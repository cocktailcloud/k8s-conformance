I0628 18:12:12.798567      22 test_context.go:416] Using a temporary kubeconfig file from in-cluster config : /tmp/kubeconfig-108209140
I0628 18:12:12.798731      22 test_context.go:429] Tolerating taints "node-role.kubernetes.io/master" when considering if nodes are ready
I0628 18:12:12.798969      22 e2e.go:129] Starting e2e run "b5c16fbf-adda-4cec-91a4-4348765d9b5b" on Ginkgo node 1
{"msg":"Test Suite starting","total":305,"completed":0,"skipped":0,"failed":0}
Running Suite: Kubernetes e2e suite
===================================
Random Seed: 1624903931 - Will randomize all specs
Will run 305 of 5232 specs

Jun 28 18:12:12.811: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
E0628 18:12:12.815853      22 progress.go:119] Failed to post progress update to http://localhost:8099/progress: Post "http://localhost:8099/progress": dial tcp [::1]:8099: connect: connection refused
Jun 28 18:12:12.820: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
Jun 28 18:12:12.916: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Jun 28 18:12:13.134: INFO: 13 / 13 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Jun 28 18:12:13.134: INFO: expected 4 pod replicas in namespace 'kube-system', 4 are Running and Ready.
Jun 28 18:12:13.134: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Jun 28 18:12:13.161: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'ibm-keepalived-watcher' (0 seconds elapsed)
Jun 28 18:12:13.161: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'ibmcloud-block-storage-driver' (0 seconds elapsed)
Jun 28 18:12:13.161: INFO: e2e test version: v1.19.0
Jun 28 18:12:13.167: INFO: kube-apiserver version: v1.19.0+c3e2e69
Jun 28 18:12:13.167: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
Jun 28 18:12:13.191: INFO: Cluster IP family: ipv4
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 18:12:13.194: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename projected
Jun 28 18:12:13.366: INFO: No PodSecurityPolicies found; assuming PodSecurityPolicy is disabled.
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name projected-configmap-test-volume-cf991f73-5c12-42d3-80c4-a5f34ca362a2
STEP: Creating a pod to test consume configMaps
Jun 28 18:12:13.518: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-e0a393f7-a204-41fe-808a-a5ab55a82175" in namespace "projected-9095" to be "Succeeded or Failed"
Jun 28 18:12:13.540: INFO: Pod "pod-projected-configmaps-e0a393f7-a204-41fe-808a-a5ab55a82175": Phase="Pending", Reason="", readiness=false. Elapsed: 21.177247ms
Jun 28 18:12:15.594: INFO: Pod "pod-projected-configmaps-e0a393f7-a204-41fe-808a-a5ab55a82175": Phase="Pending", Reason="", readiness=false. Elapsed: 2.075360209s
Jun 28 18:12:17.628: INFO: Pod "pod-projected-configmaps-e0a393f7-a204-41fe-808a-a5ab55a82175": Phase="Pending", Reason="", readiness=false. Elapsed: 4.109484409s
Jun 28 18:12:19.683: INFO: Pod "pod-projected-configmaps-e0a393f7-a204-41fe-808a-a5ab55a82175": Phase="Pending", Reason="", readiness=false. Elapsed: 6.164673356s
Jun 28 18:12:21.705: INFO: Pod "pod-projected-configmaps-e0a393f7-a204-41fe-808a-a5ab55a82175": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.18635108s
STEP: Saw pod success
Jun 28 18:12:21.705: INFO: Pod "pod-projected-configmaps-e0a393f7-a204-41fe-808a-a5ab55a82175" satisfied condition "Succeeded or Failed"
Jun 28 18:12:21.722: INFO: Trying to get logs from node 10.13.107.37 pod pod-projected-configmaps-e0a393f7-a204-41fe-808a-a5ab55a82175 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jun 28 18:12:21.860: INFO: Waiting for pod pod-projected-configmaps-e0a393f7-a204-41fe-808a-a5ab55a82175 to disappear
Jun 28 18:12:21.870: INFO: Pod pod-projected-configmaps-e0a393f7-a204-41fe-808a-a5ab55a82175 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 18:12:21.871: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9095" for this suite.

• [SLOW TEST:8.719 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:36
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume [NodeConformance] [Conformance]","total":305,"completed":1,"skipped":41,"failed":0}
SSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 18:12:21.917: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[It] should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Starting the proxy
Jun 28 18:12:22.045: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-108209140 proxy --unix-socket=/tmp/kubectl-proxy-unix832205507/test'
STEP: retrieving proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 18:12:22.179: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9045" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support --unix-socket=/path  [Conformance]","total":305,"completed":2,"skipped":51,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 18:12:22.214: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jun 28 18:12:23.279: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jun 28 18:12:25.324: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63760500743, loc:(*time.Location)(0x7702840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63760500743, loc:(*time.Location)(0x7702840)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63760500743, loc:(*time.Location)(0x7702840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63760500743, loc:(*time.Location)(0x7702840)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun 28 18:12:28.381: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Creating a dummy validating-webhook-configuration object
STEP: Deleting the validating-webhook-configuration, which should be possible to remove
STEP: Creating a dummy mutating-webhook-configuration object
STEP: Deleting the mutating-webhook-configuration, which should be possible to remove
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 18:12:28.824: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1743" for this suite.
STEP: Destroying namespace "webhook-1743-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:6.783 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]","total":305,"completed":3,"skipped":65,"failed":0}
[sig-storage] Secrets 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 18:12:28.998: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating secret with name secret-test-daf43906-ba08-4598-a079-e171f73f3e85
STEP: Creating a pod to test consume secrets
Jun 28 18:12:29.242: INFO: Waiting up to 5m0s for pod "pod-secrets-a068f9cb-c681-45d4-a351-d1e06c2a50a9" in namespace "secrets-80" to be "Succeeded or Failed"
Jun 28 18:12:29.268: INFO: Pod "pod-secrets-a068f9cb-c681-45d4-a351-d1e06c2a50a9": Phase="Pending", Reason="", readiness=false. Elapsed: 26.279122ms
Jun 28 18:12:31.282: INFO: Pod "pod-secrets-a068f9cb-c681-45d4-a351-d1e06c2a50a9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.040379593s
Jun 28 18:12:33.312: INFO: Pod "pod-secrets-a068f9cb-c681-45d4-a351-d1e06c2a50a9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.070022932s
STEP: Saw pod success
Jun 28 18:12:33.312: INFO: Pod "pod-secrets-a068f9cb-c681-45d4-a351-d1e06c2a50a9" satisfied condition "Succeeded or Failed"
Jun 28 18:12:33.323: INFO: Trying to get logs from node 10.13.107.37 pod pod-secrets-a068f9cb-c681-45d4-a351-d1e06c2a50a9 container secret-volume-test: <nil>
STEP: delete the pod
Jun 28 18:12:33.386: INFO: Waiting for pod pod-secrets-a068f9cb-c681-45d4-a351-d1e06c2a50a9 to disappear
Jun 28 18:12:33.396: INFO: Pod pod-secrets-a068f9cb-c681-45d4-a351-d1e06c2a50a9 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 18:12:33.396: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-80" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","total":305,"completed":4,"skipped":65,"failed":0}
SSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 18:12:33.431: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating projection with secret that has name projected-secret-test-f2a756ef-a10f-4f61-a9b5-9c6dfa45f355
STEP: Creating a pod to test consume secrets
Jun 28 18:12:33.627: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-23c6a12e-8d9b-4ce4-af12-ea21d367b4e5" in namespace "projected-7092" to be "Succeeded or Failed"
Jun 28 18:12:33.641: INFO: Pod "pod-projected-secrets-23c6a12e-8d9b-4ce4-af12-ea21d367b4e5": Phase="Pending", Reason="", readiness=false. Elapsed: 13.876309ms
Jun 28 18:12:35.677: INFO: Pod "pod-projected-secrets-23c6a12e-8d9b-4ce4-af12-ea21d367b4e5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.050080191s
Jun 28 18:12:37.713: INFO: Pod "pod-projected-secrets-23c6a12e-8d9b-4ce4-af12-ea21d367b4e5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.085528145s
STEP: Saw pod success
Jun 28 18:12:37.713: INFO: Pod "pod-projected-secrets-23c6a12e-8d9b-4ce4-af12-ea21d367b4e5" satisfied condition "Succeeded or Failed"
Jun 28 18:12:37.740: INFO: Trying to get logs from node 10.13.107.37 pod pod-projected-secrets-23c6a12e-8d9b-4ce4-af12-ea21d367b4e5 container projected-secret-volume-test: <nil>
STEP: delete the pod
Jun 28 18:12:37.852: INFO: Waiting for pod pod-projected-secrets-23c6a12e-8d9b-4ce4-af12-ea21d367b4e5 to disappear
Jun 28 18:12:37.883: INFO: Pod pod-projected-secrets-23c6a12e-8d9b-4ce4-af12-ea21d367b4e5 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 18:12:37.884: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7092" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":5,"skipped":70,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 18:12:38.007: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:181
[It] should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Jun 28 18:12:46.954: INFO: Successfully updated pod "pod-update-9588e0ad-d75c-4f7b-bb96-d095de17c5a6"
STEP: verifying the updated pod is in kubernetes
Jun 28 18:12:47.036: INFO: Pod update OK
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 18:12:47.036: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-9347" for this suite.

• [SLOW TEST:9.180 seconds]
[k8s.io] Pods
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Pods should be updated [NodeConformance] [Conformance]","total":305,"completed":6,"skipped":120,"failed":0}
SSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 18:12:47.189: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Performing setup for networking test in namespace pod-network-test-2305
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Jun 28 18:12:47.587: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Jun 28 18:12:48.177: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jun 28 18:12:50.226: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun 28 18:12:52.232: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun 28 18:12:54.194: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun 28 18:12:56.198: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun 28 18:12:58.208: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun 28 18:13:00.204: INFO: The status of Pod netserver-0 is Running (Ready = true)
Jun 28 18:13:00.230: INFO: The status of Pod netserver-1 is Running (Ready = false)
Jun 28 18:13:02.250: INFO: The status of Pod netserver-1 is Running (Ready = false)
Jun 28 18:13:04.286: INFO: The status of Pod netserver-1 is Running (Ready = false)
Jun 28 18:13:06.252: INFO: The status of Pod netserver-1 is Running (Ready = true)
Jun 28 18:13:06.282: INFO: The status of Pod netserver-2 is Running (Ready = false)
Jun 28 18:13:08.299: INFO: The status of Pod netserver-2 is Running (Ready = true)
STEP: Creating test pods
Jun 28 18:13:10.554: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.30.221.173:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-2305 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 28 18:13:10.554: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
Jun 28 18:13:10.829: INFO: Found all expected endpoints: [netserver-0]
Jun 28 18:13:10.859: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.30.54.42:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-2305 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 28 18:13:10.859: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
Jun 28 18:13:11.079: INFO: Found all expected endpoints: [netserver-1]
Jun 28 18:13:11.098: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.30.196.117:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-2305 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 28 18:13:11.098: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
Jun 28 18:13:11.375: INFO: Found all expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 18:13:11.375: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-2305" for this suite.

• [SLOW TEST:24.245 seconds]
[sig-network] Networking
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:26
  Granular Checks: Pods
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:29
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":7,"skipped":129,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 18:13:11.439: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir volume type on node default medium
Jun 28 18:13:11.649: INFO: Waiting up to 5m0s for pod "pod-aad032a1-722f-403f-9673-74379de92e89" in namespace "emptydir-9659" to be "Succeeded or Failed"
Jun 28 18:13:11.676: INFO: Pod "pod-aad032a1-722f-403f-9673-74379de92e89": Phase="Pending", Reason="", readiness=false. Elapsed: 26.912906ms
Jun 28 18:13:13.775: INFO: Pod "pod-aad032a1-722f-403f-9673-74379de92e89": Phase="Pending", Reason="", readiness=false. Elapsed: 2.126067526s
Jun 28 18:13:15.798: INFO: Pod "pod-aad032a1-722f-403f-9673-74379de92e89": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.148348073s
STEP: Saw pod success
Jun 28 18:13:15.798: INFO: Pod "pod-aad032a1-722f-403f-9673-74379de92e89" satisfied condition "Succeeded or Failed"
Jun 28 18:13:15.811: INFO: Trying to get logs from node 10.13.107.37 pod pod-aad032a1-722f-403f-9673-74379de92e89 container test-container: <nil>
STEP: delete the pod
Jun 28 18:13:15.888: INFO: Waiting for pod pod-aad032a1-722f-403f-9673-74379de92e89 to disappear
Jun 28 18:13:15.900: INFO: Pod pod-aad032a1-722f-403f-9673-74379de92e89 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 18:13:15.901: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9659" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":8,"skipped":152,"failed":0}

------------------------------
[sig-api-machinery] Watchers 
  should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 18:13:15.958: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: starting a background goroutine to produce watch events
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 18:13:21.252: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-3924" for this suite.

• [SLOW TEST:5.393 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should receive events on concurrent watches in same order [Conformance]","total":305,"completed":9,"skipped":152,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 18:13:21.357: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename crd-webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:126
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
Jun 28 18:13:22.210: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
Jun 28 18:13:24.303: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63760500802, loc:(*time.Location)(0x7702840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63760500802, loc:(*time.Location)(0x7702840)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63760500802, loc:(*time.Location)(0x7702840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63760500802, loc:(*time.Location)(0x7702840)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-85d57b96d6\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun 28 18:13:27.382: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jun 28 18:13:27.404: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Creating a v1 custom resource
STEP: v2 custom resource should be converted
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 18:13:28.769: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-2921" for this suite.
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:137

• [SLOW TEST:7.626 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert from CR v1 to CR v2 [Conformance]","total":305,"completed":10,"skipped":188,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 18:13:28.983: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:134
[It] should run and stop complex daemon [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jun 28 18:13:29.342: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes.
Jun 28 18:13:29.425: INFO: Number of nodes with available pods: 0
Jun 28 18:13:29.425: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Change node label to blue, check that daemon pod is launched.
Jun 28 18:13:29.599: INFO: Number of nodes with available pods: 0
Jun 28 18:13:29.599: INFO: Node 10.13.107.57 is running more than one daemon pod
Jun 28 18:13:30.612: INFO: Number of nodes with available pods: 0
Jun 28 18:13:30.612: INFO: Node 10.13.107.57 is running more than one daemon pod
Jun 28 18:13:31.617: INFO: Number of nodes with available pods: 0
Jun 28 18:13:31.617: INFO: Node 10.13.107.57 is running more than one daemon pod
Jun 28 18:13:32.613: INFO: Number of nodes with available pods: 0
Jun 28 18:13:32.614: INFO: Node 10.13.107.57 is running more than one daemon pod
Jun 28 18:13:33.616: INFO: Number of nodes with available pods: 0
Jun 28 18:13:33.616: INFO: Node 10.13.107.57 is running more than one daemon pod
Jun 28 18:13:34.616: INFO: Number of nodes with available pods: 0
Jun 28 18:13:34.616: INFO: Node 10.13.107.57 is running more than one daemon pod
Jun 28 18:13:35.620: INFO: Number of nodes with available pods: 0
Jun 28 18:13:35.620: INFO: Node 10.13.107.57 is running more than one daemon pod
Jun 28 18:13:36.615: INFO: Number of nodes with available pods: 0
Jun 28 18:13:36.615: INFO: Node 10.13.107.57 is running more than one daemon pod
Jun 28 18:13:37.615: INFO: Number of nodes with available pods: 0
Jun 28 18:13:37.615: INFO: Node 10.13.107.57 is running more than one daemon pod
Jun 28 18:13:38.615: INFO: Number of nodes with available pods: 0
Jun 28 18:13:38.615: INFO: Node 10.13.107.57 is running more than one daemon pod
Jun 28 18:13:39.615: INFO: Number of nodes with available pods: 1
Jun 28 18:13:39.615: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Update the node label to green, and wait for daemons to be unscheduled
Jun 28 18:13:39.704: INFO: Number of nodes with available pods: 0
Jun 28 18:13:39.704: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate
Jun 28 18:13:39.757: INFO: Number of nodes with available pods: 0
Jun 28 18:13:39.757: INFO: Node 10.13.107.57 is running more than one daemon pod
Jun 28 18:13:40.776: INFO: Number of nodes with available pods: 0
Jun 28 18:13:40.776: INFO: Node 10.13.107.57 is running more than one daemon pod
Jun 28 18:13:41.788: INFO: Number of nodes with available pods: 0
Jun 28 18:13:41.788: INFO: Node 10.13.107.57 is running more than one daemon pod
Jun 28 18:13:42.779: INFO: Number of nodes with available pods: 0
Jun 28 18:13:42.779: INFO: Node 10.13.107.57 is running more than one daemon pod
Jun 28 18:13:43.776: INFO: Number of nodes with available pods: 0
Jun 28 18:13:43.776: INFO: Node 10.13.107.57 is running more than one daemon pod
Jun 28 18:13:44.778: INFO: Number of nodes with available pods: 0
Jun 28 18:13:44.778: INFO: Node 10.13.107.57 is running more than one daemon pod
Jun 28 18:13:45.774: INFO: Number of nodes with available pods: 0
Jun 28 18:13:45.774: INFO: Node 10.13.107.57 is running more than one daemon pod
Jun 28 18:13:46.775: INFO: Number of nodes with available pods: 0
Jun 28 18:13:46.775: INFO: Node 10.13.107.57 is running more than one daemon pod
Jun 28 18:13:47.795: INFO: Number of nodes with available pods: 0
Jun 28 18:13:47.795: INFO: Node 10.13.107.57 is running more than one daemon pod
Jun 28 18:13:48.794: INFO: Number of nodes with available pods: 0
Jun 28 18:13:48.794: INFO: Node 10.13.107.57 is running more than one daemon pod
Jun 28 18:13:49.817: INFO: Number of nodes with available pods: 0
Jun 28 18:13:49.817: INFO: Node 10.13.107.57 is running more than one daemon pod
Jun 28 18:13:50.804: INFO: Number of nodes with available pods: 0
Jun 28 18:13:50.804: INFO: Node 10.13.107.57 is running more than one daemon pod
Jun 28 18:13:51.798: INFO: Number of nodes with available pods: 0
Jun 28 18:13:51.798: INFO: Node 10.13.107.57 is running more than one daemon pod
Jun 28 18:13:52.780: INFO: Number of nodes with available pods: 0
Jun 28 18:13:52.780: INFO: Node 10.13.107.57 is running more than one daemon pod
Jun 28 18:13:53.791: INFO: Number of nodes with available pods: 0
Jun 28 18:13:53.792: INFO: Node 10.13.107.57 is running more than one daemon pod
Jun 28 18:13:54.774: INFO: Number of nodes with available pods: 0
Jun 28 18:13:54.774: INFO: Node 10.13.107.57 is running more than one daemon pod
Jun 28 18:13:55.782: INFO: Number of nodes with available pods: 0
Jun 28 18:13:55.782: INFO: Node 10.13.107.57 is running more than one daemon pod
Jun 28 18:13:56.783: INFO: Number of nodes with available pods: 1
Jun 28 18:13:56.783: INFO: Number of running nodes: 1, number of available pods: 1
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:100
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-3197, will wait for the garbage collector to delete the pods
Jun 28 18:13:56.982: INFO: Deleting DaemonSet.extensions daemon-set took: 43.326001ms
Jun 28 18:13:57.082: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.422029ms
Jun 28 18:14:05.000: INFO: Number of nodes with available pods: 0
Jun 28 18:14:05.000: INFO: Number of running nodes: 0, number of available pods: 0
Jun 28 18:14:05.018: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-3197/daemonsets","resourceVersion":"51295"},"items":null}

Jun 28 18:14:05.034: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-3197/pods","resourceVersion":"51295"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 18:14:05.171: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-3197" for this suite.

• [SLOW TEST:36.231 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop complex daemon [Conformance]","total":305,"completed":11,"skipped":225,"failed":0}
[sig-apps] Daemon set [Serial] 
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 18:14:05.213: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:134
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jun 28 18:14:05.498: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster.
Jun 28 18:14:05.566: INFO: Number of nodes with available pods: 0
Jun 28 18:14:05.566: INFO: Node 10.13.107.37 is running more than one daemon pod
Jun 28 18:14:06.616: INFO: Number of nodes with available pods: 0
Jun 28 18:14:06.616: INFO: Node 10.13.107.37 is running more than one daemon pod
Jun 28 18:14:07.624: INFO: Number of nodes with available pods: 1
Jun 28 18:14:07.624: INFO: Node 10.13.107.37 is running more than one daemon pod
Jun 28 18:14:08.611: INFO: Number of nodes with available pods: 1
Jun 28 18:14:08.611: INFO: Node 10.13.107.37 is running more than one daemon pod
Jun 28 18:14:09.602: INFO: Number of nodes with available pods: 1
Jun 28 18:14:09.602: INFO: Node 10.13.107.37 is running more than one daemon pod
Jun 28 18:14:10.600: INFO: Number of nodes with available pods: 1
Jun 28 18:14:10.600: INFO: Node 10.13.107.37 is running more than one daemon pod
Jun 28 18:14:11.619: INFO: Number of nodes with available pods: 1
Jun 28 18:14:11.619: INFO: Node 10.13.107.37 is running more than one daemon pod
Jun 28 18:14:12.602: INFO: Number of nodes with available pods: 1
Jun 28 18:14:12.602: INFO: Node 10.13.107.37 is running more than one daemon pod
Jun 28 18:14:13.648: INFO: Number of nodes with available pods: 1
Jun 28 18:14:13.648: INFO: Node 10.13.107.37 is running more than one daemon pod
Jun 28 18:14:14.608: INFO: Number of nodes with available pods: 2
Jun 28 18:14:14.608: INFO: Node 10.13.107.60 is running more than one daemon pod
Jun 28 18:14:15.619: INFO: Number of nodes with available pods: 2
Jun 28 18:14:15.619: INFO: Node 10.13.107.60 is running more than one daemon pod
Jun 28 18:14:16.617: INFO: Number of nodes with available pods: 2
Jun 28 18:14:16.617: INFO: Node 10.13.107.60 is running more than one daemon pod
Jun 28 18:14:17.601: INFO: Number of nodes with available pods: 3
Jun 28 18:14:17.601: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Update daemon pods image.
STEP: Check that daemon pods images are updated.
Jun 28 18:14:17.711: INFO: Wrong image for pod: daemon-set-h4g4p. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jun 28 18:14:17.711: INFO: Wrong image for pod: daemon-set-mrwk7. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jun 28 18:14:17.711: INFO: Wrong image for pod: daemon-set-tqhxm. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jun 28 18:14:18.753: INFO: Wrong image for pod: daemon-set-h4g4p. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jun 28 18:14:18.753: INFO: Wrong image for pod: daemon-set-mrwk7. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jun 28 18:14:18.753: INFO: Wrong image for pod: daemon-set-tqhxm. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jun 28 18:14:19.809: INFO: Wrong image for pod: daemon-set-h4g4p. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jun 28 18:14:19.809: INFO: Wrong image for pod: daemon-set-mrwk7. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jun 28 18:14:19.809: INFO: Wrong image for pod: daemon-set-tqhxm. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jun 28 18:14:19.809: INFO: Pod daemon-set-tqhxm is not available
Jun 28 18:14:20.770: INFO: Wrong image for pod: daemon-set-h4g4p. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jun 28 18:14:20.770: INFO: Wrong image for pod: daemon-set-mrwk7. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jun 28 18:14:20.770: INFO: Wrong image for pod: daemon-set-tqhxm. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jun 28 18:14:20.770: INFO: Pod daemon-set-tqhxm is not available
Jun 28 18:14:21.746: INFO: Wrong image for pod: daemon-set-h4g4p. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jun 28 18:14:21.746: INFO: Wrong image for pod: daemon-set-mrwk7. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jun 28 18:14:21.746: INFO: Wrong image for pod: daemon-set-tqhxm. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jun 28 18:14:21.746: INFO: Pod daemon-set-tqhxm is not available
Jun 28 18:14:22.797: INFO: Wrong image for pod: daemon-set-h4g4p. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jun 28 18:14:22.797: INFO: Wrong image for pod: daemon-set-mrwk7. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jun 28 18:14:22.797: INFO: Wrong image for pod: daemon-set-tqhxm. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jun 28 18:14:22.797: INFO: Pod daemon-set-tqhxm is not available
Jun 28 18:14:23.753: INFO: Wrong image for pod: daemon-set-h4g4p. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jun 28 18:14:23.754: INFO: Wrong image for pod: daemon-set-mrwk7. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jun 28 18:14:23.754: INFO: Wrong image for pod: daemon-set-tqhxm. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jun 28 18:14:23.754: INFO: Pod daemon-set-tqhxm is not available
Jun 28 18:14:24.759: INFO: Wrong image for pod: daemon-set-h4g4p. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jun 28 18:14:24.759: INFO: Wrong image for pod: daemon-set-mrwk7. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jun 28 18:14:24.759: INFO: Wrong image for pod: daemon-set-tqhxm. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jun 28 18:14:24.759: INFO: Pod daemon-set-tqhxm is not available
Jun 28 18:14:25.770: INFO: Pod daemon-set-dp5k8 is not available
Jun 28 18:14:25.770: INFO: Wrong image for pod: daemon-set-h4g4p. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jun 28 18:14:25.770: INFO: Wrong image for pod: daemon-set-mrwk7. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jun 28 18:14:26.772: INFO: Pod daemon-set-dp5k8 is not available
Jun 28 18:14:26.772: INFO: Wrong image for pod: daemon-set-h4g4p. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jun 28 18:14:26.772: INFO: Wrong image for pod: daemon-set-mrwk7. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jun 28 18:14:27.759: INFO: Wrong image for pod: daemon-set-h4g4p. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jun 28 18:14:27.759: INFO: Wrong image for pod: daemon-set-mrwk7. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jun 28 18:14:28.760: INFO: Wrong image for pod: daemon-set-h4g4p. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jun 28 18:14:28.760: INFO: Pod daemon-set-h4g4p is not available
Jun 28 18:14:28.760: INFO: Wrong image for pod: daemon-set-mrwk7. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jun 28 18:14:29.765: INFO: Wrong image for pod: daemon-set-mrwk7. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jun 28 18:14:29.765: INFO: Pod daemon-set-wsx48 is not available
Jun 28 18:14:30.752: INFO: Wrong image for pod: daemon-set-mrwk7. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jun 28 18:14:30.752: INFO: Pod daemon-set-wsx48 is not available
Jun 28 18:14:31.772: INFO: Wrong image for pod: daemon-set-mrwk7. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jun 28 18:14:32.758: INFO: Wrong image for pod: daemon-set-mrwk7. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jun 28 18:14:33.749: INFO: Wrong image for pod: daemon-set-mrwk7. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jun 28 18:14:33.749: INFO: Pod daemon-set-mrwk7 is not available
Jun 28 18:14:34.745: INFO: Wrong image for pod: daemon-set-mrwk7. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jun 28 18:14:34.746: INFO: Pod daemon-set-mrwk7 is not available
Jun 28 18:14:35.749: INFO: Wrong image for pod: daemon-set-mrwk7. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jun 28 18:14:35.749: INFO: Pod daemon-set-mrwk7 is not available
Jun 28 18:14:36.749: INFO: Wrong image for pod: daemon-set-mrwk7. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jun 28 18:14:36.749: INFO: Pod daemon-set-mrwk7 is not available
Jun 28 18:14:37.748: INFO: Wrong image for pod: daemon-set-mrwk7. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jun 28 18:14:37.748: INFO: Pod daemon-set-mrwk7 is not available
Jun 28 18:14:38.750: INFO: Pod daemon-set-84fpq is not available
STEP: Check that daemon pods are still running on every node of the cluster.
Jun 28 18:14:38.826: INFO: Number of nodes with available pods: 2
Jun 28 18:14:38.826: INFO: Node 10.13.107.60 is running more than one daemon pod
Jun 28 18:14:39.879: INFO: Number of nodes with available pods: 2
Jun 28 18:14:39.879: INFO: Node 10.13.107.60 is running more than one daemon pod
Jun 28 18:14:40.872: INFO: Number of nodes with available pods: 3
Jun 28 18:14:40.872: INFO: Number of running nodes: 3, number of available pods: 3
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:100
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-9708, will wait for the garbage collector to delete the pods
Jun 28 18:14:41.032: INFO: Deleting DaemonSet.extensions daemon-set took: 25.49535ms
Jun 28 18:14:41.132: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.396374ms
Jun 28 18:14:55.051: INFO: Number of nodes with available pods: 0
Jun 28 18:14:55.051: INFO: Number of running nodes: 0, number of available pods: 0
Jun 28 18:14:55.065: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-9708/daemonsets","resourceVersion":"51905"},"items":null}

Jun 28 18:14:55.080: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-9708/pods","resourceVersion":"51905"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 18:14:55.153: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-9708" for this suite.

• [SLOW TEST:49.995 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]","total":305,"completed":12,"skipped":225,"failed":0}
SSSSSSSS
------------------------------
[k8s.io] Security Context When creating a pod with privileged 
  should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 18:14:55.211: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:41
[It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jun 28 18:14:55.410: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-3f2fce5e-36f5-4c95-8e0e-0119633cb753" in namespace "security-context-test-9110" to be "Succeeded or Failed"
Jun 28 18:14:55.424: INFO: Pod "busybox-privileged-false-3f2fce5e-36f5-4c95-8e0e-0119633cb753": Phase="Pending", Reason="", readiness=false. Elapsed: 14.041064ms
Jun 28 18:14:57.459: INFO: Pod "busybox-privileged-false-3f2fce5e-36f5-4c95-8e0e-0119633cb753": Phase="Pending", Reason="", readiness=false. Elapsed: 2.049221566s
Jun 28 18:14:59.525: INFO: Pod "busybox-privileged-false-3f2fce5e-36f5-4c95-8e0e-0119633cb753": Phase="Pending", Reason="", readiness=false. Elapsed: 4.114898769s
Jun 28 18:15:01.594: INFO: Pod "busybox-privileged-false-3f2fce5e-36f5-4c95-8e0e-0119633cb753": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.184292477s
Jun 28 18:15:01.595: INFO: Pod "busybox-privileged-false-3f2fce5e-36f5-4c95-8e0e-0119633cb753" satisfied condition "Succeeded or Failed"
Jun 28 18:15:01.725: INFO: Got logs for pod "busybox-privileged-false-3f2fce5e-36f5-4c95-8e0e-0119633cb753": "ip: RTNETLINK answers: Operation not permitted\n"
[AfterEach] [k8s.io] Security Context
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 18:15:01.725: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-9110" for this suite.

• [SLOW TEST:6.766 seconds]
[k8s.io] Security Context
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  When creating a pod with privileged
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:227
    should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Security Context When creating a pod with privileged should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":13,"skipped":233,"failed":0}
SSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of different groups [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 18:15:01.978: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of different groups [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation
Jun 28 18:15:02.314: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
Jun 28 18:15:13.338: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 18:15:53.265: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-7227" for this suite.

• [SLOW TEST:51.458 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of different groups [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of different groups [Conformance]","total":305,"completed":14,"skipped":236,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 18:15:53.437: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-645.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-645.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-645.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-645.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-645.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-645.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe /etc/hosts
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jun 28 18:16:03.781: INFO: DNS probes using dns-645/dns-test-cf48aa3a-faf4-4bfb-aecc-4c4549e2a5e9 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 18:16:03.810: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-645" for this suite.

• [SLOW TEST:10.425 seconds]
[sig-network] DNS
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] DNS should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]","total":305,"completed":15,"skipped":250,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD with validation schema [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 18:16:03.864: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD with validation schema [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jun 28 18:16:04.049: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: client-side validation (kubectl create and apply) allows request with known and required properties
Jun 28 18:16:15.204: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-108209140 --namespace=crd-publish-openapi-6077 create -f -'
Jun 28 18:16:16.253: INFO: stderr: ""
Jun 28 18:16:16.253: INFO: stdout: "e2e-test-crd-publish-openapi-2552-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Jun 28 18:16:16.253: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-108209140 --namespace=crd-publish-openapi-6077 delete e2e-test-crd-publish-openapi-2552-crds test-foo'
Jun 28 18:16:16.511: INFO: stderr: ""
Jun 28 18:16:16.511: INFO: stdout: "e2e-test-crd-publish-openapi-2552-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
Jun 28 18:16:16.511: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-108209140 --namespace=crd-publish-openapi-6077 apply -f -'
Jun 28 18:16:17.101: INFO: stderr: ""
Jun 28 18:16:17.101: INFO: stdout: "e2e-test-crd-publish-openapi-2552-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Jun 28 18:16:17.101: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-108209140 --namespace=crd-publish-openapi-6077 delete e2e-test-crd-publish-openapi-2552-crds test-foo'
Jun 28 18:16:17.332: INFO: stderr: ""
Jun 28 18:16:17.332: INFO: stdout: "e2e-test-crd-publish-openapi-2552-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
STEP: client-side validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema
Jun 28 18:16:17.332: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-108209140 --namespace=crd-publish-openapi-6077 create -f -'
Jun 28 18:16:18.140: INFO: rc: 1
Jun 28 18:16:18.141: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-108209140 --namespace=crd-publish-openapi-6077 apply -f -'
Jun 28 18:16:18.688: INFO: rc: 1
STEP: client-side validation (kubectl create and apply) rejects request without required properties
Jun 28 18:16:18.688: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-108209140 --namespace=crd-publish-openapi-6077 create -f -'
Jun 28 18:16:19.381: INFO: rc: 1
Jun 28 18:16:19.381: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-108209140 --namespace=crd-publish-openapi-6077 apply -f -'
Jun 28 18:16:20.395: INFO: rc: 1
STEP: kubectl explain works to explain CR properties
Jun 28 18:16:20.395: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-108209140 explain e2e-test-crd-publish-openapi-2552-crds'
Jun 28 18:16:21.049: INFO: stderr: ""
Jun 28 18:16:21.049: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-2552-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
STEP: kubectl explain works to explain CR properties recursively
Jun 28 18:16:21.050: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-108209140 explain e2e-test-crd-publish-openapi-2552-crds.metadata'
Jun 28 18:16:21.915: INFO: stderr: ""
Jun 28 18:16:21.915: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-2552-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   clusterName\t<string>\n     The name of the cluster which the object belongs to. This is used to\n     distinguish resources with same name and namespace in different clusters.\n     This field is not set anywhere right now and apiserver is going to ignore\n     it if set in create or update request.\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC.\n\n     Populated by the system. Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested.\n\n     Populated by the system when a graceful deletion is requested. Read-only.\n     More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server.\n\n     If this field is specified and the generated name exists, the server will\n     NOT return a 409 - instead, it will either return 201 Created or 500 with\n     Reason ServerTimeout indicating a unique name could not be found in the\n     time allotted, and the client should retry (optionally after the time\n     indicated in the Retry-After header).\n\n     Applied only if Name is not specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within which each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty.\n\n     Must be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources.\n\n     Populated by the system. Read-only. Value must be treated as opaque by\n     clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     SelfLink is a URL representing this object. Populated by the system.\n     Read-only.\n\n     DEPRECATED Kubernetes will stop propagating this field in 1.20 release and\n     the field is planned to be removed in 1.21 release.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations.\n\n     Populated by the system. Read-only. More info:\n     http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
Jun 28 18:16:21.915: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-108209140 explain e2e-test-crd-publish-openapi-2552-crds.spec'
Jun 28 18:16:22.815: INFO: stderr: ""
Jun 28 18:16:22.815: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-2552-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
Jun 28 18:16:22.815: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-108209140 explain e2e-test-crd-publish-openapi-2552-crds.spec.bars'
Jun 28 18:16:23.563: INFO: stderr: ""
Jun 28 18:16:23.563: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-2552-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
STEP: kubectl explain works to return error when explain is called on property that doesn't exist
Jun 28 18:16:23.564: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-108209140 explain e2e-test-crd-publish-openapi-2552-crds.spec.bars2'
Jun 28 18:16:24.324: INFO: rc: 1
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 18:16:31.357: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-6077" for this suite.

• [SLOW TEST:27.534 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD with validation schema [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD with validation schema [Conformance]","total":305,"completed":16,"skipped":258,"failed":0}
SS
------------------------------
[k8s.io] Pods 
  should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 18:16:31.398: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:181
[It] should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating pod
Jun 28 18:16:35.629: INFO: Pod pod-hostip-9da24e86-3476-4a59-b29c-e496aaa369f7 has hostIP: 10.13.107.37
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 18:16:35.629: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-2314" for this suite.
•{"msg":"PASSED [k8s.io] Pods should get a host IP [NodeConformance] [Conformance]","total":305,"completed":17,"skipped":260,"failed":0}
SSSSSSSSS
------------------------------
[k8s.io] [sig-node] PreStop 
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 18:16:35.662: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename prestop
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pre_stop.go:171
[It] should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating server pod server in namespace prestop-7059
STEP: Waiting for pods to come up.
STEP: Creating tester pod tester in namespace prestop-7059
STEP: Deleting pre-stop pod
Jun 28 18:16:45.006: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod
[AfterEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 18:16:45.026: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "prestop-7059" for this suite.

• [SLOW TEST:9.406 seconds]
[k8s.io] [sig-node] PreStop
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] [sig-node] PreStop should call prestop when killing a pod  [Conformance]","total":305,"completed":18,"skipped":269,"failed":0}
SSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should scale a replication controller  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 18:16:45.069: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[BeforeEach] Update Demo
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:308
[It] should scale a replication controller  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a replication controller
Jun 28 18:16:45.195: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-108209140 create -f - --namespace=kubectl-5010'
Jun 28 18:16:46.044: INFO: stderr: ""
Jun 28 18:16:46.044: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jun 28 18:16:46.044: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-108209140 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-5010'
Jun 28 18:16:46.246: INFO: stderr: ""
Jun 28 18:16:46.246: INFO: stdout: "update-demo-nautilus-665mx update-demo-nautilus-6rrz2 "
Jun 28 18:16:46.246: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-108209140 get pods update-demo-nautilus-665mx -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-5010'
Jun 28 18:16:46.453: INFO: stderr: ""
Jun 28 18:16:46.453: INFO: stdout: ""
Jun 28 18:16:46.453: INFO: update-demo-nautilus-665mx is created but not running
Jun 28 18:16:51.453: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-108209140 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-5010'
Jun 28 18:16:51.707: INFO: stderr: ""
Jun 28 18:16:51.707: INFO: stdout: "update-demo-nautilus-665mx update-demo-nautilus-6rrz2 "
Jun 28 18:16:51.707: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-108209140 get pods update-demo-nautilus-665mx -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-5010'
Jun 28 18:16:51.939: INFO: stderr: ""
Jun 28 18:16:51.939: INFO: stdout: "true"
Jun 28 18:16:51.940: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-108209140 get pods update-demo-nautilus-665mx -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-5010'
Jun 28 18:16:52.175: INFO: stderr: ""
Jun 28 18:16:52.175: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jun 28 18:16:52.175: INFO: validating pod update-demo-nautilus-665mx
Jun 28 18:16:52.226: INFO: got data: {
  "image": "nautilus.jpg"
}

Jun 28 18:16:52.226: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jun 28 18:16:52.226: INFO: update-demo-nautilus-665mx is verified up and running
Jun 28 18:16:52.226: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-108209140 get pods update-demo-nautilus-6rrz2 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-5010'
Jun 28 18:16:52.416: INFO: stderr: ""
Jun 28 18:16:52.416: INFO: stdout: "true"
Jun 28 18:16:52.416: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-108209140 get pods update-demo-nautilus-6rrz2 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-5010'
Jun 28 18:16:52.662: INFO: stderr: ""
Jun 28 18:16:52.662: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jun 28 18:16:52.662: INFO: validating pod update-demo-nautilus-6rrz2
Jun 28 18:16:52.713: INFO: got data: {
  "image": "nautilus.jpg"
}

Jun 28 18:16:52.713: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jun 28 18:16:52.713: INFO: update-demo-nautilus-6rrz2 is verified up and running
STEP: scaling down the replication controller
Jun 28 18:16:52.721: INFO: scanned /root for discovery docs: <nil>
Jun 28 18:16:52.721: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-108209140 scale rc update-demo-nautilus --replicas=1 --timeout=5m --namespace=kubectl-5010'
Jun 28 18:16:52.964: INFO: stderr: ""
Jun 28 18:16:52.964: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jun 28 18:16:52.964: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-108209140 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-5010'
Jun 28 18:16:53.183: INFO: stderr: ""
Jun 28 18:16:53.183: INFO: stdout: "update-demo-nautilus-665mx update-demo-nautilus-6rrz2 "
STEP: Replicas for name=update-demo: expected=1 actual=2
Jun 28 18:16:58.184: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-108209140 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-5010'
Jun 28 18:16:58.402: INFO: stderr: ""
Jun 28 18:16:58.402: INFO: stdout: "update-demo-nautilus-665mx update-demo-nautilus-6rrz2 "
STEP: Replicas for name=update-demo: expected=1 actual=2
Jun 28 18:17:03.402: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-108209140 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-5010'
Jun 28 18:17:03.586: INFO: stderr: ""
Jun 28 18:17:03.586: INFO: stdout: "update-demo-nautilus-665mx update-demo-nautilus-6rrz2 "
STEP: Replicas for name=update-demo: expected=1 actual=2
Jun 28 18:17:08.588: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-108209140 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-5010'
Jun 28 18:17:08.785: INFO: stderr: ""
Jun 28 18:17:08.785: INFO: stdout: "update-demo-nautilus-665mx "
Jun 28 18:17:08.785: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-108209140 get pods update-demo-nautilus-665mx -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-5010'
Jun 28 18:17:09.016: INFO: stderr: ""
Jun 28 18:17:09.017: INFO: stdout: "true"
Jun 28 18:17:09.017: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-108209140 get pods update-demo-nautilus-665mx -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-5010'
Jun 28 18:17:09.184: INFO: stderr: ""
Jun 28 18:17:09.184: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jun 28 18:17:09.184: INFO: validating pod update-demo-nautilus-665mx
Jun 28 18:17:09.241: INFO: got data: {
  "image": "nautilus.jpg"
}

Jun 28 18:17:09.241: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jun 28 18:17:09.241: INFO: update-demo-nautilus-665mx is verified up and running
STEP: scaling up the replication controller
Jun 28 18:17:09.254: INFO: scanned /root for discovery docs: <nil>
Jun 28 18:17:09.254: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-108209140 scale rc update-demo-nautilus --replicas=2 --timeout=5m --namespace=kubectl-5010'
Jun 28 18:17:09.588: INFO: stderr: ""
Jun 28 18:17:09.588: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jun 28 18:17:09.588: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-108209140 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-5010'
Jun 28 18:17:09.782: INFO: stderr: ""
Jun 28 18:17:09.782: INFO: stdout: "update-demo-nautilus-665mx update-demo-nautilus-fmjrw "
Jun 28 18:17:09.783: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-108209140 get pods update-demo-nautilus-665mx -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-5010'
Jun 28 18:17:09.990: INFO: stderr: ""
Jun 28 18:17:09.990: INFO: stdout: "true"
Jun 28 18:17:09.990: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-108209140 get pods update-demo-nautilus-665mx -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-5010'
Jun 28 18:17:10.179: INFO: stderr: ""
Jun 28 18:17:10.179: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jun 28 18:17:10.179: INFO: validating pod update-demo-nautilus-665mx
Jun 28 18:17:10.219: INFO: got data: {
  "image": "nautilus.jpg"
}

Jun 28 18:17:10.219: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jun 28 18:17:10.219: INFO: update-demo-nautilus-665mx is verified up and running
Jun 28 18:17:10.219: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-108209140 get pods update-demo-nautilus-fmjrw -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-5010'
Jun 28 18:17:10.441: INFO: stderr: ""
Jun 28 18:17:10.441: INFO: stdout: ""
Jun 28 18:17:10.441: INFO: update-demo-nautilus-fmjrw is created but not running
Jun 28 18:17:15.441: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-108209140 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-5010'
Jun 28 18:17:15.658: INFO: stderr: ""
Jun 28 18:17:15.658: INFO: stdout: "update-demo-nautilus-665mx update-demo-nautilus-fmjrw "
Jun 28 18:17:15.658: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-108209140 get pods update-demo-nautilus-665mx -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-5010'
Jun 28 18:17:15.861: INFO: stderr: ""
Jun 28 18:17:15.861: INFO: stdout: "true"
Jun 28 18:17:15.861: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-108209140 get pods update-demo-nautilus-665mx -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-5010'
Jun 28 18:17:16.064: INFO: stderr: ""
Jun 28 18:17:16.064: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jun 28 18:17:16.064: INFO: validating pod update-demo-nautilus-665mx
Jun 28 18:17:16.091: INFO: got data: {
  "image": "nautilus.jpg"
}

Jun 28 18:17:16.091: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jun 28 18:17:16.091: INFO: update-demo-nautilus-665mx is verified up and running
Jun 28 18:17:16.091: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-108209140 get pods update-demo-nautilus-fmjrw -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-5010'
Jun 28 18:17:16.283: INFO: stderr: ""
Jun 28 18:17:16.283: INFO: stdout: "true"
Jun 28 18:17:16.283: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-108209140 get pods update-demo-nautilus-fmjrw -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-5010'
Jun 28 18:17:16.462: INFO: stderr: ""
Jun 28 18:17:16.462: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jun 28 18:17:16.462: INFO: validating pod update-demo-nautilus-fmjrw
Jun 28 18:17:16.506: INFO: got data: {
  "image": "nautilus.jpg"
}

Jun 28 18:17:16.506: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jun 28 18:17:16.506: INFO: update-demo-nautilus-fmjrw is verified up and running
STEP: using delete to clean up resources
Jun 28 18:17:16.506: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-108209140 delete --grace-period=0 --force -f - --namespace=kubectl-5010'
Jun 28 18:17:16.677: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jun 28 18:17:16.677: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Jun 28 18:17:16.677: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-108209140 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-5010'
Jun 28 18:17:17.026: INFO: stderr: "No resources found in kubectl-5010 namespace.\n"
Jun 28 18:17:17.026: INFO: stdout: ""
Jun 28 18:17:17.026: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-108209140 get pods -l name=update-demo --namespace=kubectl-5010 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jun 28 18:17:17.227: INFO: stderr: ""
Jun 28 18:17:17.228: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 18:17:17.228: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5010" for this suite.

• [SLOW TEST:32.241 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:306
    should scale a replication controller  [Conformance]
    /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should scale a replication controller  [Conformance]","total":305,"completed":19,"skipped":277,"failed":0}
SSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 18:17:17.310: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Jun 28 18:17:17.536: INFO: Waiting up to 5m0s for pod "downwardapi-volume-4c7da2a0-abfd-4f24-8ceb-75d5dc594e8d" in namespace "projected-6890" to be "Succeeded or Failed"
Jun 28 18:17:17.560: INFO: Pod "downwardapi-volume-4c7da2a0-abfd-4f24-8ceb-75d5dc594e8d": Phase="Pending", Reason="", readiness=false. Elapsed: 23.386232ms
Jun 28 18:17:19.573: INFO: Pod "downwardapi-volume-4c7da2a0-abfd-4f24-8ceb-75d5dc594e8d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.036549211s
Jun 28 18:17:21.585: INFO: Pod "downwardapi-volume-4c7da2a0-abfd-4f24-8ceb-75d5dc594e8d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.048925309s
STEP: Saw pod success
Jun 28 18:17:21.586: INFO: Pod "downwardapi-volume-4c7da2a0-abfd-4f24-8ceb-75d5dc594e8d" satisfied condition "Succeeded or Failed"
Jun 28 18:17:21.596: INFO: Trying to get logs from node 10.13.107.37 pod downwardapi-volume-4c7da2a0-abfd-4f24-8ceb-75d5dc594e8d container client-container: <nil>
STEP: delete the pod
Jun 28 18:17:21.673: INFO: Waiting for pod downwardapi-volume-4c7da2a0-abfd-4f24-8ceb-75d5dc594e8d to disappear
Jun 28 18:17:21.683: INFO: Pod downwardapi-volume-4c7da2a0-abfd-4f24-8ceb-75d5dc594e8d no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 18:17:21.683: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6890" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","total":305,"completed":20,"skipped":282,"failed":0}
SSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 18:17:21.718: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0777 on node default medium
Jun 28 18:17:21.915: INFO: Waiting up to 5m0s for pod "pod-6f112ac0-9141-4931-b5c3-cc878b858149" in namespace "emptydir-4198" to be "Succeeded or Failed"
Jun 28 18:17:21.926: INFO: Pod "pod-6f112ac0-9141-4931-b5c3-cc878b858149": Phase="Pending", Reason="", readiness=false. Elapsed: 10.733362ms
Jun 28 18:17:23.936: INFO: Pod "pod-6f112ac0-9141-4931-b5c3-cc878b858149": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.020958499s
STEP: Saw pod success
Jun 28 18:17:23.937: INFO: Pod "pod-6f112ac0-9141-4931-b5c3-cc878b858149" satisfied condition "Succeeded or Failed"
Jun 28 18:17:23.946: INFO: Trying to get logs from node 10.13.107.37 pod pod-6f112ac0-9141-4931-b5c3-cc878b858149 container test-container: <nil>
STEP: delete the pod
Jun 28 18:17:24.038: INFO: Waiting for pod pod-6f112ac0-9141-4931-b5c3-cc878b858149 to disappear
Jun 28 18:17:24.053: INFO: Pod pod-6f112ac0-9141-4931-b5c3-cc878b858149 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 18:17:24.054: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4198" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":21,"skipped":288,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 18:17:24.087: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating projection with secret that has name projected-secret-test-551ecdd1-2666-4974-b989-05c94a937e25
STEP: Creating a pod to test consume secrets
Jun 28 18:17:24.272: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-999b8285-55f8-4049-8ee6-0de6989146b6" in namespace "projected-1143" to be "Succeeded or Failed"
Jun 28 18:17:24.291: INFO: Pod "pod-projected-secrets-999b8285-55f8-4049-8ee6-0de6989146b6": Phase="Pending", Reason="", readiness=false. Elapsed: 19.140536ms
Jun 28 18:17:26.303: INFO: Pod "pod-projected-secrets-999b8285-55f8-4049-8ee6-0de6989146b6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030885829s
Jun 28 18:17:28.323: INFO: Pod "pod-projected-secrets-999b8285-55f8-4049-8ee6-0de6989146b6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.05102909s
STEP: Saw pod success
Jun 28 18:17:28.325: INFO: Pod "pod-projected-secrets-999b8285-55f8-4049-8ee6-0de6989146b6" satisfied condition "Succeeded or Failed"
Jun 28 18:17:28.345: INFO: Trying to get logs from node 10.13.107.37 pod pod-projected-secrets-999b8285-55f8-4049-8ee6-0de6989146b6 container projected-secret-volume-test: <nil>
STEP: delete the pod
Jun 28 18:17:28.437: INFO: Waiting for pod pod-projected-secrets-999b8285-55f8-4049-8ee6-0de6989146b6 to disappear
Jun 28 18:17:28.457: INFO: Pod pod-projected-secrets-999b8285-55f8-4049-8ee6-0de6989146b6 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 18:17:28.457: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1143" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume [NodeConformance] [Conformance]","total":305,"completed":22,"skipped":305,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 18:17:28.511: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:78
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jun 28 18:17:28.645: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Jun 28 18:17:28.673: INFO: Pod name sample-pod: Found 0 pods out of 1
Jun 28 18:17:33.684: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Jun 28 18:17:33.685: INFO: Creating deployment "test-rolling-update-deployment"
Jun 28 18:17:33.711: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Jun 28 18:17:33.749: INFO: deployment "test-rolling-update-deployment" doesn't have the required revision set
Jun 28 18:17:35.802: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Jun 28 18:17:35.834: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:72
Jun 28 18:17:35.899: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-4165 /apis/apps/v1/namespaces/deployment-4165/deployments/test-rolling-update-deployment a0d7a3fe-ff4a-4eca-8eb0-24176d6ed289 53703 1 2021-06-28 18:17:33 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] []  [{e2e.test Update apps/v1 2021-06-28 18:17:33 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{}}},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-06-28 18:17:35 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}}}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.20 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00afcc3c8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2021-06-28 18:17:33 +0000 UTC,LastTransitionTime:2021-06-28 18:17:33 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-c4cb8d6d9" has successfully progressed.,LastUpdateTime:2021-06-28 18:17:35 +0000 UTC,LastTransitionTime:2021-06-28 18:17:33 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Jun 28 18:17:35.913: INFO: New ReplicaSet "test-rolling-update-deployment-c4cb8d6d9" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:{test-rolling-update-deployment-c4cb8d6d9  deployment-4165 /apis/apps/v1/namespaces/deployment-4165/replicasets/test-rolling-update-deployment-c4cb8d6d9 1ef0f04e-5f23-4e58-b24f-083f30f71176 53692 1 2021-06-28 18:17:33 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:c4cb8d6d9] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment a0d7a3fe-ff4a-4eca-8eb0-24176d6ed289 0xc00afccc50 0xc00afccc51}] []  [{kube-controller-manager Update apps/v1 2021-06-28 18:17:35 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a0d7a3fe-ff4a-4eca-8eb0-24176d6ed289\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: c4cb8d6d9,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:c4cb8d6d9] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.20 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00afcccc8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jun 28 18:17:35.914: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Jun 28 18:17:35.914: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-4165 /apis/apps/v1/namespaces/deployment-4165/replicasets/test-rolling-update-controller e5a38954-94e7-4e26-9bc0-2a2aa476cc97 53702 2 2021-06-28 18:17:28 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment a0d7a3fe-ff4a-4eca-8eb0-24176d6ed289 0xc00afccaa7 0xc00afccaa8}] []  [{e2e.test Update apps/v1 2021-06-28 18:17:28 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{"f:matchLabels":{".":{},"f:name":{},"f:pod":{}}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-06-28 18:17:35 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a0d7a3fe-ff4a-4eca-8eb0-24176d6ed289\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{}},"f:status":{"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc00afccbd8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jun 28 18:17:35.929: INFO: Pod "test-rolling-update-deployment-c4cb8d6d9-gjch8" is available:
&Pod{ObjectMeta:{test-rolling-update-deployment-c4cb8d6d9-gjch8 test-rolling-update-deployment-c4cb8d6d9- deployment-4165 /api/v1/namespaces/deployment-4165/pods/test-rolling-update-deployment-c4cb8d6d9-gjch8 5fee82f6-cd2d-44b9-909d-8b03809eb257 53691 0 2021-06-28 18:17:33 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:c4cb8d6d9] map[cni.projectcalico.org/podIP:172.30.54.51/32 cni.projectcalico.org/podIPs:172.30.54.51/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "",
    "ips": [
        "172.30.54.51"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "",
    "ips": [
        "172.30.54.51"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet test-rolling-update-deployment-c4cb8d6d9 1ef0f04e-5f23-4e58-b24f-083f30f71176 0xc00afcd627 0xc00afcd628}] []  [{kube-controller-manager Update v1 2021-06-28 18:17:33 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1ef0f04e-5f23-4e58-b24f-083f30f71176\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-06-28 18:17:34 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {multus Update v1 2021-06-28 18:17:34 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}}} {kubelet Update v1 2021-06-28 18:17:35 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.54.51\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-r8jwh,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-r8jwh,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:k8s.gcr.io/e2e-test-images/agnhost:2.20,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-r8jwh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.13.107.57,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c32,c24,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-x7hhm,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-28 18:17:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-28 18:17:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-28 18:17:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-28 18:17:33 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.13.107.57,PodIP:172.30.54.51,StartTime:2021-06-28 18:17:33 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-06-28 18:17:35 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.20,ImageID:k8s.gcr.io/e2e-test-images/agnhost@sha256:17e61a0b9e498b6c73ed97670906be3d5a3ae394739c1bd5b619e1a004885cf0,ContainerID:cri-o://68ddd5092836797b074b8228f10ce525cb04301d5c6c2293e69592abe60f0920,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.54.51,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 18:17:35.929: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-4165" for this suite.

• [SLOW TEST:7.463 seconds]
[sig-apps] Deployment
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Deployment RollingUpdateDeployment should delete old pods and create new ones [Conformance]","total":305,"completed":23,"skipped":320,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should test the lifecycle of an Endpoint [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 18:17:35.979: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should test the lifecycle of an Endpoint [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating an Endpoint
STEP: waiting for available Endpoint
STEP: listing all Endpoints
STEP: updating the Endpoint
STEP: fetching the Endpoint
STEP: patching the Endpoint
STEP: fetching the Endpoint
STEP: deleting the Endpoint by Collection
STEP: waiting for Endpoint deletion
STEP: fetching the Endpoint
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 18:17:36.303: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-9078" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786
•{"msg":"PASSED [sig-network] Services should test the lifecycle of an Endpoint [Conformance]","total":305,"completed":24,"skipped":358,"failed":0}

------------------------------
[sig-storage] Projected configMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 18:17:36.360: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jun 28 18:17:36.527: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
STEP: Creating configMap with name cm-test-opt-del-bea117a7-053c-4f83-857a-00b3c8633b29
STEP: Creating configMap with name cm-test-opt-upd-6e070e3c-f73d-41bf-afbc-d79843272642
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-bea117a7-053c-4f83-857a-00b3c8633b29
STEP: Updating configmap cm-test-opt-upd-6e070e3c-f73d-41bf-afbc-d79843272642
STEP: Creating configMap with name cm-test-opt-create-395f523b-2bd3-4337-b04d-8fe6f946d248
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 18:18:56.627: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2108" for this suite.

• [SLOW TEST:80.305 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:36
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap optional updates should be reflected in volume [NodeConformance] [Conformance]","total":305,"completed":25,"skipped":358,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 18:18:56.669: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jun 28 18:18:56.823: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Jun 28 18:19:07.457: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-108209140 --namespace=crd-publish-openapi-7793 create -f -'
Jun 28 18:19:09.496: INFO: stderr: ""
Jun 28 18:19:09.496: INFO: stdout: "e2e-test-crd-publish-openapi-8549-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Jun 28 18:19:09.496: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-108209140 --namespace=crd-publish-openapi-7793 delete e2e-test-crd-publish-openapi-8549-crds test-cr'
Jun 28 18:19:09.851: INFO: stderr: ""
Jun 28 18:19:09.851: INFO: stdout: "e2e-test-crd-publish-openapi-8549-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
Jun 28 18:19:09.852: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-108209140 --namespace=crd-publish-openapi-7793 apply -f -'
Jun 28 18:19:10.791: INFO: stderr: ""
Jun 28 18:19:10.791: INFO: stdout: "e2e-test-crd-publish-openapi-8549-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Jun 28 18:19:10.791: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-108209140 --namespace=crd-publish-openapi-7793 delete e2e-test-crd-publish-openapi-8549-crds test-cr'
Jun 28 18:19:11.079: INFO: stderr: ""
Jun 28 18:19:11.079: INFO: stdout: "e2e-test-crd-publish-openapi-8549-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
Jun 28 18:19:11.079: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-108209140 explain e2e-test-crd-publish-openapi-8549-crds'
Jun 28 18:19:11.938: INFO: stderr: ""
Jun 28 18:19:11.938: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-8549-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<map[string]>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 18:19:23.336: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-7793" for this suite.

• [SLOW TEST:26.707 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields in an embedded object [Conformance]","total":305,"completed":26,"skipped":434,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 18:19:23.377: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jun 28 18:21:23.613: INFO: Deleting pod "var-expansion-b9a9ccca-3056-4e63-8fdd-35714b87e9f9" in namespace "var-expansion-7211"
Jun 28 18:21:23.635: INFO: Wait up to 5m0s for pod "var-expansion-b9a9ccca-3056-4e63-8fdd-35714b87e9f9" to be fully deleted
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 18:21:31.663: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-7211" for this suite.

• [SLOW TEST:128.330 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]","total":305,"completed":27,"skipped":459,"failed":0}
[sig-api-machinery] Watchers 
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 18:21:31.708: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a watch on configmaps with label A
STEP: creating a watch on configmaps with label B
STEP: creating a watch on configmaps with label A or B
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification
Jun 28 18:21:31.877: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-3228 /api/v1/namespaces/watch-3228/configmaps/e2e-watch-test-configmap-a 84b6e34c-bb2b-4da2-a61a-b76ebac3bf53 55368 0 2021-06-28 18:21:31 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-06-28 18:21:31 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Jun 28 18:21:31.878: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-3228 /api/v1/namespaces/watch-3228/configmaps/e2e-watch-test-configmap-a 84b6e34c-bb2b-4da2-a61a-b76ebac3bf53 55368 0 2021-06-28 18:21:31 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-06-28 18:21:31 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A and ensuring the correct watchers observe the notification
Jun 28 18:21:41.932: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-3228 /api/v1/namespaces/watch-3228/configmaps/e2e-watch-test-configmap-a 84b6e34c-bb2b-4da2-a61a-b76ebac3bf53 55439 0 2021-06-28 18:21:31 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-06-28 18:21:41 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Jun 28 18:21:41.932: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-3228 /api/v1/namespaces/watch-3228/configmaps/e2e-watch-test-configmap-a 84b6e34c-bb2b-4da2-a61a-b76ebac3bf53 55439 0 2021-06-28 18:21:31 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-06-28 18:21:41 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification
Jun 28 18:21:51.986: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-3228 /api/v1/namespaces/watch-3228/configmaps/e2e-watch-test-configmap-a 84b6e34c-bb2b-4da2-a61a-b76ebac3bf53 55498 0 2021-06-28 18:21:31 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-06-28 18:21:51 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Jun 28 18:21:51.986: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-3228 /api/v1/namespaces/watch-3228/configmaps/e2e-watch-test-configmap-a 84b6e34c-bb2b-4da2-a61a-b76ebac3bf53 55498 0 2021-06-28 18:21:31 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-06-28 18:21:51 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap A and ensuring the correct watchers observe the notification
Jun 28 18:22:02.019: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-3228 /api/v1/namespaces/watch-3228/configmaps/e2e-watch-test-configmap-a 84b6e34c-bb2b-4da2-a61a-b76ebac3bf53 55543 0 2021-06-28 18:21:31 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-06-28 18:21:51 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Jun 28 18:22:02.019: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-3228 /api/v1/namespaces/watch-3228/configmaps/e2e-watch-test-configmap-a 84b6e34c-bb2b-4da2-a61a-b76ebac3bf53 55543 0 2021-06-28 18:21:31 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-06-28 18:21:51 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification
Jun 28 18:22:12.048: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-3228 /api/v1/namespaces/watch-3228/configmaps/e2e-watch-test-configmap-b 1870ffe7-5ff2-4149-825e-d8e9a2eb2a6c 55587 0 2021-06-28 18:22:12 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2021-06-28 18:22:12 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Jun 28 18:22:12.048: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-3228 /api/v1/namespaces/watch-3228/configmaps/e2e-watch-test-configmap-b 1870ffe7-5ff2-4149-825e-d8e9a2eb2a6c 55587 0 2021-06-28 18:22:12 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2021-06-28 18:22:12 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap B and ensuring the correct watchers observe the notification
Jun 28 18:22:22.081: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-3228 /api/v1/namespaces/watch-3228/configmaps/e2e-watch-test-configmap-b 1870ffe7-5ff2-4149-825e-d8e9a2eb2a6c 55634 0 2021-06-28 18:22:12 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2021-06-28 18:22:12 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Jun 28 18:22:22.082: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-3228 /api/v1/namespaces/watch-3228/configmaps/e2e-watch-test-configmap-b 1870ffe7-5ff2-4149-825e-d8e9a2eb2a6c 55634 0 2021-06-28 18:22:12 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2021-06-28 18:22:12 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 18:22:32.082: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-3228" for this suite.

• [SLOW TEST:60.423 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should observe add, update, and delete watch notifications on configmaps [Conformance]","total":305,"completed":28,"skipped":459,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  updates the published spec when one version gets renamed [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 18:22:32.131: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates the published spec when one version gets renamed [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: set up a multi version CRD
Jun 28 18:22:32.264: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: rename a version
STEP: check the new version name is served
STEP: check the old version name is removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 18:23:24.463: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-4484" for this suite.

• [SLOW TEST:52.367 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  updates the published spec when one version gets renamed [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] updates the published spec when one version gets renamed [Conformance]","total":305,"completed":29,"skipped":474,"failed":0}
SSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 18:23:24.504: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a test externalName service
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-4945.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-4945.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-4945.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-4945.svc.cluster.local; sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jun 28 18:23:26.771: INFO: DNS probes using dns-test-4f33eb6b-d18e-431e-a973-f50f96f21a0b succeeded

STEP: deleting the pod
STEP: changing the externalName to bar.example.com
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-4945.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-4945.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-4945.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-4945.svc.cluster.local; sleep 1; done

STEP: creating a second pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jun 28 18:23:30.928: INFO: File wheezy_udp@dns-test-service-3.dns-4945.svc.cluster.local from pod  dns-4945/dns-test-cf443325-efeb-4c0d-a0a9-c6a6327c0b03 contains 'foo.example.com.
' instead of 'bar.example.com.'
Jun 28 18:23:30.947: INFO: File jessie_udp@dns-test-service-3.dns-4945.svc.cluster.local from pod  dns-4945/dns-test-cf443325-efeb-4c0d-a0a9-c6a6327c0b03 contains 'foo.example.com.
' instead of 'bar.example.com.'
Jun 28 18:23:30.947: INFO: Lookups using dns-4945/dns-test-cf443325-efeb-4c0d-a0a9-c6a6327c0b03 failed for: [wheezy_udp@dns-test-service-3.dns-4945.svc.cluster.local jessie_udp@dns-test-service-3.dns-4945.svc.cluster.local]

Jun 28 18:23:35.996: INFO: DNS probes using dns-test-cf443325-efeb-4c0d-a0a9-c6a6327c0b03 succeeded

STEP: deleting the pod
STEP: changing the service to type=ClusterIP
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-4945.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-4945.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-4945.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-4945.svc.cluster.local; sleep 1; done

STEP: creating a third pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jun 28 18:23:40.241: INFO: DNS probes using dns-test-cc0baf11-a905-42ea-be8f-c08f27943247 succeeded

STEP: deleting the pod
STEP: deleting the test externalName service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 18:23:40.310: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-4945" for this suite.

• [SLOW TEST:15.845 seconds]
[sig-network] DNS
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for ExternalName services [Conformance]","total":305,"completed":30,"skipped":480,"failed":0}
SSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with different stored version [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 18:23:40.349: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jun 28 18:23:41.005: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jun 28 18:23:43.036: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63760501421, loc:(*time.Location)(0x7702840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63760501421, loc:(*time.Location)(0x7702840)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63760501421, loc:(*time.Location)(0x7702840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63760501421, loc:(*time.Location)(0x7702840)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun 28 18:23:46.079: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with different stored version [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jun 28 18:23:46.097: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-7931-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource while v1 is storage version
STEP: Patching Custom Resource Definition to set v2 as storage
STEP: Patching the custom resource while v2 is storage version
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 18:23:47.632: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8169" for this suite.
STEP: Destroying namespace "webhook-8169-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:7.453 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource with different stored version [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with different stored version [Conformance]","total":305,"completed":31,"skipped":490,"failed":0}
SSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Guestbook application 
  should create and stop a working application  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 18:23:47.802: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[It] should create and stop a working application  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating all guestbook components
Jun 28 18:23:47.928: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-replica
  labels:
    app: agnhost
    role: replica
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: agnhost
    role: replica
    tier: backend

Jun 28 18:23:47.928: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-108209140 create -f - --namespace=kubectl-9204'
Jun 28 18:23:48.791: INFO: stderr: ""
Jun 28 18:23:48.791: INFO: stdout: "service/agnhost-replica created\n"
Jun 28 18:23:48.791: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-primary
  labels:
    app: agnhost
    role: primary
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: agnhost
    role: primary
    tier: backend

Jun 28 18:23:48.791: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-108209140 create -f - --namespace=kubectl-9204'
Jun 28 18:23:51.114: INFO: stderr: ""
Jun 28 18:23:51.114: INFO: stdout: "service/agnhost-primary created\n"
Jun 28 18:23:51.114: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Jun 28 18:23:51.114: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-108209140 create -f - --namespace=kubectl-9204'
Jun 28 18:23:52.575: INFO: stderr: ""
Jun 28 18:23:52.575: INFO: stdout: "service/frontend created\n"
Jun 28 18:23:52.575: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: guestbook-frontend
        image: k8s.gcr.io/e2e-test-images/agnhost:2.20
        args: [ "guestbook", "--backend-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 80

Jun 28 18:23:52.575: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-108209140 create -f - --namespace=kubectl-9204'
Jun 28 18:23:53.559: INFO: stderr: ""
Jun 28 18:23:53.559: INFO: stdout: "deployment.apps/frontend created\n"
Jun 28 18:23:53.559: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-primary
spec:
  replicas: 1
  selector:
    matchLabels:
      app: agnhost
      role: primary
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: primary
        tier: backend
    spec:
      containers:
      - name: primary
        image: k8s.gcr.io/e2e-test-images/agnhost:2.20
        args: [ "guestbook", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Jun 28 18:23:53.559: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-108209140 create -f - --namespace=kubectl-9204'
Jun 28 18:23:54.511: INFO: stderr: ""
Jun 28 18:23:54.511: INFO: stdout: "deployment.apps/agnhost-primary created\n"
Jun 28 18:23:54.511: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-replica
spec:
  replicas: 2
  selector:
    matchLabels:
      app: agnhost
      role: replica
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: replica
        tier: backend
    spec:
      containers:
      - name: replica
        image: k8s.gcr.io/e2e-test-images/agnhost:2.20
        args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Jun 28 18:23:54.511: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-108209140 create -f - --namespace=kubectl-9204'
Jun 28 18:23:55.480: INFO: stderr: ""
Jun 28 18:23:55.480: INFO: stdout: "deployment.apps/agnhost-replica created\n"
STEP: validating guestbook app
Jun 28 18:23:55.480: INFO: Waiting for all frontend pods to be Running.
Jun 28 18:24:00.531: INFO: Waiting for frontend to serve content.
Jun 28 18:24:00.579: INFO: Trying to add a new entry to the guestbook.
Jun 28 18:24:00.643: INFO: Verifying that added entry can be retrieved.
STEP: using delete to clean up resources
Jun 28 18:24:00.672: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-108209140 delete --grace-period=0 --force -f - --namespace=kubectl-9204'
Jun 28 18:24:00.948: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jun 28 18:24:00.948: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
STEP: using delete to clean up resources
Jun 28 18:24:00.949: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-108209140 delete --grace-period=0 --force -f - --namespace=kubectl-9204'
Jun 28 18:24:01.155: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jun 28 18:24:01.155: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources
Jun 28 18:24:01.156: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-108209140 delete --grace-period=0 --force -f - --namespace=kubectl-9204'
Jun 28 18:24:01.400: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jun 28 18:24:01.400: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Jun 28 18:24:01.401: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-108209140 delete --grace-period=0 --force -f - --namespace=kubectl-9204'
Jun 28 18:24:01.662: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jun 28 18:24:01.662: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Jun 28 18:24:01.663: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-108209140 delete --grace-period=0 --force -f - --namespace=kubectl-9204'
Jun 28 18:24:01.860: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jun 28 18:24:01.860: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources
Jun 28 18:24:01.861: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-108209140 delete --grace-period=0 --force -f - --namespace=kubectl-9204'
Jun 28 18:24:02.210: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jun 28 18:24:02.210: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 18:24:02.210: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9204" for this suite.

• [SLOW TEST:14.454 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Guestbook application
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:351
    should create and stop a working application  [Conformance]
    /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Guestbook application should create and stop a working application  [Conformance]","total":305,"completed":32,"skipped":500,"failed":0}
SS
------------------------------
[sig-cli] Kubectl client Kubectl expose 
  should create services for rc  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 18:24:02.256: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[It] should create services for rc  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating Agnhost RC
Jun 28 18:24:02.392: INFO: namespace kubectl-5044
Jun 28 18:24:02.392: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-108209140 create -f - --namespace=kubectl-5044'
Jun 28 18:24:03.450: INFO: stderr: ""
Jun 28 18:24:03.450: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
Jun 28 18:24:04.466: INFO: Selector matched 1 pods for map[app:agnhost]
Jun 28 18:24:04.466: INFO: Found 0 / 1
Jun 28 18:24:05.466: INFO: Selector matched 1 pods for map[app:agnhost]
Jun 28 18:24:05.467: INFO: Found 0 / 1
Jun 28 18:24:06.468: INFO: Selector matched 1 pods for map[app:agnhost]
Jun 28 18:24:06.468: INFO: Found 1 / 1
Jun 28 18:24:06.468: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Jun 28 18:24:06.479: INFO: Selector matched 1 pods for map[app:agnhost]
Jun 28 18:24:06.479: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jun 28 18:24:06.479: INFO: wait on agnhost-primary startup in kubectl-5044 
Jun 28 18:24:06.479: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-108209140 logs agnhost-primary-zqwcs agnhost-primary --namespace=kubectl-5044'
Jun 28 18:24:06.766: INFO: stderr: ""
Jun 28 18:24:06.766: INFO: stdout: "Paused\n"
STEP: exposing RC
Jun 28 18:24:06.766: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-108209140 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379 --namespace=kubectl-5044'
Jun 28 18:24:07.066: INFO: stderr: ""
Jun 28 18:24:07.066: INFO: stdout: "service/rm2 exposed\n"
Jun 28 18:24:07.075: INFO: Service rm2 in namespace kubectl-5044 found.
STEP: exposing service
Jun 28 18:24:09.099: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-108209140 expose service rm2 --name=rm3 --port=2345 --target-port=6379 --namespace=kubectl-5044'
Jun 28 18:24:09.393: INFO: stderr: ""
Jun 28 18:24:09.393: INFO: stdout: "service/rm3 exposed\n"
Jun 28 18:24:09.404: INFO: Service rm3 in namespace kubectl-5044 found.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 18:24:11.433: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5044" for this suite.

• [SLOW TEST:9.211 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl expose
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1246
    should create services for rc  [Conformance]
    /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl expose should create services for rc  [Conformance]","total":305,"completed":33,"skipped":502,"failed":0}
SSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not conflict [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 18:24:11.468: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not conflict [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Cleaning up the secret
STEP: Cleaning up the configmap
STEP: Cleaning up the pod
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 18:24:15.817: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-3687" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not conflict [Conformance]","total":305,"completed":34,"skipped":509,"failed":0}
SSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 18:24:15.859: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:162
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod
Jun 28 18:24:15.982: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 18:24:19.845: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-8451" for this suite.
•{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]","total":305,"completed":35,"skipped":520,"failed":0}
SS
------------------------------
[sig-network] Services 
  should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 18:24:19.880: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a service nodeport-service with the type=NodePort in namespace services-4438
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-4438
STEP: creating replication controller externalsvc in namespace services-4438
I0628 18:24:20.121429      22 runners.go:190] Created replication controller with name: externalsvc, namespace: services-4438, replica count: 2
I0628 18:24:23.174186      22 runners.go:190] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the NodePort service to type=ExternalName
Jun 28 18:24:23.238: INFO: Creating new exec pod
Jun 28 18:24:25.309: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-108209140 exec --namespace=services-4438 execpoddm9cs -- /bin/sh -x -c nslookup nodeport-service.services-4438.svc.cluster.local'
Jun 28 18:24:25.694: INFO: stderr: "+ nslookup nodeport-service.services-4438.svc.cluster.local\n"
Jun 28 18:24:25.694: INFO: stdout: "Server:\t\t172.21.0.10\nAddress:\t172.21.0.10#53\n\nnodeport-service.services-4438.svc.cluster.local\tcanonical name = externalsvc.services-4438.svc.cluster.local.\nName:\texternalsvc.services-4438.svc.cluster.local\nAddress: 172.21.80.178\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-4438, will wait for the garbage collector to delete the pods
Jun 28 18:24:25.779: INFO: Deleting ReplicationController externalsvc took: 19.701358ms
Jun 28 18:24:25.880: INFO: Terminating ReplicationController externalsvc pods took: 101.122206ms
Jun 28 18:24:35.032: INFO: Cleaning up the NodePort to ExternalName test service
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 18:24:35.068: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4438" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

• [SLOW TEST:15.219 seconds]
[sig-network] Services
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from NodePort to ExternalName [Conformance]","total":305,"completed":36,"skipped":522,"failed":0}
SSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 18:24:35.098: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test substitution in container's args
Jun 28 18:24:35.264: INFO: Waiting up to 5m0s for pod "var-expansion-6c0643c4-0bd4-44ae-ae67-77e79b7458ce" in namespace "var-expansion-8733" to be "Succeeded or Failed"
Jun 28 18:24:35.274: INFO: Pod "var-expansion-6c0643c4-0bd4-44ae-ae67-77e79b7458ce": Phase="Pending", Reason="", readiness=false. Elapsed: 10.021861ms
Jun 28 18:24:37.297: INFO: Pod "var-expansion-6c0643c4-0bd4-44ae-ae67-77e79b7458ce": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033393158s
Jun 28 18:24:39.310: INFO: Pod "var-expansion-6c0643c4-0bd4-44ae-ae67-77e79b7458ce": Phase="Pending", Reason="", readiness=false. Elapsed: 4.046450927s
Jun 28 18:24:41.330: INFO: Pod "var-expansion-6c0643c4-0bd4-44ae-ae67-77e79b7458ce": Phase="Pending", Reason="", readiness=false. Elapsed: 6.065910109s
Jun 28 18:24:43.347: INFO: Pod "var-expansion-6c0643c4-0bd4-44ae-ae67-77e79b7458ce": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.082969688s
STEP: Saw pod success
Jun 28 18:24:43.347: INFO: Pod "var-expansion-6c0643c4-0bd4-44ae-ae67-77e79b7458ce" satisfied condition "Succeeded or Failed"
Jun 28 18:24:43.358: INFO: Trying to get logs from node 10.13.107.57 pod var-expansion-6c0643c4-0bd4-44ae-ae67-77e79b7458ce container dapi-container: <nil>
STEP: delete the pod
Jun 28 18:24:43.438: INFO: Waiting for pod var-expansion-6c0643c4-0bd4-44ae-ae67-77e79b7458ce to disappear
Jun 28 18:24:43.449: INFO: Pod var-expansion-6c0643c4-0bd4-44ae-ae67-77e79b7458ce no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 18:24:43.449: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-8733" for this suite.

• [SLOW TEST:8.402 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Variable Expansion should allow substituting values in a container's args [NodeConformance] [Conformance]","total":305,"completed":37,"skipped":525,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 18:24:43.501: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:162
[It] should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod
Jun 28 18:24:43.637: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 18:24:46.941: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-2122" for this suite.
•{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should invoke init containers on a RestartNever pod [Conformance]","total":305,"completed":38,"skipped":555,"failed":0}
SSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 18:24:46.982: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap configmap-4524/configmap-test-ccd0f912-d297-42a0-9e2f-e6e3bcbb8428
STEP: Creating a pod to test consume configMaps
Jun 28 18:24:47.199: INFO: Waiting up to 5m0s for pod "pod-configmaps-44a6294f-246a-4486-a4ba-44d2cf02d9e8" in namespace "configmap-4524" to be "Succeeded or Failed"
Jun 28 18:24:47.211: INFO: Pod "pod-configmaps-44a6294f-246a-4486-a4ba-44d2cf02d9e8": Phase="Pending", Reason="", readiness=false. Elapsed: 11.552644ms
Jun 28 18:24:49.226: INFO: Pod "pod-configmaps-44a6294f-246a-4486-a4ba-44d2cf02d9e8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.027128396s
STEP: Saw pod success
Jun 28 18:24:49.227: INFO: Pod "pod-configmaps-44a6294f-246a-4486-a4ba-44d2cf02d9e8" satisfied condition "Succeeded or Failed"
Jun 28 18:24:49.239: INFO: Trying to get logs from node 10.13.107.37 pod pod-configmaps-44a6294f-246a-4486-a4ba-44d2cf02d9e8 container env-test: <nil>
STEP: delete the pod
Jun 28 18:24:49.443: INFO: Waiting for pod pod-configmaps-44a6294f-246a-4486-a4ba-44d2cf02d9e8 to disappear
Jun 28 18:24:49.458: INFO: Pod pod-configmaps-44a6294f-246a-4486-a4ba-44d2cf02d9e8 no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 18:24:49.458: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4524" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should be consumable via the environment [NodeConformance] [Conformance]","total":305,"completed":39,"skipped":561,"failed":0}
SSSS
------------------------------
[k8s.io] Container Runtime blackbox test when starting a container that exits 
  should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 18:24:49.496: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpa': should get the expected 'State'
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpof': should get the expected 'State'
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpn': should get the expected 'State'
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance]
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 18:25:12.407: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-8106" for this suite.

• [SLOW TEST:22.942 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  blackbox test
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:41
    when starting a container that exits
    /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:42
      should run with the expected status [NodeConformance] [Conformance]
      /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Container Runtime blackbox test when starting a container that exits should run with the expected status [NodeConformance] [Conformance]","total":305,"completed":40,"skipped":565,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 18:25:12.439: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod pod-subpath-test-configmap-rsbb
STEP: Creating a pod to test atomic-volume-subpath
Jun 28 18:25:12.642: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-rsbb" in namespace "subpath-4086" to be "Succeeded or Failed"
Jun 28 18:25:12.654: INFO: Pod "pod-subpath-test-configmap-rsbb": Phase="Pending", Reason="", readiness=false. Elapsed: 11.699374ms
Jun 28 18:25:14.668: INFO: Pod "pod-subpath-test-configmap-rsbb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025694307s
Jun 28 18:25:16.687: INFO: Pod "pod-subpath-test-configmap-rsbb": Phase="Running", Reason="", readiness=true. Elapsed: 4.044715741s
Jun 28 18:25:18.700: INFO: Pod "pod-subpath-test-configmap-rsbb": Phase="Running", Reason="", readiness=true. Elapsed: 6.058051125s
Jun 28 18:25:20.718: INFO: Pod "pod-subpath-test-configmap-rsbb": Phase="Running", Reason="", readiness=true. Elapsed: 8.076044239s
Jun 28 18:25:22.735: INFO: Pod "pod-subpath-test-configmap-rsbb": Phase="Running", Reason="", readiness=true. Elapsed: 10.092939153s
Jun 28 18:25:24.747: INFO: Pod "pod-subpath-test-configmap-rsbb": Phase="Running", Reason="", readiness=true. Elapsed: 12.105520161s
Jun 28 18:25:26.763: INFO: Pod "pod-subpath-test-configmap-rsbb": Phase="Running", Reason="", readiness=true. Elapsed: 14.121087541s
Jun 28 18:25:28.780: INFO: Pod "pod-subpath-test-configmap-rsbb": Phase="Running", Reason="", readiness=true. Elapsed: 16.13849525s
Jun 28 18:25:30.799: INFO: Pod "pod-subpath-test-configmap-rsbb": Phase="Running", Reason="", readiness=true. Elapsed: 18.156657548s
Jun 28 18:25:32.814: INFO: Pod "pod-subpath-test-configmap-rsbb": Phase="Running", Reason="", readiness=true. Elapsed: 20.172476371s
Jun 28 18:25:34.826: INFO: Pod "pod-subpath-test-configmap-rsbb": Phase="Running", Reason="", readiness=true. Elapsed: 22.183697803s
Jun 28 18:25:36.839: INFO: Pod "pod-subpath-test-configmap-rsbb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.196696346s
STEP: Saw pod success
Jun 28 18:25:36.839: INFO: Pod "pod-subpath-test-configmap-rsbb" satisfied condition "Succeeded or Failed"
Jun 28 18:25:36.849: INFO: Trying to get logs from node 10.13.107.37 pod pod-subpath-test-configmap-rsbb container test-container-subpath-configmap-rsbb: <nil>
STEP: delete the pod
Jun 28 18:25:36.911: INFO: Waiting for pod pod-subpath-test-configmap-rsbb to disappear
Jun 28 18:25:36.920: INFO: Pod pod-subpath-test-configmap-rsbb no longer exists
STEP: Deleting pod pod-subpath-test-configmap-rsbb
Jun 28 18:25:36.921: INFO: Deleting pod "pod-subpath-test-configmap-rsbb" in namespace "subpath-4086"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 18:25:36.936: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-4086" for this suite.

• [SLOW TEST:24.531 seconds]
[sig-storage] Subpath
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod [LinuxOnly] [Conformance]","total":305,"completed":41,"skipped":576,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 18:25:36.970: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating secret secrets-3045/secret-test-13b73d50-bd86-49b6-ab66-02e574375912
STEP: Creating a pod to test consume secrets
Jun 28 18:25:37.147: INFO: Waiting up to 5m0s for pod "pod-configmaps-b2bcea69-d5b3-4c13-ba73-417b8dc3a92c" in namespace "secrets-3045" to be "Succeeded or Failed"
Jun 28 18:25:37.157: INFO: Pod "pod-configmaps-b2bcea69-d5b3-4c13-ba73-417b8dc3a92c": Phase="Pending", Reason="", readiness=false. Elapsed: 9.791365ms
Jun 28 18:25:39.170: INFO: Pod "pod-configmaps-b2bcea69-d5b3-4c13-ba73-417b8dc3a92c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.023111657s
STEP: Saw pod success
Jun 28 18:25:39.170: INFO: Pod "pod-configmaps-b2bcea69-d5b3-4c13-ba73-417b8dc3a92c" satisfied condition "Succeeded or Failed"
Jun 28 18:25:39.181: INFO: Trying to get logs from node 10.13.107.37 pod pod-configmaps-b2bcea69-d5b3-4c13-ba73-417b8dc3a92c container env-test: <nil>
STEP: delete the pod
Jun 28 18:25:39.237: INFO: Waiting for pod pod-configmaps-b2bcea69-d5b3-4c13-ba73-417b8dc3a92c to disappear
Jun 28 18:25:39.249: INFO: Pod pod-configmaps-b2bcea69-d5b3-4c13-ba73-417b8dc3a92c no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 18:25:39.250: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3045" for this suite.
•{"msg":"PASSED [sig-api-machinery] Secrets should be consumable via the environment [NodeConformance] [Conformance]","total":305,"completed":42,"skipped":609,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates 
  should run the lifecycle of PodTemplates [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-node] PodTemplates
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 18:25:39.287: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename podtemplate
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run the lifecycle of PodTemplates [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [sig-node] PodTemplates
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 18:25:39.584: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-427" for this suite.
•{"msg":"PASSED [sig-node] PodTemplates should run the lifecycle of PodTemplates [Conformance]","total":305,"completed":43,"skipped":625,"failed":0}
SSSSS
------------------------------
[sig-instrumentation] Events API 
  should delete a collection of events [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-instrumentation] Events API
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 18:25:39.620: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-instrumentation] Events API
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/instrumentation/events.go:81
[It] should delete a collection of events [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Create set of events
STEP: get a list of Events with a label in the current namespace
STEP: delete a list of events
Jun 28 18:25:39.805: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity
[AfterEach] [sig-instrumentation] Events API
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 18:25:39.864: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-415" for this suite.
•{"msg":"PASSED [sig-instrumentation] Events API should delete a collection of events [Conformance]","total":305,"completed":44,"skipped":630,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 18:25:39.898: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Jun 28 18:25:40.071: INFO: Waiting up to 5m0s for pod "downwardapi-volume-bfe61eeb-2dd6-41c3-8100-bc27ec166f6a" in namespace "downward-api-4917" to be "Succeeded or Failed"
Jun 28 18:25:40.082: INFO: Pod "downwardapi-volume-bfe61eeb-2dd6-41c3-8100-bc27ec166f6a": Phase="Pending", Reason="", readiness=false. Elapsed: 11.163772ms
Jun 28 18:25:42.092: INFO: Pod "downwardapi-volume-bfe61eeb-2dd6-41c3-8100-bc27ec166f6a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021403593s
Jun 28 18:25:44.116: INFO: Pod "downwardapi-volume-bfe61eeb-2dd6-41c3-8100-bc27ec166f6a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.044603148s
STEP: Saw pod success
Jun 28 18:25:44.116: INFO: Pod "downwardapi-volume-bfe61eeb-2dd6-41c3-8100-bc27ec166f6a" satisfied condition "Succeeded or Failed"
Jun 28 18:25:44.127: INFO: Trying to get logs from node 10.13.107.37 pod downwardapi-volume-bfe61eeb-2dd6-41c3-8100-bc27ec166f6a container client-container: <nil>
STEP: delete the pod
Jun 28 18:25:44.188: INFO: Waiting for pod downwardapi-volume-bfe61eeb-2dd6-41c3-8100-bc27ec166f6a to disappear
Jun 28 18:25:44.199: INFO: Pod downwardapi-volume-bfe61eeb-2dd6-41c3-8100-bc27ec166f6a no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 18:25:44.199: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4917" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","total":305,"completed":45,"skipped":645,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 18:25:44.237: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward api env vars
Jun 28 18:25:44.414: INFO: Waiting up to 5m0s for pod "downward-api-db90d621-fbda-4b90-a7c8-e787d82fe1f3" in namespace "downward-api-9238" to be "Succeeded or Failed"
Jun 28 18:25:44.427: INFO: Pod "downward-api-db90d621-fbda-4b90-a7c8-e787d82fe1f3": Phase="Pending", Reason="", readiness=false. Elapsed: 13.336394ms
Jun 28 18:25:46.444: INFO: Pod "downward-api-db90d621-fbda-4b90-a7c8-e787d82fe1f3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.030701901s
STEP: Saw pod success
Jun 28 18:25:46.445: INFO: Pod "downward-api-db90d621-fbda-4b90-a7c8-e787d82fe1f3" satisfied condition "Succeeded or Failed"
Jun 28 18:25:46.457: INFO: Trying to get logs from node 10.13.107.37 pod downward-api-db90d621-fbda-4b90-a7c8-e787d82fe1f3 container dapi-container: <nil>
STEP: delete the pod
Jun 28 18:25:46.522: INFO: Waiting for pod downward-api-db90d621-fbda-4b90-a7c8-e787d82fe1f3 to disappear
Jun 28 18:25:46.532: INFO: Pod downward-api-db90d621-fbda-4b90-a7c8-e787d82fe1f3 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 18:25:46.532: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9238" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]","total":305,"completed":46,"skipped":666,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 18:25:46.566: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Jun 28 18:25:46.751: INFO: Waiting up to 5m0s for pod "downwardapi-volume-80859462-4bc9-40c0-9b60-a8d10687bfc0" in namespace "downward-api-4011" to be "Succeeded or Failed"
Jun 28 18:25:46.762: INFO: Pod "downwardapi-volume-80859462-4bc9-40c0-9b60-a8d10687bfc0": Phase="Pending", Reason="", readiness=false. Elapsed: 11.580272ms
Jun 28 18:25:48.780: INFO: Pod "downwardapi-volume-80859462-4bc9-40c0-9b60-a8d10687bfc0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029033834s
Jun 28 18:25:50.796: INFO: Pod "downwardapi-volume-80859462-4bc9-40c0-9b60-a8d10687bfc0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.045005334s
STEP: Saw pod success
Jun 28 18:25:50.796: INFO: Pod "downwardapi-volume-80859462-4bc9-40c0-9b60-a8d10687bfc0" satisfied condition "Succeeded or Failed"
Jun 28 18:25:50.806: INFO: Trying to get logs from node 10.13.107.37 pod downwardapi-volume-80859462-4bc9-40c0-9b60-a8d10687bfc0 container client-container: <nil>
STEP: delete the pod
Jun 28 18:25:50.864: INFO: Waiting for pod downwardapi-volume-80859462-4bc9-40c0-9b60-a8d10687bfc0 to disappear
Jun 28 18:25:50.875: INFO: Pod downwardapi-volume-80859462-4bc9-40c0-9b60-a8d10687bfc0 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 18:25:50.876: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4011" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide podname only [NodeConformance] [Conformance]","total":305,"completed":47,"skipped":696,"failed":0}
S
------------------------------
[k8s.io] Pods 
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 18:25:50.914: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:181
[It] should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jun 28 18:25:55.186: INFO: Waiting up to 5m0s for pod "client-envvars-d5572a75-7302-4265-9cbc-94bdcf83b10c" in namespace "pods-7191" to be "Succeeded or Failed"
Jun 28 18:25:55.199: INFO: Pod "client-envvars-d5572a75-7302-4265-9cbc-94bdcf83b10c": Phase="Pending", Reason="", readiness=false. Elapsed: 12.93287ms
Jun 28 18:25:57.210: INFO: Pod "client-envvars-d5572a75-7302-4265-9cbc-94bdcf83b10c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.023508855s
STEP: Saw pod success
Jun 28 18:25:57.210: INFO: Pod "client-envvars-d5572a75-7302-4265-9cbc-94bdcf83b10c" satisfied condition "Succeeded or Failed"
Jun 28 18:25:57.220: INFO: Trying to get logs from node 10.13.107.57 pod client-envvars-d5572a75-7302-4265-9cbc-94bdcf83b10c container env3cont: <nil>
STEP: delete the pod
Jun 28 18:25:57.285: INFO: Waiting for pod client-envvars-d5572a75-7302-4265-9cbc-94bdcf83b10c to disappear
Jun 28 18:25:57.296: INFO: Pod client-envvars-d5572a75-7302-4265-9cbc-94bdcf83b10c no longer exists
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 18:25:57.297: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-7191" for this suite.

• [SLOW TEST:6.415 seconds]
[k8s.io] Pods
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Pods should contain environment variables for services [NodeConformance] [Conformance]","total":305,"completed":48,"skipped":697,"failed":0}
S
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 18:25:57.332: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation
Jun 28 18:25:57.471: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
Jun 28 18:26:08.676: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 18:26:47.006: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-8015" for this suite.

• [SLOW TEST:49.709 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group and version but different kinds [Conformance]","total":305,"completed":49,"skipped":698,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with pruning [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 18:26:47.041: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jun 28 18:26:47.997: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Jun 28 18:26:50.037: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63760501608, loc:(*time.Location)(0x7702840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63760501608, loc:(*time.Location)(0x7702840)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63760501608, loc:(*time.Location)(0x7702840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63760501608, loc:(*time.Location)(0x7702840)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun 28 18:26:53.084: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with pruning [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jun 28 18:26:53.099: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-607-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 18:26:54.442: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5538" for this suite.
STEP: Destroying namespace "webhook-5538-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:7.562 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource with pruning [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with pruning [Conformance]","total":305,"completed":50,"skipped":724,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] KubeletManagedEtcHosts 
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 18:26:54.604: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Setting up the test
STEP: Creating hostNetwork=false pod
STEP: Creating hostNetwork=true pod
STEP: Running the test
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false
Jun 28 18:27:00.883: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-5159 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 28 18:27:00.883: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
Jun 28 18:27:01.258: INFO: Exec stderr: ""
Jun 28 18:27:01.258: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-5159 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 28 18:27:01.258: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
Jun 28 18:27:01.580: INFO: Exec stderr: ""
Jun 28 18:27:01.580: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-5159 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 28 18:27:01.580: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
Jun 28 18:27:01.982: INFO: Exec stderr: ""
Jun 28 18:27:01.982: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-5159 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 28 18:27:01.982: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
Jun 28 18:27:02.206: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount
Jun 28 18:27:02.207: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-5159 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 28 18:27:02.207: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
Jun 28 18:27:02.404: INFO: Exec stderr: ""
Jun 28 18:27:02.404: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-5159 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 28 18:27:02.404: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
Jun 28 18:27:02.569: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true
Jun 28 18:27:02.569: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-5159 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 28 18:27:02.569: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
Jun 28 18:27:02.850: INFO: Exec stderr: ""
Jun 28 18:27:02.850: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-5159 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 28 18:27:02.850: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
Jun 28 18:27:03.029: INFO: Exec stderr: ""
Jun 28 18:27:03.029: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-5159 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 28 18:27:03.029: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
Jun 28 18:27:03.263: INFO: Exec stderr: ""
Jun 28 18:27:03.263: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-5159 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 28 18:27:03.263: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
Jun 28 18:27:03.545: INFO: Exec stderr: ""
[AfterEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 18:27:03.545: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-kubelet-etc-hosts-5159" for this suite.

• [SLOW TEST:8.978 seconds]
[k8s.io] KubeletManagedEtcHosts
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] KubeletManagedEtcHosts should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":51,"skipped":755,"failed":0}
[k8s.io] Probing container 
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 18:27:03.582: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 18:28:03.768: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-6416" for this suite.

• [SLOW TEST:60.220 seconds]
[k8s.io] Probing container
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Probing container with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]","total":305,"completed":52,"skipped":755,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial] 
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 18:28:03.803: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename taint-multiple-pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/taints.go:345
Jun 28 18:28:03.924: INFO: Waiting up to 1m0s for all nodes to be ready
Jun 28 18:29:04.122: INFO: Waiting for terminating namespaces to be deleted...
[It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jun 28 18:29:04.137: INFO: Starting informer...
STEP: Starting pods...
Jun 28 18:29:04.408: INFO: Pod1 is running on 10.13.107.37. Tainting Node
Jun 28 18:29:06.695: INFO: Pod2 is running on 10.13.107.37. Tainting Node
STEP: Trying to apply a taint on the Node
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting for Pod1 and Pod2 to be deleted
Jun 28 18:29:17.329: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
Jun 28 18:29:37.224: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
[AfterEach] [k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 18:29:37.275: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-multiple-pods-1248" for this suite.

• [SLOW TEST:93.506 seconds]
[k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]","total":305,"completed":53,"skipped":796,"failed":0}
SSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 18:29:37.309: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap configmap-2257/configmap-test-1901ed4a-fb5f-4b6c-85c0-8ac36d265f8e
STEP: Creating a pod to test consume configMaps
Jun 28 18:29:37.500: INFO: Waiting up to 5m0s for pod "pod-configmaps-a51e3b00-e0d5-427e-96b9-21d14628f750" in namespace "configmap-2257" to be "Succeeded or Failed"
Jun 28 18:29:37.512: INFO: Pod "pod-configmaps-a51e3b00-e0d5-427e-96b9-21d14628f750": Phase="Pending", Reason="", readiness=false. Elapsed: 11.81724ms
Jun 28 18:29:39.523: INFO: Pod "pod-configmaps-a51e3b00-e0d5-427e-96b9-21d14628f750": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.022908648s
STEP: Saw pod success
Jun 28 18:29:39.523: INFO: Pod "pod-configmaps-a51e3b00-e0d5-427e-96b9-21d14628f750" satisfied condition "Succeeded or Failed"
Jun 28 18:29:39.535: INFO: Trying to get logs from node 10.13.107.37 pod pod-configmaps-a51e3b00-e0d5-427e-96b9-21d14628f750 container env-test: <nil>
STEP: delete the pod
Jun 28 18:29:39.622: INFO: Waiting for pod pod-configmaps-a51e3b00-e0d5-427e-96b9-21d14628f750 to disappear
Jun 28 18:29:39.634: INFO: Pod pod-configmaps-a51e3b00-e0d5-427e-96b9-21d14628f750 no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 18:29:39.634: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2257" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should be consumable via environment variable [NodeConformance] [Conformance]","total":305,"completed":54,"skipped":803,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 18:29:39.666: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[BeforeEach] Update Demo
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:308
[It] should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a replication controller
Jun 28 18:29:39.776: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-108209140 create -f - --namespace=kubectl-9063'
Jun 28 18:29:40.876: INFO: stderr: ""
Jun 28 18:29:40.876: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jun 28 18:29:40.877: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-108209140 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-9063'
Jun 28 18:29:41.085: INFO: stderr: ""
Jun 28 18:29:41.085: INFO: stdout: "update-demo-nautilus-6kxpz update-demo-nautilus-fdsw7 "
Jun 28 18:29:41.086: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-108209140 get pods update-demo-nautilus-6kxpz -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-9063'
Jun 28 18:29:41.272: INFO: stderr: ""
Jun 28 18:29:41.272: INFO: stdout: ""
Jun 28 18:29:41.273: INFO: update-demo-nautilus-6kxpz is created but not running
Jun 28 18:29:46.273: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-108209140 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-9063'
Jun 28 18:29:46.459: INFO: stderr: ""
Jun 28 18:29:46.459: INFO: stdout: "update-demo-nautilus-6kxpz update-demo-nautilus-fdsw7 "
Jun 28 18:29:46.460: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-108209140 get pods update-demo-nautilus-6kxpz -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-9063'
Jun 28 18:29:46.662: INFO: stderr: ""
Jun 28 18:29:46.662: INFO: stdout: "true"
Jun 28 18:29:46.662: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-108209140 get pods update-demo-nautilus-6kxpz -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-9063'
Jun 28 18:29:46.817: INFO: stderr: ""
Jun 28 18:29:46.817: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jun 28 18:29:46.817: INFO: validating pod update-demo-nautilus-6kxpz
Jun 28 18:29:46.851: INFO: got data: {
  "image": "nautilus.jpg"
}

Jun 28 18:29:46.851: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jun 28 18:29:46.851: INFO: update-demo-nautilus-6kxpz is verified up and running
Jun 28 18:29:46.851: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-108209140 get pods update-demo-nautilus-fdsw7 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-9063'
Jun 28 18:29:47.102: INFO: stderr: ""
Jun 28 18:29:47.102: INFO: stdout: "true"
Jun 28 18:29:47.103: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-108209140 get pods update-demo-nautilus-fdsw7 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-9063'
Jun 28 18:29:47.384: INFO: stderr: ""
Jun 28 18:29:47.384: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jun 28 18:29:47.384: INFO: validating pod update-demo-nautilus-fdsw7
Jun 28 18:29:47.439: INFO: got data: {
  "image": "nautilus.jpg"
}

Jun 28 18:29:47.439: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jun 28 18:29:47.439: INFO: update-demo-nautilus-fdsw7 is verified up and running
STEP: using delete to clean up resources
Jun 28 18:29:47.439: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-108209140 delete --grace-period=0 --force -f - --namespace=kubectl-9063'
Jun 28 18:29:47.633: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jun 28 18:29:47.633: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Jun 28 18:29:47.633: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-108209140 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-9063'
Jun 28 18:29:47.936: INFO: stderr: "No resources found in kubectl-9063 namespace.\n"
Jun 28 18:29:47.936: INFO: stdout: ""
Jun 28 18:29:47.936: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-108209140 get pods -l name=update-demo --namespace=kubectl-9063 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jun 28 18:29:48.154: INFO: stderr: ""
Jun 28 18:29:48.154: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 18:29:48.154: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9063" for this suite.

• [SLOW TEST:8.526 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:306
    should create and stop a replication controller  [Conformance]
    /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should create and stop a replication controller  [Conformance]","total":305,"completed":55,"skipped":844,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should have a working scale subresource [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 18:29:48.193: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-2162
[It] should have a working scale subresource [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating statefulset ss in namespace statefulset-2162
Jun 28 18:29:48.356: INFO: Found 0 stateful pods, waiting for 1
Jun 28 18:29:58.372: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: getting scale subresource
STEP: updating a scale subresource
STEP: verifying the statefulset Spec.Replicas was modified
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
Jun 28 18:29:58.432: INFO: Deleting all statefulset in ns statefulset-2162
Jun 28 18:29:58.443: INFO: Scaling statefulset ss to 0
Jun 28 18:30:28.533: INFO: Waiting for statefulset status.replicas updated to 0
Jun 28 18:30:28.545: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 18:30:28.588: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-2162" for this suite.

• [SLOW TEST:40.430 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
    should have a working scale subresource [Conformance]
    /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should have a working scale subresource [Conformance]","total":305,"completed":56,"skipped":855,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 18:30:28.637: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the container
STEP: wait for the container to reach Failed
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Jun 28 18:30:30.902: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 18:30:30.948: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-8025" for this suite.
•{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":305,"completed":57,"skipped":887,"failed":0}

------------------------------
[sig-cli] Kubectl client Kubectl run pod 
  should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 18:30:30.992: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[BeforeEach] Kubectl run pod
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1545
[It] should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Jun 28 18:30:31.119: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-108209140 run e2e-test-httpd-pod --restart=Never --image=docker.io/library/httpd:2.4.38-alpine --namespace=kubectl-7931'
Jun 28 18:30:31.369: INFO: stderr: ""
Jun 28 18:30:31.369: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod was created
[AfterEach] Kubectl run pod
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1550
Jun 28 18:30:31.381: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-108209140 delete pods e2e-test-httpd-pod --namespace=kubectl-7931'
Jun 28 18:30:37.194: INFO: stderr: ""
Jun 28 18:30:37.194: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 18:30:37.194: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7931" for this suite.

• [SLOW TEST:6.245 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl run pod
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1541
    should create a pod from an image when restart is Never  [Conformance]
    /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl run pod should create a pod from an image when restart is Never  [Conformance]","total":305,"completed":58,"skipped":887,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 18:30:37.236: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:181
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jun 28 18:30:37.371: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 18:30:39.709: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-328" for this suite.
•{"msg":"PASSED [k8s.io] Pods should support remote command execution over websockets [NodeConformance] [Conformance]","total":305,"completed":59,"skipped":934,"failed":0}
SS
------------------------------
[sig-instrumentation] Events API 
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-instrumentation] Events API
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 18:30:39.744: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-instrumentation] Events API
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/instrumentation/events.go:81
[It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a test event
STEP: listing events in all namespaces
STEP: listing events in test namespace
STEP: listing events with field selection filtering on source
STEP: listing events with field selection filtering on reportingController
STEP: getting the test event
STEP: patching the test event
STEP: getting the test event
STEP: updating the test event
STEP: getting the test event
STEP: deleting the test event
STEP: listing events in all namespaces
STEP: listing events in test namespace
[AfterEach] [sig-instrumentation] Events API
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 18:30:40.285: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-4169" for this suite.
•{"msg":"PASSED [sig-instrumentation] Events API should ensure that an event can be fetched, patched, deleted, and listed [Conformance]","total":305,"completed":60,"skipped":936,"failed":0}
SSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 18:30:40.320: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicationController
STEP: Ensuring resource quota status captures replication controller creation
STEP: Deleting a ReplicationController
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 18:30:51.626: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-2865" for this suite.

• [SLOW TEST:11.345 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replication controller. [Conformance]","total":305,"completed":61,"skipped":946,"failed":0}
SSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 18:30:51.667: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name configmap-test-volume-map-9dd0422d-8b8c-458a-9e2d-cfe08a70e0e6
STEP: Creating a pod to test consume configMaps
Jun 28 18:30:51.861: INFO: Waiting up to 5m0s for pod "pod-configmaps-d067ea29-1c9a-4cfc-999a-294fc4f7854c" in namespace "configmap-1281" to be "Succeeded or Failed"
Jun 28 18:30:51.876: INFO: Pod "pod-configmaps-d067ea29-1c9a-4cfc-999a-294fc4f7854c": Phase="Pending", Reason="", readiness=false. Elapsed: 14.242415ms
Jun 28 18:30:53.903: INFO: Pod "pod-configmaps-d067ea29-1c9a-4cfc-999a-294fc4f7854c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.041867886s
Jun 28 18:30:55.919: INFO: Pod "pod-configmaps-d067ea29-1c9a-4cfc-999a-294fc4f7854c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.057555404s
STEP: Saw pod success
Jun 28 18:30:55.919: INFO: Pod "pod-configmaps-d067ea29-1c9a-4cfc-999a-294fc4f7854c" satisfied condition "Succeeded or Failed"
Jun 28 18:30:55.930: INFO: Trying to get logs from node 10.13.107.37 pod pod-configmaps-d067ea29-1c9a-4cfc-999a-294fc4f7854c container configmap-volume-test: <nil>
STEP: delete the pod
Jun 28 18:30:56.005: INFO: Waiting for pod pod-configmaps-d067ea29-1c9a-4cfc-999a-294fc4f7854c to disappear
Jun 28 18:30:56.014: INFO: Pod pod-configmaps-d067ea29-1c9a-4cfc-999a-294fc4f7854c no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 18:30:56.015: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1281" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","total":305,"completed":62,"skipped":950,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 18:30:56.055: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward api env vars
Jun 28 18:30:56.225: INFO: Waiting up to 5m0s for pod "downward-api-8bd0d8b2-55cd-46d9-bbe1-179d2df66188" in namespace "downward-api-9014" to be "Succeeded or Failed"
Jun 28 18:30:56.237: INFO: Pod "downward-api-8bd0d8b2-55cd-46d9-bbe1-179d2df66188": Phase="Pending", Reason="", readiness=false. Elapsed: 11.646048ms
Jun 28 18:30:58.248: INFO: Pod "downward-api-8bd0d8b2-55cd-46d9-bbe1-179d2df66188": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.0233278s
STEP: Saw pod success
Jun 28 18:30:58.248: INFO: Pod "downward-api-8bd0d8b2-55cd-46d9-bbe1-179d2df66188" satisfied condition "Succeeded or Failed"
Jun 28 18:30:58.259: INFO: Trying to get logs from node 10.13.107.37 pod downward-api-8bd0d8b2-55cd-46d9-bbe1-179d2df66188 container dapi-container: <nil>
STEP: delete the pod
Jun 28 18:30:58.321: INFO: Waiting for pod downward-api-8bd0d8b2-55cd-46d9-bbe1-179d2df66188 to disappear
Jun 28 18:30:58.331: INFO: Pod downward-api-8bd0d8b2-55cd-46d9-bbe1-179d2df66188 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 18:30:58.331: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9014" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide pod UID as env vars [NodeConformance] [Conformance]","total":305,"completed":63,"skipped":963,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 18:30:58.366: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a ResourceQuota with best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a best-effort pod
STEP: Ensuring resource quota with best effort scope captures the pod usage
STEP: Ensuring resource quota with not best effort ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a not best-effort pod
STEP: Ensuring resource quota with not best effort scope captures the pod usage
STEP: Ensuring resource quota with best effort scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 18:31:14.817: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-2257" for this suite.

• [SLOW TEST:16.485 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with best effort scope. [Conformance]","total":305,"completed":64,"skipped":969,"failed":0}
SSSSSSS
------------------------------
[sig-node] ConfigMap 
  should run through a ConfigMap lifecycle [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 18:31:14.851: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run through a ConfigMap lifecycle [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a ConfigMap
STEP: fetching the ConfigMap
STEP: patching the ConfigMap
STEP: listing all ConfigMaps in all namespaces with a label selector
STEP: deleting the ConfigMap by collection with a label selector
STEP: listing all ConfigMaps in test namespace
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 18:31:15.167: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3557" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should run through a ConfigMap lifecycle [Conformance]","total":305,"completed":65,"skipped":976,"failed":0}
SSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: http [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 18:31:15.203: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: http [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Performing setup for networking test in namespace pod-network-test-733
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Jun 28 18:31:15.328: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Jun 28 18:31:15.477: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jun 28 18:31:17.494: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun 28 18:31:19.492: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun 28 18:31:21.493: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun 28 18:31:23.493: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun 28 18:31:25.492: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun 28 18:31:27.496: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun 28 18:31:29.497: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun 28 18:31:31.498: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun 28 18:31:33.489: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun 28 18:31:35.490: INFO: The status of Pod netserver-0 is Running (Ready = true)
Jun 28 18:31:35.510: INFO: The status of Pod netserver-1 is Running (Ready = true)
Jun 28 18:31:35.530: INFO: The status of Pod netserver-2 is Running (Ready = true)
STEP: Creating test pods
Jun 28 18:31:37.617: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.221.174:8080/dial?request=hostname&protocol=http&host=172.30.221.172&port=8080&tries=1'] Namespace:pod-network-test-733 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 28 18:31:37.617: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
Jun 28 18:31:37.986: INFO: Waiting for responses: map[]
Jun 28 18:31:38.000: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.221.174:8080/dial?request=hostname&protocol=http&host=172.30.54.28&port=8080&tries=1'] Namespace:pod-network-test-733 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 28 18:31:38.000: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
Jun 28 18:31:38.303: INFO: Waiting for responses: map[]
Jun 28 18:31:38.316: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.221.174:8080/dial?request=hostname&protocol=http&host=172.30.196.89&port=8080&tries=1'] Namespace:pod-network-test-733 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 28 18:31:38.316: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
Jun 28 18:31:38.525: INFO: Waiting for responses: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 18:31:38.526: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-733" for this suite.

• [SLOW TEST:23.357 seconds]
[sig-network] Networking
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:26
  Granular Checks: Pods
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:29
    should function for intra-pod communication: http [NodeConformance] [Conformance]
    /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: http [NodeConformance] [Conformance]","total":305,"completed":66,"skipped":979,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 18:31:38.572: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jun 28 18:31:38.728: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
STEP: Creating rc "condition-test" that asks for more than the allowed pod quota
STEP: Checking rc "condition-test" has the desired failure condition set
STEP: Scaling down rc "condition-test" to satisfy pod quota
Jun 28 18:31:40.841: INFO: Updating replication controller "condition-test"
STEP: Checking rc "condition-test" has no failure condition set
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 18:31:41.871: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-4668" for this suite.
•{"msg":"PASSED [sig-apps] ReplicationController should surface a failure condition on a common issue like exceeded quota [Conformance]","total":305,"completed":67,"skipped":1016,"failed":0}
S
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 18:31:41.914: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Jun 28 18:31:42.117: INFO: Waiting up to 5m0s for pod "downwardapi-volume-10246d8e-e959-409d-a98e-39f9297d3d1e" in namespace "projected-3336" to be "Succeeded or Failed"
Jun 28 18:31:42.130: INFO: Pod "downwardapi-volume-10246d8e-e959-409d-a98e-39f9297d3d1e": Phase="Pending", Reason="", readiness=false. Elapsed: 12.56875ms
Jun 28 18:31:44.141: INFO: Pod "downwardapi-volume-10246d8e-e959-409d-a98e-39f9297d3d1e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024456122s
Jun 28 18:31:46.155: INFO: Pod "downwardapi-volume-10246d8e-e959-409d-a98e-39f9297d3d1e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.038143137s
STEP: Saw pod success
Jun 28 18:31:46.156: INFO: Pod "downwardapi-volume-10246d8e-e959-409d-a98e-39f9297d3d1e" satisfied condition "Succeeded or Failed"
Jun 28 18:31:46.166: INFO: Trying to get logs from node 10.13.107.37 pod downwardapi-volume-10246d8e-e959-409d-a98e-39f9297d3d1e container client-container: <nil>
STEP: delete the pod
Jun 28 18:31:46.223: INFO: Waiting for pod downwardapi-volume-10246d8e-e959-409d-a98e-39f9297d3d1e to disappear
Jun 28 18:31:46.234: INFO: Pod downwardapi-volume-10246d8e-e959-409d-a98e-39f9297d3d1e no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 18:31:46.235: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3336" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu limit [NodeConformance] [Conformance]","total":305,"completed":68,"skipped":1017,"failed":0}
SSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 18:31:46.273: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jun 28 18:31:46.417: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 18:31:47.057: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-7628" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition getting/updating/patching custom resource definition status sub-resource works  [Conformance]","total":305,"completed":69,"skipped":1021,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl replace 
  should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 18:31:47.093: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[BeforeEach] Kubectl replace
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1581
[It] should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Jun 28 18:31:47.215: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-108209140 run e2e-test-httpd-pod --image=docker.io/library/httpd:2.4.38-alpine --labels=run=e2e-test-httpd-pod --namespace=kubectl-9756'
Jun 28 18:31:47.666: INFO: stderr: ""
Jun 28 18:31:47.666: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod is running
STEP: verifying the pod e2e-test-httpd-pod was created
Jun 28 18:31:52.717: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-108209140 get pod e2e-test-httpd-pod --namespace=kubectl-9756 -o json'
Jun 28 18:31:52.938: INFO: stderr: ""
Jun 28 18:31:52.938: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"cni.projectcalico.org/podIP\": \"172.30.221.176/32\",\n            \"cni.projectcalico.org/podIPs\": \"172.30.221.176/32\",\n            \"k8s.v1.cni.cncf.io/network-status\": \"[{\\n    \\\"name\\\": \\\"\\\",\\n    \\\"ips\\\": [\\n        \\\"172.30.221.176\\\"\\n    ],\\n    \\\"default\\\": true,\\n    \\\"dns\\\": {}\\n}]\",\n            \"k8s.v1.cni.cncf.io/networks-status\": \"[{\\n    \\\"name\\\": \\\"\\\",\\n    \\\"ips\\\": [\\n        \\\"172.30.221.176\\\"\\n    ],\\n    \\\"default\\\": true,\\n    \\\"dns\\\": {}\\n}]\",\n            \"openshift.io/scc\": \"anyuid\"\n        },\n        \"creationTimestamp\": \"2021-06-28T18:31:47Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"managedFields\": [\n            {\n                \"apiVersion\": \"v1\",\n                \"fieldsType\": \"FieldsV1\",\n                \"fieldsV1\": {\n                    \"f:metadata\": {\n                        \"f:labels\": {\n                            \".\": {},\n                            \"f:run\": {}\n                        }\n                    },\n                    \"f:spec\": {\n                        \"f:containers\": {\n                            \"k:{\\\"name\\\":\\\"e2e-test-httpd-pod\\\"}\": {\n                                \".\": {},\n                                \"f:image\": {},\n                                \"f:imagePullPolicy\": {},\n                                \"f:name\": {},\n                                \"f:resources\": {},\n                                \"f:terminationMessagePath\": {},\n                                \"f:terminationMessagePolicy\": {}\n                            }\n                        },\n                        \"f:dnsPolicy\": {},\n                        \"f:enableServiceLinks\": {},\n                        \"f:restartPolicy\": {},\n                        \"f:schedulerName\": {},\n                        \"f:securityContext\": {},\n                        \"f:terminationGracePeriodSeconds\": {}\n                    }\n                },\n                \"manager\": \"kubectl-run\",\n                \"operation\": \"Update\",\n                \"time\": \"2021-06-28T18:31:47Z\"\n            },\n            {\n                \"apiVersion\": \"v1\",\n                \"fieldsType\": \"FieldsV1\",\n                \"fieldsV1\": {\n                    \"f:metadata\": {\n                        \"f:annotations\": {\n                            \"f:cni.projectcalico.org/podIP\": {},\n                            \"f:cni.projectcalico.org/podIPs\": {}\n                        }\n                    }\n                },\n                \"manager\": \"calico\",\n                \"operation\": \"Update\",\n                \"time\": \"2021-06-28T18:31:48Z\"\n            },\n            {\n                \"apiVersion\": \"v1\",\n                \"fieldsType\": \"FieldsV1\",\n                \"fieldsV1\": {\n                    \"f:metadata\": {\n                        \"f:annotations\": {\n                            \"f:k8s.v1.cni.cncf.io/network-status\": {},\n                            \"f:k8s.v1.cni.cncf.io/networks-status\": {}\n                        }\n                    }\n                },\n                \"manager\": \"multus\",\n                \"operation\": \"Update\",\n                \"time\": \"2021-06-28T18:31:48Z\"\n            },\n            {\n                \"apiVersion\": \"v1\",\n                \"fieldsType\": \"FieldsV1\",\n                \"fieldsV1\": {\n                    \"f:status\": {\n                        \"f:conditions\": {\n                            \"k:{\\\"type\\\":\\\"ContainersReady\\\"}\": {\n                                \".\": {},\n                                \"f:lastProbeTime\": {},\n                                \"f:lastTransitionTime\": {},\n                                \"f:status\": {},\n                                \"f:type\": {}\n                            },\n                            \"k:{\\\"type\\\":\\\"Initialized\\\"}\": {\n                                \".\": {},\n                                \"f:lastProbeTime\": {},\n                                \"f:lastTransitionTime\": {},\n                                \"f:status\": {},\n                                \"f:type\": {}\n                            },\n                            \"k:{\\\"type\\\":\\\"Ready\\\"}\": {\n                                \".\": {},\n                                \"f:lastProbeTime\": {},\n                                \"f:lastTransitionTime\": {},\n                                \"f:status\": {},\n                                \"f:type\": {}\n                            }\n                        },\n                        \"f:containerStatuses\": {},\n                        \"f:hostIP\": {},\n                        \"f:phase\": {},\n                        \"f:podIP\": {},\n                        \"f:podIPs\": {\n                            \".\": {},\n                            \"k:{\\\"ip\\\":\\\"172.30.221.176\\\"}\": {\n                                \".\": {},\n                                \"f:ip\": {}\n                            }\n                        },\n                        \"f:startTime\": {}\n                    }\n                },\n                \"manager\": \"kubelet\",\n                \"operation\": \"Update\",\n                \"time\": \"2021-06-28T18:31:49Z\"\n            }\n        ],\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-9756\",\n        \"resourceVersion\": \"63436\",\n        \"selfLink\": \"/api/v1/namespaces/kubectl-9756/pods/e2e-test-httpd-pod\",\n        \"uid\": \"24ffc551-9d12-4dff-8d60-647ec759d3cd\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"docker.io/library/httpd:2.4.38-alpine\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"securityContext\": {\n                    \"capabilities\": {\n                        \"drop\": [\n                            \"MKNOD\"\n                        ]\n                    }\n                },\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"default-token-zwv9x\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"imagePullSecrets\": [\n            {\n                \"name\": \"default-dockercfg-hvvrl\"\n            }\n        ],\n        \"nodeName\": \"10.13.107.37\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {\n            \"seLinuxOptions\": {\n                \"level\": \"s0:c39,c24\"\n            }\n        },\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"default-token-zwv9x\",\n                \"secret\": {\n                    \"defaultMode\": 420,\n                    \"secretName\": \"default-token-zwv9x\"\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2021-06-28T18:31:47Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2021-06-28T18:31:49Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2021-06-28T18:31:49Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2021-06-28T18:31:47Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"cri-o://dff6ab4b704447e2b5116d876e5a91b2577e9b18a60f435712cccf1267f5deb5\",\n                \"image\": \"docker.io/library/httpd:2.4.38-alpine\",\n                \"imageID\": \"docker.io/library/httpd@sha256:6feb0ea7b0967367da66e8d58ba813fde32bdb92f63bfc21a9e170d211539db4\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2021-06-28T18:31:48Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"10.13.107.37\",\n        \"phase\": \"Running\",\n        \"podIP\": \"172.30.221.176\",\n        \"podIPs\": [\n            {\n                \"ip\": \"172.30.221.176\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2021-06-28T18:31:47Z\"\n    }\n}\n"
STEP: replace the image in the pod
Jun 28 18:31:52.938: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-108209140 replace -f - --namespace=kubectl-9756'
Jun 28 18:31:53.879: INFO: stderr: ""
Jun 28 18:31:53.879: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image docker.io/library/busybox:1.29
[AfterEach] Kubectl replace
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1586
Jun 28 18:31:53.892: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-108209140 delete pods e2e-test-httpd-pod --namespace=kubectl-9756'
Jun 28 18:31:55.327: INFO: stderr: ""
Jun 28 18:31:55.327: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 18:31:55.327: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9756" for this suite.

• [SLOW TEST:8.266 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl replace
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1577
    should update a single-container pod's image  [Conformance]
    /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl replace should update a single-container pod's image  [Conformance]","total":305,"completed":70,"skipped":1064,"failed":0}
SS
------------------------------
[sig-api-machinery] Garbage collector 
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 18:31:55.360: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W0628 18:32:01.594866      22 metrics_grabber.go:98] Can't find kube-scheduler pod. Grabbing metrics from kube-scheduler is disabled.
W0628 18:32:01.594914      22 metrics_grabber.go:102] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
W0628 18:32:01.594927      22 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
Jun 28 18:32:01.594: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 18:32:01.595: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-7825" for this suite.

• [SLOW TEST:6.295 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]","total":305,"completed":71,"skipped":1066,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 18:32:01.668: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jun 28 18:32:01.823: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
STEP: Creating secret with name s-test-opt-del-c2d66043-36e3-4fc2-9d1c-8ae2cc1fbdcd
STEP: Creating secret with name s-test-opt-upd-cff77d03-5b8d-42bc-908c-a22ccb9fe3a4
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-c2d66043-36e3-4fc2-9d1c-8ae2cc1fbdcd
STEP: Updating secret s-test-opt-upd-cff77d03-5b8d-42bc-908c-a22ccb9fe3a4
STEP: Creating secret with name s-test-opt-create-5f91792a-7233-4e7d-9276-c25baaa1af53
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 18:33:09.142: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-299" for this suite.

• [SLOW TEST:67.522 seconds]
[sig-storage] Secrets
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:36
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Secrets optional updates should be reflected in volume [NodeConformance] [Conformance]","total":305,"completed":72,"skipped":1138,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  listing custom resource definition objects works  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 18:33:09.190: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] listing custom resource definition objects works  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jun 28 18:33:09.381: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 18:33:16.715: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-8214" for this suite.

• [SLOW TEST:7.562 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:48
    listing custom resource definition objects works  [Conformance]
    /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition listing custom resource definition objects works  [Conformance]","total":305,"completed":73,"skipped":1144,"failed":0}
S
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 18:33:16.753: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating projection with secret that has name projected-secret-test-4c4e86b8-2547-424a-9dd7-5cf4958c837e
STEP: Creating a pod to test consume secrets
Jun 28 18:33:16.960: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-d7b30cf8-a2e5-465d-b720-9dfa8b97c8b2" in namespace "projected-7585" to be "Succeeded or Failed"
Jun 28 18:33:16.972: INFO: Pod "pod-projected-secrets-d7b30cf8-a2e5-465d-b720-9dfa8b97c8b2": Phase="Pending", Reason="", readiness=false. Elapsed: 11.330638ms
Jun 28 18:33:18.992: INFO: Pod "pod-projected-secrets-d7b30cf8-a2e5-465d-b720-9dfa8b97c8b2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.03160843s
STEP: Saw pod success
Jun 28 18:33:18.992: INFO: Pod "pod-projected-secrets-d7b30cf8-a2e5-465d-b720-9dfa8b97c8b2" satisfied condition "Succeeded or Failed"
Jun 28 18:33:19.003: INFO: Trying to get logs from node 10.13.107.37 pod pod-projected-secrets-d7b30cf8-a2e5-465d-b720-9dfa8b97c8b2 container projected-secret-volume-test: <nil>
STEP: delete the pod
Jun 28 18:33:19.068: INFO: Waiting for pod pod-projected-secrets-d7b30cf8-a2e5-465d-b720-9dfa8b97c8b2 to disappear
Jun 28 18:33:19.080: INFO: Pod pod-projected-secrets-d7b30cf8-a2e5-465d-b720-9dfa8b97c8b2 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 18:33:19.081: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7585" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":74,"skipped":1145,"failed":0}
SSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 18:33:19.120: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:134
[It] should run and stop simple daemon [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Jun 28 18:33:19.345: INFO: Number of nodes with available pods: 0
Jun 28 18:33:19.345: INFO: Node 10.13.107.37 is running more than one daemon pod
Jun 28 18:33:20.373: INFO: Number of nodes with available pods: 0
Jun 28 18:33:20.374: INFO: Node 10.13.107.37 is running more than one daemon pod
Jun 28 18:33:21.376: INFO: Number of nodes with available pods: 2
Jun 28 18:33:21.376: INFO: Node 10.13.107.57 is running more than one daemon pod
Jun 28 18:33:22.373: INFO: Number of nodes with available pods: 3
Jun 28 18:33:22.373: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Stop a daemon pod, check that the daemon pod is revived.
Jun 28 18:33:22.441: INFO: Number of nodes with available pods: 2
Jun 28 18:33:22.441: INFO: Node 10.13.107.57 is running more than one daemon pod
Jun 28 18:33:23.476: INFO: Number of nodes with available pods: 2
Jun 28 18:33:23.476: INFO: Node 10.13.107.57 is running more than one daemon pod
Jun 28 18:33:24.472: INFO: Number of nodes with available pods: 2
Jun 28 18:33:24.472: INFO: Node 10.13.107.57 is running more than one daemon pod
Jun 28 18:33:25.477: INFO: Number of nodes with available pods: 2
Jun 28 18:33:25.477: INFO: Node 10.13.107.57 is running more than one daemon pod
Jun 28 18:33:26.475: INFO: Number of nodes with available pods: 2
Jun 28 18:33:26.475: INFO: Node 10.13.107.57 is running more than one daemon pod
Jun 28 18:33:27.475: INFO: Number of nodes with available pods: 2
Jun 28 18:33:27.475: INFO: Node 10.13.107.57 is running more than one daemon pod
Jun 28 18:33:28.473: INFO: Number of nodes with available pods: 2
Jun 28 18:33:28.473: INFO: Node 10.13.107.57 is running more than one daemon pod
Jun 28 18:33:29.476: INFO: Number of nodes with available pods: 2
Jun 28 18:33:29.476: INFO: Node 10.13.107.57 is running more than one daemon pod
Jun 28 18:33:30.475: INFO: Number of nodes with available pods: 2
Jun 28 18:33:30.475: INFO: Node 10.13.107.57 is running more than one daemon pod
Jun 28 18:33:31.474: INFO: Number of nodes with available pods: 2
Jun 28 18:33:31.474: INFO: Node 10.13.107.57 is running more than one daemon pod
Jun 28 18:33:32.472: INFO: Number of nodes with available pods: 2
Jun 28 18:33:32.472: INFO: Node 10.13.107.57 is running more than one daemon pod
Jun 28 18:33:33.474: INFO: Number of nodes with available pods: 2
Jun 28 18:33:33.475: INFO: Node 10.13.107.57 is running more than one daemon pod
Jun 28 18:33:34.473: INFO: Number of nodes with available pods: 2
Jun 28 18:33:34.474: INFO: Node 10.13.107.57 is running more than one daemon pod
Jun 28 18:33:35.474: INFO: Number of nodes with available pods: 2
Jun 28 18:33:35.474: INFO: Node 10.13.107.57 is running more than one daemon pod
Jun 28 18:33:36.475: INFO: Number of nodes with available pods: 2
Jun 28 18:33:36.476: INFO: Node 10.13.107.57 is running more than one daemon pod
Jun 28 18:33:37.473: INFO: Number of nodes with available pods: 3
Jun 28 18:33:37.473: INFO: Number of running nodes: 3, number of available pods: 3
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:100
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-267, will wait for the garbage collector to delete the pods
Jun 28 18:33:37.569: INFO: Deleting DaemonSet.extensions daemon-set took: 23.888061ms
Jun 28 18:33:37.669: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.837422ms
Jun 28 18:33:44.980: INFO: Number of nodes with available pods: 0
Jun 28 18:33:44.981: INFO: Number of running nodes: 0, number of available pods: 0
Jun 28 18:33:44.990: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-267/daemonsets","resourceVersion":"64760"},"items":null}

Jun 28 18:33:45.000: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-267/pods","resourceVersion":"64760"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 18:33:45.051: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-267" for this suite.

• [SLOW TEST:25.974 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop simple daemon [Conformance]","total":305,"completed":75,"skipped":1155,"failed":0}
[k8s.io] Docker Containers 
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 18:33:45.094: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 18:33:47.348: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-4870" for this suite.
•{"msg":"PASSED [k8s.io] Docker Containers should use the image defaults if command and args are blank [NodeConformance] [Conformance]","total":305,"completed":76,"skipped":1155,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 18:33:47.382: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name configmap-test-volume-0c58d2a8-992d-4aea-8af3-6075d3a7574f
STEP: Creating a pod to test consume configMaps
Jun 28 18:33:47.569: INFO: Waiting up to 5m0s for pod "pod-configmaps-fb1feacd-8ce8-47f2-bbbd-4903fda014ea" in namespace "configmap-5282" to be "Succeeded or Failed"
Jun 28 18:33:47.580: INFO: Pod "pod-configmaps-fb1feacd-8ce8-47f2-bbbd-4903fda014ea": Phase="Pending", Reason="", readiness=false. Elapsed: 11.054593ms
Jun 28 18:33:49.599: INFO: Pod "pod-configmaps-fb1feacd-8ce8-47f2-bbbd-4903fda014ea": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029863597s
Jun 28 18:33:51.621: INFO: Pod "pod-configmaps-fb1feacd-8ce8-47f2-bbbd-4903fda014ea": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.05215346s
STEP: Saw pod success
Jun 28 18:33:51.621: INFO: Pod "pod-configmaps-fb1feacd-8ce8-47f2-bbbd-4903fda014ea" satisfied condition "Succeeded or Failed"
Jun 28 18:33:51.650: INFO: Trying to get logs from node 10.13.107.37 pod pod-configmaps-fb1feacd-8ce8-47f2-bbbd-4903fda014ea container configmap-volume-test: <nil>
STEP: delete the pod
Jun 28 18:33:51.720: INFO: Waiting for pod pod-configmaps-fb1feacd-8ce8-47f2-bbbd-4903fda014ea to disappear
Jun 28 18:33:51.731: INFO: Pod pod-configmaps-fb1feacd-8ce8-47f2-bbbd-4903fda014ea no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 18:33:51.731: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5282" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","total":305,"completed":77,"skipped":1170,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 18:33:51.768: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a watch on configmaps with a certain label
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: changing the label value of the configmap
STEP: Expecting to observe a delete notification for the watched object
Jun 28 18:33:52.116: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9803 /api/v1/namespaces/watch-9803/configmaps/e2e-watch-test-label-changed 16c5d7e1-70c8-418b-bd3e-25cc0c7bd350 64974 0 2021-06-28 18:33:51 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-06-28 18:33:51 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Jun 28 18:33:52.116: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9803 /api/v1/namespaces/watch-9803/configmaps/e2e-watch-test-label-changed 16c5d7e1-70c8-418b-bd3e-25cc0c7bd350 64976 0 2021-06-28 18:33:51 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-06-28 18:33:52 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Jun 28 18:33:52.116: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9803 /api/v1/namespaces/watch-9803/configmaps/e2e-watch-test-label-changed 16c5d7e1-70c8-418b-bd3e-25cc0c7bd350 64977 0 2021-06-28 18:33:51 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-06-28 18:33:52 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements
STEP: changing the label value of the configmap back
STEP: modifying the configmap a third time
STEP: deleting the configmap
STEP: Expecting to observe an add notification for the watched object when the label value was restored
Jun 28 18:34:02.256: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9803 /api/v1/namespaces/watch-9803/configmaps/e2e-watch-test-label-changed 16c5d7e1-70c8-418b-bd3e-25cc0c7bd350 65112 0 2021-06-28 18:33:51 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-06-28 18:34:02 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Jun 28 18:34:02.256: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9803 /api/v1/namespaces/watch-9803/configmaps/e2e-watch-test-label-changed 16c5d7e1-70c8-418b-bd3e-25cc0c7bd350 65113 0 2021-06-28 18:33:51 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-06-28 18:34:02 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
Jun 28 18:34:02.257: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9803 /api/v1/namespaces/watch-9803/configmaps/e2e-watch-test-label-changed 16c5d7e1-70c8-418b-bd3e-25cc0c7bd350 65114 0 2021-06-28 18:33:51 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-06-28 18:34:02 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 18:34:02.257: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-9803" for this suite.

• [SLOW TEST:10.524 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should observe an object deletion if it stops meeting the requirements of the selector [Conformance]","total":305,"completed":78,"skipped":1215,"failed":0}
SSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 18:34:02.296: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Jun 28 18:34:02.465: INFO: Waiting up to 5m0s for pod "downwardapi-volume-be120e01-9399-48a8-86fe-23a127e82513" in namespace "downward-api-5882" to be "Succeeded or Failed"
Jun 28 18:34:02.476: INFO: Pod "downwardapi-volume-be120e01-9399-48a8-86fe-23a127e82513": Phase="Pending", Reason="", readiness=false. Elapsed: 11.044551ms
Jun 28 18:34:04.496: INFO: Pod "downwardapi-volume-be120e01-9399-48a8-86fe-23a127e82513": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031566352s
Jun 28 18:34:06.510: INFO: Pod "downwardapi-volume-be120e01-9399-48a8-86fe-23a127e82513": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.045397641s
STEP: Saw pod success
Jun 28 18:34:06.511: INFO: Pod "downwardapi-volume-be120e01-9399-48a8-86fe-23a127e82513" satisfied condition "Succeeded or Failed"
Jun 28 18:34:06.522: INFO: Trying to get logs from node 10.13.107.37 pod downwardapi-volume-be120e01-9399-48a8-86fe-23a127e82513 container client-container: <nil>
STEP: delete the pod
Jun 28 18:34:06.580: INFO: Waiting for pod downwardapi-volume-be120e01-9399-48a8-86fe-23a127e82513 to disappear
Jun 28 18:34:06.590: INFO: Pod downwardapi-volume-be120e01-9399-48a8-86fe-23a127e82513 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 18:34:06.590: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5882" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu limit [NodeConformance] [Conformance]","total":305,"completed":79,"skipped":1219,"failed":0}
SSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 18:34:06.623: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Pod that fits quota
STEP: Ensuring ResourceQuota status captures the pod usage
STEP: Not allowing a pod to be created that exceeds remaining quota
STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources)
STEP: Ensuring a pod cannot update its resource requirements
STEP: Ensuring attempts to update pod resource requirements did not change quota usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 18:34:20.044: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-632" for this suite.

• [SLOW TEST:13.461 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a pod. [Conformance]","total":305,"completed":80,"skipped":1226,"failed":0}
SSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 18:34:20.085: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:78
[It] deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jun 28 18:34:20.264: INFO: Pod name cleanup-pod: Found 0 pods out of 1
Jun 28 18:34:25.277: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Jun 28 18:34:25.277: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:72
Jun 28 18:34:25.333: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:{test-cleanup-deployment  deployment-1114 /apis/apps/v1/namespaces/deployment-1114/deployments/test-cleanup-deployment 155ab8ad-7c06-4be4-92ef-f1be34fe23c0 65409 1 2021-06-28 18:34:25 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] []  [{e2e.test Update apps/v1 2021-06-28 18:34:25 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{}}},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.20 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00c8494b8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[]DeploymentCondition{},ReadyReplicas:0,CollisionCount:nil,},}

Jun 28 18:34:25.342: INFO: New ReplicaSet of Deployment "test-cleanup-deployment" is nil.
Jun 28 18:34:25.342: INFO: All old ReplicaSets of Deployment "test-cleanup-deployment":
Jun 28 18:34:25.343: INFO: &ReplicaSet{ObjectMeta:{test-cleanup-controller  deployment-1114 /apis/apps/v1/namespaces/deployment-1114/replicasets/test-cleanup-controller 2b0e4494-9d06-49b2-be3a-b4f5a3a1d4ca 65410 1 2021-06-28 18:34:20 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 Deployment test-cleanup-deployment 155ab8ad-7c06-4be4-92ef-f1be34fe23c0 0xc00c8498b7 0xc00c8498b8}] []  [{e2e.test Update apps/v1 2021-06-28 18:34:20 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:replicas":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{},"f:pod":{}}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-06-28 18:34:25 +0000 UTC FieldsV1 {"f:metadata":{"f:ownerReferences":{".":{},"k:{\"uid\":\"155ab8ad-7c06-4be4-92ef-f1be34fe23c0\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc00c849958 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jun 28 18:34:25.353: INFO: Pod "test-cleanup-controller-5zqd9" is available:
&Pod{ObjectMeta:{test-cleanup-controller-5zqd9 test-cleanup-controller- deployment-1114 /api/v1/namespaces/deployment-1114/pods/test-cleanup-controller-5zqd9 ecaf1a9d-20a3-49ae-8cfa-90372c0c0074 65388 0 2021-06-28 18:34:20 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[cni.projectcalico.org/podIP:172.30.221.187/32 cni.projectcalico.org/podIPs:172.30.221.187/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "",
    "ips": [
        "172.30.221.187"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "",
    "ips": [
        "172.30.221.187"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet test-cleanup-controller 2b0e4494-9d06-49b2-be3a-b4f5a3a1d4ca 0xc00c849d17 0xc00c849d18}] []  [{kube-controller-manager Update v1 2021-06-28 18:34:20 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2b0e4494-9d06-49b2-be3a-b4f5a3a1d4ca\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-06-28 18:34:21 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2021-06-28 18:34:21 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.221.187\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}} {multus Update v1 2021-06-28 18:34:21 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-gdtgk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-gdtgk,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-gdtgk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.13.107.37,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c41,c0,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-28 18:34:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-28 18:34:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-28 18:34:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-28 18:34:20 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.13.107.37,PodIP:172.30.221.187,StartTime:2021-06-28 18:34:20 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-06-28 18:34:21 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:6feb0ea7b0967367da66e8d58ba813fde32bdb92f63bfc21a9e170d211539db4,ContainerID:cri-o://b2fba7d88dc0d92d4f142f1eca766ec034d0d52658d2a82e27bdfe7be2802aca,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.221.187,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 18:34:25.353: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-1114" for this suite.

• [SLOW TEST:5.300 seconds]
[sig-apps] Deployment
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should delete old replica sets [Conformance]","total":305,"completed":81,"skipped":1235,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 18:34:25.390: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0777 on tmpfs
Jun 28 18:34:25.633: INFO: Waiting up to 5m0s for pod "pod-f5c6538b-e775-4096-a80f-19b38a00a0f0" in namespace "emptydir-3795" to be "Succeeded or Failed"
Jun 28 18:34:25.644: INFO: Pod "pod-f5c6538b-e775-4096-a80f-19b38a00a0f0": Phase="Pending", Reason="", readiness=false. Elapsed: 10.693647ms
Jun 28 18:34:27.657: INFO: Pod "pod-f5c6538b-e775-4096-a80f-19b38a00a0f0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023861998s
Jun 28 18:34:29.676: INFO: Pod "pod-f5c6538b-e775-4096-a80f-19b38a00a0f0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.0421437s
STEP: Saw pod success
Jun 28 18:34:29.676: INFO: Pod "pod-f5c6538b-e775-4096-a80f-19b38a00a0f0" satisfied condition "Succeeded or Failed"
Jun 28 18:34:29.685: INFO: Trying to get logs from node 10.13.107.37 pod pod-f5c6538b-e775-4096-a80f-19b38a00a0f0 container test-container: <nil>
STEP: delete the pod
Jun 28 18:34:29.748: INFO: Waiting for pod pod-f5c6538b-e775-4096-a80f-19b38a00a0f0 to disappear
Jun 28 18:34:29.760: INFO: Pod pod-f5c6538b-e775-4096-a80f-19b38a00a0f0 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 18:34:29.765: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3795" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":82,"skipped":1247,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Security Context When creating a container with runAsUser 
  should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 18:34:29.800: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:41
[It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jun 28 18:34:29.972: INFO: Waiting up to 5m0s for pod "busybox-user-65534-6a42043f-0254-4fb3-ae42-09509629a466" in namespace "security-context-test-5985" to be "Succeeded or Failed"
Jun 28 18:34:29.984: INFO: Pod "busybox-user-65534-6a42043f-0254-4fb3-ae42-09509629a466": Phase="Pending", Reason="", readiness=false. Elapsed: 11.962397ms
Jun 28 18:34:31.999: INFO: Pod "busybox-user-65534-6a42043f-0254-4fb3-ae42-09509629a466": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.026186612s
Jun 28 18:34:31.999: INFO: Pod "busybox-user-65534-6a42043f-0254-4fb3-ae42-09509629a466" satisfied condition "Succeeded or Failed"
[AfterEach] [k8s.io] Security Context
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 18:34:32.000: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-5985" for this suite.
•{"msg":"PASSED [k8s.io] Security Context When creating a container with runAsUser should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":83,"skipped":1272,"failed":0}
SSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 18:34:32.046: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: set up a multi version CRD
Jun 28 18:34:32.205: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: mark a version not serverd
STEP: check the unserved version gets removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 18:35:19.794: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-4026" for this suite.

• [SLOW TEST:47.791 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] removes definition from spec when one version gets changed to not be served [Conformance]","total":305,"completed":84,"skipped":1275,"failed":0}
SS
------------------------------
[sig-cli] Kubectl client Kubectl api-versions 
  should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 18:35:19.839: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[It] should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: validating api versions
Jun 28 18:35:19.987: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-108209140 api-versions'
Jun 28 18:35:20.199: INFO: stderr: ""
Jun 28 18:35:20.199: INFO: stdout: "admissionregistration.k8s.io/v1\nadmissionregistration.k8s.io/v1beta1\napiextensions.k8s.io/v1\napiextensions.k8s.io/v1beta1\napiregistration.k8s.io/v1\napiregistration.k8s.io/v1beta1\napps.openshift.io/v1\napps/v1\nauthentication.k8s.io/v1\nauthentication.k8s.io/v1beta1\nauthorization.k8s.io/v1\nauthorization.k8s.io/v1beta1\nauthorization.openshift.io/v1\nautoscaling/v1\nautoscaling/v2beta1\nautoscaling/v2beta2\nbatch/v1\nbatch/v1beta1\nbuild.openshift.io/v1\ncertificates.k8s.io/v1\ncertificates.k8s.io/v1beta1\ncloudcredential.openshift.io/v1\nconfig.openshift.io/v1\nconsole.openshift.io/v1\ncoordination.k8s.io/v1\ncoordination.k8s.io/v1beta1\ncrd.projectcalico.org/v1\ndiscovery.k8s.io/v1beta1\nevents.k8s.io/v1\nevents.k8s.io/v1beta1\nextensions/v1beta1\nflowcontrol.apiserver.k8s.io/v1alpha1\nhelm.openshift.io/v1beta1\nibm.com/v1alpha1\nimage.openshift.io/v1\nimageregistry.operator.openshift.io/v1\ningress.operator.openshift.io/v1\nk8s.cni.cncf.io/v1\nmachineconfiguration.openshift.io/v1\nmetal3.io/v1alpha1\nmetrics.k8s.io/v1beta1\nmigration.k8s.io/v1alpha1\nmonitoring.coreos.com/v1\nnetwork.operator.openshift.io/v1\nnetworking.k8s.io/v1\nnetworking.k8s.io/v1beta1\nnode.k8s.io/v1beta1\noauth.openshift.io/v1\noperator.openshift.io/v1\noperator.openshift.io/v1alpha1\noperator.tigera.io/v1\noperators.coreos.com/v1\noperators.coreos.com/v1alpha1\noperators.coreos.com/v1alpha2\npackages.operators.coreos.com/v1\npolicy/v1beta1\nproject.openshift.io/v1\nquota.openshift.io/v1\nrbac.authorization.k8s.io/v1\nrbac.authorization.k8s.io/v1beta1\nroute.openshift.io/v1\nsamples.operator.openshift.io/v1\nscheduling.k8s.io/v1\nscheduling.k8s.io/v1beta1\nsecurity.internal.openshift.io/v1\nsecurity.openshift.io/v1\nsnapshot.storage.k8s.io/v1beta1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\ntemplate.openshift.io/v1\ntuned.openshift.io/v1\nuser.openshift.io/v1\nv1\nwhereabouts.cni.cncf.io/v1alpha1\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 18:35:20.199: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1153" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl api-versions should check if v1 is in available api versions  [Conformance]","total":305,"completed":85,"skipped":1277,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should fail to create secret due to empty secret key [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 18:35:20.245: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create secret due to empty secret key [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating projection with secret that has name secret-emptykey-test-41ccfac4-acbe-4818-bc51-1f7d28039eab
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 18:35:20.379: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4334" for this suite.
•{"msg":"PASSED [sig-api-machinery] Secrets should fail to create secret due to empty secret key [Conformance]","total":305,"completed":86,"skipped":1376,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 18:35:20.414: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name projected-configmap-test-volume-a50e36ed-eda1-4e16-b428-8dff5a9abac9
STEP: Creating a pod to test consume configMaps
Jun 28 18:35:20.606: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-0ab5d319-759a-4810-9448-e9bc77dafc29" in namespace "projected-6193" to be "Succeeded or Failed"
Jun 28 18:35:20.618: INFO: Pod "pod-projected-configmaps-0ab5d319-759a-4810-9448-e9bc77dafc29": Phase="Pending", Reason="", readiness=false. Elapsed: 12.185253ms
Jun 28 18:35:22.637: INFO: Pod "pod-projected-configmaps-0ab5d319-759a-4810-9448-e9bc77dafc29": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031187621s
Jun 28 18:35:24.649: INFO: Pod "pod-projected-configmaps-0ab5d319-759a-4810-9448-e9bc77dafc29": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.043138804s
STEP: Saw pod success
Jun 28 18:35:24.649: INFO: Pod "pod-projected-configmaps-0ab5d319-759a-4810-9448-e9bc77dafc29" satisfied condition "Succeeded or Failed"
Jun 28 18:35:24.659: INFO: Trying to get logs from node 10.13.107.37 pod pod-projected-configmaps-0ab5d319-759a-4810-9448-e9bc77dafc29 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jun 28 18:35:24.738: INFO: Waiting for pod pod-projected-configmaps-0ab5d319-759a-4810-9448-e9bc77dafc29 to disappear
Jun 28 18:35:24.750: INFO: Pod pod-projected-configmaps-0ab5d319-759a-4810-9448-e9bc77dafc29 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 18:35:24.750: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6193" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","total":305,"completed":87,"skipped":1395,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 18:35:24.786: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Jun 28 18:35:24.959: INFO: Waiting up to 5m0s for pod "downwardapi-volume-2416c4eb-4cd3-4d17-bd32-33d6893786c6" in namespace "projected-6218" to be "Succeeded or Failed"
Jun 28 18:35:24.971: INFO: Pod "downwardapi-volume-2416c4eb-4cd3-4d17-bd32-33d6893786c6": Phase="Pending", Reason="", readiness=false. Elapsed: 12.11315ms
Jun 28 18:35:26.987: INFO: Pod "downwardapi-volume-2416c4eb-4cd3-4d17-bd32-33d6893786c6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.028220736s
STEP: Saw pod success
Jun 28 18:35:26.987: INFO: Pod "downwardapi-volume-2416c4eb-4cd3-4d17-bd32-33d6893786c6" satisfied condition "Succeeded or Failed"
Jun 28 18:35:26.997: INFO: Trying to get logs from node 10.13.107.37 pod downwardapi-volume-2416c4eb-4cd3-4d17-bd32-33d6893786c6 container client-container: <nil>
STEP: delete the pod
Jun 28 18:35:27.049: INFO: Waiting for pod downwardapi-volume-2416c4eb-4cd3-4d17-bd32-33d6893786c6 to disappear
Jun 28 18:35:27.062: INFO: Pod downwardapi-volume-2416c4eb-4cd3-4d17-bd32-33d6893786c6 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 18:35:27.063: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6218" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":88,"skipped":1403,"failed":0}
SSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 18:35:27.095: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name projected-configmap-test-volume-6c75e8c1-af58-4c09-9ce8-a3da034d8c63
STEP: Creating a pod to test consume configMaps
Jun 28 18:35:27.400: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-890e30a3-496e-426a-8fdc-530e7c702da7" in namespace "projected-3012" to be "Succeeded or Failed"
Jun 28 18:35:27.413: INFO: Pod "pod-projected-configmaps-890e30a3-496e-426a-8fdc-530e7c702da7": Phase="Pending", Reason="", readiness=false. Elapsed: 12.660661ms
Jun 28 18:35:29.427: INFO: Pod "pod-projected-configmaps-890e30a3-496e-426a-8fdc-530e7c702da7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.026997143s
STEP: Saw pod success
Jun 28 18:35:29.428: INFO: Pod "pod-projected-configmaps-890e30a3-496e-426a-8fdc-530e7c702da7" satisfied condition "Succeeded or Failed"
Jun 28 18:35:29.439: INFO: Trying to get logs from node 10.13.107.37 pod pod-projected-configmaps-890e30a3-496e-426a-8fdc-530e7c702da7 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jun 28 18:35:29.497: INFO: Waiting for pod pod-projected-configmaps-890e30a3-496e-426a-8fdc-530e7c702da7 to disappear
Jun 28 18:35:29.510: INFO: Pod pod-projected-configmaps-890e30a3-496e-426a-8fdc-530e7c702da7 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 18:35:29.510: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3012" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","total":305,"completed":89,"skipped":1408,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 18:35:29.547: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Jun 28 18:35:35.880: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jun 28 18:35:35.896: INFO: Pod pod-with-prestop-http-hook still exists
Jun 28 18:35:37.896: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jun 28 18:35:37.909: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 18:35:37.934: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-5250" for this suite.

• [SLOW TEST:8.420 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  when create a pod with lifecycle hook
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute prestop http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop http hook properly [NodeConformance] [Conformance]","total":305,"completed":90,"skipped":1443,"failed":0}
SSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class 
  should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 18:35:37.969: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods Set QOS Class
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:161
[It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying QOS class is set on the pod
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 18:35:38.152: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-2973" for this suite.
•{"msg":"PASSED [k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]","total":305,"completed":91,"skipped":1454,"failed":0}
SSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 18:35:38.186: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:162
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod
Jun 28 18:35:38.321: INFO: PodSpec: initContainers in spec.initContainers
Jun 28 18:36:27.016: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-53a8ab10-bcbc-47d4-8436-06c92f8ff2f1", GenerateName:"", Namespace:"init-container-2611", SelfLink:"/api/v1/namespaces/init-container-2611/pods/pod-init-53a8ab10-bcbc-47d4-8436-06c92f8ff2f1", UID:"9e6237a1-f8f5-4a0c-8b54-8e937cff0f72", ResourceVersion:"66999", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63760502138, loc:(*time.Location)(0x7702840)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"321890925"}, Annotations:map[string]string{"cni.projectcalico.org/podIP":"172.30.221.146/32", "cni.projectcalico.org/podIPs":"172.30.221.146/32", "k8s.v1.cni.cncf.io/network-status":"[{\n    \"name\": \"\",\n    \"ips\": [\n        \"172.30.221.146\"\n    ],\n    \"default\": true,\n    \"dns\": {}\n}]", "k8s.v1.cni.cncf.io/networks-status":"[{\n    \"name\": \"\",\n    \"ips\": [\n        \"172.30.221.146\"\n    ],\n    \"default\": true,\n    \"dns\": {}\n}]", "openshift.io/scc":"anyuid"}, OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:(*v1.Time)(0xc00235c2a0), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc00235c2c0)}, v1.ManagedFieldsEntry{Manager:"calico", Operation:"Update", APIVersion:"v1", Time:(*v1.Time)(0xc00235c2e0), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc00235c300)}, v1.ManagedFieldsEntry{Manager:"multus", Operation:"Update", APIVersion:"v1", Time:(*v1.Time)(0xc00235c360), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc00235c3a0)}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:(*v1.Time)(0xc00235c3c0), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc00235c3e0)}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"default-token-d5vbs", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(0xc001bd0300), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-d5vbs", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(0xc0035302a0), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-d5vbs", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(0xc003530300), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"k8s.gcr.io/pause:3.2", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-d5vbs", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(0xc003530240), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc004954b28), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"10.13.107.37", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc0038927e0), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc004954ca0)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc004954cc0)}, v1.Toleration{Key:"node.kubernetes.io/memory-pressure", Operator:"Exists", Value:"", Effect:"NoSchedule", TolerationSeconds:(*int64)(nil)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc004954cdc), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc004954ce0), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc003c02060), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63760502138, loc:(*time.Location)(0x7702840)}}, Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63760502138, loc:(*time.Location)(0x7702840)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63760502138, loc:(*time.Location)(0x7702840)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63760502138, loc:(*time.Location)(0x7702840)}}, Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"10.13.107.37", PodIP:"172.30.221.146", PodIPs:[]v1.PodIP{v1.PodIP{IP:"172.30.221.146"}}, StartTime:(*v1.Time)(0xc00235c400), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0038928c0)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc003892930)}, Ready:false, RestartCount:3, Image:"docker.io/library/busybox:1.29", ImageID:"docker.io/library/busybox@sha256:8ccbac733d19c0dd4d70b4f0c1e12245b5fa3ad24758a11035ee505c629c0796", ContainerID:"cri-o://b3178dbcfb68c605dc14121d642ff497f2eb32e3223f2c104594f44e651a1446", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc00235c500), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"docker.io/library/busybox:1.29", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc00235c4c0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/pause:3.2", ImageID:"", ContainerID:"", Started:(*bool)(0xc004954d5f)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 18:36:27.017: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-2611" for this suite.

• [SLOW TEST:48.869 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should not start app containers if init containers fail on a RestartAlways pod [Conformance]","total":305,"completed":92,"skipped":1463,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 18:36:27.057: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test override arguments
Jun 28 18:36:27.273: INFO: Waiting up to 5m0s for pod "client-containers-5d10735f-1600-4961-ad61-755ba6c86db9" in namespace "containers-2638" to be "Succeeded or Failed"
Jun 28 18:36:27.284: INFO: Pod "client-containers-5d10735f-1600-4961-ad61-755ba6c86db9": Phase="Pending", Reason="", readiness=false. Elapsed: 11.294377ms
Jun 28 18:36:29.297: INFO: Pod "client-containers-5d10735f-1600-4961-ad61-755ba6c86db9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.024174416s
STEP: Saw pod success
Jun 28 18:36:29.298: INFO: Pod "client-containers-5d10735f-1600-4961-ad61-755ba6c86db9" satisfied condition "Succeeded or Failed"
Jun 28 18:36:29.307: INFO: Trying to get logs from node 10.13.107.37 pod client-containers-5d10735f-1600-4961-ad61-755ba6c86db9 container test-container: <nil>
STEP: delete the pod
Jun 28 18:36:29.399: INFO: Waiting for pod client-containers-5d10735f-1600-4961-ad61-755ba6c86db9 to disappear
Jun 28 18:36:29.409: INFO: Pod client-containers-5d10735f-1600-4961-ad61-755ba6c86db9 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 18:36:29.409: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-2638" for this suite.
•{"msg":"PASSED [k8s.io] Docker Containers should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]","total":305,"completed":93,"skipped":1517,"failed":0}

------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a mutating webhook should work [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 18:36:29.445: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jun 28 18:36:30.233: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jun 28 18:36:32.266: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63760502190, loc:(*time.Location)(0x7702840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63760502190, loc:(*time.Location)(0x7702840)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63760502190, loc:(*time.Location)(0x7702840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63760502190, loc:(*time.Location)(0x7702840)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun 28 18:36:35.306: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a mutating webhook should work [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a mutating webhook configuration
STEP: Updating a mutating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that should not be mutated
STEP: Patching a mutating webhook configuration's rules to include the create operation
STEP: Creating a configMap that should be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 18:36:35.578: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3085" for this suite.
STEP: Destroying namespace "webhook-3085-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:6.292 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  patching/updating a mutating webhook should work [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a mutating webhook should work [Conformance]","total":305,"completed":94,"skipped":1517,"failed":0}
SSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 18:36:35.740: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod liveness-c7fa5ac1-a7b8-479c-8634-9ba1834845c5 in namespace container-probe-3375
Jun 28 18:36:40.046: INFO: Started pod liveness-c7fa5ac1-a7b8-479c-8634-9ba1834845c5 in namespace container-probe-3375
STEP: checking the pod's current state and verifying that restartCount is present
Jun 28 18:36:40.056: INFO: Initial restart count of pod liveness-c7fa5ac1-a7b8-479c-8634-9ba1834845c5 is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 18:40:41.450: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-3375" for this suite.

• [SLOW TEST:245.744 seconds]
[k8s.io] Probing container
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Probing container should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]","total":305,"completed":95,"skipped":1529,"failed":0}
S
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 18:40:41.484: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-4796
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a new StatefulSet
Jun 28 18:40:41.664: INFO: Found 0 stateful pods, waiting for 3
Jun 28 18:40:51.679: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jun 28 18:40:51.679: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jun 28 18:40:51.679: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from docker.io/library/httpd:2.4.38-alpine to docker.io/library/httpd:2.4.39-alpine
Jun 28 18:40:51.755: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Not applying an update when the partition is greater than the number of replicas
STEP: Performing a canary update
Jun 28 18:41:01.836: INFO: Updating stateful set ss2
Jun 28 18:41:01.869: INFO: Waiting for Pod statefulset-4796/ss2-2 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
STEP: Restoring Pods to the correct revision when they are deleted
Jun 28 18:41:11.975: INFO: Found 1 stateful pods, waiting for 3
Jun 28 18:41:21.996: INFO: Found 2 stateful pods, waiting for 3
Jun 28 18:41:32.087: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jun 28 18:41:32.088: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jun 28 18:41:32.088: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update
Jun 28 18:41:32.154: INFO: Updating stateful set ss2
Jun 28 18:41:32.196: INFO: Waiting for Pod statefulset-4796/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Jun 28 18:41:42.275: INFO: Waiting for Pod statefulset-4796/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Jun 28 18:41:52.426: INFO: Updating stateful set ss2
Jun 28 18:41:52.445: INFO: Waiting for StatefulSet statefulset-4796/ss2 to complete update
Jun 28 18:41:52.445: INFO: Waiting for Pod statefulset-4796/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Jun 28 18:42:02.541: INFO: Waiting for StatefulSet statefulset-4796/ss2 to complete update
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
Jun 28 18:42:12.489: INFO: Deleting all statefulset in ns statefulset-4796
Jun 28 18:42:12.499: INFO: Scaling statefulset ss2 to 0
Jun 28 18:42:42.695: INFO: Waiting for statefulset status.replicas updated to 0
Jun 28 18:42:42.705: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 18:42:42.753: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-4796" for this suite.

• [SLOW TEST:121.302 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should perform canary updates and phased rolling updates of template modifications [Conformance]","total":305,"completed":96,"skipped":1530,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 18:42:42.787: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Jun 28 18:42:43.108: INFO: Waiting up to 5m0s for pod "downwardapi-volume-255e46c1-1be2-48ac-b5e0-17c78886d35f" in namespace "projected-3458" to be "Succeeded or Failed"
Jun 28 18:42:43.123: INFO: Pod "downwardapi-volume-255e46c1-1be2-48ac-b5e0-17c78886d35f": Phase="Pending", Reason="", readiness=false. Elapsed: 14.43667ms
Jun 28 18:42:45.136: INFO: Pod "downwardapi-volume-255e46c1-1be2-48ac-b5e0-17c78886d35f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.027491891s
STEP: Saw pod success
Jun 28 18:42:45.136: INFO: Pod "downwardapi-volume-255e46c1-1be2-48ac-b5e0-17c78886d35f" satisfied condition "Succeeded or Failed"
Jun 28 18:42:45.145: INFO: Trying to get logs from node 10.13.107.37 pod downwardapi-volume-255e46c1-1be2-48ac-b5e0-17c78886d35f container client-container: <nil>
STEP: delete the pod
Jun 28 18:42:45.249: INFO: Waiting for pod downwardapi-volume-255e46c1-1be2-48ac-b5e0-17c78886d35f to disappear
Jun 28 18:42:45.265: INFO: Pod downwardapi-volume-255e46c1-1be2-48ac-b5e0-17c78886d35f no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 18:42:45.265: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3458" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory request [NodeConformance] [Conformance]","total":305,"completed":97,"skipped":1551,"failed":0}
SSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath 
  runs ReplicaSets to verify preemption running path [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 18:42:45.299: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename sched-preemption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:89
Jun 28 18:42:45.474: INFO: Waiting up to 1m0s for all nodes to be ready
Jun 28 18:43:45.708: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PreemptionExecutionPath
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 18:43:45.723: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename sched-preemption-path
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] PreemptionExecutionPath
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:487
STEP: Finding an available node
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
Jun 28 18:43:47.967: INFO: found a healthy node: 10.13.107.37
[It] runs ReplicaSets to verify preemption running path [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jun 28 18:44:00.223: INFO: pods created so far: [1 1 1]
Jun 28 18:44:00.223: INFO: length of pods created so far: 3
Jun 28 18:44:12.273: INFO: pods created so far: [2 2 1]
[AfterEach] PreemptionExecutionPath
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 18:44:19.275: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-path-570" for this suite.
[AfterEach] PreemptionExecutionPath
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:461
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 18:44:19.447: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-3879" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:77

• [SLOW TEST:94.338 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  PreemptionExecutionPath
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:450
    runs ReplicaSets to verify preemption running path [Conformance]
    /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath runs ReplicaSets to verify preemption running path [Conformance]","total":305,"completed":98,"skipped":1560,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 18:44:19.637: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating Pod
STEP: Waiting for the pod running
STEP: Geting the pod
STEP: Reading file content from the nginx-container
Jun 28 18:44:23.923: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-4517 PodName:pod-sharedvolume-f7a387e0-2b65-4571-9cbe-ccf789ba68e3 ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 28 18:44:23.923: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
Jun 28 18:44:24.114: INFO: Exec stderr: ""
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 18:44:24.114: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4517" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes pod should support shared volumes between containers [Conformance]","total":305,"completed":99,"skipped":1568,"failed":0}
S
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 18:44:24.154: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a service in the namespace
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there is no service in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 18:44:31.740: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-3870" for this suite.
STEP: Destroying namespace "nsdeletetest-5924" for this suite.
Jun 28 18:44:31.784: INFO: Namespace nsdeletetest-5924 was already deleted
STEP: Destroying namespace "nsdeletetest-5980" for this suite.

• [SLOW TEST:7.645 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all services are removed when a namespace is deleted [Conformance]","total":305,"completed":100,"skipped":1569,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 18:44:31.809: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a service externalname-service with the type=ExternalName in namespace services-996
STEP: changing the ExternalName service to type=NodePort
STEP: creating replication controller externalname-service in namespace services-996
I0628 18:44:32.055079      22 runners.go:190] Created replication controller with name: externalname-service, namespace: services-996, replica count: 2
Jun 28 18:44:35.106: INFO: Creating new exec pod
I0628 18:44:35.105931      22 runners.go:190] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jun 28 18:44:38.202: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-108209140 exec --namespace=services-996 execpodwj8p4 -- /bin/sh -x -c nc -zv -t -w 2 externalname-service 80'
Jun 28 18:44:38.827: INFO: stderr: "+ nc -zv -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Jun 28 18:44:38.827: INFO: stdout: ""
Jun 28 18:44:38.828: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-108209140 exec --namespace=services-996 execpodwj8p4 -- /bin/sh -x -c nc -zv -t -w 2 172.21.158.65 80'
Jun 28 18:44:39.216: INFO: stderr: "+ nc -zv -t -w 2 172.21.158.65 80\nConnection to 172.21.158.65 80 port [tcp/http] succeeded!\n"
Jun 28 18:44:39.216: INFO: stdout: ""
Jun 28 18:44:39.216: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-108209140 exec --namespace=services-996 execpodwj8p4 -- /bin/sh -x -c nc -zv -t -w 2 10.13.107.37 30103'
Jun 28 18:44:39.707: INFO: stderr: "+ nc -zv -t -w 2 10.13.107.37 30103\nConnection to 10.13.107.37 30103 port [tcp/30103] succeeded!\n"
Jun 28 18:44:39.707: INFO: stdout: ""
Jun 28 18:44:39.707: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-108209140 exec --namespace=services-996 execpodwj8p4 -- /bin/sh -x -c nc -zv -t -w 2 10.13.107.57 30103'
Jun 28 18:44:40.170: INFO: stderr: "+ nc -zv -t -w 2 10.13.107.57 30103\nConnection to 10.13.107.57 30103 port [tcp/30103] succeeded!\n"
Jun 28 18:44:40.170: INFO: stdout: ""
Jun 28 18:44:40.170: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-108209140 exec --namespace=services-996 execpodwj8p4 -- /bin/sh -x -c nc -zv -t -w 2 149.81.178.52 30103'
Jun 28 18:44:40.614: INFO: stderr: "+ nc -zv -t -w 2 149.81.178.52 30103\nConnection to 149.81.178.52 30103 port [tcp/30103] succeeded!\n"
Jun 28 18:44:40.614: INFO: stdout: ""
Jun 28 18:44:40.614: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-108209140 exec --namespace=services-996 execpodwj8p4 -- /bin/sh -x -c nc -zv -t -w 2 149.81.178.54 30103'
Jun 28 18:44:41.022: INFO: stderr: "+ nc -zv -t -w 2 149.81.178.54 30103\nConnection to 149.81.178.54 30103 port [tcp/30103] succeeded!\n"
Jun 28 18:44:41.022: INFO: stdout: ""
Jun 28 18:44:41.022: INFO: Cleaning up the ExternalName to NodePort test service
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 18:44:41.082: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-996" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

• [SLOW TEST:9.307 seconds]
[sig-network] Services
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to NodePort [Conformance]","total":305,"completed":101,"skipped":1635,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 18:44:41.116: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Jun 28 18:44:41.294: INFO: Waiting up to 5m0s for pod "downwardapi-volume-cc81fd17-9b39-4154-9f1d-bf3b053e9ce7" in namespace "downward-api-2964" to be "Succeeded or Failed"
Jun 28 18:44:41.304: INFO: Pod "downwardapi-volume-cc81fd17-9b39-4154-9f1d-bf3b053e9ce7": Phase="Pending", Reason="", readiness=false. Elapsed: 10.208162ms
Jun 28 18:44:43.317: INFO: Pod "downwardapi-volume-cc81fd17-9b39-4154-9f1d-bf3b053e9ce7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.022694022s
STEP: Saw pod success
Jun 28 18:44:43.317: INFO: Pod "downwardapi-volume-cc81fd17-9b39-4154-9f1d-bf3b053e9ce7" satisfied condition "Succeeded or Failed"
Jun 28 18:44:43.329: INFO: Trying to get logs from node 10.13.107.37 pod downwardapi-volume-cc81fd17-9b39-4154-9f1d-bf3b053e9ce7 container client-container: <nil>
STEP: delete the pod
Jun 28 18:44:43.414: INFO: Waiting for pod downwardapi-volume-cc81fd17-9b39-4154-9f1d-bf3b053e9ce7 to disappear
Jun 28 18:44:43.424: INFO: Pod downwardapi-volume-cc81fd17-9b39-4154-9f1d-bf3b053e9ce7 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 18:44:43.425: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2964" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":102,"skipped":1653,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 18:44:43.465: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating service multi-endpoint-test in namespace services-7970
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-7970 to expose endpoints map[]
Jun 28 18:44:43.640: INFO: Failed go get Endpoints object: endpoints "multi-endpoint-test" not found
Jun 28 18:44:44.707: INFO: successfully validated that service multi-endpoint-test in namespace services-7970 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-7970
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-7970 to expose endpoints map[pod1:[100]]
Jun 28 18:44:47.832: INFO: successfully validated that service multi-endpoint-test in namespace services-7970 exposes endpoints map[pod1:[100]]
STEP: Creating pod pod2 in namespace services-7970
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-7970 to expose endpoints map[pod1:[100] pod2:[101]]
Jun 28 18:44:49.966: INFO: successfully validated that service multi-endpoint-test in namespace services-7970 exposes endpoints map[pod1:[100] pod2:[101]]
STEP: Deleting pod pod1 in namespace services-7970
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-7970 to expose endpoints map[pod2:[101]]
Jun 28 18:44:50.027: INFO: successfully validated that service multi-endpoint-test in namespace services-7970 exposes endpoints map[pod2:[101]]
STEP: Deleting pod pod2 in namespace services-7970
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-7970 to expose endpoints map[]
Jun 28 18:44:50.104: INFO: successfully validated that service multi-endpoint-test in namespace services-7970 exposes endpoints map[]
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 18:44:50.151: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7970" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

• [SLOW TEST:6.722 seconds]
[sig-network] Services
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should serve multiport endpoints from pods  [Conformance]","total":305,"completed":103,"skipped":1664,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a validating webhook should work [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 18:44:50.188: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jun 28 18:44:50.703: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jun 28 18:44:52.733: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63760502690, loc:(*time.Location)(0x7702840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63760502690, loc:(*time.Location)(0x7702840)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63760502690, loc:(*time.Location)(0x7702840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63760502690, loc:(*time.Location)(0x7702840)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun 28 18:44:55.827: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a validating webhook should work [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a validating webhook configuration
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Updating a validating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Patching a validating webhook configuration's rules to include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 18:44:56.117: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1755" for this suite.
STEP: Destroying namespace "webhook-1755-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:6.091 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  patching/updating a validating webhook should work [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a validating webhook should work [Conformance]","total":305,"completed":104,"skipped":1695,"failed":0}
SS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 18:44:56.282: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating secret with name secret-test-93b57a7c-08a5-44ec-b5da-0017fedf4971
STEP: Creating a pod to test consume secrets
Jun 28 18:44:56.469: INFO: Waiting up to 5m0s for pod "pod-secrets-9415bc36-6c2c-487f-9de8-a87acee18414" in namespace "secrets-2683" to be "Succeeded or Failed"
Jun 28 18:44:56.482: INFO: Pod "pod-secrets-9415bc36-6c2c-487f-9de8-a87acee18414": Phase="Pending", Reason="", readiness=false. Elapsed: 12.74524ms
Jun 28 18:44:58.500: INFO: Pod "pod-secrets-9415bc36-6c2c-487f-9de8-a87acee18414": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030959768s
Jun 28 18:45:00.516: INFO: Pod "pod-secrets-9415bc36-6c2c-487f-9de8-a87acee18414": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.046693195s
STEP: Saw pod success
Jun 28 18:45:00.516: INFO: Pod "pod-secrets-9415bc36-6c2c-487f-9de8-a87acee18414" satisfied condition "Succeeded or Failed"
Jun 28 18:45:00.526: INFO: Trying to get logs from node 10.13.107.37 pod pod-secrets-9415bc36-6c2c-487f-9de8-a87acee18414 container secret-volume-test: <nil>
STEP: delete the pod
Jun 28 18:45:00.581: INFO: Waiting for pod pod-secrets-9415bc36-6c2c-487f-9de8-a87acee18414 to disappear
Jun 28 18:45:00.590: INFO: Pod pod-secrets-9415bc36-6c2c-487f-9de8-a87acee18414 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 18:45:00.590: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2683" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume [NodeConformance] [Conformance]","total":305,"completed":105,"skipped":1697,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate configmap [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 18:45:00.630: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jun 28 18:45:01.495: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jun 28 18:45:03.530: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63760502701, loc:(*time.Location)(0x7702840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63760502701, loc:(*time.Location)(0x7702840)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63760502701, loc:(*time.Location)(0x7702840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63760502701, loc:(*time.Location)(0x7702840)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun 28 18:45:06.569: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate configmap [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Registering the mutating configmap webhook via the AdmissionRegistration API
STEP: create a configmap that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 18:45:06.705: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6911" for this suite.
STEP: Destroying namespace "webhook-6911-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:6.240 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate configmap [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate configmap [Conformance]","total":305,"completed":106,"skipped":1708,"failed":0}
SS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 18:45:06.870: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Jun 28 18:45:07.061: INFO: Waiting up to 5m0s for pod "downwardapi-volume-dee80341-58a8-42a9-b01e-9650b4f3ea0b" in namespace "downward-api-3760" to be "Succeeded or Failed"
Jun 28 18:45:07.074: INFO: Pod "downwardapi-volume-dee80341-58a8-42a9-b01e-9650b4f3ea0b": Phase="Pending", Reason="", readiness=false. Elapsed: 12.468104ms
Jun 28 18:45:09.086: INFO: Pod "downwardapi-volume-dee80341-58a8-42a9-b01e-9650b4f3ea0b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.024274771s
STEP: Saw pod success
Jun 28 18:45:09.086: INFO: Pod "downwardapi-volume-dee80341-58a8-42a9-b01e-9650b4f3ea0b" satisfied condition "Succeeded or Failed"
Jun 28 18:45:09.097: INFO: Trying to get logs from node 10.13.107.37 pod downwardapi-volume-dee80341-58a8-42a9-b01e-9650b4f3ea0b container client-container: <nil>
STEP: delete the pod
Jun 28 18:45:09.160: INFO: Waiting for pod downwardapi-volume-dee80341-58a8-42a9-b01e-9650b4f3ea0b to disappear
Jun 28 18:45:09.172: INFO: Pod downwardapi-volume-dee80341-58a8-42a9-b01e-9650b4f3ea0b no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 18:45:09.172: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3760" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","total":305,"completed":107,"skipped":1710,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl patch 
  should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 18:45:09.203: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[It] should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating Agnhost RC
Jun 28 18:45:09.324: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-108209140 create -f - --namespace=kubectl-1520'
Jun 28 18:45:10.129: INFO: stderr: ""
Jun 28 18:45:10.129: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
Jun 28 18:45:11.143: INFO: Selector matched 1 pods for map[app:agnhost]
Jun 28 18:45:11.143: INFO: Found 0 / 1
Jun 28 18:45:12.144: INFO: Selector matched 1 pods for map[app:agnhost]
Jun 28 18:45:12.145: INFO: Found 1 / 1
Jun 28 18:45:12.145: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods
Jun 28 18:45:12.155: INFO: Selector matched 1 pods for map[app:agnhost]
Jun 28 18:45:12.155: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jun 28 18:45:12.155: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-108209140 patch pod agnhost-primary-mnztk --namespace=kubectl-1520 -p {"metadata":{"annotations":{"x":"y"}}}'
Jun 28 18:45:12.403: INFO: stderr: ""
Jun 28 18:45:12.403: INFO: stdout: "pod/agnhost-primary-mnztk patched\n"
STEP: checking annotations
Jun 28 18:45:12.413: INFO: Selector matched 1 pods for map[app:agnhost]
Jun 28 18:45:12.413: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 18:45:12.413: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1520" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl patch should add annotations for pods in rc  [Conformance]","total":305,"completed":108,"skipped":1751,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should deny crd creation [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 18:45:12.449: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jun 28 18:45:12.966: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jun 28 18:45:15.051: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63760502712, loc:(*time.Location)(0x7702840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63760502712, loc:(*time.Location)(0x7702840)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63760502713, loc:(*time.Location)(0x7702840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63760502712, loc:(*time.Location)(0x7702840)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun 28 18:45:18.091: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should deny crd creation [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Registering the crd webhook via the AdmissionRegistration API
STEP: Creating a custom resource definition that should be denied by the webhook
Jun 28 18:45:18.158: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 18:45:18.224: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3088" for this suite.
STEP: Destroying namespace "webhook-3088-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:5.941 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should deny crd creation [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should deny crd creation [Conformance]","total":305,"completed":109,"skipped":1769,"failed":0}
SSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 18:45:18.392: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-8475.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-8475.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jun 28 18:45:22.843: INFO: DNS probes using dns-8475/dns-test-deeefdef-a7e5-4034-a6a9-bb7c6b42c647 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 18:45:22.874: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-8475" for this suite.
•{"msg":"PASSED [sig-network] DNS should provide DNS for the cluster  [Conformance]","total":305,"completed":110,"skipped":1774,"failed":0}
SSS
------------------------------
[sig-auth] Certificates API [Privileged:ClusterAdmin] 
  should support CSR API operations [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 18:45:22.907: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename certificates
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support CSR API operations [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: getting /apis
STEP: getting /apis/certificates.k8s.io
STEP: getting /apis/certificates.k8s.io/v1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Jun 28 18:45:23.857: INFO: starting watch
STEP: patching
STEP: updating
Jun 28 18:45:23.906: INFO: waiting for watch events with expected annotations
Jun 28 18:45:23.906: INFO: saw patched and updated annotations
STEP: getting /approval
STEP: patching /approval
STEP: updating /approval
STEP: getting /status
STEP: patching /status
STEP: updating /status
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 18:45:24.117: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "certificates-9971" for this suite.
•{"msg":"PASSED [sig-auth] Certificates API [Privileged:ClusterAdmin] should support CSR API operations [Conformance]","total":305,"completed":111,"skipped":1777,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 18:45:24.150: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:162
[It] should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod
Jun 28 18:45:24.271: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 18:45:28.092: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-6158" for this suite.
•{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should invoke init containers on a RestartAlways pod [Conformance]","total":305,"completed":112,"skipped":1811,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a read only busybox container 
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 18:45:28.134: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 18:45:30.369: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-9711" for this suite.
•{"msg":"PASSED [k8s.io] Kubelet when scheduling a read only busybox container should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":113,"skipped":1825,"failed":0}
SSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 18:45:30.405: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:90
Jun 28 18:45:30.542: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jun 28 18:45:30.573: INFO: Waiting for terminating namespaces to be deleted...
Jun 28 18:45:30.591: INFO: 
Logging pods the apiserver thinks is on node 10.13.107.37 before test
Jun 28 18:45:30.636: INFO: calico-node-7ntd9 from calico-system started at 2021-06-28 16:43:04 +0000 UTC (1 container statuses recorded)
Jun 28 18:45:30.636: INFO: 	Container calico-node ready: true, restart count 0
Jun 28 18:45:30.636: INFO: calico-typha-649969bb55-mj7tk from calico-system started at 2021-06-28 16:45:03 +0000 UTC (1 container statuses recorded)
Jun 28 18:45:30.636: INFO: 	Container calico-typha ready: true, restart count 0
Jun 28 18:45:30.637: INFO: pod-init-a00e6e95-14f3-448d-b307-0264c86379d7 from init-container-6158 started at 2021-06-28 18:45:24 +0000 UTC (1 container statuses recorded)
Jun 28 18:45:30.637: INFO: 	Container run1 ready: true, restart count 0
Jun 28 18:45:30.637: INFO: ibm-keepalived-watcher-f9nwv from kube-system started at 2021-06-28 16:41:30 +0000 UTC (1 container statuses recorded)
Jun 28 18:45:30.637: INFO: 	Container keepalived-watcher ready: true, restart count 0
Jun 28 18:45:30.637: INFO: ibm-master-proxy-static-10.13.107.37 from kube-system started at 2021-06-28 16:40:48 +0000 UTC (2 container statuses recorded)
Jun 28 18:45:30.637: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Jun 28 18:45:30.637: INFO: 	Container pause ready: true, restart count 0
Jun 28 18:45:30.637: INFO: ibmcloud-block-storage-driver-kbnst from kube-system started at 2021-06-28 16:41:30 +0000 UTC (1 container statuses recorded)
Jun 28 18:45:30.637: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Jun 28 18:45:30.638: INFO: busybox-readonly-fsfcfaa580-c4fe-48bf-8cce-29d7680c17f9 from kubelet-test-9711 started at 2021-06-28 18:45:28 +0000 UTC (1 container statuses recorded)
Jun 28 18:45:30.638: INFO: 	Container busybox-readonly-fsfcfaa580-c4fe-48bf-8cce-29d7680c17f9 ready: true, restart count 0
Jun 28 18:45:30.638: INFO: tuned-2tw2w from openshift-cluster-node-tuning-operator started at 2021-06-28 16:43:24 +0000 UTC (1 container statuses recorded)
Jun 28 18:45:30.638: INFO: 	Container tuned ready: true, restart count 0
Jun 28 18:45:30.638: INFO: dns-default-crxtq from openshift-dns started at 2021-06-28 16:44:04 +0000 UTC (3 container statuses recorded)
Jun 28 18:45:30.638: INFO: 	Container dns ready: true, restart count 0
Jun 28 18:45:30.638: INFO: 	Container dns-node-resolver ready: true, restart count 0
Jun 28 18:45:30.638: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 28 18:45:30.638: INFO: node-ca-z64gj from openshift-image-registry started at 2021-06-28 16:44:14 +0000 UTC (1 container statuses recorded)
Jun 28 18:45:30.639: INFO: 	Container node-ca ready: true, restart count 0
Jun 28 18:45:30.639: INFO: registry-pvc-permissions-lfjks from openshift-image-registry started at 2021-06-28 16:46:50 +0000 UTC (1 container statuses recorded)
Jun 28 18:45:30.639: INFO: 	Container pvc-permissions ready: false, restart count 0
Jun 28 18:45:30.639: INFO: openshift-kube-proxy-8l7d5 from openshift-kube-proxy started at 2021-06-28 16:42:08 +0000 UTC (1 container statuses recorded)
Jun 28 18:45:30.639: INFO: 	Container kube-proxy ready: true, restart count 0
Jun 28 18:45:30.639: INFO: node-exporter-qj7hs from openshift-monitoring started at 2021-06-28 16:44:29 +0000 UTC (2 container statuses recorded)
Jun 28 18:45:30.639: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 28 18:45:30.639: INFO: 	Container node-exporter ready: true, restart count 0
Jun 28 18:45:30.639: INFO: multus-admission-controller-qz22l from openshift-multus started at 2021-06-28 18:29:47 +0000 UTC (2 container statuses recorded)
Jun 28 18:45:30.639: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 28 18:45:30.640: INFO: 	Container multus-admission-controller ready: true, restart count 0
Jun 28 18:45:30.640: INFO: multus-brfr7 from openshift-multus started at 2021-06-28 16:42:07 +0000 UTC (1 container statuses recorded)
Jun 28 18:45:30.640: INFO: 	Container kube-multus ready: true, restart count 0
Jun 28 18:45:30.640: INFO: network-metrics-daemon-5rjvj from openshift-multus started at 2021-06-28 16:42:08 +0000 UTC (2 container statuses recorded)
Jun 28 18:45:30.640: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 28 18:45:30.640: INFO: 	Container network-metrics-daemon ready: true, restart count 0
Jun 28 18:45:30.640: INFO: sonobuoy-systemd-logs-daemon-set-bc3c64b0befe49cb-gp9nl from sonobuoy started at 2021-06-28 18:11:47 +0000 UTC (2 container statuses recorded)
Jun 28 18:45:30.640: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 28 18:45:30.641: INFO: 	Container systemd-logs ready: true, restart count 0
Jun 28 18:45:30.641: INFO: 
Logging pods the apiserver thinks is on node 10.13.107.57 before test
Jun 28 18:45:30.693: INFO: calico-node-tr9lp from calico-system started at 2021-06-28 16:43:04 +0000 UTC (1 container statuses recorded)
Jun 28 18:45:30.693: INFO: 	Container calico-node ready: true, restart count 0
Jun 28 18:45:30.693: INFO: calico-typha-649969bb55-tpj68 from calico-system started at 2021-06-28 16:43:04 +0000 UTC (1 container statuses recorded)
Jun 28 18:45:30.693: INFO: 	Container calico-typha ready: true, restart count 0
Jun 28 18:45:30.693: INFO: ibm-cloud-provider-ip-149-81-71-226-77554565fb-clmlm from ibm-system started at 2021-06-28 16:58:09 +0000 UTC (1 container statuses recorded)
Jun 28 18:45:30.693: INFO: 	Container ibm-cloud-provider-ip-149-81-71-226 ready: true, restart count 0
Jun 28 18:45:30.693: INFO: ibm-file-plugin-7d974bf47c-bw8qh from kube-system started at 2021-06-28 16:43:14 +0000 UTC (1 container statuses recorded)
Jun 28 18:45:30.693: INFO: 	Container ibm-file-plugin-container ready: true, restart count 0
Jun 28 18:45:30.693: INFO: ibm-keepalived-watcher-rqj9s from kube-system started at 2021-06-28 16:41:35 +0000 UTC (1 container statuses recorded)
Jun 28 18:45:30.694: INFO: 	Container keepalived-watcher ready: true, restart count 0
Jun 28 18:45:30.694: INFO: ibm-master-proxy-static-10.13.107.57 from kube-system started at 2021-06-28 16:40:56 +0000 UTC (2 container statuses recorded)
Jun 28 18:45:30.694: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Jun 28 18:45:30.694: INFO: 	Container pause ready: true, restart count 0
Jun 28 18:45:30.694: INFO: ibm-storage-watcher-869677c8b-q8rh5 from kube-system started at 2021-06-28 16:43:14 +0000 UTC (1 container statuses recorded)
Jun 28 18:45:30.694: INFO: 	Container ibm-storage-watcher-container ready: true, restart count 0
Jun 28 18:45:30.694: INFO: ibmcloud-block-storage-driver-p9gg7 from kube-system started at 2021-06-28 16:41:35 +0000 UTC (1 container statuses recorded)
Jun 28 18:45:30.694: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Jun 28 18:45:30.694: INFO: ibmcloud-block-storage-plugin-665488684b-h5fhz from kube-system started at 2021-06-28 16:43:14 +0000 UTC (1 container statuses recorded)
Jun 28 18:45:30.694: INFO: 	Container ibmcloud-block-storage-plugin-container ready: true, restart count 0
Jun 28 18:45:30.694: INFO: vpn-6c58c4756c-rw748 from kube-system started at 2021-06-28 16:50:23 +0000 UTC (1 container statuses recorded)
Jun 28 18:45:30.694: INFO: 	Container vpn ready: true, restart count 0
Jun 28 18:45:30.694: INFO: cluster-node-tuning-operator-77dfdd89b8-chjmh from openshift-cluster-node-tuning-operator started at 2021-06-28 16:43:14 +0000 UTC (1 container statuses recorded)
Jun 28 18:45:30.694: INFO: 	Container cluster-node-tuning-operator ready: true, restart count 0
Jun 28 18:45:30.694: INFO: tuned-r5kts from openshift-cluster-node-tuning-operator started at 2021-06-28 16:43:24 +0000 UTC (1 container statuses recorded)
Jun 28 18:45:30.694: INFO: 	Container tuned ready: true, restart count 0
Jun 28 18:45:30.694: INFO: cluster-samples-operator-56dd9bbcb-8mjfl from openshift-cluster-samples-operator started at 2021-06-28 16:43:14 +0000 UTC (2 container statuses recorded)
Jun 28 18:45:30.694: INFO: 	Container cluster-samples-operator ready: true, restart count 0
Jun 28 18:45:30.694: INFO: 	Container cluster-samples-operator-watch ready: true, restart count 0
Jun 28 18:45:30.694: INFO: cluster-storage-operator-56664f46b8-kjjbs from openshift-cluster-storage-operator started at 2021-06-28 16:43:14 +0000 UTC (1 container statuses recorded)
Jun 28 18:45:30.695: INFO: 	Container cluster-storage-operator ready: true, restart count 0
Jun 28 18:45:30.695: INFO: console-operator-cd789fcfb-4j2b8 from openshift-console-operator started at 2021-06-28 16:43:14 +0000 UTC (1 container statuses recorded)
Jun 28 18:45:30.695: INFO: 	Container console-operator ready: true, restart count 1
Jun 28 18:45:30.695: INFO: console-74dd6cb4c6-vnhx8 from openshift-console started at 2021-06-28 18:29:06 +0000 UTC (1 container statuses recorded)
Jun 28 18:45:30.695: INFO: 	Container console ready: true, restart count 0
Jun 28 18:45:30.695: INFO: downloads-65dcc64f75-2bzn8 from openshift-console started at 2021-06-28 16:43:14 +0000 UTC (1 container statuses recorded)
Jun 28 18:45:30.695: INFO: 	Container download-server ready: true, restart count 0
Jun 28 18:45:30.695: INFO: downloads-65dcc64f75-5hm2s from openshift-console started at 2021-06-28 16:43:14 +0000 UTC (1 container statuses recorded)
Jun 28 18:45:30.695: INFO: 	Container download-server ready: true, restart count 0
Jun 28 18:45:30.695: INFO: dns-operator-7d8cb9bb6d-kkfvf from openshift-dns-operator started at 2021-06-28 16:43:14 +0000 UTC (2 container statuses recorded)
Jun 28 18:45:30.695: INFO: 	Container dns-operator ready: true, restart count 0
Jun 28 18:45:30.695: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 28 18:45:30.695: INFO: dns-default-48b6x from openshift-dns started at 2021-06-28 16:44:04 +0000 UTC (3 container statuses recorded)
Jun 28 18:45:30.695: INFO: 	Container dns ready: true, restart count 0
Jun 28 18:45:30.695: INFO: 	Container dns-node-resolver ready: true, restart count 0
Jun 28 18:45:30.695: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 28 18:45:30.695: INFO: cluster-image-registry-operator-999d7b49c-d458n from openshift-image-registry started at 2021-06-28 16:43:14 +0000 UTC (1 container statuses recorded)
Jun 28 18:45:30.695: INFO: 	Container cluster-image-registry-operator ready: true, restart count 0
Jun 28 18:45:30.695: INFO: node-ca-49d6t from openshift-image-registry started at 2021-06-28 16:44:14 +0000 UTC (1 container statuses recorded)
Jun 28 18:45:30.695: INFO: 	Container node-ca ready: true, restart count 0
Jun 28 18:45:30.695: INFO: ingress-operator-795fb4477f-sgnmp from openshift-ingress-operator started at 2021-06-28 16:43:14 +0000 UTC (2 container statuses recorded)
Jun 28 18:45:30.696: INFO: 	Container ingress-operator ready: true, restart count 0
Jun 28 18:45:30.696: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 28 18:45:30.696: INFO: router-default-8b99f5968-b84k7 from openshift-ingress started at 2021-06-28 18:29:06 +0000 UTC (1 container statuses recorded)
Jun 28 18:45:30.696: INFO: 	Container router ready: true, restart count 0
Jun 28 18:45:30.696: INFO: openshift-kube-proxy-xq2t8 from openshift-kube-proxy started at 2021-06-28 16:42:08 +0000 UTC (1 container statuses recorded)
Jun 28 18:45:30.696: INFO: 	Container kube-proxy ready: true, restart count 0
Jun 28 18:45:30.696: INFO: kube-storage-version-migrator-operator-77d5dd5f6c-z9m4j from openshift-kube-storage-version-migrator-operator started at 2021-06-28 16:43:14 +0000 UTC (1 container statuses recorded)
Jun 28 18:45:30.696: INFO: 	Container kube-storage-version-migrator-operator ready: true, restart count 1
Jun 28 18:45:30.696: INFO: marketplace-operator-7b9d5dcb99-nzgf8 from openshift-marketplace started at 2021-06-28 16:43:14 +0000 UTC (1 container statuses recorded)
Jun 28 18:45:30.696: INFO: 	Container marketplace-operator ready: true, restart count 0
Jun 28 18:45:30.696: INFO: alertmanager-main-1 from openshift-monitoring started at 2021-06-28 16:45:25 +0000 UTC (5 container statuses recorded)
Jun 28 18:45:30.696: INFO: 	Container alertmanager ready: true, restart count 0
Jun 28 18:45:30.696: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Jun 28 18:45:30.696: INFO: 	Container config-reloader ready: true, restart count 0
Jun 28 18:45:30.696: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 28 18:45:30.696: INFO: 	Container prom-label-proxy ready: true, restart count 0
Jun 28 18:45:30.696: INFO: cluster-monitoring-operator-6964464f67-jbfjk from openshift-monitoring started at 2021-06-28 16:43:14 +0000 UTC (2 container statuses recorded)
Jun 28 18:45:30.696: INFO: 	Container cluster-monitoring-operator ready: true, restart count 0
Jun 28 18:45:30.696: INFO: 	Container kube-rbac-proxy ready: true, restart count 2
Jun 28 18:45:30.696: INFO: node-exporter-ld8qc from openshift-monitoring started at 2021-06-28 16:44:29 +0000 UTC (2 container statuses recorded)
Jun 28 18:45:30.696: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 28 18:45:30.697: INFO: 	Container node-exporter ready: true, restart count 0
Jun 28 18:45:30.697: INFO: prometheus-adapter-8646bbc64d-stfnh from openshift-monitoring started at 2021-06-28 18:29:07 +0000 UTC (1 container statuses recorded)
Jun 28 18:45:30.697: INFO: 	Container prometheus-adapter ready: true, restart count 0
Jun 28 18:45:30.697: INFO: prometheus-k8s-0 from openshift-monitoring started at 2021-06-28 18:29:17 +0000 UTC (6 container statuses recorded)
Jun 28 18:45:30.697: INFO: 	Container config-reloader ready: true, restart count 0
Jun 28 18:45:30.697: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 28 18:45:30.697: INFO: 	Container prom-label-proxy ready: true, restart count 0
Jun 28 18:45:30.697: INFO: 	Container prometheus ready: true, restart count 1
Jun 28 18:45:30.697: INFO: 	Container prometheus-proxy ready: true, restart count 0
Jun 28 18:45:30.697: INFO: 	Container thanos-sidecar ready: true, restart count 0
Jun 28 18:45:30.697: INFO: thanos-querier-b46bb8497-dcbxw from openshift-monitoring started at 2021-06-28 18:29:07 +0000 UTC (5 container statuses recorded)
Jun 28 18:45:30.697: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 28 18:45:30.697: INFO: 	Container kube-rbac-proxy-rules ready: true, restart count 0
Jun 28 18:45:30.697: INFO: 	Container oauth-proxy ready: true, restart count 0
Jun 28 18:45:30.697: INFO: 	Container prom-label-proxy ready: true, restart count 0
Jun 28 18:45:30.697: INFO: 	Container thanos-query ready: true, restart count 0
Jun 28 18:45:30.697: INFO: multus-admission-controller-58k5w from openshift-multus started at 2021-06-28 16:43:13 +0000 UTC (2 container statuses recorded)
Jun 28 18:45:30.697: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 28 18:45:30.697: INFO: 	Container multus-admission-controller ready: true, restart count 0
Jun 28 18:45:30.697: INFO: multus-hvm6b from openshift-multus started at 2021-06-28 16:42:08 +0000 UTC (1 container statuses recorded)
Jun 28 18:45:30.698: INFO: 	Container kube-multus ready: true, restart count 0
Jun 28 18:45:30.698: INFO: network-metrics-daemon-78dfb from openshift-multus started at 2021-06-28 16:42:08 +0000 UTC (2 container statuses recorded)
Jun 28 18:45:30.698: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 28 18:45:30.698: INFO: 	Container network-metrics-daemon ready: true, restart count 0
Jun 28 18:45:30.698: INFO: network-operator-5b78d575b8-gvzvg from openshift-network-operator started at 2021-06-28 16:41:35 +0000 UTC (1 container statuses recorded)
Jun 28 18:45:30.698: INFO: 	Container network-operator ready: true, restart count 0
Jun 28 18:45:30.698: INFO: catalog-operator-67c774db7d-5ps99 from openshift-operator-lifecycle-manager started at 2021-06-28 16:43:14 +0000 UTC (1 container statuses recorded)
Jun 28 18:45:30.698: INFO: 	Container catalog-operator ready: true, restart count 0
Jun 28 18:45:30.698: INFO: olm-operator-68fcf6954f-nfsgq from openshift-operator-lifecycle-manager started at 2021-06-28 16:43:14 +0000 UTC (1 container statuses recorded)
Jun 28 18:45:30.698: INFO: 	Container olm-operator ready: true, restart count 0
Jun 28 18:45:30.698: INFO: packageserver-65544748bd-hhnld from openshift-operator-lifecycle-manager started at 2021-06-28 18:29:07 +0000 UTC (1 container statuses recorded)
Jun 28 18:45:30.698: INFO: 	Container packageserver ready: true, restart count 0
Jun 28 18:45:30.698: INFO: metrics-7df79584fc-f6g2g from openshift-roks-metrics started at 2021-06-28 16:43:14 +0000 UTC (1 container statuses recorded)
Jun 28 18:45:30.698: INFO: 	Container metrics ready: true, restart count 1
Jun 28 18:45:30.698: INFO: push-gateway-6dc86bc94f-dz7nt from openshift-roks-metrics started at 2021-06-28 16:43:14 +0000 UTC (1 container statuses recorded)
Jun 28 18:45:30.698: INFO: 	Container push-gateway ready: true, restart count 0
Jun 28 18:45:30.699: INFO: service-ca-operator-d4bfd498b-4xhnv from openshift-service-ca-operator started at 2021-06-28 16:43:14 +0000 UTC (1 container statuses recorded)
Jun 28 18:45:30.699: INFO: 	Container service-ca-operator ready: true, restart count 1
Jun 28 18:45:30.699: INFO: service-ca-665fb97685-mn77q from openshift-service-ca started at 2021-06-28 18:29:06 +0000 UTC (1 container statuses recorded)
Jun 28 18:45:30.699: INFO: 	Container service-ca-controller ready: true, restart count 0
Jun 28 18:45:30.699: INFO: sonobuoy from sonobuoy started at 2021-06-28 18:11:40 +0000 UTC (1 container statuses recorded)
Jun 28 18:45:30.699: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jun 28 18:45:30.699: INFO: sonobuoy-systemd-logs-daemon-set-bc3c64b0befe49cb-s48t5 from sonobuoy started at 2021-06-28 18:11:47 +0000 UTC (2 container statuses recorded)
Jun 28 18:45:30.699: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 28 18:45:30.699: INFO: 	Container systemd-logs ready: true, restart count 0
Jun 28 18:45:30.699: INFO: tigera-operator-64c8f4c7d7-hzwkg from tigera-operator started at 2021-06-28 18:29:06 +0000 UTC (1 container statuses recorded)
Jun 28 18:45:30.699: INFO: 	Container tigera-operator ready: true, restart count 0
Jun 28 18:45:30.699: INFO: 
Logging pods the apiserver thinks is on node 10.13.107.60 before test
Jun 28 18:45:30.775: INFO: calico-kube-controllers-57df785794-xp76v from calico-system started at 2021-06-28 18:29:06 +0000 UTC (1 container statuses recorded)
Jun 28 18:45:30.775: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Jun 28 18:45:30.775: INFO: calico-node-vr9g2 from calico-system started at 2021-06-28 16:43:04 +0000 UTC (1 container statuses recorded)
Jun 28 18:45:30.775: INFO: 	Container calico-node ready: true, restart count 0
Jun 28 18:45:30.775: INFO: calico-typha-649969bb55-mc6s8 from calico-system started at 2021-06-28 16:45:02 +0000 UTC (1 container statuses recorded)
Jun 28 18:45:30.775: INFO: 	Container calico-typha ready: true, restart count 0
Jun 28 18:45:30.775: INFO: test-k8s-e2e-pvg-master-verification from default started at 2021-06-28 16:51:21 +0000 UTC (1 container statuses recorded)
Jun 28 18:45:30.775: INFO: 	Container test-k8s-e2e-pvg-master-verification ready: true, restart count 0
Jun 28 18:45:30.775: INFO: ibm-cloud-provider-ip-149-81-71-226-77554565fb-djwh7 from ibm-system started at 2021-06-28 18:29:06 +0000 UTC (1 container statuses recorded)
Jun 28 18:45:30.775: INFO: 	Container ibm-cloud-provider-ip-149-81-71-226 ready: true, restart count 0
Jun 28 18:45:30.775: INFO: ibm-keepalived-watcher-w6fhq from kube-system started at 2021-06-28 16:42:37 +0000 UTC (1 container statuses recorded)
Jun 28 18:45:30.776: INFO: 	Container keepalived-watcher ready: true, restart count 0
Jun 28 18:45:30.776: INFO: ibm-master-proxy-static-10.13.107.60 from kube-system started at 2021-06-28 16:41:49 +0000 UTC (2 container statuses recorded)
Jun 28 18:45:30.776: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Jun 28 18:45:30.776: INFO: 	Container pause ready: true, restart count 0
Jun 28 18:45:30.776: INFO: ibmcloud-block-storage-driver-kwxt2 from kube-system started at 2021-06-28 16:42:37 +0000 UTC (1 container statuses recorded)
Jun 28 18:45:30.776: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Jun 28 18:45:30.776: INFO: tuned-djhn5 from openshift-cluster-node-tuning-operator started at 2021-06-28 16:43:24 +0000 UTC (1 container statuses recorded)
Jun 28 18:45:30.776: INFO: 	Container tuned ready: true, restart count 0
Jun 28 18:45:30.776: INFO: console-74dd6cb4c6-gbthm from openshift-console started at 2021-06-28 17:20:10 +0000 UTC (1 container statuses recorded)
Jun 28 18:45:30.776: INFO: 	Container console ready: true, restart count 0
Jun 28 18:45:30.776: INFO: dns-default-84l5j from openshift-dns started at 2021-06-28 16:44:04 +0000 UTC (3 container statuses recorded)
Jun 28 18:45:30.776: INFO: 	Container dns ready: true, restart count 0
Jun 28 18:45:30.776: INFO: 	Container dns-node-resolver ready: true, restart count 0
Jun 28 18:45:30.776: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 28 18:45:30.776: INFO: image-registry-7f9574d5c6-tmmns from openshift-image-registry started at 2021-06-28 16:46:50 +0000 UTC (1 container statuses recorded)
Jun 28 18:45:30.776: INFO: 	Container registry ready: true, restart count 0
Jun 28 18:45:30.776: INFO: node-ca-42txp from openshift-image-registry started at 2021-06-28 16:44:14 +0000 UTC (1 container statuses recorded)
Jun 28 18:45:30.776: INFO: 	Container node-ca ready: true, restart count 0
Jun 28 18:45:30.776: INFO: router-default-8b99f5968-8nzw5 from openshift-ingress started at 2021-06-28 17:18:25 +0000 UTC (1 container statuses recorded)
Jun 28 18:45:30.776: INFO: 	Container router ready: true, restart count 0
Jun 28 18:45:30.776: INFO: openshift-kube-proxy-n6pzf from openshift-kube-proxy started at 2021-06-28 16:42:37 +0000 UTC (1 container statuses recorded)
Jun 28 18:45:30.776: INFO: 	Container kube-proxy ready: true, restart count 0
Jun 28 18:45:30.776: INFO: migrator-858cfc6f4c-tvj6h from openshift-kube-storage-version-migrator started at 2021-06-28 18:29:06 +0000 UTC (1 container statuses recorded)
Jun 28 18:45:30.776: INFO: 	Container migrator ready: true, restart count 0
Jun 28 18:45:30.776: INFO: certified-operators-d5d5w from openshift-marketplace started at 2021-06-28 16:44:19 +0000 UTC (1 container statuses recorded)
Jun 28 18:45:30.776: INFO: 	Container registry-server ready: true, restart count 0
Jun 28 18:45:30.776: INFO: community-operators-dbmgh from openshift-marketplace started at 2021-06-28 17:01:52 +0000 UTC (1 container statuses recorded)
Jun 28 18:45:30.777: INFO: 	Container registry-server ready: true, restart count 0
Jun 28 18:45:30.777: INFO: redhat-marketplace-4t4vz from openshift-marketplace started at 2021-06-28 16:44:21 +0000 UTC (1 container statuses recorded)
Jun 28 18:45:30.777: INFO: 	Container registry-server ready: true, restart count 0
Jun 28 18:45:30.777: INFO: redhat-operators-dvpnz from openshift-marketplace started at 2021-06-28 16:44:19 +0000 UTC (1 container statuses recorded)
Jun 28 18:45:30.777: INFO: 	Container registry-server ready: true, restart count 0
Jun 28 18:45:30.777: INFO: alertmanager-main-0 from openshift-monitoring started at 2021-06-28 16:45:25 +0000 UTC (5 container statuses recorded)
Jun 28 18:45:30.777: INFO: 	Container alertmanager ready: true, restart count 0
Jun 28 18:45:30.777: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Jun 28 18:45:30.777: INFO: 	Container config-reloader ready: true, restart count 0
Jun 28 18:45:30.777: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 28 18:45:30.777: INFO: 	Container prom-label-proxy ready: true, restart count 0
Jun 28 18:45:30.777: INFO: alertmanager-main-2 from openshift-monitoring started at 2021-06-28 18:29:17 +0000 UTC (5 container statuses recorded)
Jun 28 18:45:30.777: INFO: 	Container alertmanager ready: true, restart count 0
Jun 28 18:45:30.777: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Jun 28 18:45:30.777: INFO: 	Container config-reloader ready: true, restart count 0
Jun 28 18:45:30.777: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 28 18:45:30.777: INFO: 	Container prom-label-proxy ready: true, restart count 0
Jun 28 18:45:30.777: INFO: grafana-85d4b8dbd-jn67b from openshift-monitoring started at 2021-06-28 16:44:32 +0000 UTC (2 container statuses recorded)
Jun 28 18:45:30.777: INFO: 	Container grafana ready: true, restart count 0
Jun 28 18:45:30.777: INFO: 	Container grafana-proxy ready: true, restart count 0
Jun 28 18:45:30.777: INFO: kube-state-metrics-659c7b865d-974cs from openshift-monitoring started at 2021-06-28 16:44:29 +0000 UTC (3 container statuses recorded)
Jun 28 18:45:30.777: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Jun 28 18:45:30.777: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Jun 28 18:45:30.777: INFO: 	Container kube-state-metrics ready: true, restart count 0
Jun 28 18:45:30.777: INFO: node-exporter-6s54d from openshift-monitoring started at 2021-06-28 16:44:29 +0000 UTC (2 container statuses recorded)
Jun 28 18:45:30.777: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 28 18:45:30.777: INFO: 	Container node-exporter ready: true, restart count 0
Jun 28 18:45:30.777: INFO: openshift-state-metrics-7cf4dc694b-d9w6x from openshift-monitoring started at 2021-06-28 16:44:29 +0000 UTC (3 container statuses recorded)
Jun 28 18:45:30.777: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Jun 28 18:45:30.778: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Jun 28 18:45:30.778: INFO: 	Container openshift-state-metrics ready: true, restart count 0
Jun 28 18:45:30.778: INFO: prometheus-adapter-8646bbc64d-nq9hj from openshift-monitoring started at 2021-06-28 16:44:29 +0000 UTC (1 container statuses recorded)
Jun 28 18:45:30.778: INFO: 	Container prometheus-adapter ready: true, restart count 0
Jun 28 18:45:30.778: INFO: prometheus-k8s-1 from openshift-monitoring started at 2021-06-28 16:45:26 +0000 UTC (6 container statuses recorded)
Jun 28 18:45:30.778: INFO: 	Container config-reloader ready: true, restart count 0
Jun 28 18:45:30.778: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 28 18:45:30.778: INFO: 	Container prom-label-proxy ready: true, restart count 0
Jun 28 18:45:30.778: INFO: 	Container prometheus ready: true, restart count 1
Jun 28 18:45:30.778: INFO: 	Container prometheus-proxy ready: true, restart count 0
Jun 28 18:45:30.778: INFO: 	Container thanos-sidecar ready: true, restart count 0
Jun 28 18:45:30.778: INFO: prometheus-operator-5d48db6d9c-z9sxl from openshift-monitoring started at 2021-06-28 16:44:29 +0000 UTC (2 container statuses recorded)
Jun 28 18:45:30.778: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 28 18:45:30.778: INFO: 	Container prometheus-operator ready: true, restart count 0
Jun 28 18:45:30.778: INFO: telemeter-client-5584d54f85-bhqql from openshift-monitoring started at 2021-06-28 16:44:29 +0000 UTC (3 container statuses recorded)
Jun 28 18:45:30.778: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 28 18:45:30.778: INFO: 	Container reload ready: true, restart count 0
Jun 28 18:45:30.778: INFO: 	Container telemeter-client ready: true, restart count 0
Jun 28 18:45:30.778: INFO: thanos-querier-b46bb8497-wxrxb from openshift-monitoring started at 2021-06-28 16:44:31 +0000 UTC (5 container statuses recorded)
Jun 28 18:45:30.778: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 28 18:45:30.778: INFO: 	Container kube-rbac-proxy-rules ready: true, restart count 0
Jun 28 18:45:30.778: INFO: 	Container oauth-proxy ready: true, restart count 0
Jun 28 18:45:30.778: INFO: 	Container prom-label-proxy ready: true, restart count 0
Jun 28 18:45:30.778: INFO: 	Container thanos-query ready: true, restart count 0
Jun 28 18:45:30.778: INFO: multus-admission-controller-5rwk7 from openshift-multus started at 2021-06-28 16:43:56 +0000 UTC (2 container statuses recorded)
Jun 28 18:45:30.778: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 28 18:45:30.778: INFO: 	Container multus-admission-controller ready: true, restart count 0
Jun 28 18:45:30.778: INFO: multus-tsj42 from openshift-multus started at 2021-06-28 16:42:37 +0000 UTC (1 container statuses recorded)
Jun 28 18:45:30.778: INFO: 	Container kube-multus ready: true, restart count 0
Jun 28 18:45:30.779: INFO: network-metrics-daemon-gzxgq from openshift-multus started at 2021-06-28 16:42:37 +0000 UTC (2 container statuses recorded)
Jun 28 18:45:30.779: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 28 18:45:30.779: INFO: 	Container network-metrics-daemon ready: true, restart count 0
Jun 28 18:45:30.779: INFO: packageserver-65544748bd-h6vfz from openshift-operator-lifecycle-manager started at 2021-06-28 16:44:29 +0000 UTC (1 container statuses recorded)
Jun 28 18:45:30.779: INFO: 	Container packageserver ready: true, restart count 0
Jun 28 18:45:30.779: INFO: sonobuoy-e2e-job-3182b6be48fa4a64 from sonobuoy started at 2021-06-28 18:11:47 +0000 UTC (2 container statuses recorded)
Jun 28 18:45:30.779: INFO: 	Container e2e ready: true, restart count 0
Jun 28 18:45:30.779: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 28 18:45:30.779: INFO: sonobuoy-systemd-logs-daemon-set-bc3c64b0befe49cb-47fm7 from sonobuoy started at 2021-06-28 18:11:47 +0000 UTC (2 container statuses recorded)
Jun 28 18:45:30.779: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 28 18:45:30.779: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-87fa6d91-a82e-42c1-bbe4-fa6713a1545d 42
STEP: Trying to relaunch the pod, now with labels.
STEP: removing the label kubernetes.io/e2e-87fa6d91-a82e-42c1-bbe4-fa6713a1545d off the node 10.13.107.37
STEP: verifying the node doesn't have the label kubernetes.io/e2e-87fa6d91-a82e-42c1-bbe4-fa6713a1545d
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 18:45:37.094: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-3253" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81

• [SLOW TEST:6.721 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if matching  [Conformance]","total":305,"completed":114,"skipped":1828,"failed":0}
SSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 18:45:37.128: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap that has name configmap-test-emptyKey-313b6e5f-420e-4a12-959e-d119d2cfc6ef
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 18:45:37.268: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9596" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should fail to create ConfigMap with empty key [Conformance]","total":305,"completed":115,"skipped":1836,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 18:45:37.322: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod busybox-36034059-42f4-4833-b5fc-b1d1eb6569cd in namespace container-probe-4934
Jun 28 18:45:39.520: INFO: Started pod busybox-36034059-42f4-4833-b5fc-b1d1eb6569cd in namespace container-probe-4934
STEP: checking the pod's current state and verifying that restartCount is present
Jun 28 18:45:39.531: INFO: Initial restart count of pod busybox-36034059-42f4-4833-b5fc-b1d1eb6569cd is 0
Jun 28 18:46:25.919: INFO: Restart count of pod container-probe-4934/busybox-36034059-42f4-4833-b5fc-b1d1eb6569cd is now 1 (46.388719564s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 18:46:25.950: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-4934" for this suite.

• [SLOW TEST:48.676 seconds]
[k8s.io] Probing container
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Probing container should be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","total":305,"completed":116,"skipped":1872,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [sig-storage][Slow] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 18:46:25.999: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify that a failing subpath expansion can be modified during the lifecycle of a container [sig-storage][Slow] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod with failed condition
STEP: updating the pod
Jun 28 18:48:26.762: INFO: Successfully updated pod "var-expansion-ef059880-de90-4da3-8bc3-ad0ea9c559af"
STEP: waiting for pod running
STEP: deleting the pod gracefully
Jun 28 18:48:28.790: INFO: Deleting pod "var-expansion-ef059880-de90-4da3-8bc3-ad0ea9c559af" in namespace "var-expansion-5819"
Jun 28 18:48:28.811: INFO: Wait up to 5m0s for pod "var-expansion-ef059880-de90-4da3-8bc3-ad0ea9c559af" to be fully deleted
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 18:49:02.843: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-5819" for this suite.

• [SLOW TEST:156.883 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [sig-storage][Slow] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Variable Expansion should verify that a failing subpath expansion can be modified during the lifecycle of a container [sig-storage][Slow] [Conformance]","total":305,"completed":117,"skipped":1895,"failed":0}
SSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 18:49:02.884: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:134
[It] should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jun 28 18:49:03.099: INFO: Create a RollingUpdate DaemonSet
Jun 28 18:49:03.116: INFO: Check that daemon pods launch on every node of the cluster
Jun 28 18:49:03.144: INFO: Number of nodes with available pods: 0
Jun 28 18:49:03.144: INFO: Node 10.13.107.37 is running more than one daemon pod
Jun 28 18:49:04.175: INFO: Number of nodes with available pods: 0
Jun 28 18:49:04.175: INFO: Node 10.13.107.37 is running more than one daemon pod
Jun 28 18:49:05.175: INFO: Number of nodes with available pods: 1
Jun 28 18:49:05.175: INFO: Node 10.13.107.57 is running more than one daemon pod
Jun 28 18:49:06.173: INFO: Number of nodes with available pods: 3
Jun 28 18:49:06.173: INFO: Number of running nodes: 3, number of available pods: 3
Jun 28 18:49:06.173: INFO: Update the DaemonSet to trigger a rollout
Jun 28 18:49:06.237: INFO: Updating DaemonSet daemon-set
Jun 28 18:49:15.293: INFO: Roll back the DaemonSet before rollout is complete
Jun 28 18:49:15.325: INFO: Updating DaemonSet daemon-set
Jun 28 18:49:15.325: INFO: Make sure DaemonSet rollback is complete
Jun 28 18:49:15.340: INFO: Wrong image for pod: daemon-set-fdrvz. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Jun 28 18:49:15.340: INFO: Pod daemon-set-fdrvz is not available
Jun 28 18:49:16.382: INFO: Wrong image for pod: daemon-set-fdrvz. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Jun 28 18:49:16.382: INFO: Pod daemon-set-fdrvz is not available
Jun 28 18:49:17.384: INFO: Wrong image for pod: daemon-set-fdrvz. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Jun 28 18:49:17.385: INFO: Pod daemon-set-fdrvz is not available
Jun 28 18:49:18.380: INFO: Pod daemon-set-zmsg5 is not available
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:100
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-6424, will wait for the garbage collector to delete the pods
Jun 28 18:49:18.491: INFO: Deleting DaemonSet.extensions daemon-set took: 20.449237ms
Jun 28 18:49:18.595: INFO: Terminating DaemonSet.extensions daemon-set pods took: 103.388874ms
Jun 28 18:49:27.917: INFO: Number of nodes with available pods: 0
Jun 28 18:49:27.917: INFO: Number of running nodes: 0, number of available pods: 0
Jun 28 18:49:27.927: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-6424/daemonsets","resourceVersion":"74324"},"items":null}

Jun 28 18:49:27.937: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-6424/pods","resourceVersion":"74324"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 18:49:27.987: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-6424" for this suite.

• [SLOW TEST:25.136 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","total":305,"completed":118,"skipped":1899,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 18:49:28.022: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating projection with secret that has name projected-secret-test-map-91e32e47-6ba7-4d07-bcd3-29332717eb2e
STEP: Creating a pod to test consume secrets
Jun 28 18:49:28.342: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-321888ca-333c-46d8-ab66-3b27247a40fb" in namespace "projected-6106" to be "Succeeded or Failed"
Jun 28 18:49:28.354: INFO: Pod "pod-projected-secrets-321888ca-333c-46d8-ab66-3b27247a40fb": Phase="Pending", Reason="", readiness=false. Elapsed: 9.416107ms
Jun 28 18:49:30.370: INFO: Pod "pod-projected-secrets-321888ca-333c-46d8-ab66-3b27247a40fb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024791222s
Jun 28 18:49:32.394: INFO: Pod "pod-projected-secrets-321888ca-333c-46d8-ab66-3b27247a40fb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.048920918s
STEP: Saw pod success
Jun 28 18:49:32.395: INFO: Pod "pod-projected-secrets-321888ca-333c-46d8-ab66-3b27247a40fb" satisfied condition "Succeeded or Failed"
Jun 28 18:49:32.405: INFO: Trying to get logs from node 10.13.107.37 pod pod-projected-secrets-321888ca-333c-46d8-ab66-3b27247a40fb container projected-secret-volume-test: <nil>
STEP: delete the pod
Jun 28 18:49:32.508: INFO: Waiting for pod pod-projected-secrets-321888ca-333c-46d8-ab66-3b27247a40fb to disappear
Jun 28 18:49:32.519: INFO: Pod pod-projected-secrets-321888ca-333c-46d8-ab66-3b27247a40fb no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 18:49:32.519: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6106" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":305,"completed":119,"skipped":1928,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 18:49:32.548: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs
STEP: Gathering metrics
W0628 18:49:33.458975      22 metrics_grabber.go:98] Can't find kube-scheduler pod. Grabbing metrics from kube-scheduler is disabled.
W0628 18:49:33.459392      22 metrics_grabber.go:102] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
W0628 18:49:33.459652      22 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
Jun 28 18:49:33.459: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 18:49:33.460: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-7954" for this suite.
•{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]","total":305,"completed":120,"skipped":1934,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 18:49:33.502: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0666 on node default medium
Jun 28 18:49:33.690: INFO: Waiting up to 5m0s for pod "pod-fdee58ea-ceb0-4400-964b-4b62af91624c" in namespace "emptydir-5566" to be "Succeeded or Failed"
Jun 28 18:49:33.708: INFO: Pod "pod-fdee58ea-ceb0-4400-964b-4b62af91624c": Phase="Pending", Reason="", readiness=false. Elapsed: 17.551496ms
Jun 28 18:49:35.719: INFO: Pod "pod-fdee58ea-ceb0-4400-964b-4b62af91624c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.028960337s
STEP: Saw pod success
Jun 28 18:49:35.719: INFO: Pod "pod-fdee58ea-ceb0-4400-964b-4b62af91624c" satisfied condition "Succeeded or Failed"
Jun 28 18:49:35.732: INFO: Trying to get logs from node 10.13.107.37 pod pod-fdee58ea-ceb0-4400-964b-4b62af91624c container test-container: <nil>
STEP: delete the pod
Jun 28 18:49:35.800: INFO: Waiting for pod pod-fdee58ea-ceb0-4400-964b-4b62af91624c to disappear
Jun 28 18:49:35.809: INFO: Pod pod-fdee58ea-ceb0-4400-964b-4b62af91624c no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 18:49:35.809: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5566" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":121,"skipped":1948,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 18:49:35.842: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating secret with name secret-test-16acb1ef-9dd5-4878-824d-d401958b6d64
STEP: Creating a pod to test consume secrets
Jun 28 18:49:36.043: INFO: Waiting up to 5m0s for pod "pod-secrets-fda85149-bd57-4c53-8e9d-7df337e2dea9" in namespace "secrets-4369" to be "Succeeded or Failed"
Jun 28 18:49:36.051: INFO: Pod "pod-secrets-fda85149-bd57-4c53-8e9d-7df337e2dea9": Phase="Pending", Reason="", readiness=false. Elapsed: 8.728184ms
Jun 28 18:49:38.064: INFO: Pod "pod-secrets-fda85149-bd57-4c53-8e9d-7df337e2dea9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.021127737s
STEP: Saw pod success
Jun 28 18:49:38.064: INFO: Pod "pod-secrets-fda85149-bd57-4c53-8e9d-7df337e2dea9" satisfied condition "Succeeded or Failed"
Jun 28 18:49:38.077: INFO: Trying to get logs from node 10.13.107.37 pod pod-secrets-fda85149-bd57-4c53-8e9d-7df337e2dea9 container secret-env-test: <nil>
STEP: delete the pod
Jun 28 18:49:38.130: INFO: Waiting for pod pod-secrets-fda85149-bd57-4c53-8e9d-7df337e2dea9 to disappear
Jun 28 18:49:38.140: INFO: Pod pod-secrets-fda85149-bd57-4c53-8e9d-7df337e2dea9 no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 18:49:38.140: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4369" for this suite.
•{"msg":"PASSED [sig-api-machinery] Secrets should be consumable from pods in env vars [NodeConformance] [Conformance]","total":305,"completed":122,"skipped":1961,"failed":0}
SSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 18:49:38.178: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Jun 28 18:49:38.470: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d5403fe1-9d9a-4afe-9ccf-fc0e0bf6ba35" in namespace "downward-api-4185" to be "Succeeded or Failed"
Jun 28 18:49:38.479: INFO: Pod "downwardapi-volume-d5403fe1-9d9a-4afe-9ccf-fc0e0bf6ba35": Phase="Pending", Reason="", readiness=false. Elapsed: 9.191744ms
Jun 28 18:49:40.490: INFO: Pod "downwardapi-volume-d5403fe1-9d9a-4afe-9ccf-fc0e0bf6ba35": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01970715s
Jun 28 18:49:42.502: INFO: Pod "downwardapi-volume-d5403fe1-9d9a-4afe-9ccf-fc0e0bf6ba35": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.03179569s
STEP: Saw pod success
Jun 28 18:49:42.502: INFO: Pod "downwardapi-volume-d5403fe1-9d9a-4afe-9ccf-fc0e0bf6ba35" satisfied condition "Succeeded or Failed"
Jun 28 18:49:42.512: INFO: Trying to get logs from node 10.13.107.37 pod downwardapi-volume-d5403fe1-9d9a-4afe-9ccf-fc0e0bf6ba35 container client-container: <nil>
STEP: delete the pod
Jun 28 18:49:42.584: INFO: Waiting for pod downwardapi-volume-d5403fe1-9d9a-4afe-9ccf-fc0e0bf6ba35 to disappear
Jun 28 18:49:42.593: INFO: Pod downwardapi-volume-d5403fe1-9d9a-4afe-9ccf-fc0e0bf6ba35 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 18:49:42.594: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4185" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory request [NodeConformance] [Conformance]","total":305,"completed":123,"skipped":1967,"failed":0}
SSSS
------------------------------
[sig-api-machinery] server version 
  should find the server version [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] server version
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 18:49:42.632: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename server-version
STEP: Waiting for a default service account to be provisioned in namespace
[It] should find the server version [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Request ServerVersion
STEP: Confirm major version
Jun 28 18:49:42.771: INFO: Major version: 1
STEP: Confirm minor version
Jun 28 18:49:42.771: INFO: cleanMinorVersion: 19
Jun 28 18:49:42.771: INFO: Minor version: 19
[AfterEach] [sig-api-machinery] server version
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 18:49:42.771: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "server-version-9927" for this suite.
•{"msg":"PASSED [sig-api-machinery] server version should find the server version [Conformance]","total":305,"completed":124,"skipped":1971,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should support configurable pod DNS nameservers [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 18:49:42.804: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support configurable pod DNS nameservers [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod with dnsPolicy=None and customized dnsConfig...
Jun 28 18:49:42.970: INFO: Created pod &Pod{ObjectMeta:{dns-4023  dns-4023 /api/v1/namespaces/dns-4023/pods/dns-4023 1f6271a0-0e77-481f-a23c-59c8a67db72e 74914 0 2021-06-28 18:49:42 +0000 UTC <nil> <nil> map[] map[openshift.io/scc:anyuid] [] []  [{e2e.test Update v1 2021-06-28 18:49:42 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-6klws,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-6klws,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:k8s.gcr.io/e2e-test-images/agnhost:2.20,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-6klws,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c46,c40,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 28 18:49:42.981: INFO: The status of Pod dns-4023 is Pending, waiting for it to be Running (with Ready = true)
Jun 28 18:49:44.996: INFO: The status of Pod dns-4023 is Running (Ready = true)
STEP: Verifying customized DNS suffix list is configured on pod...
Jun 28 18:49:44.997: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-4023 PodName:dns-4023 ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 28 18:49:44.997: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Verifying customized DNS server is configured on pod...
Jun 28 18:49:45.217: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-4023 PodName:dns-4023 ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 28 18:49:45.217: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
Jun 28 18:49:45.428: INFO: Deleting pod dns-4023...
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 18:49:45.460: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-4023" for this suite.
•{"msg":"PASSED [sig-network] DNS should support configurable pod DNS nameservers [Conformance]","total":305,"completed":125,"skipped":1988,"failed":0}

------------------------------
[sig-storage] Downward API volume 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 18:49:45.493: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Jun 28 18:49:45.668: INFO: Waiting up to 5m0s for pod "downwardapi-volume-4263730e-9e65-4ad7-ba41-9d221ae24bcd" in namespace "downward-api-166" to be "Succeeded or Failed"
Jun 28 18:49:45.683: INFO: Pod "downwardapi-volume-4263730e-9e65-4ad7-ba41-9d221ae24bcd": Phase="Pending", Reason="", readiness=false. Elapsed: 13.858022ms
Jun 28 18:49:47.697: INFO: Pod "downwardapi-volume-4263730e-9e65-4ad7-ba41-9d221ae24bcd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.027677481s
STEP: Saw pod success
Jun 28 18:49:47.697: INFO: Pod "downwardapi-volume-4263730e-9e65-4ad7-ba41-9d221ae24bcd" satisfied condition "Succeeded or Failed"
Jun 28 18:49:47.707: INFO: Trying to get logs from node 10.13.107.37 pod downwardapi-volume-4263730e-9e65-4ad7-ba41-9d221ae24bcd container client-container: <nil>
STEP: delete the pod
Jun 28 18:49:47.768: INFO: Waiting for pod downwardapi-volume-4263730e-9e65-4ad7-ba41-9d221ae24bcd to disappear
Jun 28 18:49:47.780: INFO: Pod downwardapi-volume-4263730e-9e65-4ad7-ba41-9d221ae24bcd no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 18:49:47.780: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-166" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory limit [NodeConformance] [Conformance]","total":305,"completed":126,"skipped":1988,"failed":0}
SSSSSSSS
------------------------------
[sig-network] Services 
  should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 18:49:47.814: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating service in namespace services-5499
Jun 28 18:49:50.026: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-108209140 exec --namespace=services-5499 kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
Jun 28 18:49:50.458: INFO: rc: 7
Jun 28 18:49:50.481: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Jun 28 18:49:50.492: INFO: Pod kube-proxy-mode-detector still exists
Jun 28 18:49:52.493: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Jun 28 18:49:52.505: INFO: Pod kube-proxy-mode-detector still exists
Jun 28 18:49:54.493: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Jun 28 18:49:54.503: INFO: Pod kube-proxy-mode-detector still exists
Jun 28 18:49:56.493: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Jun 28 18:49:56.507: INFO: Pod kube-proxy-mode-detector still exists
Jun 28 18:49:58.493: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Jun 28 18:49:58.512: INFO: Pod kube-proxy-mode-detector no longer exists
Jun 28 18:49:58.512: INFO: Couldn't detect KubeProxy mode - test failure may be expected: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-108209140 exec --namespace=services-5499 kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode:
Command stdout:

stderr:
+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode
command terminated with exit code 7

error:
exit status 7
STEP: creating service affinity-clusterip-timeout in namespace services-5499
STEP: creating replication controller affinity-clusterip-timeout in namespace services-5499
I0628 18:49:58.662914      22 runners.go:190] Created replication controller with name: affinity-clusterip-timeout, namespace: services-5499, replica count: 3
I0628 18:50:01.713425      22 runners.go:190] affinity-clusterip-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jun 28 18:50:01.740: INFO: Creating new exec pod
Jun 28 18:50:04.811: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-108209140 exec --namespace=services-5499 execpod-affinitypn52m -- /bin/sh -x -c nc -zv -t -w 2 affinity-clusterip-timeout 80'
Jun 28 18:50:05.354: INFO: stderr: "+ nc -zv -t -w 2 affinity-clusterip-timeout 80\nConnection to affinity-clusterip-timeout 80 port [tcp/http] succeeded!\n"
Jun 28 18:50:05.354: INFO: stdout: ""
Jun 28 18:50:05.355: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-108209140 exec --namespace=services-5499 execpod-affinitypn52m -- /bin/sh -x -c nc -zv -t -w 2 172.21.32.132 80'
Jun 28 18:50:05.773: INFO: stderr: "+ nc -zv -t -w 2 172.21.32.132 80\nConnection to 172.21.32.132 80 port [tcp/http] succeeded!\n"
Jun 28 18:50:05.773: INFO: stdout: ""
Jun 28 18:50:05.773: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-108209140 exec --namespace=services-5499 execpod-affinitypn52m -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.21.32.132:80/ ; done'
Jun 28 18:50:06.253: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.32.132:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.32.132:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.32.132:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.32.132:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.32.132:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.32.132:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.32.132:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.32.132:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.32.132:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.32.132:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.32.132:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.32.132:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.32.132:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.32.132:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.32.132:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.32.132:80/\n"
Jun 28 18:50:06.253: INFO: stdout: "\naffinity-clusterip-timeout-xxtb4\naffinity-clusterip-timeout-xxtb4\naffinity-clusterip-timeout-xxtb4\naffinity-clusterip-timeout-xxtb4\naffinity-clusterip-timeout-xxtb4\naffinity-clusterip-timeout-xxtb4\naffinity-clusterip-timeout-xxtb4\naffinity-clusterip-timeout-xxtb4\naffinity-clusterip-timeout-xxtb4\naffinity-clusterip-timeout-xxtb4\naffinity-clusterip-timeout-xxtb4\naffinity-clusterip-timeout-xxtb4\naffinity-clusterip-timeout-xxtb4\naffinity-clusterip-timeout-xxtb4\naffinity-clusterip-timeout-xxtb4\naffinity-clusterip-timeout-xxtb4"
Jun 28 18:50:06.253: INFO: Received response from host: affinity-clusterip-timeout-xxtb4
Jun 28 18:50:06.253: INFO: Received response from host: affinity-clusterip-timeout-xxtb4
Jun 28 18:50:06.253: INFO: Received response from host: affinity-clusterip-timeout-xxtb4
Jun 28 18:50:06.253: INFO: Received response from host: affinity-clusterip-timeout-xxtb4
Jun 28 18:50:06.253: INFO: Received response from host: affinity-clusterip-timeout-xxtb4
Jun 28 18:50:06.253: INFO: Received response from host: affinity-clusterip-timeout-xxtb4
Jun 28 18:50:06.253: INFO: Received response from host: affinity-clusterip-timeout-xxtb4
Jun 28 18:50:06.253: INFO: Received response from host: affinity-clusterip-timeout-xxtb4
Jun 28 18:50:06.253: INFO: Received response from host: affinity-clusterip-timeout-xxtb4
Jun 28 18:50:06.253: INFO: Received response from host: affinity-clusterip-timeout-xxtb4
Jun 28 18:50:06.253: INFO: Received response from host: affinity-clusterip-timeout-xxtb4
Jun 28 18:50:06.253: INFO: Received response from host: affinity-clusterip-timeout-xxtb4
Jun 28 18:50:06.254: INFO: Received response from host: affinity-clusterip-timeout-xxtb4
Jun 28 18:50:06.254: INFO: Received response from host: affinity-clusterip-timeout-xxtb4
Jun 28 18:50:06.254: INFO: Received response from host: affinity-clusterip-timeout-xxtb4
Jun 28 18:50:06.254: INFO: Received response from host: affinity-clusterip-timeout-xxtb4
Jun 28 18:50:06.254: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-108209140 exec --namespace=services-5499 execpod-affinitypn52m -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://172.21.32.132:80/'
Jun 28 18:50:06.754: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://172.21.32.132:80/\n"
Jun 28 18:50:06.754: INFO: stdout: "affinity-clusterip-timeout-xxtb4"
Jun 28 18:50:21.754: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-108209140 exec --namespace=services-5499 execpod-affinitypn52m -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://172.21.32.132:80/'
Jun 28 18:50:22.266: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://172.21.32.132:80/\n"
Jun 28 18:50:22.266: INFO: stdout: "affinity-clusterip-timeout-97tbs"
Jun 28 18:50:22.266: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-timeout in namespace services-5499, will wait for the garbage collector to delete the pods
Jun 28 18:50:22.386: INFO: Deleting ReplicationController affinity-clusterip-timeout took: 18.13344ms
Jun 28 18:50:22.486: INFO: Terminating ReplicationController affinity-clusterip-timeout pods took: 100.407633ms
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 18:50:37.949: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-5499" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

• [SLOW TEST:50.172 seconds]
[sig-network] Services
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]","total":305,"completed":127,"skipped":1996,"failed":0}
SSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 18:50:37.986: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod pod-subpath-test-secret-4tlb
STEP: Creating a pod to test atomic-volume-subpath
Jun 28 18:50:38.202: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-4tlb" in namespace "subpath-5092" to be "Succeeded or Failed"
Jun 28 18:50:38.215: INFO: Pod "pod-subpath-test-secret-4tlb": Phase="Pending", Reason="", readiness=false. Elapsed: 13.69948ms
Jun 28 18:50:40.228: INFO: Pod "pod-subpath-test-secret-4tlb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026406621s
Jun 28 18:50:42.244: INFO: Pod "pod-subpath-test-secret-4tlb": Phase="Running", Reason="", readiness=true. Elapsed: 4.042283911s
Jun 28 18:50:44.256: INFO: Pod "pod-subpath-test-secret-4tlb": Phase="Running", Reason="", readiness=true. Elapsed: 6.054112332s
Jun 28 18:50:46.266: INFO: Pod "pod-subpath-test-secret-4tlb": Phase="Running", Reason="", readiness=true. Elapsed: 8.064271385s
Jun 28 18:50:48.278: INFO: Pod "pod-subpath-test-secret-4tlb": Phase="Running", Reason="", readiness=true. Elapsed: 10.07655418s
Jun 28 18:50:50.293: INFO: Pod "pod-subpath-test-secret-4tlb": Phase="Running", Reason="", readiness=true. Elapsed: 12.091629686s
Jun 28 18:50:52.310: INFO: Pod "pod-subpath-test-secret-4tlb": Phase="Running", Reason="", readiness=true. Elapsed: 14.108593165s
Jun 28 18:50:54.329: INFO: Pod "pod-subpath-test-secret-4tlb": Phase="Running", Reason="", readiness=true. Elapsed: 16.127281808s
Jun 28 18:50:56.343: INFO: Pod "pod-subpath-test-secret-4tlb": Phase="Running", Reason="", readiness=true. Elapsed: 18.141696722s
Jun 28 18:50:58.361: INFO: Pod "pod-subpath-test-secret-4tlb": Phase="Running", Reason="", readiness=true. Elapsed: 20.15917738s
Jun 28 18:51:00.378: INFO: Pod "pod-subpath-test-secret-4tlb": Phase="Running", Reason="", readiness=true. Elapsed: 22.176104819s
Jun 28 18:51:02.396: INFO: Pod "pod-subpath-test-secret-4tlb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.194542206s
STEP: Saw pod success
Jun 28 18:51:02.396: INFO: Pod "pod-subpath-test-secret-4tlb" satisfied condition "Succeeded or Failed"
Jun 28 18:51:02.406: INFO: Trying to get logs from node 10.13.107.37 pod pod-subpath-test-secret-4tlb container test-container-subpath-secret-4tlb: <nil>
STEP: delete the pod
Jun 28 18:51:02.469: INFO: Waiting for pod pod-subpath-test-secret-4tlb to disappear
Jun 28 18:51:02.479: INFO: Pod pod-subpath-test-secret-4tlb no longer exists
STEP: Deleting pod pod-subpath-test-secret-4tlb
Jun 28 18:51:02.479: INFO: Deleting pod "pod-subpath-test-secret-4tlb" in namespace "subpath-5092"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 18:51:02.489: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-5092" for this suite.

• [SLOW TEST:24.535 seconds]
[sig-storage] Subpath
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with secret pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with secret pod [LinuxOnly] [Conformance]","total":305,"completed":128,"skipped":2002,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 18:51:02.526: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Given a Pod with a 'name' label pod-adoption is created
STEP: When a replication controller with a matching selector is created
STEP: Then the orphan pod is adopted
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 18:51:07.787: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-7918" for this suite.

• [SLOW TEST:5.301 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should adopt matching pods on creation [Conformance]","total":305,"completed":129,"skipped":2014,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 18:51:07.838: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 18:51:15.033: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-743" for this suite.

• [SLOW TEST:7.313 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]","total":305,"completed":130,"skipped":2040,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 18:51:15.154: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod test-webserver-343c37e8-040e-4ac0-b675-4fd8487589b6 in namespace container-probe-4208
Jun 28 18:51:17.353: INFO: Started pod test-webserver-343c37e8-040e-4ac0-b675-4fd8487589b6 in namespace container-probe-4208
STEP: checking the pod's current state and verifying that restartCount is present
Jun 28 18:51:17.364: INFO: Initial restart count of pod test-webserver-343c37e8-040e-4ac0-b675-4fd8487589b6 is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 18:55:17.479: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-4208" for this suite.

• [SLOW TEST:242.358 seconds]
[k8s.io] Probing container
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Probing container should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","total":305,"completed":131,"skipped":2058,"failed":0}
SSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 18:55:17.515: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name configmap-test-volume-f3db8c53-3269-49e3-b5ca-79ebd066c6e1
STEP: Creating a pod to test consume configMaps
Jun 28 18:55:17.705: INFO: Waiting up to 5m0s for pod "pod-configmaps-33d2e5b4-ec4a-41fd-8b34-40a64c0e5d23" in namespace "configmap-9517" to be "Succeeded or Failed"
Jun 28 18:55:17.718: INFO: Pod "pod-configmaps-33d2e5b4-ec4a-41fd-8b34-40a64c0e5d23": Phase="Pending", Reason="", readiness=false. Elapsed: 12.258322ms
Jun 28 18:55:19.736: INFO: Pod "pod-configmaps-33d2e5b4-ec4a-41fd-8b34-40a64c0e5d23": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.030154555s
STEP: Saw pod success
Jun 28 18:55:19.736: INFO: Pod "pod-configmaps-33d2e5b4-ec4a-41fd-8b34-40a64c0e5d23" satisfied condition "Succeeded or Failed"
Jun 28 18:55:19.748: INFO: Trying to get logs from node 10.13.107.37 pod pod-configmaps-33d2e5b4-ec4a-41fd-8b34-40a64c0e5d23 container configmap-volume-test: <nil>
STEP: delete the pod
Jun 28 18:55:19.931: INFO: Waiting for pod pod-configmaps-33d2e5b4-ec4a-41fd-8b34-40a64c0e5d23 to disappear
Jun 28 18:55:19.942: INFO: Pod pod-configmaps-33d2e5b4-ec4a-41fd-8b34-40a64c0e5d23 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 18:55:19.942: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9517" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","total":305,"completed":132,"skipped":2064,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 18:55:19.979: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0666 on tmpfs
Jun 28 18:55:20.180: INFO: Waiting up to 5m0s for pod "pod-4861ad30-4167-4f9f-89a0-2311de96c0db" in namespace "emptydir-3635" to be "Succeeded or Failed"
Jun 28 18:55:20.192: INFO: Pod "pod-4861ad30-4167-4f9f-89a0-2311de96c0db": Phase="Pending", Reason="", readiness=false. Elapsed: 12.219041ms
Jun 28 18:55:22.216: INFO: Pod "pod-4861ad30-4167-4f9f-89a0-2311de96c0db": Phase="Pending", Reason="", readiness=false. Elapsed: 2.035489372s
Jun 28 18:55:24.229: INFO: Pod "pod-4861ad30-4167-4f9f-89a0-2311de96c0db": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.048762899s
STEP: Saw pod success
Jun 28 18:55:24.229: INFO: Pod "pod-4861ad30-4167-4f9f-89a0-2311de96c0db" satisfied condition "Succeeded or Failed"
Jun 28 18:55:24.239: INFO: Trying to get logs from node 10.13.107.37 pod pod-4861ad30-4167-4f9f-89a0-2311de96c0db container test-container: <nil>
STEP: delete the pod
Jun 28 18:55:24.296: INFO: Waiting for pod pod-4861ad30-4167-4f9f-89a0-2311de96c0db to disappear
Jun 28 18:55:24.306: INFO: Pod pod-4861ad30-4167-4f9f-89a0-2311de96c0db no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 18:55:24.306: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3635" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":133,"skipped":2072,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 18:55:24.345: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward api env vars
Jun 28 18:55:24.513: INFO: Waiting up to 5m0s for pod "downward-api-6d2a0880-7d5e-486d-805c-58646934cc75" in namespace "downward-api-1417" to be "Succeeded or Failed"
Jun 28 18:55:24.528: INFO: Pod "downward-api-6d2a0880-7d5e-486d-805c-58646934cc75": Phase="Pending", Reason="", readiness=false. Elapsed: 15.106164ms
Jun 28 18:55:26.540: INFO: Pod "downward-api-6d2a0880-7d5e-486d-805c-58646934cc75": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.026505301s
STEP: Saw pod success
Jun 28 18:55:26.540: INFO: Pod "downward-api-6d2a0880-7d5e-486d-805c-58646934cc75" satisfied condition "Succeeded or Failed"
Jun 28 18:55:26.550: INFO: Trying to get logs from node 10.13.107.37 pod downward-api-6d2a0880-7d5e-486d-805c-58646934cc75 container dapi-container: <nil>
STEP: delete the pod
Jun 28 18:55:26.622: INFO: Waiting for pod downward-api-6d2a0880-7d5e-486d-805c-58646934cc75 to disappear
Jun 28 18:55:26.635: INFO: Pod downward-api-6d2a0880-7d5e-486d-805c-58646934cc75 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 18:55:26.636: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1417" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]","total":305,"completed":134,"skipped":2101,"failed":0}

------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 18:55:26.669: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name configmap-test-volume-map-929e7863-d9eb-44ce-a31f-56e74e52bf32
STEP: Creating a pod to test consume configMaps
Jun 28 18:55:26.861: INFO: Waiting up to 5m0s for pod "pod-configmaps-3f0cc25c-95bf-4249-823c-a5f1a2599b9a" in namespace "configmap-2414" to be "Succeeded or Failed"
Jun 28 18:55:26.880: INFO: Pod "pod-configmaps-3f0cc25c-95bf-4249-823c-a5f1a2599b9a": Phase="Pending", Reason="", readiness=false. Elapsed: 18.87502ms
Jun 28 18:55:28.898: INFO: Pod "pod-configmaps-3f0cc25c-95bf-4249-823c-a5f1a2599b9a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.036947692s
Jun 28 18:55:30.910: INFO: Pod "pod-configmaps-3f0cc25c-95bf-4249-823c-a5f1a2599b9a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.048718307s
STEP: Saw pod success
Jun 28 18:55:30.910: INFO: Pod "pod-configmaps-3f0cc25c-95bf-4249-823c-a5f1a2599b9a" satisfied condition "Succeeded or Failed"
Jun 28 18:55:30.921: INFO: Trying to get logs from node 10.13.107.37 pod pod-configmaps-3f0cc25c-95bf-4249-823c-a5f1a2599b9a container configmap-volume-test: <nil>
STEP: delete the pod
Jun 28 18:55:30.976: INFO: Waiting for pod pod-configmaps-3f0cc25c-95bf-4249-823c-a5f1a2599b9a to disappear
Jun 28 18:55:30.986: INFO: Pod pod-configmaps-3f0cc25c-95bf-4249-823c-a5f1a2599b9a no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 18:55:30.986: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2414" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":135,"skipped":2101,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 18:55:31.020: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-6612
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-6612
STEP: creating replication controller externalsvc in namespace services-6612
I0628 18:55:31.220712      22 runners.go:190] Created replication controller with name: externalsvc, namespace: services-6612, replica count: 2
I0628 18:55:34.279462      22 runners.go:190] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the ClusterIP service to type=ExternalName
Jun 28 18:55:34.334: INFO: Creating new exec pod
Jun 28 18:55:36.409: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-108209140 exec --namespace=services-6612 execpodr5hvg -- /bin/sh -x -c nslookup clusterip-service.services-6612.svc.cluster.local'
Jun 28 18:55:37.113: INFO: stderr: "+ nslookup clusterip-service.services-6612.svc.cluster.local\n"
Jun 28 18:55:37.114: INFO: stdout: "Server:\t\t172.21.0.10\nAddress:\t172.21.0.10#53\n\nclusterip-service.services-6612.svc.cluster.local\tcanonical name = externalsvc.services-6612.svc.cluster.local.\nName:\texternalsvc.services-6612.svc.cluster.local\nAddress: 172.21.104.82\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-6612, will wait for the garbage collector to delete the pods
Jun 28 18:55:37.194: INFO: Deleting ReplicationController externalsvc took: 20.860938ms
Jun 28 18:55:37.294: INFO: Terminating ReplicationController externalsvc pods took: 100.269649ms
Jun 28 18:55:47.943: INFO: Cleaning up the ClusterIP to ExternalName test service
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 18:55:47.975: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6612" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

• [SLOW TEST:16.985 seconds]
[sig-network] Services
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ClusterIP to ExternalName [Conformance]","total":305,"completed":136,"skipped":2139,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl server-side dry-run 
  should check if kubectl can dry-run update Pods [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 18:55:48.007: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[It] should check if kubectl can dry-run update Pods [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Jun 28 18:55:48.142: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-108209140 run e2e-test-httpd-pod --image=docker.io/library/httpd:2.4.38-alpine --labels=run=e2e-test-httpd-pod --namespace=kubectl-7782'
Jun 28 18:55:48.434: INFO: stderr: ""
Jun 28 18:55:48.434: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: replace the image in the pod with server-side dry-run
Jun 28 18:55:48.434: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-108209140 get pod e2e-test-httpd-pod -o json --namespace=kubectl-7782'
Jun 28 18:55:48.621: INFO: stderr: ""
Jun 28 18:55:48.621: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"openshift.io/scc\": \"anyuid\"\n        },\n        \"creationTimestamp\": \"2021-06-28T18:55:48Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"managedFields\": [\n            {\n                \"apiVersion\": \"v1\",\n                \"fieldsType\": \"FieldsV1\",\n                \"fieldsV1\": {\n                    \"f:metadata\": {\n                        \"f:labels\": {\n                            \".\": {},\n                            \"f:run\": {}\n                        }\n                    },\n                    \"f:spec\": {\n                        \"f:containers\": {\n                            \"k:{\\\"name\\\":\\\"e2e-test-httpd-pod\\\"}\": {\n                                \".\": {},\n                                \"f:image\": {},\n                                \"f:imagePullPolicy\": {},\n                                \"f:name\": {},\n                                \"f:resources\": {},\n                                \"f:terminationMessagePath\": {},\n                                \"f:terminationMessagePolicy\": {}\n                            }\n                        },\n                        \"f:dnsPolicy\": {},\n                        \"f:enableServiceLinks\": {},\n                        \"f:restartPolicy\": {},\n                        \"f:schedulerName\": {},\n                        \"f:securityContext\": {},\n                        \"f:terminationGracePeriodSeconds\": {}\n                    }\n                },\n                \"manager\": \"kubectl-run\",\n                \"operation\": \"Update\",\n                \"time\": \"2021-06-28T18:55:48Z\"\n            },\n            {\n                \"apiVersion\": \"v1\",\n                \"fieldsType\": \"FieldsV1\",\n                \"fieldsV1\": {\n                    \"f:status\": {\n                        \"f:conditions\": {\n                            \"k:{\\\"type\\\":\\\"ContainersReady\\\"}\": {\n                                \".\": {},\n                                \"f:lastProbeTime\": {},\n                                \"f:lastTransitionTime\": {},\n                                \"f:message\": {},\n                                \"f:reason\": {},\n                                \"f:status\": {},\n                                \"f:type\": {}\n                            },\n                            \"k:{\\\"type\\\":\\\"Initialized\\\"}\": {\n                                \".\": {},\n                                \"f:lastProbeTime\": {},\n                                \"f:lastTransitionTime\": {},\n                                \"f:status\": {},\n                                \"f:type\": {}\n                            },\n                            \"k:{\\\"type\\\":\\\"Ready\\\"}\": {\n                                \".\": {},\n                                \"f:lastProbeTime\": {},\n                                \"f:lastTransitionTime\": {},\n                                \"f:message\": {},\n                                \"f:reason\": {},\n                                \"f:status\": {},\n                                \"f:type\": {}\n                            }\n                        },\n                        \"f:containerStatuses\": {},\n                        \"f:hostIP\": {},\n                        \"f:startTime\": {}\n                    }\n                },\n                \"manager\": \"kubelet\",\n                \"operation\": \"Update\",\n                \"time\": \"2021-06-28T18:55:48Z\"\n            }\n        ],\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-7782\",\n        \"resourceVersion\": \"78015\",\n        \"selfLink\": \"/api/v1/namespaces/kubectl-7782/pods/e2e-test-httpd-pod\",\n        \"uid\": \"65818d92-5135-4b68-80e2-2d90da9b0b4a\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"docker.io/library/httpd:2.4.38-alpine\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"securityContext\": {\n                    \"capabilities\": {\n                        \"drop\": [\n                            \"MKNOD\"\n                        ]\n                    }\n                },\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"default-token-9fqfx\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"imagePullSecrets\": [\n            {\n                \"name\": \"default-dockercfg-wqs8m\"\n            }\n        ],\n        \"nodeName\": \"10.13.107.37\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {\n            \"seLinuxOptions\": {\n                \"level\": \"s0:c48,c7\"\n            }\n        },\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"default-token-9fqfx\",\n                \"secret\": {\n                    \"defaultMode\": 420,\n                    \"secretName\": \"default-token-9fqfx\"\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2021-06-28T18:55:48Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2021-06-28T18:55:48Z\",\n                \"message\": \"containers with unready status: [e2e-test-httpd-pod]\",\n                \"reason\": \"ContainersNotReady\",\n                \"status\": \"False\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2021-06-28T18:55:48Z\",\n                \"message\": \"containers with unready status: [e2e-test-httpd-pod]\",\n                \"reason\": \"ContainersNotReady\",\n                \"status\": \"False\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2021-06-28T18:55:48Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"image\": \"docker.io/library/httpd:2.4.38-alpine\",\n                \"imageID\": \"\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": false,\n                \"restartCount\": 0,\n                \"started\": false,\n                \"state\": {\n                    \"waiting\": {\n                        \"reason\": \"ContainerCreating\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"10.13.107.37\",\n        \"phase\": \"Pending\",\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2021-06-28T18:55:48Z\"\n    }\n}\n"
Jun 28 18:55:48.622: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-108209140 replace -f - --dry-run server --namespace=kubectl-7782'
Jun 28 18:55:49.225: INFO: stderr: "W0628 18:55:48.749735     729 helpers.go:553] --dry-run is deprecated and can be replaced with --dry-run=client.\n"
Jun 28 18:55:49.226: INFO: stdout: "pod/e2e-test-httpd-pod replaced (dry run)\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image docker.io/library/httpd:2.4.38-alpine
Jun 28 18:55:49.239: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-108209140 delete pods e2e-test-httpd-pod --namespace=kubectl-7782'
Jun 28 18:57:23.486: INFO: stderr: ""
Jun 28 18:57:23.486: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 18:57:23.486: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7782" for this suite.

• [SLOW TEST:95.529 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl server-side dry-run
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:919
    should check if kubectl can dry-run update Pods [Conformance]
    /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl server-side dry-run should check if kubectl can dry-run update Pods [Conformance]","total":305,"completed":137,"skipped":2195,"failed":0}
SSSS
------------------------------
[sig-network] Services 
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 18:57:23.536: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating service in namespace services-2109
STEP: creating service affinity-clusterip-transition in namespace services-2109
STEP: creating replication controller affinity-clusterip-transition in namespace services-2109
I0628 18:57:23.712436      22 runners.go:190] Created replication controller with name: affinity-clusterip-transition, namespace: services-2109, replica count: 3
I0628 18:57:26.763021      22 runners.go:190] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jun 28 18:57:26.789: INFO: Creating new exec pod
Jun 28 18:57:31.865: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-108209140 exec --namespace=services-2109 execpod-affinity8884f -- /bin/sh -x -c nc -zv -t -w 2 affinity-clusterip-transition 80'
Jun 28 18:57:32.256: INFO: stderr: "+ nc -zv -t -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
Jun 28 18:57:32.256: INFO: stdout: ""
Jun 28 18:57:32.257: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-108209140 exec --namespace=services-2109 execpod-affinity8884f -- /bin/sh -x -c nc -zv -t -w 2 172.21.99.74 80'
Jun 28 18:57:32.699: INFO: stderr: "+ nc -zv -t -w 2 172.21.99.74 80\nConnection to 172.21.99.74 80 port [tcp/http] succeeded!\n"
Jun 28 18:57:32.699: INFO: stdout: ""
Jun 28 18:57:32.739: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-108209140 exec --namespace=services-2109 execpod-affinity8884f -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.21.99.74:80/ ; done'
Jun 28 18:57:33.209: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.99.74:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.99.74:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.99.74:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.99.74:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.99.74:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.99.74:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.99.74:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.99.74:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.99.74:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.99.74:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.99.74:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.99.74:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.99.74:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.99.74:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.99.74:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.99.74:80/\n"
Jun 28 18:57:33.210: INFO: stdout: "\naffinity-clusterip-transition-f8x4q\naffinity-clusterip-transition-8vb99\naffinity-clusterip-transition-8vb99\naffinity-clusterip-transition-8vb99\naffinity-clusterip-transition-8vb99\naffinity-clusterip-transition-tft2x\naffinity-clusterip-transition-f8x4q\naffinity-clusterip-transition-f8x4q\naffinity-clusterip-transition-f8x4q\naffinity-clusterip-transition-8vb99\naffinity-clusterip-transition-tft2x\naffinity-clusterip-transition-f8x4q\naffinity-clusterip-transition-tft2x\naffinity-clusterip-transition-tft2x\naffinity-clusterip-transition-tft2x\naffinity-clusterip-transition-f8x4q"
Jun 28 18:57:33.210: INFO: Received response from host: affinity-clusterip-transition-f8x4q
Jun 28 18:57:33.210: INFO: Received response from host: affinity-clusterip-transition-8vb99
Jun 28 18:57:33.210: INFO: Received response from host: affinity-clusterip-transition-8vb99
Jun 28 18:57:33.210: INFO: Received response from host: affinity-clusterip-transition-8vb99
Jun 28 18:57:33.210: INFO: Received response from host: affinity-clusterip-transition-8vb99
Jun 28 18:57:33.210: INFO: Received response from host: affinity-clusterip-transition-tft2x
Jun 28 18:57:33.210: INFO: Received response from host: affinity-clusterip-transition-f8x4q
Jun 28 18:57:33.210: INFO: Received response from host: affinity-clusterip-transition-f8x4q
Jun 28 18:57:33.210: INFO: Received response from host: affinity-clusterip-transition-f8x4q
Jun 28 18:57:33.210: INFO: Received response from host: affinity-clusterip-transition-8vb99
Jun 28 18:57:33.210: INFO: Received response from host: affinity-clusterip-transition-tft2x
Jun 28 18:57:33.210: INFO: Received response from host: affinity-clusterip-transition-f8x4q
Jun 28 18:57:33.210: INFO: Received response from host: affinity-clusterip-transition-tft2x
Jun 28 18:57:33.210: INFO: Received response from host: affinity-clusterip-transition-tft2x
Jun 28 18:57:33.210: INFO: Received response from host: affinity-clusterip-transition-tft2x
Jun 28 18:57:33.210: INFO: Received response from host: affinity-clusterip-transition-f8x4q
Jun 28 18:57:33.243: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-108209140 exec --namespace=services-2109 execpod-affinity8884f -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.21.99.74:80/ ; done'
Jun 28 18:57:33.763: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.99.74:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.99.74:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.99.74:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.99.74:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.99.74:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.99.74:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.99.74:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.99.74:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.99.74:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.99.74:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.99.74:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.99.74:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.99.74:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.99.74:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.99.74:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.99.74:80/\n"
Jun 28 18:57:33.763: INFO: stdout: "\naffinity-clusterip-transition-f8x4q\naffinity-clusterip-transition-f8x4q\naffinity-clusterip-transition-f8x4q\naffinity-clusterip-transition-f8x4q\naffinity-clusterip-transition-f8x4q\naffinity-clusterip-transition-f8x4q\naffinity-clusterip-transition-f8x4q\naffinity-clusterip-transition-f8x4q\naffinity-clusterip-transition-f8x4q\naffinity-clusterip-transition-f8x4q\naffinity-clusterip-transition-f8x4q\naffinity-clusterip-transition-f8x4q\naffinity-clusterip-transition-f8x4q\naffinity-clusterip-transition-f8x4q\naffinity-clusterip-transition-f8x4q\naffinity-clusterip-transition-f8x4q"
Jun 28 18:57:33.764: INFO: Received response from host: affinity-clusterip-transition-f8x4q
Jun 28 18:57:33.764: INFO: Received response from host: affinity-clusterip-transition-f8x4q
Jun 28 18:57:33.764: INFO: Received response from host: affinity-clusterip-transition-f8x4q
Jun 28 18:57:33.764: INFO: Received response from host: affinity-clusterip-transition-f8x4q
Jun 28 18:57:33.764: INFO: Received response from host: affinity-clusterip-transition-f8x4q
Jun 28 18:57:33.764: INFO: Received response from host: affinity-clusterip-transition-f8x4q
Jun 28 18:57:33.764: INFO: Received response from host: affinity-clusterip-transition-f8x4q
Jun 28 18:57:33.764: INFO: Received response from host: affinity-clusterip-transition-f8x4q
Jun 28 18:57:33.764: INFO: Received response from host: affinity-clusterip-transition-f8x4q
Jun 28 18:57:33.764: INFO: Received response from host: affinity-clusterip-transition-f8x4q
Jun 28 18:57:33.764: INFO: Received response from host: affinity-clusterip-transition-f8x4q
Jun 28 18:57:33.764: INFO: Received response from host: affinity-clusterip-transition-f8x4q
Jun 28 18:57:33.764: INFO: Received response from host: affinity-clusterip-transition-f8x4q
Jun 28 18:57:33.764: INFO: Received response from host: affinity-clusterip-transition-f8x4q
Jun 28 18:57:33.764: INFO: Received response from host: affinity-clusterip-transition-f8x4q
Jun 28 18:57:33.764: INFO: Received response from host: affinity-clusterip-transition-f8x4q
Jun 28 18:57:33.764: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-2109, will wait for the garbage collector to delete the pods
Jun 28 18:57:33.883: INFO: Deleting ReplicationController affinity-clusterip-transition took: 18.832545ms
Jun 28 18:57:33.984: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 100.397362ms
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 18:57:47.941: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-2109" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

• [SLOW TEST:24.440 seconds]
[sig-network] Services
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","total":305,"completed":138,"skipped":2199,"failed":0}
SSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] 
  validates basic preemption works [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 18:57:47.976: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename sched-preemption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:89
Jun 28 18:57:48.168: INFO: Waiting up to 1m0s for all nodes to be ready
Jun 28 18:58:48.379: INFO: Waiting for terminating namespaces to be deleted...
[It] validates basic preemption works [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Create pods that use 2/3 of node resources.
Jun 28 18:58:48.474: INFO: Created pod: pod0-sched-preemption-low-priority
Jun 28 18:58:48.546: INFO: Created pod: pod1-sched-preemption-medium-priority
Jun 28 18:58:48.608: INFO: Created pod: pod2-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled.
STEP: Run a high priority pod that has same requirements as that of lower priority pod
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 18:59:20.793: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-160" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:77

• [SLOW TEST:92.986 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates basic preemption works [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] validates basic preemption works [Conformance]","total":305,"completed":139,"skipped":2204,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 18:59:20.963: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: getting the auto-created API token
Jun 28 18:59:21.695: INFO: created pod pod-service-account-defaultsa
Jun 28 18:59:21.695: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Jun 28 18:59:21.735: INFO: created pod pod-service-account-mountsa
Jun 28 18:59:21.735: INFO: pod pod-service-account-mountsa service account token volume mount: true
Jun 28 18:59:21.767: INFO: created pod pod-service-account-nomountsa
Jun 28 18:59:21.768: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Jun 28 18:59:21.802: INFO: created pod pod-service-account-defaultsa-mountspec
Jun 28 18:59:21.802: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Jun 28 18:59:21.844: INFO: created pod pod-service-account-mountsa-mountspec
Jun 28 18:59:21.844: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Jun 28 18:59:21.917: INFO: created pod pod-service-account-nomountsa-mountspec
Jun 28 18:59:21.917: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Jun 28 18:59:21.953: INFO: created pod pod-service-account-defaultsa-nomountspec
Jun 28 18:59:21.953: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Jun 28 18:59:21.986: INFO: created pod pod-service-account-mountsa-nomountspec
Jun 28 18:59:21.986: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Jun 28 18:59:22.021: INFO: created pod pod-service-account-nomountsa-nomountspec
Jun 28 18:59:22.021: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 18:59:22.021: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-5225" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should allow opting out of API token automount  [Conformance]","total":305,"completed":140,"skipped":2236,"failed":0}

------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 18:59:22.085: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0644 on node default medium
Jun 28 18:59:22.272: INFO: Waiting up to 5m0s for pod "pod-2ec7563f-5a4d-45a7-83a1-89987fe83366" in namespace "emptydir-9536" to be "Succeeded or Failed"
Jun 28 18:59:22.290: INFO: Pod "pod-2ec7563f-5a4d-45a7-83a1-89987fe83366": Phase="Pending", Reason="", readiness=false. Elapsed: 17.853558ms
Jun 28 18:59:24.305: INFO: Pod "pod-2ec7563f-5a4d-45a7-83a1-89987fe83366": Phase="Running", Reason="", readiness=true. Elapsed: 2.033037321s
Jun 28 18:59:26.376: INFO: Pod "pod-2ec7563f-5a4d-45a7-83a1-89987fe83366": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.103558787s
STEP: Saw pod success
Jun 28 18:59:26.376: INFO: Pod "pod-2ec7563f-5a4d-45a7-83a1-89987fe83366" satisfied condition "Succeeded or Failed"
Jun 28 18:59:26.388: INFO: Trying to get logs from node 10.13.107.37 pod pod-2ec7563f-5a4d-45a7-83a1-89987fe83366 container test-container: <nil>
STEP: delete the pod
Jun 28 18:59:26.458: INFO: Waiting for pod pod-2ec7563f-5a4d-45a7-83a1-89987fe83366 to disappear
Jun 28 18:59:26.467: INFO: Pod pod-2ec7563f-5a4d-45a7-83a1-89987fe83366 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 18:59:26.467: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9536" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":141,"skipped":2236,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: udp [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 18:59:26.508: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Performing setup for networking test in namespace pod-network-test-5149
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Jun 28 18:59:26.652: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Jun 28 18:59:26.806: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jun 28 18:59:28.831: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jun 28 18:59:30.817: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun 28 18:59:32.820: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun 28 18:59:34.824: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun 28 18:59:36.819: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun 28 18:59:38.819: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun 28 18:59:40.824: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun 28 18:59:42.824: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun 28 18:59:44.823: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun 28 18:59:46.820: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun 28 18:59:48.823: INFO: The status of Pod netserver-0 is Running (Ready = true)
Jun 28 18:59:48.843: INFO: The status of Pod netserver-1 is Running (Ready = true)
Jun 28 18:59:48.862: INFO: The status of Pod netserver-2 is Running (Ready = true)
STEP: Creating test pods
Jun 28 18:59:50.954: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.221.150:8080/dial?request=hostname&protocol=udp&host=172.30.221.149&port=8081&tries=1'] Namespace:pod-network-test-5149 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 28 18:59:50.955: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
Jun 28 18:59:51.263: INFO: Waiting for responses: map[]
Jun 28 18:59:51.275: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.221.150:8080/dial?request=hostname&protocol=udp&host=172.30.54.46&port=8081&tries=1'] Namespace:pod-network-test-5149 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 28 18:59:51.275: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
Jun 28 18:59:51.573: INFO: Waiting for responses: map[]
Jun 28 18:59:51.585: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.221.150:8080/dial?request=hostname&protocol=udp&host=172.30.196.119&port=8081&tries=1'] Namespace:pod-network-test-5149 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 28 18:59:51.586: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
Jun 28 18:59:51.832: INFO: Waiting for responses: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 18:59:51.832: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-5149" for this suite.

• [SLOW TEST:25.365 seconds]
[sig-network] Networking
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:26
  Granular Checks: Pods
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:29
    should function for intra-pod communication: udp [NodeConformance] [Conformance]
    /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: udp [NodeConformance] [Conformance]","total":305,"completed":142,"skipped":2249,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 18:59:51.874: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jun 28 18:59:52.023: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Jun 28 19:00:02.659: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-108209140 --namespace=crd-publish-openapi-4011 create -f -'
Jun 28 19:00:03.438: INFO: stderr: ""
Jun 28 19:00:03.438: INFO: stdout: "e2e-test-crd-publish-openapi-866-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Jun 28 19:00:03.438: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-108209140 --namespace=crd-publish-openapi-4011 delete e2e-test-crd-publish-openapi-866-crds test-cr'
Jun 28 19:00:03.698: INFO: stderr: ""
Jun 28 19:00:03.698: INFO: stdout: "e2e-test-crd-publish-openapi-866-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
Jun 28 19:00:03.698: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-108209140 --namespace=crd-publish-openapi-4011 apply -f -'
Jun 28 19:00:04.573: INFO: stderr: ""
Jun 28 19:00:04.573: INFO: stdout: "e2e-test-crd-publish-openapi-866-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Jun 28 19:00:04.573: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-108209140 --namespace=crd-publish-openapi-4011 delete e2e-test-crd-publish-openapi-866-crds test-cr'
Jun 28 19:00:04.765: INFO: stderr: ""
Jun 28 19:00:04.765: INFO: stdout: "e2e-test-crd-publish-openapi-866-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
Jun 28 19:00:04.765: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-108209140 explain e2e-test-crd-publish-openapi-866-crds'
Jun 28 19:00:05.589: INFO: stderr: ""
Jun 28 19:00:05.589: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-866-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 19:00:15.624: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-4011" for this suite.

• [SLOW TEST:23.789 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields at the schema root [Conformance]","total":305,"completed":143,"skipped":2282,"failed":0}
SSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 19:00:15.663: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-5010
[It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating stateful set ss in namespace statefulset-5010
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-5010
Jun 28 19:00:15.898: INFO: Found 0 stateful pods, waiting for 1
Jun 28 19:00:25.913: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod
Jun 28 19:00:25.923: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-108209140 exec --namespace=statefulset-5010 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jun 28 19:00:26.417: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jun 28 19:00:26.417: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jun 28 19:00:26.417: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jun 28 19:00:26.430: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Jun 28 19:00:36.443: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jun 28 19:00:36.443: INFO: Waiting for statefulset status.replicas updated to 0
Jun 28 19:00:36.493: INFO: POD   NODE          PHASE    GRACE  CONDITIONS
Jun 28 19:00:36.493: INFO: ss-0  10.13.107.37  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-06-28 19:00:15 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-06-28 19:00:26 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-06-28 19:00:26 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-06-28 19:00:15 +0000 UTC  }]
Jun 28 19:00:36.493: INFO: 
Jun 28 19:00:36.493: INFO: StatefulSet ss has not reached scale 3, at 1
Jun 28 19:00:37.507: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.988854006s
Jun 28 19:00:38.521: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.975391933s
Jun 28 19:00:39.537: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.960884382s
Jun 28 19:00:40.552: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.945084333s
Jun 28 19:00:41.567: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.930012767s
Jun 28 19:00:42.584: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.914117023s
Jun 28 19:00:43.600: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.897397691s
Jun 28 19:00:44.619: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.881756716s
Jun 28 19:00:45.639: INFO: Verifying statefulset ss doesn't scale past 3 for another 862.541121ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-5010
Jun 28 19:00:46.653: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-108209140 exec --namespace=statefulset-5010 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 28 19:00:47.091: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jun 28 19:00:47.091: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jun 28 19:00:47.091: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jun 28 19:00:47.092: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-108209140 exec --namespace=statefulset-5010 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 28 19:00:47.638: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Jun 28 19:00:47.638: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jun 28 19:00:47.638: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jun 28 19:00:47.638: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-108209140 exec --namespace=statefulset-5010 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 28 19:00:48.181: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Jun 28 19:00:48.181: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jun 28 19:00:48.181: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jun 28 19:00:48.197: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Jun 28 19:00:48.198: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Jun 28 19:00:48.198: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod
Jun 28 19:00:48.209: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-108209140 exec --namespace=statefulset-5010 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jun 28 19:00:48.649: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jun 28 19:00:48.649: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jun 28 19:00:48.649: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jun 28 19:00:48.649: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-108209140 exec --namespace=statefulset-5010 ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jun 28 19:00:49.176: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jun 28 19:00:49.176: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jun 28 19:00:49.176: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jun 28 19:00:49.176: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-108209140 exec --namespace=statefulset-5010 ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jun 28 19:00:49.607: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jun 28 19:00:49.607: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jun 28 19:00:49.607: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jun 28 19:00:49.607: INFO: Waiting for statefulset status.replicas updated to 0
Jun 28 19:00:49.622: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
Jun 28 19:00:59.655: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jun 28 19:00:59.655: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Jun 28 19:00:59.655: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Jun 28 19:00:59.695: INFO: POD   NODE          PHASE    GRACE  CONDITIONS
Jun 28 19:00:59.695: INFO: ss-0  10.13.107.37  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-06-28 19:00:15 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-06-28 19:00:48 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-06-28 19:00:48 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-06-28 19:00:15 +0000 UTC  }]
Jun 28 19:00:59.695: INFO: ss-1  10.13.107.60  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-06-28 19:00:36 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-06-28 19:00:49 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-06-28 19:00:49 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-06-28 19:00:36 +0000 UTC  }]
Jun 28 19:00:59.695: INFO: ss-2  10.13.107.57  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-06-28 19:00:36 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-06-28 19:00:50 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-06-28 19:00:50 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-06-28 19:00:36 +0000 UTC  }]
Jun 28 19:00:59.695: INFO: 
Jun 28 19:00:59.695: INFO: StatefulSet ss has not reached scale 0, at 3
Jun 28 19:01:00.710: INFO: POD   NODE          PHASE    GRACE  CONDITIONS
Jun 28 19:01:00.710: INFO: ss-0  10.13.107.37  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-06-28 19:00:15 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-06-28 19:00:48 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-06-28 19:00:48 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-06-28 19:00:15 +0000 UTC  }]
Jun 28 19:01:00.711: INFO: ss-1  10.13.107.60  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-06-28 19:00:36 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-06-28 19:00:49 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-06-28 19:00:49 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-06-28 19:00:36 +0000 UTC  }]
Jun 28 19:01:00.711: INFO: ss-2  10.13.107.57  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-06-28 19:00:36 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-06-28 19:00:50 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-06-28 19:00:50 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-06-28 19:00:36 +0000 UTC  }]
Jun 28 19:01:00.711: INFO: 
Jun 28 19:01:00.711: INFO: StatefulSet ss has not reached scale 0, at 3
Jun 28 19:01:01.734: INFO: POD   NODE          PHASE    GRACE  CONDITIONS
Jun 28 19:01:01.734: INFO: ss-0  10.13.107.37  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-06-28 19:00:15 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-06-28 19:00:48 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-06-28 19:00:48 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-06-28 19:00:15 +0000 UTC  }]
Jun 28 19:01:01.734: INFO: ss-1  10.13.107.60  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-06-28 19:00:36 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-06-28 19:00:49 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-06-28 19:00:49 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-06-28 19:00:36 +0000 UTC  }]
Jun 28 19:01:01.734: INFO: ss-2  10.13.107.57  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-06-28 19:00:36 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-06-28 19:00:50 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-06-28 19:00:50 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-06-28 19:00:36 +0000 UTC  }]
Jun 28 19:01:01.734: INFO: 
Jun 28 19:01:01.734: INFO: StatefulSet ss has not reached scale 0, at 3
Jun 28 19:01:02.748: INFO: POD   NODE          PHASE    GRACE  CONDITIONS
Jun 28 19:01:02.749: INFO: ss-2  10.13.107.57  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-06-28 19:00:36 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-06-28 19:00:50 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-06-28 19:00:50 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-06-28 19:00:36 +0000 UTC  }]
Jun 28 19:01:02.749: INFO: 
Jun 28 19:01:02.749: INFO: StatefulSet ss has not reached scale 0, at 1
Jun 28 19:01:03.763: INFO: POD   NODE          PHASE    GRACE  CONDITIONS
Jun 28 19:01:03.763: INFO: ss-2  10.13.107.57  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-06-28 19:00:36 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-06-28 19:00:50 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-06-28 19:00:50 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-06-28 19:00:36 +0000 UTC  }]
Jun 28 19:01:03.763: INFO: 
Jun 28 19:01:03.764: INFO: StatefulSet ss has not reached scale 0, at 1
Jun 28 19:01:04.777: INFO: POD   NODE          PHASE    GRACE  CONDITIONS
Jun 28 19:01:04.777: INFO: ss-2  10.13.107.57  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-06-28 19:00:36 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-06-28 19:00:50 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-06-28 19:00:50 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-06-28 19:00:36 +0000 UTC  }]
Jun 28 19:01:04.778: INFO: 
Jun 28 19:01:04.778: INFO: StatefulSet ss has not reached scale 0, at 1
Jun 28 19:01:05.801: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.906137183s
Jun 28 19:01:06.816: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.882842265s
Jun 28 19:01:07.827: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.867961995s
Jun 28 19:01:08.846: INFO: Verifying statefulset ss doesn't scale past 0 for another 856.946782ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-5010
Jun 28 19:01:09.858: INFO: Scaling statefulset ss to 0
Jun 28 19:01:09.894: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
Jun 28 19:01:09.902: INFO: Deleting all statefulset in ns statefulset-5010
Jun 28 19:01:09.911: INFO: Scaling statefulset ss to 0
Jun 28 19:01:09.945: INFO: Waiting for statefulset status.replicas updated to 0
Jun 28 19:01:09.954: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 19:01:09.992: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-5010" for this suite.

• [SLOW TEST:54.360 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
    Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
    /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]","total":305,"completed":144,"skipped":2286,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 19:01:10.024: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the rc
STEP: delete the rc
STEP: wait for all pods to be garbage collected
STEP: Gathering metrics
W0628 19:01:20.252458      22 metrics_grabber.go:98] Can't find kube-scheduler pod. Grabbing metrics from kube-scheduler is disabled.
W0628 19:01:20.252524      22 metrics_grabber.go:102] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
W0628 19:01:20.252544      22 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
Jun 28 19:01:20.252: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 19:01:20.252: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-2076" for this suite.

• [SLOW TEST:10.262 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should delete pods created by rc when not orphaning [Conformance]","total":305,"completed":145,"skipped":2309,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] 
  validates lower priority pod preemption by critical pod [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 19:01:20.287: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename sched-preemption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:89
Jun 28 19:01:20.456: INFO: Waiting up to 1m0s for all nodes to be ready
Jun 28 19:02:20.675: INFO: Waiting for terminating namespaces to be deleted...
[It] validates lower priority pod preemption by critical pod [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Create pods that use 2/3 of node resources.
Jun 28 19:02:20.779: INFO: Created pod: pod0-sched-preemption-low-priority
Jun 28 19:02:20.844: INFO: Created pod: pod1-sched-preemption-medium-priority
Jun 28 19:02:20.909: INFO: Created pod: pod2-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled.
STEP: Run a critical pod that use same resources as that of a lower priority pod
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 19:02:31.103: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-6336" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:77

• [SLOW TEST:71.015 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates lower priority pod preemption by critical pod [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]","total":305,"completed":146,"skipped":2320,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny pod and configmap creation [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 19:02:31.304: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jun 28 19:02:31.990: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Jun 28 19:02:34.021: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63760503752, loc:(*time.Location)(0x7702840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63760503752, loc:(*time.Location)(0x7702840)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63760503752, loc:(*time.Location)(0x7702840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63760503751, loc:(*time.Location)(0x7702840)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun 28 19:02:37.073: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny pod and configmap creation [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod that should be denied by the webhook
STEP: create a pod that causes the webhook to hang
STEP: create a configmap that should be denied by the webhook
STEP: create a configmap that should be admitted by the webhook
STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: create a namespace that bypass the webhook
STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 19:02:47.643: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5936" for this suite.
STEP: Destroying namespace "webhook-5936-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:16.507 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny pod and configmap creation [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny pod and configmap creation [Conformance]","total":305,"completed":147,"skipped":2409,"failed":0}
S
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 19:02:47.811: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0777 on node default medium
Jun 28 19:02:47.979: INFO: Waiting up to 5m0s for pod "pod-fd767c76-c666-4141-b098-fb40e57bfe47" in namespace "emptydir-2880" to be "Succeeded or Failed"
Jun 28 19:02:47.989: INFO: Pod "pod-fd767c76-c666-4141-b098-fb40e57bfe47": Phase="Pending", Reason="", readiness=false. Elapsed: 10.249557ms
Jun 28 19:02:50.005: INFO: Pod "pod-fd767c76-c666-4141-b098-fb40e57bfe47": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025470227s
Jun 28 19:02:52.032: INFO: Pod "pod-fd767c76-c666-4141-b098-fb40e57bfe47": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.052933728s
STEP: Saw pod success
Jun 28 19:02:52.032: INFO: Pod "pod-fd767c76-c666-4141-b098-fb40e57bfe47" satisfied condition "Succeeded or Failed"
Jun 28 19:02:52.046: INFO: Trying to get logs from node 10.13.107.37 pod pod-fd767c76-c666-4141-b098-fb40e57bfe47 container test-container: <nil>
STEP: delete the pod
Jun 28 19:02:52.171: INFO: Waiting for pod pod-fd767c76-c666-4141-b098-fb40e57bfe47 to disappear
Jun 28 19:02:52.184: INFO: Pod pod-fd767c76-c666-4141-b098-fb40e57bfe47 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 19:02:52.184: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2880" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":148,"skipped":2410,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 19:02:52.215: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a ResourceQuota with terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a long running pod
STEP: Ensuring resource quota with not terminating scope captures the pod usage
STEP: Ensuring resource quota with terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a terminating pod
STEP: Ensuring resource quota with terminating scope captures the pod usage
STEP: Ensuring resource quota with not terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 19:03:08.686: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-4611" for this suite.

• [SLOW TEST:16.504 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with terminating scopes. [Conformance]","total":305,"completed":149,"skipped":2423,"failed":0}
SS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 19:03:08.720: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Jun 28 19:03:10.966: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 19:03:11.008: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-1818" for this suite.
•{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":305,"completed":150,"skipped":2425,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 19:03:11.047: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward api env vars
Jun 28 19:03:11.211: INFO: Waiting up to 5m0s for pod "downward-api-b19f1b46-dd9d-4d14-b0af-9a35bbbeae84" in namespace "downward-api-5344" to be "Succeeded or Failed"
Jun 28 19:03:11.230: INFO: Pod "downward-api-b19f1b46-dd9d-4d14-b0af-9a35bbbeae84": Phase="Pending", Reason="", readiness=false. Elapsed: 18.540544ms
Jun 28 19:03:13.254: INFO: Pod "downward-api-b19f1b46-dd9d-4d14-b0af-9a35bbbeae84": Phase="Pending", Reason="", readiness=false. Elapsed: 2.042415557s
Jun 28 19:03:15.269: INFO: Pod "downward-api-b19f1b46-dd9d-4d14-b0af-9a35bbbeae84": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.057731048s
STEP: Saw pod success
Jun 28 19:03:15.269: INFO: Pod "downward-api-b19f1b46-dd9d-4d14-b0af-9a35bbbeae84" satisfied condition "Succeeded or Failed"
Jun 28 19:03:15.279: INFO: Trying to get logs from node 10.13.107.37 pod downward-api-b19f1b46-dd9d-4d14-b0af-9a35bbbeae84 container dapi-container: <nil>
STEP: delete the pod
Jun 28 19:03:15.372: INFO: Waiting for pod downward-api-b19f1b46-dd9d-4d14-b0af-9a35bbbeae84 to disappear
Jun 28 19:03:15.382: INFO: Pod downward-api-b19f1b46-dd9d-4d14-b0af-9a35bbbeae84 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 19:03:15.382: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5344" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide host IP as an env var [NodeConformance] [Conformance]","total":305,"completed":151,"skipped":2442,"failed":0}
SSSSSSS
------------------------------
[sig-network] Ingress API 
  should support creating Ingress API operations [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Ingress API
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 19:03:15.413: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename ingress
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support creating Ingress API operations [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: getting /apis
STEP: getting /apis/networking.k8s.io
STEP: getting /apis/networking.k8s.iov1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Jun 28 19:03:15.618: INFO: starting watch
STEP: cluster-wide listing
STEP: cluster-wide watching
Jun 28 19:03:15.633: INFO: starting watch
STEP: patching
STEP: updating
Jun 28 19:03:15.670: INFO: waiting for watch events with expected annotations
Jun 28 19:03:15.670: INFO: saw patched and updated annotations
STEP: patching /status
STEP: updating /status
STEP: get /status
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-network] Ingress API
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 19:03:15.797: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ingress-399" for this suite.
•{"msg":"PASSED [sig-network] Ingress API should support creating Ingress API operations [Conformance]","total":305,"completed":152,"skipped":2449,"failed":0}
S
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 19:03:15.832: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-3528
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a new StatefulSet
Jun 28 19:03:15.995: INFO: Found 0 stateful pods, waiting for 3
Jun 28 19:03:26.016: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jun 28 19:03:26.017: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jun 28 19:03:26.017: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Jun 28 19:03:26.062: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-108209140 exec --namespace=statefulset-3528 ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jun 28 19:03:26.663: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jun 28 19:03:26.663: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jun 28 19:03:26.663: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from docker.io/library/httpd:2.4.38-alpine to docker.io/library/httpd:2.4.39-alpine
Jun 28 19:03:36.835: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Updating Pods in reverse ordinal order
Jun 28 19:03:46.898: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-108209140 exec --namespace=statefulset-3528 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 28 19:03:48.121: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jun 28 19:03:48.121: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jun 28 19:03:48.121: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

STEP: Rolling back to a previous revision
Jun 28 19:04:18.204: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-108209140 exec --namespace=statefulset-3528 ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jun 28 19:04:18.863: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jun 28 19:04:18.863: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jun 28 19:04:18.863: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jun 28 19:04:28.959: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order
Jun 28 19:04:39.027: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-108209140 exec --namespace=statefulset-3528 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 28 19:04:39.543: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jun 28 19:04:39.543: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jun 28 19:04:39.543: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
Jun 28 19:05:09.608: INFO: Deleting all statefulset in ns statefulset-3528
Jun 28 19:05:09.616: INFO: Scaling statefulset ss2 to 0
Jun 28 19:05:29.660: INFO: Waiting for statefulset status.replicas updated to 0
Jun 28 19:05:29.672: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 19:05:29.717: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-3528" for this suite.

• [SLOW TEST:133.916 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
    should perform rolling updates and roll backs of template modifications [Conformance]
    /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should perform rolling updates and roll backs of template modifications [Conformance]","total":305,"completed":153,"skipped":2450,"failed":0}
SSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should patch a secret [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 19:05:29.751: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should patch a secret [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a secret
STEP: listing secrets in all namespaces to ensure that there are more than zero
STEP: patching the secret
STEP: deleting the secret using a LabelSelector
STEP: listing secrets in all namespaces, searching for label name and value in patch
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 19:05:30.580: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4040" for this suite.
•{"msg":"PASSED [sig-api-machinery] Secrets should patch a secret [Conformance]","total":305,"completed":154,"skipped":2459,"failed":0}
SSSSSSSSS
------------------------------
[sig-scheduling] LimitRange 
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-scheduling] LimitRange
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 19:05:30.618: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename limitrange
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a LimitRange
STEP: Setting up watch
STEP: Submitting a LimitRange
Jun 28 19:05:30.766: INFO: observed the limitRanges list
STEP: Verifying LimitRange creation was observed
STEP: Fetching the LimitRange to ensure it has proper values
Jun 28 19:05:30.789: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Jun 28 19:05:30.790: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with no resource requirements
STEP: Ensuring Pod has resource requirements applied from LimitRange
Jun 28 19:05:30.842: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Jun 28 19:05:30.843: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with partial resource requirements
STEP: Ensuring Pod has merged resource requirements applied from LimitRange
Jun 28 19:05:30.887: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
Jun 28 19:05:30.887: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Failing to create a Pod with less than min resources
STEP: Failing to create a Pod with more than max resources
STEP: Updating a LimitRange
STEP: Verifying LimitRange updating is effective
STEP: Creating a Pod with less than former min resources
STEP: Failing to create a Pod with more than max resources
STEP: Deleting a LimitRange
STEP: Verifying the LimitRange was deleted
Jun 28 19:05:38.051: INFO: limitRange is already deleted
STEP: Creating a Pod with more than former max resources
[AfterEach] [sig-scheduling] LimitRange
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 19:05:38.094: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "limitrange-9875" for this suite.

• [SLOW TEST:7.506 seconds]
[sig-scheduling] LimitRange
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-scheduling] LimitRange should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]","total":305,"completed":155,"skipped":2468,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] IngressClass API 
   should support creating IngressClass API operations [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] IngressClass API
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 19:05:38.128: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename ingressclass
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] IngressClass API
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/ingressclass.go:148
[It]  should support creating IngressClass API operations [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: getting /apis
STEP: getting /apis/networking.k8s.io
STEP: getting /apis/networking.k8s.iov1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Jun 28 19:05:38.345: INFO: starting watch
STEP: patching
STEP: updating
Jun 28 19:05:38.385: INFO: waiting for watch events with expected annotations
Jun 28 19:05:38.385: INFO: saw patched and updated annotations
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-network] IngressClass API
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 19:05:38.470: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ingressclass-517" for this suite.
•{"msg":"PASSED [sig-network] IngressClass API  should support creating IngressClass API operations [Conformance]","total":305,"completed":156,"skipped":2514,"failed":0}
SSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 19:05:38.509: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod liveness-752dcbed-a948-48fb-a7ee-157472fef357 in namespace container-probe-5490
Jun 28 19:05:40.703: INFO: Started pod liveness-752dcbed-a948-48fb-a7ee-157472fef357 in namespace container-probe-5490
STEP: checking the pod's current state and verifying that restartCount is present
Jun 28 19:05:40.713: INFO: Initial restart count of pod liveness-752dcbed-a948-48fb-a7ee-157472fef357 is 0
Jun 28 19:05:58.866: INFO: Restart count of pod container-probe-5490/liveness-752dcbed-a948-48fb-a7ee-157472fef357 is now 1 (18.153001975s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 19:05:58.902: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-5490" for this suite.

• [SLOW TEST:20.428 seconds]
[k8s.io] Probing container
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Probing container should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","total":305,"completed":157,"skipped":2518,"failed":0}
[sig-network] Services 
  should be able to create a functioning NodePort service [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 19:05:58.938: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should be able to create a functioning NodePort service [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating service nodeport-test with type=NodePort in namespace services-3825
STEP: creating replication controller nodeport-test in namespace services-3825
I0628 19:05:59.129620      22 runners.go:190] Created replication controller with name: nodeport-test, namespace: services-3825, replica count: 2
Jun 28 19:06:02.180: INFO: Creating new exec pod
I0628 19:06:02.180150      22 runners.go:190] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jun 28 19:06:05.266: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-108209140 exec --namespace=services-3825 execpodps8hw -- /bin/sh -x -c nc -zv -t -w 2 nodeport-test 80'
Jun 28 19:06:05.855: INFO: stderr: "+ nc -zv -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Jun 28 19:06:05.855: INFO: stdout: ""
Jun 28 19:06:05.856: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-108209140 exec --namespace=services-3825 execpodps8hw -- /bin/sh -x -c nc -zv -t -w 2 172.21.47.224 80'
Jun 28 19:06:06.612: INFO: stderr: "+ nc -zv -t -w 2 172.21.47.224 80\nConnection to 172.21.47.224 80 port [tcp/http] succeeded!\n"
Jun 28 19:06:06.619: INFO: stdout: ""
Jun 28 19:06:06.619: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-108209140 exec --namespace=services-3825 execpodps8hw -- /bin/sh -x -c nc -zv -t -w 2 10.13.107.60 30879'
Jun 28 19:06:07.038: INFO: stderr: "+ nc -zv -t -w 2 10.13.107.60 30879\nConnection to 10.13.107.60 30879 port [tcp/30879] succeeded!\n"
Jun 28 19:06:07.039: INFO: stdout: ""
Jun 28 19:06:07.039: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-108209140 exec --namespace=services-3825 execpodps8hw -- /bin/sh -x -c nc -zv -t -w 2 10.13.107.57 30879'
Jun 28 19:06:07.470: INFO: stderr: "+ nc -zv -t -w 2 10.13.107.57 30879\nConnection to 10.13.107.57 30879 port [tcp/30879] succeeded!\n"
Jun 28 19:06:07.470: INFO: stdout: ""
Jun 28 19:06:07.470: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-108209140 exec --namespace=services-3825 execpodps8hw -- /bin/sh -x -c nc -zv -t -w 2 149.81.178.59 30879'
Jun 28 19:06:07.864: INFO: stderr: "+ nc -zv -t -w 2 149.81.178.59 30879\nConnection to 149.81.178.59 30879 port [tcp/30879] succeeded!\n"
Jun 28 19:06:07.864: INFO: stdout: ""
Jun 28 19:06:07.864: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-108209140 exec --namespace=services-3825 execpodps8hw -- /bin/sh -x -c nc -zv -t -w 2 149.81.178.54 30879'
Jun 28 19:06:08.366: INFO: stderr: "+ nc -zv -t -w 2 149.81.178.54 30879\nConnection to 149.81.178.54 30879 port [tcp/30879] succeeded!\n"
Jun 28 19:06:08.366: INFO: stdout: ""
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 19:06:08.366: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3825" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

• [SLOW TEST:9.470 seconds]
[sig-network] Services
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to create a functioning NodePort service [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should be able to create a functioning NodePort service [Conformance]","total":305,"completed":158,"skipped":2518,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Events 
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Events
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 19:06:08.408: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a test event
STEP: listing all events in all namespaces
STEP: patching the test event
STEP: fetching the test event
STEP: deleting the test event
STEP: listing all events in all namespaces
[AfterEach] [sig-api-machinery] Events
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 19:06:08.841: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-1861" for this suite.
•{"msg":"PASSED [sig-api-machinery] Events should ensure that an event can be fetched, patched, deleted, and listed [Conformance]","total":305,"completed":159,"skipped":2532,"failed":0}
SSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 19:06:08.884: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for all rs to be garbage collected
STEP: expected 0 rs, got 1 rs
STEP: expected 0 pods, got 2 pods
STEP: Gathering metrics
Jun 28 19:06:10.163: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

W0628 19:06:10.163044      22 metrics_grabber.go:98] Can't find kube-scheduler pod. Grabbing metrics from kube-scheduler is disabled.
W0628 19:06:10.163108      22 metrics_grabber.go:102] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
W0628 19:06:10.163128      22 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 19:06:10.163: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-3117" for this suite.
•{"msg":"PASSED [sig-api-machinery] Garbage collector should delete RS created by deployment when not orphaning [Conformance]","total":305,"completed":160,"skipped":2536,"failed":0}
SSSS
------------------------------
[sig-api-machinery] Events 
  should delete a collection of events [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Events
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 19:06:10.197: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a collection of events [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Create set of events
Jun 28 19:06:10.406: INFO: created test-event-1
Jun 28 19:06:10.428: INFO: created test-event-2
Jun 28 19:06:10.444: INFO: created test-event-3
STEP: get a list of Events with a label in the current namespace
STEP: delete collection of events
Jun 28 19:06:10.456: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity
Jun 28 19:06:10.522: INFO: requesting list of events to confirm quantity
[AfterEach] [sig-api-machinery] Events
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 19:06:10.533: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-7428" for this suite.
•{"msg":"PASSED [sig-api-machinery] Events should delete a collection of events [Conformance]","total":305,"completed":161,"skipped":2540,"failed":0}
S
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 19:06:10.566: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0644 on tmpfs
Jun 28 19:06:10.738: INFO: Waiting up to 5m0s for pod "pod-d467005e-83c8-4fba-b6e2-18e0312f52e3" in namespace "emptydir-9651" to be "Succeeded or Failed"
Jun 28 19:06:10.746: INFO: Pod "pod-d467005e-83c8-4fba-b6e2-18e0312f52e3": Phase="Pending", Reason="", readiness=false. Elapsed: 8.299623ms
Jun 28 19:06:12.757: INFO: Pod "pod-d467005e-83c8-4fba-b6e2-18e0312f52e3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.019171832s
STEP: Saw pod success
Jun 28 19:06:12.758: INFO: Pod "pod-d467005e-83c8-4fba-b6e2-18e0312f52e3" satisfied condition "Succeeded or Failed"
Jun 28 19:06:12.767: INFO: Trying to get logs from node 10.13.107.37 pod pod-d467005e-83c8-4fba-b6e2-18e0312f52e3 container test-container: <nil>
STEP: delete the pod
Jun 28 19:06:12.860: INFO: Waiting for pod pod-d467005e-83c8-4fba-b6e2-18e0312f52e3 to disappear
Jun 28 19:06:12.871: INFO: Pod pod-d467005e-83c8-4fba-b6e2-18e0312f52e3 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 19:06:12.872: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9651" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":162,"skipped":2541,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch 
  watch on custom resource definition objects [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 19:06:12.907: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename crd-watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] watch on custom resource definition objects [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jun 28 19:06:13.050: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Creating first CR 
Jun 28 19:06:13.739: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-06-28T19:06:13Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-06-28T19:06:13Z]] name:name1 resourceVersion:84542 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:ad2aa712-134b-43d3-b37e-87dfbbc7c66f] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Creating second CR
Jun 28 19:06:23.767: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-06-28T19:06:23Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-06-28T19:06:23Z]] name:name2 resourceVersion:84764 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:f6a9d59c-c1a1-459c-8fd0-f1d43bac9184] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying first CR
Jun 28 19:06:33.789: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-06-28T19:06:13Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-06-28T19:06:33Z]] name:name1 resourceVersion:84808 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:ad2aa712-134b-43d3-b37e-87dfbbc7c66f] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying second CR
Jun 28 19:06:43.817: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-06-28T19:06:23Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-06-28T19:06:43Z]] name:name2 resourceVersion:84853 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:f6a9d59c-c1a1-459c-8fd0-f1d43bac9184] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting first CR
Jun 28 19:06:53.853: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-06-28T19:06:13Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-06-28T19:06:33Z]] name:name1 resourceVersion:84897 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:ad2aa712-134b-43d3-b37e-87dfbbc7c66f] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting second CR
Jun 28 19:07:03.881: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-06-28T19:06:23Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-06-28T19:06:43Z]] name:name2 resourceVersion:84955 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:f6a9d59c-c1a1-459c-8fd0-f1d43bac9184] num:map[num1:9223372036854775807 num2:1000000]]}
[AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 19:07:14.425: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-watch-972" for this suite.

• [SLOW TEST:61.560 seconds]
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  CustomResourceDefinition Watch
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_watch.go:42
    watch on custom resource definition objects [Conformance]
    /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch watch on custom resource definition objects [Conformance]","total":305,"completed":163,"skipped":2549,"failed":0}
SSSS
------------------------------
[k8s.io] Pods 
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 19:07:14.467: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:181
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Jun 28 19:07:19.291: INFO: Successfully updated pod "pod-update-activedeadlineseconds-db33b0bb-b013-4722-9d04-179f4d77948d"
Jun 28 19:07:19.291: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-db33b0bb-b013-4722-9d04-179f4d77948d" in namespace "pods-9739" to be "terminated due to deadline exceeded"
Jun 28 19:07:19.312: INFO: Pod "pod-update-activedeadlineseconds-db33b0bb-b013-4722-9d04-179f4d77948d": Phase="Running", Reason="", readiness=true. Elapsed: 20.332654ms
Jun 28 19:07:21.323: INFO: Pod "pod-update-activedeadlineseconds-db33b0bb-b013-4722-9d04-179f4d77948d": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 2.031566107s
Jun 28 19:07:21.323: INFO: Pod "pod-update-activedeadlineseconds-db33b0bb-b013-4722-9d04-179f4d77948d" satisfied condition "terminated due to deadline exceeded"
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 19:07:21.323: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-9739" for this suite.

• [SLOW TEST:6.887 seconds]
[k8s.io] Pods
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Pods should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]","total":305,"completed":164,"skipped":2553,"failed":0}
SSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 19:07:21.356: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Given a Pod with a 'name' label pod-adoption-release is created
STEP: When a replicaset with a matching selector is created
STEP: Then the orphan pod is adopted
STEP: When the matched label of one of its pods change
Jun 28 19:07:24.626: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 19:07:25.707: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-7795" for this suite.
•{"msg":"PASSED [sig-apps] ReplicaSet should adopt matching pods on creation and release no longer matching pods [Conformance]","total":305,"completed":165,"skipped":2560,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected combined 
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected combined
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 19:07:25.741: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name configmap-projected-all-test-volume-e5d4d181-c321-4074-a6d8-28981ba76375
STEP: Creating secret with name secret-projected-all-test-volume-d7a2dc31-9f86-430a-8a8f-805ea7bf4162
STEP: Creating a pod to test Check all projections for projected volume plugin
Jun 28 19:07:25.953: INFO: Waiting up to 5m0s for pod "projected-volume-de5a0eac-ed83-423f-8d64-753db3c82a2c" in namespace "projected-7805" to be "Succeeded or Failed"
Jun 28 19:07:25.978: INFO: Pod "projected-volume-de5a0eac-ed83-423f-8d64-753db3c82a2c": Phase="Pending", Reason="", readiness=false. Elapsed: 24.899514ms
Jun 28 19:07:27.992: INFO: Pod "projected-volume-de5a0eac-ed83-423f-8d64-753db3c82a2c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.039055655s
STEP: Saw pod success
Jun 28 19:07:27.993: INFO: Pod "projected-volume-de5a0eac-ed83-423f-8d64-753db3c82a2c" satisfied condition "Succeeded or Failed"
Jun 28 19:07:28.006: INFO: Trying to get logs from node 10.13.107.37 pod projected-volume-de5a0eac-ed83-423f-8d64-753db3c82a2c container projected-all-volume-test: <nil>
STEP: delete the pod
Jun 28 19:07:28.079: INFO: Waiting for pod projected-volume-de5a0eac-ed83-423f-8d64-753db3c82a2c to disappear
Jun 28 19:07:28.090: INFO: Pod projected-volume-de5a0eac-ed83-423f-8d64-753db3c82a2c no longer exists
[AfterEach] [sig-storage] Projected combined
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 19:07:28.090: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7805" for this suite.
•{"msg":"PASSED [sig-storage] Projected combined should project all components that make up the projection API [Projection][NodeConformance] [Conformance]","total":305,"completed":166,"skipped":2645,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD without validation schema [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 19:07:28.124: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD without validation schema [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jun 28 19:07:28.253: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Jun 28 19:07:39.017: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-108209140 --namespace=crd-publish-openapi-7809 create -f -'
Jun 28 19:07:39.959: INFO: stderr: ""
Jun 28 19:07:39.959: INFO: stdout: "e2e-test-crd-publish-openapi-7857-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Jun 28 19:07:39.959: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-108209140 --namespace=crd-publish-openapi-7809 delete e2e-test-crd-publish-openapi-7857-crds test-cr'
Jun 28 19:07:40.190: INFO: stderr: ""
Jun 28 19:07:40.191: INFO: stdout: "e2e-test-crd-publish-openapi-7857-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
Jun 28 19:07:40.191: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-108209140 --namespace=crd-publish-openapi-7809 apply -f -'
Jun 28 19:07:40.980: INFO: stderr: ""
Jun 28 19:07:40.980: INFO: stdout: "e2e-test-crd-publish-openapi-7857-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Jun 28 19:07:40.980: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-108209140 --namespace=crd-publish-openapi-7809 delete e2e-test-crd-publish-openapi-7857-crds test-cr'
Jun 28 19:07:41.191: INFO: stderr: ""
Jun 28 19:07:41.191: INFO: stdout: "e2e-test-crd-publish-openapi-7857-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR without validation schema
Jun 28 19:07:41.191: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-108209140 explain e2e-test-crd-publish-openapi-7857-crds'
Jun 28 19:07:41.898: INFO: stderr: ""
Jun 28 19:07:41.898: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-7857-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 19:07:52.728: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-7809" for this suite.

• [SLOW TEST:24.647 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD without validation schema [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD without validation schema [Conformance]","total":305,"completed":167,"skipped":2667,"failed":0}
SSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl cluster-info 
  should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 19:07:52.772: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[It] should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: validating cluster-info
Jun 28 19:07:53.003: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-108209140 cluster-info'
Jun 28 19:07:53.217: INFO: stderr: ""
Jun 28 19:07:53.217: INFO: stdout: "\x1b[0;32mKubernetes master\x1b[0m is running at \x1b[0;33mhttps://172.21.0.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 19:07:53.217: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3191" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]","total":305,"completed":168,"skipped":2676,"failed":0}
SSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 19:07:53.251: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a watch on configmaps
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: closing the watch once it receives two notifications
Jun 28 19:07:53.435: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-5314 /api/v1/namespaces/watch-5314/configmaps/e2e-watch-test-watch-closed 31231285-820f-48c6-bb7b-71563306763f 85600 0 2021-06-28 19:07:53 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2021-06-28 19:07:53 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Jun 28 19:07:53.435: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-5314 /api/v1/namespaces/watch-5314/configmaps/e2e-watch-test-watch-closed 31231285-820f-48c6-bb7b-71563306763f 85604 0 2021-06-28 19:07:53 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2021-06-28 19:07:53 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time, while the watch is closed
STEP: creating a new watch on configmaps from the last resource version observed by the first watch
STEP: deleting the configmap
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed
Jun 28 19:07:53.493: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-5314 /api/v1/namespaces/watch-5314/configmaps/e2e-watch-test-watch-closed 31231285-820f-48c6-bb7b-71563306763f 85607 0 2021-06-28 19:07:53 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2021-06-28 19:07:53 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Jun 28 19:07:53.493: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-5314 /api/v1/namespaces/watch-5314/configmaps/e2e-watch-test-watch-closed 31231285-820f-48c6-bb7b-71563306763f 85608 0 2021-06-28 19:07:53 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2021-06-28 19:07:53 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 19:07:53.494: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-5314" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should be able to restart watching from the last resource version observed by the previous watch [Conformance]","total":305,"completed":169,"skipped":2686,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 19:07:53.545: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:181
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jun 28 19:07:53.659: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 19:07:57.892: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-8739" for this suite.
•{"msg":"PASSED [k8s.io] Pods should support retrieving logs from the container over websockets [NodeConformance] [Conformance]","total":305,"completed":170,"skipped":2723,"failed":0}
SSS
------------------------------
[sig-network] Proxy version v1 
  should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] version v1
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 19:07:57.928: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: starting an echo server on multiple ports
STEP: creating replication controller proxy-service-cnhxj in namespace proxy-5083
I0628 19:07:58.172070      22 runners.go:190] Created replication controller with name: proxy-service-cnhxj, namespace: proxy-5083, replica count: 1
I0628 19:07:59.223816      22 runners.go:190] proxy-service-cnhxj Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0628 19:08:00.224228      22 runners.go:190] proxy-service-cnhxj Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0628 19:08:01.224592      22 runners.go:190] proxy-service-cnhxj Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0628 19:08:02.225066      22 runners.go:190] proxy-service-cnhxj Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jun 28 19:08:02.244: INFO: setup took 4.130383431s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts
Jun 28 19:08:02.288: INFO: (0) /api/v1/namespaces/proxy-5083/pods/http:proxy-service-cnhxj-x7ztg:160/proxy/: foo (200; 43.535839ms)
Jun 28 19:08:02.294: INFO: (0) /api/v1/namespaces/proxy-5083/pods/proxy-service-cnhxj-x7ztg/proxy/: <a href="/api/v1/namespaces/proxy-5083/pods/proxy-service-cnhxj-x7ztg/proxy/rewriteme">test</a> (200; 48.19005ms)
Jun 28 19:08:02.294: INFO: (0) /api/v1/namespaces/proxy-5083/pods/proxy-service-cnhxj-x7ztg:162/proxy/: bar (200; 49.054713ms)
Jun 28 19:08:02.294: INFO: (0) /api/v1/namespaces/proxy-5083/pods/http:proxy-service-cnhxj-x7ztg:162/proxy/: bar (200; 48.315327ms)
Jun 28 19:08:02.294: INFO: (0) /api/v1/namespaces/proxy-5083/pods/http:proxy-service-cnhxj-x7ztg:1080/proxy/: <a href="/api/v1/namespaces/proxy-5083/pods/http:proxy-service-cnhxj-x7ztg:1080/proxy/rewriteme">... (200; 48.862876ms)
Jun 28 19:08:02.294: INFO: (0) /api/v1/namespaces/proxy-5083/pods/proxy-service-cnhxj-x7ztg:160/proxy/: foo (200; 48.329058ms)
Jun 28 19:08:02.294: INFO: (0) /api/v1/namespaces/proxy-5083/services/http:proxy-service-cnhxj:portname2/proxy/: bar (200; 49.222868ms)
Jun 28 19:08:02.295: INFO: (0) /api/v1/namespaces/proxy-5083/pods/proxy-service-cnhxj-x7ztg:1080/proxy/: <a href="/api/v1/namespaces/proxy-5083/pods/proxy-service-cnhxj-x7ztg:1080/proxy/rewriteme">test<... (200; 49.830143ms)
Jun 28 19:08:02.299: INFO: (0) /api/v1/namespaces/proxy-5083/services/proxy-service-cnhxj:portname1/proxy/: foo (200; 53.686875ms)
Jun 28 19:08:02.299: INFO: (0) /api/v1/namespaces/proxy-5083/services/http:proxy-service-cnhxj:portname1/proxy/: foo (200; 53.597996ms)
Jun 28 19:08:02.299: INFO: (0) /api/v1/namespaces/proxy-5083/services/proxy-service-cnhxj:portname2/proxy/: bar (200; 54.292854ms)
Jun 28 19:08:02.314: INFO: (0) /api/v1/namespaces/proxy-5083/pods/https:proxy-service-cnhxj-x7ztg:460/proxy/: tls baz (200; 68.573689ms)
Jun 28 19:08:02.314: INFO: (0) /api/v1/namespaces/proxy-5083/pods/https:proxy-service-cnhxj-x7ztg:462/proxy/: tls qux (200; 69.006492ms)
Jun 28 19:08:02.314: INFO: (0) /api/v1/namespaces/proxy-5083/services/https:proxy-service-cnhxj:tlsportname1/proxy/: tls baz (200; 68.705531ms)
Jun 28 19:08:02.314: INFO: (0) /api/v1/namespaces/proxy-5083/pods/https:proxy-service-cnhxj-x7ztg:443/proxy/: <a href="/api/v1/namespaces/proxy-5083/pods/https:proxy-service-cnhxj-x7ztg:443/proxy/tlsrewritem... (200; 69.222753ms)
Jun 28 19:08:02.314: INFO: (0) /api/v1/namespaces/proxy-5083/services/https:proxy-service-cnhxj:tlsportname2/proxy/: tls qux (200; 69.476938ms)
Jun 28 19:08:02.345: INFO: (1) /api/v1/namespaces/proxy-5083/pods/http:proxy-service-cnhxj-x7ztg:162/proxy/: bar (200; 30.089754ms)
Jun 28 19:08:02.355: INFO: (1) /api/v1/namespaces/proxy-5083/pods/http:proxy-service-cnhxj-x7ztg:1080/proxy/: <a href="/api/v1/namespaces/proxy-5083/pods/http:proxy-service-cnhxj-x7ztg:1080/proxy/rewriteme">... (200; 40.118123ms)
Jun 28 19:08:02.356: INFO: (1) /api/v1/namespaces/proxy-5083/pods/https:proxy-service-cnhxj-x7ztg:462/proxy/: tls qux (200; 40.3854ms)
Jun 28 19:08:02.363: INFO: (1) /api/v1/namespaces/proxy-5083/services/https:proxy-service-cnhxj:tlsportname2/proxy/: tls qux (200; 48.178468ms)
Jun 28 19:08:02.364: INFO: (1) /api/v1/namespaces/proxy-5083/pods/proxy-service-cnhxj-x7ztg:162/proxy/: bar (200; 48.144043ms)
Jun 28 19:08:02.364: INFO: (1) /api/v1/namespaces/proxy-5083/pods/http:proxy-service-cnhxj-x7ztg:160/proxy/: foo (200; 48.391733ms)
Jun 28 19:08:02.364: INFO: (1) /api/v1/namespaces/proxy-5083/pods/https:proxy-service-cnhxj-x7ztg:460/proxy/: tls baz (200; 48.418089ms)
Jun 28 19:08:02.364: INFO: (1) /api/v1/namespaces/proxy-5083/pods/proxy-service-cnhxj-x7ztg/proxy/: <a href="/api/v1/namespaces/proxy-5083/pods/proxy-service-cnhxj-x7ztg/proxy/rewriteme">test</a> (200; 48.818713ms)
Jun 28 19:08:02.364: INFO: (1) /api/v1/namespaces/proxy-5083/pods/proxy-service-cnhxj-x7ztg:160/proxy/: foo (200; 48.912892ms)
Jun 28 19:08:02.364: INFO: (1) /api/v1/namespaces/proxy-5083/pods/https:proxy-service-cnhxj-x7ztg:443/proxy/: <a href="/api/v1/namespaces/proxy-5083/pods/https:proxy-service-cnhxj-x7ztg:443/proxy/tlsrewritem... (200; 49.056289ms)
Jun 28 19:08:02.365: INFO: (1) /api/v1/namespaces/proxy-5083/pods/proxy-service-cnhxj-x7ztg:1080/proxy/: <a href="/api/v1/namespaces/proxy-5083/pods/proxy-service-cnhxj-x7ztg:1080/proxy/rewriteme">test<... (200; 48.951382ms)
Jun 28 19:08:02.365: INFO: (1) /api/v1/namespaces/proxy-5083/services/https:proxy-service-cnhxj:tlsportname1/proxy/: tls baz (200; 49.235427ms)
Jun 28 19:08:02.373: INFO: (1) /api/v1/namespaces/proxy-5083/services/http:proxy-service-cnhxj:portname2/proxy/: bar (200; 57.816747ms)
Jun 28 19:08:02.377: INFO: (1) /api/v1/namespaces/proxy-5083/services/proxy-service-cnhxj:portname2/proxy/: bar (200; 61.061569ms)
Jun 28 19:08:02.377: INFO: (1) /api/v1/namespaces/proxy-5083/services/proxy-service-cnhxj:portname1/proxy/: foo (200; 61.356094ms)
Jun 28 19:08:02.377: INFO: (1) /api/v1/namespaces/proxy-5083/services/http:proxy-service-cnhxj:portname1/proxy/: foo (200; 61.432761ms)
Jun 28 19:08:02.410: INFO: (2) /api/v1/namespaces/proxy-5083/pods/https:proxy-service-cnhxj-x7ztg:443/proxy/: <a href="/api/v1/namespaces/proxy-5083/pods/https:proxy-service-cnhxj-x7ztg:443/proxy/tlsrewritem... (200; 30.371773ms)
Jun 28 19:08:02.420: INFO: (2) /api/v1/namespaces/proxy-5083/pods/http:proxy-service-cnhxj-x7ztg:160/proxy/: foo (200; 40.706236ms)
Jun 28 19:08:02.420: INFO: (2) /api/v1/namespaces/proxy-5083/pods/https:proxy-service-cnhxj-x7ztg:460/proxy/: tls baz (200; 38.893176ms)
Jun 28 19:08:02.420: INFO: (2) /api/v1/namespaces/proxy-5083/pods/https:proxy-service-cnhxj-x7ztg:462/proxy/: tls qux (200; 39.930114ms)
Jun 28 19:08:02.420: INFO: (2) /api/v1/namespaces/proxy-5083/pods/proxy-service-cnhxj-x7ztg/proxy/: <a href="/api/v1/namespaces/proxy-5083/pods/proxy-service-cnhxj-x7ztg/proxy/rewriteme">test</a> (200; 39.729717ms)
Jun 28 19:08:02.420: INFO: (2) /api/v1/namespaces/proxy-5083/pods/proxy-service-cnhxj-x7ztg:162/proxy/: bar (200; 39.296171ms)
Jun 28 19:08:02.420: INFO: (2) /api/v1/namespaces/proxy-5083/pods/http:proxy-service-cnhxj-x7ztg:1080/proxy/: <a href="/api/v1/namespaces/proxy-5083/pods/http:proxy-service-cnhxj-x7ztg:1080/proxy/rewriteme">... (200; 40.386296ms)
Jun 28 19:08:02.422: INFO: (2) /api/v1/namespaces/proxy-5083/services/https:proxy-service-cnhxj:tlsportname2/proxy/: tls qux (200; 42.116916ms)
Jun 28 19:08:02.422: INFO: (2) /api/v1/namespaces/proxy-5083/pods/proxy-service-cnhxj-x7ztg:1080/proxy/: <a href="/api/v1/namespaces/proxy-5083/pods/proxy-service-cnhxj-x7ztg:1080/proxy/rewriteme">test<... (200; 40.290189ms)
Jun 28 19:08:02.422: INFO: (2) /api/v1/namespaces/proxy-5083/pods/proxy-service-cnhxj-x7ztg:160/proxy/: foo (200; 40.224499ms)
Jun 28 19:08:02.422: INFO: (2) /api/v1/namespaces/proxy-5083/pods/http:proxy-service-cnhxj-x7ztg:162/proxy/: bar (200; 40.285642ms)
Jun 28 19:08:02.428: INFO: (2) /api/v1/namespaces/proxy-5083/services/https:proxy-service-cnhxj:tlsportname1/proxy/: tls baz (200; 46.910117ms)
Jun 28 19:08:02.437: INFO: (2) /api/v1/namespaces/proxy-5083/services/http:proxy-service-cnhxj:portname1/proxy/: foo (200; 55.750544ms)
Jun 28 19:08:02.437: INFO: (2) /api/v1/namespaces/proxy-5083/services/proxy-service-cnhxj:portname2/proxy/: bar (200; 58.090589ms)
Jun 28 19:08:02.439: INFO: (2) /api/v1/namespaces/proxy-5083/services/proxy-service-cnhxj:portname1/proxy/: foo (200; 58.27529ms)
Jun 28 19:08:02.439: INFO: (2) /api/v1/namespaces/proxy-5083/services/http:proxy-service-cnhxj:portname2/proxy/: bar (200; 59.291664ms)
Jun 28 19:08:02.466: INFO: (3) /api/v1/namespaces/proxy-5083/pods/http:proxy-service-cnhxj-x7ztg:160/proxy/: foo (200; 26.971094ms)
Jun 28 19:08:02.471: INFO: (3) /api/v1/namespaces/proxy-5083/pods/http:proxy-service-cnhxj-x7ztg:1080/proxy/: <a href="/api/v1/namespaces/proxy-5083/pods/http:proxy-service-cnhxj-x7ztg:1080/proxy/rewriteme">... (200; 31.303825ms)
Jun 28 19:08:02.473: INFO: (3) /api/v1/namespaces/proxy-5083/pods/proxy-service-cnhxj-x7ztg:1080/proxy/: <a href="/api/v1/namespaces/proxy-5083/pods/proxy-service-cnhxj-x7ztg:1080/proxy/rewriteme">test<... (200; 31.686912ms)
Jun 28 19:08:02.473: INFO: (3) /api/v1/namespaces/proxy-5083/pods/proxy-service-cnhxj-x7ztg:162/proxy/: bar (200; 33.061564ms)
Jun 28 19:08:02.473: INFO: (3) /api/v1/namespaces/proxy-5083/pods/https:proxy-service-cnhxj-x7ztg:443/proxy/: <a href="/api/v1/namespaces/proxy-5083/pods/https:proxy-service-cnhxj-x7ztg:443/proxy/tlsrewritem... (200; 32.358759ms)
Jun 28 19:08:02.473: INFO: (3) /api/v1/namespaces/proxy-5083/pods/https:proxy-service-cnhxj-x7ztg:462/proxy/: tls qux (200; 32.327177ms)
Jun 28 19:08:02.473: INFO: (3) /api/v1/namespaces/proxy-5083/pods/http:proxy-service-cnhxj-x7ztg:162/proxy/: bar (200; 31.937851ms)
Jun 28 19:08:02.475: INFO: (3) /api/v1/namespaces/proxy-5083/pods/proxy-service-cnhxj-x7ztg/proxy/: <a href="/api/v1/namespaces/proxy-5083/pods/proxy-service-cnhxj-x7ztg/proxy/rewriteme">test</a> (200; 35.051239ms)
Jun 28 19:08:02.477: INFO: (3) /api/v1/namespaces/proxy-5083/pods/https:proxy-service-cnhxj-x7ztg:460/proxy/: tls baz (200; 36.982434ms)
Jun 28 19:08:02.478: INFO: (3) /api/v1/namespaces/proxy-5083/pods/proxy-service-cnhxj-x7ztg:160/proxy/: foo (200; 36.687535ms)
Jun 28 19:08:02.480: INFO: (3) /api/v1/namespaces/proxy-5083/services/https:proxy-service-cnhxj:tlsportname1/proxy/: tls baz (200; 39.859693ms)
Jun 28 19:08:02.482: INFO: (3) /api/v1/namespaces/proxy-5083/services/https:proxy-service-cnhxj:tlsportname2/proxy/: tls qux (200; 41.941917ms)
Jun 28 19:08:02.484: INFO: (3) /api/v1/namespaces/proxy-5083/services/http:proxy-service-cnhxj:portname1/proxy/: foo (200; 43.091253ms)
Jun 28 19:08:02.490: INFO: (3) /api/v1/namespaces/proxy-5083/services/http:proxy-service-cnhxj:portname2/proxy/: bar (200; 49.586466ms)
Jun 28 19:08:02.491: INFO: (3) /api/v1/namespaces/proxy-5083/services/proxy-service-cnhxj:portname2/proxy/: bar (200; 51.213769ms)
Jun 28 19:08:02.496: INFO: (3) /api/v1/namespaces/proxy-5083/services/proxy-service-cnhxj:portname1/proxy/: foo (200; 55.180665ms)
Jun 28 19:08:02.520: INFO: (4) /api/v1/namespaces/proxy-5083/pods/http:proxy-service-cnhxj-x7ztg:160/proxy/: foo (200; 23.684303ms)
Jun 28 19:08:02.524: INFO: (4) /api/v1/namespaces/proxy-5083/pods/https:proxy-service-cnhxj-x7ztg:462/proxy/: tls qux (200; 26.998318ms)
Jun 28 19:08:02.524: INFO: (4) /api/v1/namespaces/proxy-5083/pods/https:proxy-service-cnhxj-x7ztg:460/proxy/: tls baz (200; 27.592102ms)
Jun 28 19:08:02.524: INFO: (4) /api/v1/namespaces/proxy-5083/pods/proxy-service-cnhxj-x7ztg/proxy/: <a href="/api/v1/namespaces/proxy-5083/pods/proxy-service-cnhxj-x7ztg/proxy/rewriteme">test</a> (200; 27.60982ms)
Jun 28 19:08:02.528: INFO: (4) /api/v1/namespaces/proxy-5083/pods/http:proxy-service-cnhxj-x7ztg:162/proxy/: bar (200; 30.569631ms)
Jun 28 19:08:02.528: INFO: (4) /api/v1/namespaces/proxy-5083/pods/http:proxy-service-cnhxj-x7ztg:1080/proxy/: <a href="/api/v1/namespaces/proxy-5083/pods/http:proxy-service-cnhxj-x7ztg:1080/proxy/rewriteme">... (200; 30.936636ms)
Jun 28 19:08:02.528: INFO: (4) /api/v1/namespaces/proxy-5083/pods/proxy-service-cnhxj-x7ztg:1080/proxy/: <a href="/api/v1/namespaces/proxy-5083/pods/proxy-service-cnhxj-x7ztg:1080/proxy/rewriteme">test<... (200; 31.166872ms)
Jun 28 19:08:02.528: INFO: (4) /api/v1/namespaces/proxy-5083/pods/https:proxy-service-cnhxj-x7ztg:443/proxy/: <a href="/api/v1/namespaces/proxy-5083/pods/https:proxy-service-cnhxj-x7ztg:443/proxy/tlsrewritem... (200; 31.032975ms)
Jun 28 19:08:02.528: INFO: (4) /api/v1/namespaces/proxy-5083/pods/proxy-service-cnhxj-x7ztg:162/proxy/: bar (200; 31.374599ms)
Jun 28 19:08:02.528: INFO: (4) /api/v1/namespaces/proxy-5083/pods/proxy-service-cnhxj-x7ztg:160/proxy/: foo (200; 31.007648ms)
Jun 28 19:08:02.529: INFO: (4) /api/v1/namespaces/proxy-5083/services/https:proxy-service-cnhxj:tlsportname1/proxy/: tls baz (200; 32.388499ms)
Jun 28 19:08:02.533: INFO: (4) /api/v1/namespaces/proxy-5083/services/https:proxy-service-cnhxj:tlsportname2/proxy/: tls qux (200; 36.382996ms)
Jun 28 19:08:02.536: INFO: (4) /api/v1/namespaces/proxy-5083/services/proxy-service-cnhxj:portname1/proxy/: foo (200; 39.640115ms)
Jun 28 19:08:02.546: INFO: (4) /api/v1/namespaces/proxy-5083/services/proxy-service-cnhxj:portname2/proxy/: bar (200; 49.750417ms)
Jun 28 19:08:02.547: INFO: (4) /api/v1/namespaces/proxy-5083/services/http:proxy-service-cnhxj:portname2/proxy/: bar (200; 49.785437ms)
Jun 28 19:08:02.547: INFO: (4) /api/v1/namespaces/proxy-5083/services/http:proxy-service-cnhxj:portname1/proxy/: foo (200; 49.985338ms)
Jun 28 19:08:02.574: INFO: (5) /api/v1/namespaces/proxy-5083/pods/http:proxy-service-cnhxj-x7ztg:1080/proxy/: <a href="/api/v1/namespaces/proxy-5083/pods/http:proxy-service-cnhxj-x7ztg:1080/proxy/rewriteme">... (200; 26.951861ms)
Jun 28 19:08:02.574: INFO: (5) /api/v1/namespaces/proxy-5083/pods/https:proxy-service-cnhxj-x7ztg:443/proxy/: <a href="/api/v1/namespaces/proxy-5083/pods/https:proxy-service-cnhxj-x7ztg:443/proxy/tlsrewritem... (200; 26.264328ms)
Jun 28 19:08:02.574: INFO: (5) /api/v1/namespaces/proxy-5083/pods/proxy-service-cnhxj-x7ztg/proxy/: <a href="/api/v1/namespaces/proxy-5083/pods/proxy-service-cnhxj-x7ztg/proxy/rewriteme">test</a> (200; 26.760759ms)
Jun 28 19:08:02.577: INFO: (5) /api/v1/namespaces/proxy-5083/pods/https:proxy-service-cnhxj-x7ztg:462/proxy/: tls qux (200; 29.189453ms)
Jun 28 19:08:02.577: INFO: (5) /api/v1/namespaces/proxy-5083/pods/proxy-service-cnhxj-x7ztg:160/proxy/: foo (200; 29.219725ms)
Jun 28 19:08:02.577: INFO: (5) /api/v1/namespaces/proxy-5083/pods/http:proxy-service-cnhxj-x7ztg:162/proxy/: bar (200; 29.090999ms)
Jun 28 19:08:02.577: INFO: (5) /api/v1/namespaces/proxy-5083/pods/proxy-service-cnhxj-x7ztg:162/proxy/: bar (200; 29.067123ms)
Jun 28 19:08:02.577: INFO: (5) /api/v1/namespaces/proxy-5083/pods/proxy-service-cnhxj-x7ztg:1080/proxy/: <a href="/api/v1/namespaces/proxy-5083/pods/proxy-service-cnhxj-x7ztg:1080/proxy/rewriteme">test<... (200; 29.646899ms)
Jun 28 19:08:02.577: INFO: (5) /api/v1/namespaces/proxy-5083/pods/https:proxy-service-cnhxj-x7ztg:460/proxy/: tls baz (200; 29.13737ms)
Jun 28 19:08:02.579: INFO: (5) /api/v1/namespaces/proxy-5083/pods/http:proxy-service-cnhxj-x7ztg:160/proxy/: foo (200; 30.700614ms)
Jun 28 19:08:02.579: INFO: (5) /api/v1/namespaces/proxy-5083/services/https:proxy-service-cnhxj:tlsportname1/proxy/: tls baz (200; 31.156819ms)
Jun 28 19:08:02.579: INFO: (5) /api/v1/namespaces/proxy-5083/services/https:proxy-service-cnhxj:tlsportname2/proxy/: tls qux (200; 31.937276ms)
Jun 28 19:08:02.591: INFO: (5) /api/v1/namespaces/proxy-5083/services/http:proxy-service-cnhxj:portname1/proxy/: foo (200; 43.926584ms)
Jun 28 19:08:02.591: INFO: (5) /api/v1/namespaces/proxy-5083/services/proxy-service-cnhxj:portname2/proxy/: bar (200; 43.997672ms)
Jun 28 19:08:02.591: INFO: (5) /api/v1/namespaces/proxy-5083/services/proxy-service-cnhxj:portname1/proxy/: foo (200; 43.383543ms)
Jun 28 19:08:02.591: INFO: (5) /api/v1/namespaces/proxy-5083/services/http:proxy-service-cnhxj:portname2/proxy/: bar (200; 43.632197ms)
Jun 28 19:08:02.612: INFO: (6) /api/v1/namespaces/proxy-5083/pods/http:proxy-service-cnhxj-x7ztg:1080/proxy/: <a href="/api/v1/namespaces/proxy-5083/pods/http:proxy-service-cnhxj-x7ztg:1080/proxy/rewriteme">... (200; 20.523625ms)
Jun 28 19:08:02.614: INFO: (6) /api/v1/namespaces/proxy-5083/pods/https:proxy-service-cnhxj-x7ztg:462/proxy/: tls qux (200; 22.188679ms)
Jun 28 19:08:02.616: INFO: (6) /api/v1/namespaces/proxy-5083/pods/http:proxy-service-cnhxj-x7ztg:160/proxy/: foo (200; 24.202273ms)
Jun 28 19:08:02.617: INFO: (6) /api/v1/namespaces/proxy-5083/pods/proxy-service-cnhxj-x7ztg:1080/proxy/: <a href="/api/v1/namespaces/proxy-5083/pods/proxy-service-cnhxj-x7ztg:1080/proxy/rewriteme">test<... (200; 25.232272ms)
Jun 28 19:08:02.617: INFO: (6) /api/v1/namespaces/proxy-5083/pods/https:proxy-service-cnhxj-x7ztg:460/proxy/: tls baz (200; 25.183113ms)
Jun 28 19:08:02.617: INFO: (6) /api/v1/namespaces/proxy-5083/pods/proxy-service-cnhxj-x7ztg/proxy/: <a href="/api/v1/namespaces/proxy-5083/pods/proxy-service-cnhxj-x7ztg/proxy/rewriteme">test</a> (200; 25.463971ms)
Jun 28 19:08:02.618: INFO: (6) /api/v1/namespaces/proxy-5083/pods/proxy-service-cnhxj-x7ztg:160/proxy/: foo (200; 25.833149ms)
Jun 28 19:08:02.618: INFO: (6) /api/v1/namespaces/proxy-5083/pods/proxy-service-cnhxj-x7ztg:162/proxy/: bar (200; 25.539991ms)
Jun 28 19:08:02.619: INFO: (6) /api/v1/namespaces/proxy-5083/pods/http:proxy-service-cnhxj-x7ztg:162/proxy/: bar (200; 26.521344ms)
Jun 28 19:08:02.619: INFO: (6) /api/v1/namespaces/proxy-5083/pods/https:proxy-service-cnhxj-x7ztg:443/proxy/: <a href="/api/v1/namespaces/proxy-5083/pods/https:proxy-service-cnhxj-x7ztg:443/proxy/tlsrewritem... (200; 27.030286ms)
Jun 28 19:08:02.619: INFO: (6) /api/v1/namespaces/proxy-5083/services/https:proxy-service-cnhxj:tlsportname2/proxy/: tls qux (200; 27.050088ms)
Jun 28 19:08:02.620: INFO: (6) /api/v1/namespaces/proxy-5083/services/https:proxy-service-cnhxj:tlsportname1/proxy/: tls baz (200; 27.536199ms)
Jun 28 19:08:02.627: INFO: (6) /api/v1/namespaces/proxy-5083/services/proxy-service-cnhxj:portname2/proxy/: bar (200; 34.921618ms)
Jun 28 19:08:02.627: INFO: (6) /api/v1/namespaces/proxy-5083/services/http:proxy-service-cnhxj:portname1/proxy/: foo (200; 35.408393ms)
Jun 28 19:08:02.627: INFO: (6) /api/v1/namespaces/proxy-5083/services/http:proxy-service-cnhxj:portname2/proxy/: bar (200; 35.16472ms)
Jun 28 19:08:02.627: INFO: (6) /api/v1/namespaces/proxy-5083/services/proxy-service-cnhxj:portname1/proxy/: foo (200; 34.901239ms)
Jun 28 19:08:02.644: INFO: (7) /api/v1/namespaces/proxy-5083/pods/https:proxy-service-cnhxj-x7ztg:462/proxy/: tls qux (200; 16.320937ms)
Jun 28 19:08:02.649: INFO: (7) /api/v1/namespaces/proxy-5083/pods/proxy-service-cnhxj-x7ztg:1080/proxy/: <a href="/api/v1/namespaces/proxy-5083/pods/proxy-service-cnhxj-x7ztg:1080/proxy/rewriteme">test<... (200; 20.459802ms)
Jun 28 19:08:02.650: INFO: (7) /api/v1/namespaces/proxy-5083/pods/http:proxy-service-cnhxj-x7ztg:162/proxy/: bar (200; 20.579935ms)
Jun 28 19:08:02.650: INFO: (7) /api/v1/namespaces/proxy-5083/pods/proxy-service-cnhxj-x7ztg/proxy/: <a href="/api/v1/namespaces/proxy-5083/pods/proxy-service-cnhxj-x7ztg/proxy/rewriteme">test</a> (200; 22.292848ms)
Jun 28 19:08:02.650: INFO: (7) /api/v1/namespaces/proxy-5083/pods/https:proxy-service-cnhxj-x7ztg:460/proxy/: tls baz (200; 21.045782ms)
Jun 28 19:08:02.650: INFO: (7) /api/v1/namespaces/proxy-5083/pods/proxy-service-cnhxj-x7ztg:162/proxy/: bar (200; 22.381701ms)
Jun 28 19:08:02.652: INFO: (7) /api/v1/namespaces/proxy-5083/pods/http:proxy-service-cnhxj-x7ztg:160/proxy/: foo (200; 23.044227ms)
Jun 28 19:08:02.653: INFO: (7) /api/v1/namespaces/proxy-5083/pods/https:proxy-service-cnhxj-x7ztg:443/proxy/: <a href="/api/v1/namespaces/proxy-5083/pods/https:proxy-service-cnhxj-x7ztg:443/proxy/tlsrewritem... (200; 22.576019ms)
Jun 28 19:08:02.653: INFO: (7) /api/v1/namespaces/proxy-5083/pods/http:proxy-service-cnhxj-x7ztg:1080/proxy/: <a href="/api/v1/namespaces/proxy-5083/pods/http:proxy-service-cnhxj-x7ztg:1080/proxy/rewriteme">... (200; 22.910984ms)
Jun 28 19:08:02.653: INFO: (7) /api/v1/namespaces/proxy-5083/pods/proxy-service-cnhxj-x7ztg:160/proxy/: foo (200; 23.488049ms)
Jun 28 19:08:02.655: INFO: (7) /api/v1/namespaces/proxy-5083/services/https:proxy-service-cnhxj:tlsportname1/proxy/: tls baz (200; 26.536773ms)
Jun 28 19:08:02.658: INFO: (7) /api/v1/namespaces/proxy-5083/services/http:proxy-service-cnhxj:portname2/proxy/: bar (200; 30.898402ms)
Jun 28 19:08:02.659: INFO: (7) /api/v1/namespaces/proxy-5083/services/https:proxy-service-cnhxj:tlsportname2/proxy/: tls qux (200; 28.635132ms)
Jun 28 19:08:02.659: INFO: (7) /api/v1/namespaces/proxy-5083/services/proxy-service-cnhxj:portname2/proxy/: bar (200; 29.095336ms)
Jun 28 19:08:02.662: INFO: (7) /api/v1/namespaces/proxy-5083/services/proxy-service-cnhxj:portname1/proxy/: foo (200; 33.917065ms)
Jun 28 19:08:02.662: INFO: (7) /api/v1/namespaces/proxy-5083/services/http:proxy-service-cnhxj:portname1/proxy/: foo (200; 33.701523ms)
Jun 28 19:08:02.688: INFO: (8) /api/v1/namespaces/proxy-5083/pods/https:proxy-service-cnhxj-x7ztg:443/proxy/: <a href="/api/v1/namespaces/proxy-5083/pods/https:proxy-service-cnhxj-x7ztg:443/proxy/tlsrewritem... (200; 24.685964ms)
Jun 28 19:08:02.688: INFO: (8) /api/v1/namespaces/proxy-5083/pods/http:proxy-service-cnhxj-x7ztg:160/proxy/: foo (200; 25.641334ms)
Jun 28 19:08:02.688: INFO: (8) /api/v1/namespaces/proxy-5083/pods/https:proxy-service-cnhxj-x7ztg:462/proxy/: tls qux (200; 25.218836ms)
Jun 28 19:08:02.688: INFO: (8) /api/v1/namespaces/proxy-5083/pods/proxy-service-cnhxj-x7ztg:160/proxy/: foo (200; 25.435272ms)
Jun 28 19:08:02.688: INFO: (8) /api/v1/namespaces/proxy-5083/pods/http:proxy-service-cnhxj-x7ztg:1080/proxy/: <a href="/api/v1/namespaces/proxy-5083/pods/http:proxy-service-cnhxj-x7ztg:1080/proxy/rewriteme">... (200; 25.521729ms)
Jun 28 19:08:02.689: INFO: (8) /api/v1/namespaces/proxy-5083/pods/https:proxy-service-cnhxj-x7ztg:460/proxy/: tls baz (200; 25.355484ms)
Jun 28 19:08:02.689: INFO: (8) /api/v1/namespaces/proxy-5083/pods/proxy-service-cnhxj-x7ztg/proxy/: <a href="/api/v1/namespaces/proxy-5083/pods/proxy-service-cnhxj-x7ztg/proxy/rewriteme">test</a> (200; 26.291068ms)
Jun 28 19:08:02.689: INFO: (8) /api/v1/namespaces/proxy-5083/pods/http:proxy-service-cnhxj-x7ztg:162/proxy/: bar (200; 26.242804ms)
Jun 28 19:08:02.689: INFO: (8) /api/v1/namespaces/proxy-5083/pods/proxy-service-cnhxj-x7ztg:1080/proxy/: <a href="/api/v1/namespaces/proxy-5083/pods/proxy-service-cnhxj-x7ztg:1080/proxy/rewriteme">test<... (200; 26.115805ms)
Jun 28 19:08:02.689: INFO: (8) /api/v1/namespaces/proxy-5083/pods/proxy-service-cnhxj-x7ztg:162/proxy/: bar (200; 26.371512ms)
Jun 28 19:08:02.689: INFO: (8) /api/v1/namespaces/proxy-5083/services/https:proxy-service-cnhxj:tlsportname1/proxy/: tls baz (200; 26.534857ms)
Jun 28 19:08:02.690: INFO: (8) /api/v1/namespaces/proxy-5083/services/https:proxy-service-cnhxj:tlsportname2/proxy/: tls qux (200; 26.674859ms)
Jun 28 19:08:02.702: INFO: (8) /api/v1/namespaces/proxy-5083/services/proxy-service-cnhxj:portname1/proxy/: foo (200; 39.18529ms)
Jun 28 19:08:02.702: INFO: (8) /api/v1/namespaces/proxy-5083/services/http:proxy-service-cnhxj:portname2/proxy/: bar (200; 39.00368ms)
Jun 28 19:08:02.702: INFO: (8) /api/v1/namespaces/proxy-5083/services/proxy-service-cnhxj:portname2/proxy/: bar (200; 38.688975ms)
Jun 28 19:08:02.702: INFO: (8) /api/v1/namespaces/proxy-5083/services/http:proxy-service-cnhxj:portname1/proxy/: foo (200; 38.977031ms)
Jun 28 19:08:02.733: INFO: (9) /api/v1/namespaces/proxy-5083/pods/https:proxy-service-cnhxj-x7ztg:443/proxy/: <a href="/api/v1/namespaces/proxy-5083/pods/https:proxy-service-cnhxj-x7ztg:443/proxy/tlsrewritem... (200; 30.955301ms)
Jun 28 19:08:02.735: INFO: (9) /api/v1/namespaces/proxy-5083/pods/https:proxy-service-cnhxj-x7ztg:460/proxy/: tls baz (200; 32.580763ms)
Jun 28 19:08:02.736: INFO: (9) /api/v1/namespaces/proxy-5083/pods/proxy-service-cnhxj-x7ztg:162/proxy/: bar (200; 33.028614ms)
Jun 28 19:08:02.736: INFO: (9) /api/v1/namespaces/proxy-5083/pods/http:proxy-service-cnhxj-x7ztg:160/proxy/: foo (200; 33.877116ms)
Jun 28 19:08:02.736: INFO: (9) /api/v1/namespaces/proxy-5083/pods/http:proxy-service-cnhxj-x7ztg:162/proxy/: bar (200; 33.611929ms)
Jun 28 19:08:02.736: INFO: (9) /api/v1/namespaces/proxy-5083/pods/proxy-service-cnhxj-x7ztg/proxy/: <a href="/api/v1/namespaces/proxy-5083/pods/proxy-service-cnhxj-x7ztg/proxy/rewriteme">test</a> (200; 33.830009ms)
Jun 28 19:08:02.739: INFO: (9) /api/v1/namespaces/proxy-5083/pods/http:proxy-service-cnhxj-x7ztg:1080/proxy/: <a href="/api/v1/namespaces/proxy-5083/pods/http:proxy-service-cnhxj-x7ztg:1080/proxy/rewriteme">... (200; 35.266314ms)
Jun 28 19:08:02.739: INFO: (9) /api/v1/namespaces/proxy-5083/pods/proxy-service-cnhxj-x7ztg:1080/proxy/: <a href="/api/v1/namespaces/proxy-5083/pods/proxy-service-cnhxj-x7ztg:1080/proxy/rewriteme">test<... (200; 35.522904ms)
Jun 28 19:08:02.739: INFO: (9) /api/v1/namespaces/proxy-5083/pods/https:proxy-service-cnhxj-x7ztg:462/proxy/: tls qux (200; 35.859408ms)
Jun 28 19:08:02.739: INFO: (9) /api/v1/namespaces/proxy-5083/pods/proxy-service-cnhxj-x7ztg:160/proxy/: foo (200; 35.937777ms)
Jun 28 19:08:02.742: INFO: (9) /api/v1/namespaces/proxy-5083/services/https:proxy-service-cnhxj:tlsportname1/proxy/: tls baz (200; 38.619085ms)
Jun 28 19:08:02.743: INFO: (9) /api/v1/namespaces/proxy-5083/services/https:proxy-service-cnhxj:tlsportname2/proxy/: tls qux (200; 39.612742ms)
Jun 28 19:08:02.762: INFO: (9) /api/v1/namespaces/proxy-5083/services/proxy-service-cnhxj:portname2/proxy/: bar (200; 58.961443ms)
Jun 28 19:08:02.762: INFO: (9) /api/v1/namespaces/proxy-5083/services/http:proxy-service-cnhxj:portname2/proxy/: bar (200; 59.697238ms)
Jun 28 19:08:02.762: INFO: (9) /api/v1/namespaces/proxy-5083/services/http:proxy-service-cnhxj:portname1/proxy/: foo (200; 59.197688ms)
Jun 28 19:08:02.764: INFO: (9) /api/v1/namespaces/proxy-5083/services/proxy-service-cnhxj:portname1/proxy/: foo (200; 60.655933ms)
Jun 28 19:08:02.815: INFO: (10) /api/v1/namespaces/proxy-5083/services/proxy-service-cnhxj:portname2/proxy/: bar (200; 50.622769ms)
Jun 28 19:08:02.815: INFO: (10) /api/v1/namespaces/proxy-5083/services/http:proxy-service-cnhxj:portname1/proxy/: foo (200; 51.53674ms)
Jun 28 19:08:02.825: INFO: (10) /api/v1/namespaces/proxy-5083/services/http:proxy-service-cnhxj:portname2/proxy/: bar (200; 61.209389ms)
Jun 28 19:08:02.826: INFO: (10) /api/v1/namespaces/proxy-5083/services/proxy-service-cnhxj:portname1/proxy/: foo (200; 61.763636ms)
Jun 28 19:08:02.839: INFO: (10) /api/v1/namespaces/proxy-5083/services/https:proxy-service-cnhxj:tlsportname1/proxy/: tls baz (200; 75.090385ms)
Jun 28 19:08:02.839: INFO: (10) /api/v1/namespaces/proxy-5083/pods/https:proxy-service-cnhxj-x7ztg:460/proxy/: tls baz (200; 75.068702ms)
Jun 28 19:08:02.845: INFO: (10) /api/v1/namespaces/proxy-5083/services/https:proxy-service-cnhxj:tlsportname2/proxy/: tls qux (200; 80.627994ms)
Jun 28 19:08:02.846: INFO: (10) /api/v1/namespaces/proxy-5083/pods/https:proxy-service-cnhxj-x7ztg:462/proxy/: tls qux (200; 81.323564ms)
Jun 28 19:08:03.002: INFO: (10) /api/v1/namespaces/proxy-5083/pods/http:proxy-service-cnhxj-x7ztg:1080/proxy/: <a href="/api/v1/namespaces/proxy-5083/pods/http:proxy-service-cnhxj-x7ztg:1080/proxy/rewriteme">... (200; 237.051278ms)
Jun 28 19:08:03.003: INFO: (10) /api/v1/namespaces/proxy-5083/pods/proxy-service-cnhxj-x7ztg/proxy/: <a href="/api/v1/namespaces/proxy-5083/pods/proxy-service-cnhxj-x7ztg/proxy/rewriteme">test</a> (200; 238.201194ms)
Jun 28 19:08:03.003: INFO: (10) /api/v1/namespaces/proxy-5083/pods/proxy-service-cnhxj-x7ztg:1080/proxy/: <a href="/api/v1/namespaces/proxy-5083/pods/proxy-service-cnhxj-x7ztg:1080/proxy/rewriteme">test<... (200; 239.248299ms)
Jun 28 19:08:03.003: INFO: (10) /api/v1/namespaces/proxy-5083/pods/http:proxy-service-cnhxj-x7ztg:162/proxy/: bar (200; 239.633317ms)
Jun 28 19:08:03.004: INFO: (10) /api/v1/namespaces/proxy-5083/pods/https:proxy-service-cnhxj-x7ztg:443/proxy/: <a href="/api/v1/namespaces/proxy-5083/pods/https:proxy-service-cnhxj-x7ztg:443/proxy/tlsrewritem... (200; 239.097326ms)
Jun 28 19:08:03.005: INFO: (10) /api/v1/namespaces/proxy-5083/pods/http:proxy-service-cnhxj-x7ztg:160/proxy/: foo (200; 240.857706ms)
Jun 28 19:08:03.014: INFO: (10) /api/v1/namespaces/proxy-5083/pods/proxy-service-cnhxj-x7ztg:162/proxy/: bar (200; 249.259526ms)
Jun 28 19:08:03.014: INFO: (10) /api/v1/namespaces/proxy-5083/pods/proxy-service-cnhxj-x7ztg:160/proxy/: foo (200; 249.146436ms)
Jun 28 19:08:03.032: INFO: (11) /api/v1/namespaces/proxy-5083/pods/proxy-service-cnhxj-x7ztg:1080/proxy/: <a href="/api/v1/namespaces/proxy-5083/pods/proxy-service-cnhxj-x7ztg:1080/proxy/rewriteme">test<... (200; 17.827981ms)
Jun 28 19:08:03.036: INFO: (11) /api/v1/namespaces/proxy-5083/pods/https:proxy-service-cnhxj-x7ztg:443/proxy/: <a href="/api/v1/namespaces/proxy-5083/pods/https:proxy-service-cnhxj-x7ztg:443/proxy/tlsrewritem... (200; 20.80178ms)
Jun 28 19:08:03.036: INFO: (11) /api/v1/namespaces/proxy-5083/pods/http:proxy-service-cnhxj-x7ztg:160/proxy/: foo (200; 21.6382ms)
Jun 28 19:08:03.037: INFO: (11) /api/v1/namespaces/proxy-5083/pods/https:proxy-service-cnhxj-x7ztg:462/proxy/: tls qux (200; 21.71502ms)
Jun 28 19:08:03.037: INFO: (11) /api/v1/namespaces/proxy-5083/pods/http:proxy-service-cnhxj-x7ztg:1080/proxy/: <a href="/api/v1/namespaces/proxy-5083/pods/http:proxy-service-cnhxj-x7ztg:1080/proxy/rewriteme">... (200; 21.818067ms)
Jun 28 19:08:03.037: INFO: (11) /api/v1/namespaces/proxy-5083/pods/proxy-service-cnhxj-x7ztg:160/proxy/: foo (200; 22.605181ms)
Jun 28 19:08:03.037: INFO: (11) /api/v1/namespaces/proxy-5083/pods/http:proxy-service-cnhxj-x7ztg:162/proxy/: bar (200; 21.526453ms)
Jun 28 19:08:03.037: INFO: (11) /api/v1/namespaces/proxy-5083/pods/https:proxy-service-cnhxj-x7ztg:460/proxy/: tls baz (200; 21.451836ms)
Jun 28 19:08:03.040: INFO: (11) /api/v1/namespaces/proxy-5083/pods/proxy-service-cnhxj-x7ztg/proxy/: <a href="/api/v1/namespaces/proxy-5083/pods/proxy-service-cnhxj-x7ztg/proxy/rewriteme">test</a> (200; 24.612871ms)
Jun 28 19:08:03.040: INFO: (11) /api/v1/namespaces/proxy-5083/pods/proxy-service-cnhxj-x7ztg:162/proxy/: bar (200; 24.594904ms)
Jun 28 19:08:03.049: INFO: (11) /api/v1/namespaces/proxy-5083/services/https:proxy-service-cnhxj:tlsportname2/proxy/: tls qux (200; 33.74668ms)
Jun 28 19:08:03.051: INFO: (11) /api/v1/namespaces/proxy-5083/services/http:proxy-service-cnhxj:portname2/proxy/: bar (200; 35.788271ms)
Jun 28 19:08:03.051: INFO: (11) /api/v1/namespaces/proxy-5083/services/proxy-service-cnhxj:portname1/proxy/: foo (200; 35.277242ms)
Jun 28 19:08:03.051: INFO: (11) /api/v1/namespaces/proxy-5083/services/http:proxy-service-cnhxj:portname1/proxy/: foo (200; 35.286607ms)
Jun 28 19:08:03.052: INFO: (11) /api/v1/namespaces/proxy-5083/services/https:proxy-service-cnhxj:tlsportname1/proxy/: tls baz (200; 35.396343ms)
Jun 28 19:08:03.053: INFO: (11) /api/v1/namespaces/proxy-5083/services/proxy-service-cnhxj:portname2/proxy/: bar (200; 38.218614ms)
Jun 28 19:08:03.075: INFO: (12) /api/v1/namespaces/proxy-5083/pods/http:proxy-service-cnhxj-x7ztg:160/proxy/: foo (200; 21.714221ms)
Jun 28 19:08:03.079: INFO: (12) /api/v1/namespaces/proxy-5083/pods/proxy-service-cnhxj-x7ztg:162/proxy/: bar (200; 24.357489ms)
Jun 28 19:08:03.080: INFO: (12) /api/v1/namespaces/proxy-5083/pods/proxy-service-cnhxj-x7ztg:1080/proxy/: <a href="/api/v1/namespaces/proxy-5083/pods/proxy-service-cnhxj-x7ztg:1080/proxy/rewriteme">test<... (200; 23.957242ms)
Jun 28 19:08:03.080: INFO: (12) /api/v1/namespaces/proxy-5083/pods/http:proxy-service-cnhxj-x7ztg:1080/proxy/: <a href="/api/v1/namespaces/proxy-5083/pods/http:proxy-service-cnhxj-x7ztg:1080/proxy/rewriteme">... (200; 25.460839ms)
Jun 28 19:08:03.080: INFO: (12) /api/v1/namespaces/proxy-5083/pods/http:proxy-service-cnhxj-x7ztg:162/proxy/: bar (200; 23.814838ms)
Jun 28 19:08:03.080: INFO: (12) /api/v1/namespaces/proxy-5083/pods/https:proxy-service-cnhxj-x7ztg:443/proxy/: <a href="/api/v1/namespaces/proxy-5083/pods/https:proxy-service-cnhxj-x7ztg:443/proxy/tlsrewritem... (200; 25.444578ms)
Jun 28 19:08:03.080: INFO: (12) /api/v1/namespaces/proxy-5083/pods/https:proxy-service-cnhxj-x7ztg:462/proxy/: tls qux (200; 25.327225ms)
Jun 28 19:08:03.080: INFO: (12) /api/v1/namespaces/proxy-5083/pods/https:proxy-service-cnhxj-x7ztg:460/proxy/: tls baz (200; 23.803447ms)
Jun 28 19:08:03.080: INFO: (12) /api/v1/namespaces/proxy-5083/pods/proxy-service-cnhxj-x7ztg/proxy/: <a href="/api/v1/namespaces/proxy-5083/pods/proxy-service-cnhxj-x7ztg/proxy/rewriteme">test</a> (200; 25.055663ms)
Jun 28 19:08:03.087: INFO: (12) /api/v1/namespaces/proxy-5083/pods/proxy-service-cnhxj-x7ztg:160/proxy/: foo (200; 30.373384ms)
Jun 28 19:08:03.088: INFO: (12) /api/v1/namespaces/proxy-5083/services/https:proxy-service-cnhxj:tlsportname1/proxy/: tls baz (200; 32.110672ms)
Jun 28 19:08:03.088: INFO: (12) /api/v1/namespaces/proxy-5083/services/http:proxy-service-cnhxj:portname1/proxy/: foo (200; 32.455663ms)
Jun 28 19:08:03.088: INFO: (12) /api/v1/namespaces/proxy-5083/services/https:proxy-service-cnhxj:tlsportname2/proxy/: tls qux (200; 34.212277ms)
Jun 28 19:08:03.092: INFO: (12) /api/v1/namespaces/proxy-5083/services/proxy-service-cnhxj:portname2/proxy/: bar (200; 38.492818ms)
Jun 28 19:08:03.093: INFO: (12) /api/v1/namespaces/proxy-5083/services/http:proxy-service-cnhxj:portname2/proxy/: bar (200; 37.72969ms)
Jun 28 19:08:03.097: INFO: (12) /api/v1/namespaces/proxy-5083/services/proxy-service-cnhxj:portname1/proxy/: foo (200; 41.651548ms)
Jun 28 19:08:03.119: INFO: (13) /api/v1/namespaces/proxy-5083/pods/proxy-service-cnhxj-x7ztg:1080/proxy/: <a href="/api/v1/namespaces/proxy-5083/pods/proxy-service-cnhxj-x7ztg:1080/proxy/rewriteme">test<... (200; 20.987685ms)
Jun 28 19:08:03.138: INFO: (13) /api/v1/namespaces/proxy-5083/pods/proxy-service-cnhxj-x7ztg:162/proxy/: bar (200; 40.430664ms)
Jun 28 19:08:03.138: INFO: (13) /api/v1/namespaces/proxy-5083/pods/proxy-service-cnhxj-x7ztg/proxy/: <a href="/api/v1/namespaces/proxy-5083/pods/proxy-service-cnhxj-x7ztg/proxy/rewriteme">test</a> (200; 39.952418ms)
Jun 28 19:08:03.138: INFO: (13) /api/v1/namespaces/proxy-5083/pods/http:proxy-service-cnhxj-x7ztg:1080/proxy/: <a href="/api/v1/namespaces/proxy-5083/pods/http:proxy-service-cnhxj-x7ztg:1080/proxy/rewriteme">... (200; 40.17996ms)
Jun 28 19:08:03.138: INFO: (13) /api/v1/namespaces/proxy-5083/pods/https:proxy-service-cnhxj-x7ztg:443/proxy/: <a href="/api/v1/namespaces/proxy-5083/pods/https:proxy-service-cnhxj-x7ztg:443/proxy/tlsrewritem... (200; 40.338664ms)
Jun 28 19:08:03.138: INFO: (13) /api/v1/namespaces/proxy-5083/pods/http:proxy-service-cnhxj-x7ztg:162/proxy/: bar (200; 39.8839ms)
Jun 28 19:08:03.138: INFO: (13) /api/v1/namespaces/proxy-5083/pods/https:proxy-service-cnhxj-x7ztg:462/proxy/: tls qux (200; 40.305851ms)
Jun 28 19:08:03.138: INFO: (13) /api/v1/namespaces/proxy-5083/pods/http:proxy-service-cnhxj-x7ztg:160/proxy/: foo (200; 41.044449ms)
Jun 28 19:08:03.138: INFO: (13) /api/v1/namespaces/proxy-5083/pods/https:proxy-service-cnhxj-x7ztg:460/proxy/: tls baz (200; 40.300325ms)
Jun 28 19:08:03.138: INFO: (13) /api/v1/namespaces/proxy-5083/pods/proxy-service-cnhxj-x7ztg:160/proxy/: foo (200; 40.125801ms)
Jun 28 19:08:03.145: INFO: (13) /api/v1/namespaces/proxy-5083/services/https:proxy-service-cnhxj:tlsportname1/proxy/: tls baz (200; 46.556519ms)
Jun 28 19:08:03.145: INFO: (13) /api/v1/namespaces/proxy-5083/services/https:proxy-service-cnhxj:tlsportname2/proxy/: tls qux (200; 47.908538ms)
Jun 28 19:08:03.148: INFO: (13) /api/v1/namespaces/proxy-5083/services/http:proxy-service-cnhxj:portname2/proxy/: bar (200; 50.038614ms)
Jun 28 19:08:03.148: INFO: (13) /api/v1/namespaces/proxy-5083/services/proxy-service-cnhxj:portname1/proxy/: foo (200; 50.105018ms)
Jun 28 19:08:03.148: INFO: (13) /api/v1/namespaces/proxy-5083/services/http:proxy-service-cnhxj:portname1/proxy/: foo (200; 50.249406ms)
Jun 28 19:08:03.148: INFO: (13) /api/v1/namespaces/proxy-5083/services/proxy-service-cnhxj:portname2/proxy/: bar (200; 50.948639ms)
Jun 28 19:08:03.206: INFO: (14) /api/v1/namespaces/proxy-5083/pods/proxy-service-cnhxj-x7ztg:1080/proxy/: <a href="/api/v1/namespaces/proxy-5083/pods/proxy-service-cnhxj-x7ztg:1080/proxy/rewriteme">test<... (200; 57.324937ms)
Jun 28 19:08:03.208: INFO: (14) /api/v1/namespaces/proxy-5083/pods/https:proxy-service-cnhxj-x7ztg:462/proxy/: tls qux (200; 57.468055ms)
Jun 28 19:08:03.210: INFO: (14) /api/v1/namespaces/proxy-5083/pods/proxy-service-cnhxj-x7ztg:160/proxy/: foo (200; 60.98354ms)
Jun 28 19:08:03.211: INFO: (14) /api/v1/namespaces/proxy-5083/pods/http:proxy-service-cnhxj-x7ztg:162/proxy/: bar (200; 59.734249ms)
Jun 28 19:08:03.211: INFO: (14) /api/v1/namespaces/proxy-5083/pods/proxy-service-cnhxj-x7ztg:162/proxy/: bar (200; 59.76837ms)
Jun 28 19:08:03.211: INFO: (14) /api/v1/namespaces/proxy-5083/pods/http:proxy-service-cnhxj-x7ztg:160/proxy/: foo (200; 60.992753ms)
Jun 28 19:08:03.214: INFO: (14) /api/v1/namespaces/proxy-5083/pods/https:proxy-service-cnhxj-x7ztg:443/proxy/: <a href="/api/v1/namespaces/proxy-5083/pods/https:proxy-service-cnhxj-x7ztg:443/proxy/tlsrewritem... (200; 62.968909ms)
Jun 28 19:08:03.214: INFO: (14) /api/v1/namespaces/proxy-5083/pods/proxy-service-cnhxj-x7ztg/proxy/: <a href="/api/v1/namespaces/proxy-5083/pods/proxy-service-cnhxj-x7ztg/proxy/rewriteme">test</a> (200; 62.894207ms)
Jun 28 19:08:03.216: INFO: (14) /api/v1/namespaces/proxy-5083/pods/https:proxy-service-cnhxj-x7ztg:460/proxy/: tls baz (200; 64.813665ms)
Jun 28 19:08:03.216: INFO: (14) /api/v1/namespaces/proxy-5083/services/https:proxy-service-cnhxj:tlsportname2/proxy/: tls qux (200; 65.183425ms)
Jun 28 19:08:03.216: INFO: (14) /api/v1/namespaces/proxy-5083/services/https:proxy-service-cnhxj:tlsportname1/proxy/: tls baz (200; 64.882369ms)
Jun 28 19:08:03.217: INFO: (14) /api/v1/namespaces/proxy-5083/pods/http:proxy-service-cnhxj-x7ztg:1080/proxy/: <a href="/api/v1/namespaces/proxy-5083/pods/http:proxy-service-cnhxj-x7ztg:1080/proxy/rewriteme">... (200; 65.346232ms)
Jun 28 19:08:03.223: INFO: (14) /api/v1/namespaces/proxy-5083/services/proxy-service-cnhxj:portname2/proxy/: bar (200; 72.662009ms)
Jun 28 19:08:03.223: INFO: (14) /api/v1/namespaces/proxy-5083/services/http:proxy-service-cnhxj:portname2/proxy/: bar (200; 72.176391ms)
Jun 28 19:08:03.223: INFO: (14) /api/v1/namespaces/proxy-5083/services/proxy-service-cnhxj:portname1/proxy/: foo (200; 71.845375ms)
Jun 28 19:08:03.223: INFO: (14) /api/v1/namespaces/proxy-5083/services/http:proxy-service-cnhxj:portname1/proxy/: foo (200; 71.832439ms)
Jun 28 19:08:03.249: INFO: (15) /api/v1/namespaces/proxy-5083/pods/proxy-service-cnhxj-x7ztg:162/proxy/: bar (200; 25.225245ms)
Jun 28 19:08:03.253: INFO: (15) /api/v1/namespaces/proxy-5083/pods/https:proxy-service-cnhxj-x7ztg:443/proxy/: <a href="/api/v1/namespaces/proxy-5083/pods/https:proxy-service-cnhxj-x7ztg:443/proxy/tlsrewritem... (200; 27.02693ms)
Jun 28 19:08:03.253: INFO: (15) /api/v1/namespaces/proxy-5083/pods/http:proxy-service-cnhxj-x7ztg:162/proxy/: bar (200; 28.664508ms)
Jun 28 19:08:03.253: INFO: (15) /api/v1/namespaces/proxy-5083/pods/proxy-service-cnhxj-x7ztg:160/proxy/: foo (200; 28.334385ms)
Jun 28 19:08:03.253: INFO: (15) /api/v1/namespaces/proxy-5083/pods/proxy-service-cnhxj-x7ztg:1080/proxy/: <a href="/api/v1/namespaces/proxy-5083/pods/proxy-service-cnhxj-x7ztg:1080/proxy/rewriteme">test<... (200; 28.937349ms)
Jun 28 19:08:03.253: INFO: (15) /api/v1/namespaces/proxy-5083/pods/http:proxy-service-cnhxj-x7ztg:160/proxy/: foo (200; 28.284562ms)
Jun 28 19:08:03.253: INFO: (15) /api/v1/namespaces/proxy-5083/services/https:proxy-service-cnhxj:tlsportname1/proxy/: tls baz (200; 29.225854ms)
Jun 28 19:08:03.253: INFO: (15) /api/v1/namespaces/proxy-5083/pods/https:proxy-service-cnhxj-x7ztg:462/proxy/: tls qux (200; 27.503296ms)
Jun 28 19:08:03.253: INFO: (15) /api/v1/namespaces/proxy-5083/pods/https:proxy-service-cnhxj-x7ztg:460/proxy/: tls baz (200; 28.784973ms)
Jun 28 19:08:03.253: INFO: (15) /api/v1/namespaces/proxy-5083/pods/http:proxy-service-cnhxj-x7ztg:1080/proxy/: <a href="/api/v1/namespaces/proxy-5083/pods/http:proxy-service-cnhxj-x7ztg:1080/proxy/rewriteme">... (200; 27.989095ms)
Jun 28 19:08:03.253: INFO: (15) /api/v1/namespaces/proxy-5083/pods/proxy-service-cnhxj-x7ztg/proxy/: <a href="/api/v1/namespaces/proxy-5083/pods/proxy-service-cnhxj-x7ztg/proxy/rewriteme">test</a> (200; 27.313577ms)
Jun 28 19:08:03.255: INFO: (15) /api/v1/namespaces/proxy-5083/services/https:proxy-service-cnhxj:tlsportname2/proxy/: tls qux (200; 29.364832ms)
Jun 28 19:08:03.259: INFO: (15) /api/v1/namespaces/proxy-5083/services/http:proxy-service-cnhxj:portname2/proxy/: bar (200; 33.532302ms)
Jun 28 19:08:03.267: INFO: (15) /api/v1/namespaces/proxy-5083/services/proxy-service-cnhxj:portname2/proxy/: bar (200; 42.014053ms)
Jun 28 19:08:03.268: INFO: (15) /api/v1/namespaces/proxy-5083/services/proxy-service-cnhxj:portname1/proxy/: foo (200; 44.027111ms)
Jun 28 19:08:03.268: INFO: (15) /api/v1/namespaces/proxy-5083/services/http:proxy-service-cnhxj:portname1/proxy/: foo (200; 44.538543ms)
Jun 28 19:08:03.297: INFO: (16) /api/v1/namespaces/proxy-5083/pods/http:proxy-service-cnhxj-x7ztg:162/proxy/: bar (200; 27.271616ms)
Jun 28 19:08:03.299: INFO: (16) /api/v1/namespaces/proxy-5083/pods/https:proxy-service-cnhxj-x7ztg:443/proxy/: <a href="/api/v1/namespaces/proxy-5083/pods/https:proxy-service-cnhxj-x7ztg:443/proxy/tlsrewritem... (200; 29.436726ms)
Jun 28 19:08:03.299: INFO: (16) /api/v1/namespaces/proxy-5083/pods/proxy-service-cnhxj-x7ztg:162/proxy/: bar (200; 29.60265ms)
Jun 28 19:08:03.299: INFO: (16) /api/v1/namespaces/proxy-5083/pods/http:proxy-service-cnhxj-x7ztg:1080/proxy/: <a href="/api/v1/namespaces/proxy-5083/pods/http:proxy-service-cnhxj-x7ztg:1080/proxy/rewriteme">... (200; 30.023917ms)
Jun 28 19:08:03.299: INFO: (16) /api/v1/namespaces/proxy-5083/pods/https:proxy-service-cnhxj-x7ztg:462/proxy/: tls qux (200; 29.661292ms)
Jun 28 19:08:03.300: INFO: (16) /api/v1/namespaces/proxy-5083/pods/https:proxy-service-cnhxj-x7ztg:460/proxy/: tls baz (200; 30.180464ms)
Jun 28 19:08:03.301: INFO: (16) /api/v1/namespaces/proxy-5083/pods/proxy-service-cnhxj-x7ztg:160/proxy/: foo (200; 30.604253ms)
Jun 28 19:08:03.301: INFO: (16) /api/v1/namespaces/proxy-5083/pods/proxy-service-cnhxj-x7ztg/proxy/: <a href="/api/v1/namespaces/proxy-5083/pods/proxy-service-cnhxj-x7ztg/proxy/rewriteme">test</a> (200; 32.475475ms)
Jun 28 19:08:03.301: INFO: (16) /api/v1/namespaces/proxy-5083/services/https:proxy-service-cnhxj:tlsportname1/proxy/: tls baz (200; 32.700254ms)
Jun 28 19:08:03.302: INFO: (16) /api/v1/namespaces/proxy-5083/pods/http:proxy-service-cnhxj-x7ztg:160/proxy/: foo (200; 33.406546ms)
Jun 28 19:08:03.315: INFO: (16) /api/v1/namespaces/proxy-5083/pods/proxy-service-cnhxj-x7ztg:1080/proxy/: <a href="/api/v1/namespaces/proxy-5083/pods/proxy-service-cnhxj-x7ztg:1080/proxy/rewriteme">test<... (200; 45.384997ms)
Jun 28 19:08:03.315: INFO: (16) /api/v1/namespaces/proxy-5083/services/https:proxy-service-cnhxj:tlsportname2/proxy/: tls qux (200; 46.4467ms)
Jun 28 19:08:03.316: INFO: (16) /api/v1/namespaces/proxy-5083/services/http:proxy-service-cnhxj:portname1/proxy/: foo (200; 45.732ms)
Jun 28 19:08:03.316: INFO: (16) /api/v1/namespaces/proxy-5083/services/proxy-service-cnhxj:portname2/proxy/: bar (200; 46.559689ms)
Jun 28 19:08:03.316: INFO: (16) /api/v1/namespaces/proxy-5083/services/http:proxy-service-cnhxj:portname2/proxy/: bar (200; 45.981437ms)
Jun 28 19:08:03.316: INFO: (16) /api/v1/namespaces/proxy-5083/services/proxy-service-cnhxj:portname1/proxy/: foo (200; 46.475158ms)
Jun 28 19:08:03.335: INFO: (17) /api/v1/namespaces/proxy-5083/pods/http:proxy-service-cnhxj-x7ztg:162/proxy/: bar (200; 18.726754ms)
Jun 28 19:08:03.342: INFO: (17) /api/v1/namespaces/proxy-5083/pods/http:proxy-service-cnhxj-x7ztg:160/proxy/: foo (200; 25.143383ms)
Jun 28 19:08:03.342: INFO: (17) /api/v1/namespaces/proxy-5083/pods/https:proxy-service-cnhxj-x7ztg:462/proxy/: tls qux (200; 25.415859ms)
Jun 28 19:08:03.343: INFO: (17) /api/v1/namespaces/proxy-5083/pods/proxy-service-cnhxj-x7ztg/proxy/: <a href="/api/v1/namespaces/proxy-5083/pods/proxy-service-cnhxj-x7ztg/proxy/rewriteme">test</a> (200; 25.969439ms)
Jun 28 19:08:03.343: INFO: (17) /api/v1/namespaces/proxy-5083/pods/http:proxy-service-cnhxj-x7ztg:1080/proxy/: <a href="/api/v1/namespaces/proxy-5083/pods/http:proxy-service-cnhxj-x7ztg:1080/proxy/rewriteme">... (200; 25.796675ms)
Jun 28 19:08:03.343: INFO: (17) /api/v1/namespaces/proxy-5083/pods/proxy-service-cnhxj-x7ztg:160/proxy/: foo (200; 26.21311ms)
Jun 28 19:08:03.343: INFO: (17) /api/v1/namespaces/proxy-5083/pods/https:proxy-service-cnhxj-x7ztg:460/proxy/: tls baz (200; 26.007931ms)
Jun 28 19:08:03.343: INFO: (17) /api/v1/namespaces/proxy-5083/pods/proxy-service-cnhxj-x7ztg:1080/proxy/: <a href="/api/v1/namespaces/proxy-5083/pods/proxy-service-cnhxj-x7ztg:1080/proxy/rewriteme">test<... (200; 26.676512ms)
Jun 28 19:08:03.343: INFO: (17) /api/v1/namespaces/proxy-5083/pods/proxy-service-cnhxj-x7ztg:162/proxy/: bar (200; 26.821745ms)
Jun 28 19:08:03.344: INFO: (17) /api/v1/namespaces/proxy-5083/pods/https:proxy-service-cnhxj-x7ztg:443/proxy/: <a href="/api/v1/namespaces/proxy-5083/pods/https:proxy-service-cnhxj-x7ztg:443/proxy/tlsrewritem... (200; 26.612263ms)
Jun 28 19:08:03.348: INFO: (17) /api/v1/namespaces/proxy-5083/services/proxy-service-cnhxj:portname2/proxy/: bar (200; 31.291936ms)
Jun 28 19:08:03.348: INFO: (17) /api/v1/namespaces/proxy-5083/services/https:proxy-service-cnhxj:tlsportname1/proxy/: tls baz (200; 31.451032ms)
Jun 28 19:08:03.348: INFO: (17) /api/v1/namespaces/proxy-5083/services/https:proxy-service-cnhxj:tlsportname2/proxy/: tls qux (200; 31.253163ms)
Jun 28 19:08:03.352: INFO: (17) /api/v1/namespaces/proxy-5083/services/http:proxy-service-cnhxj:portname1/proxy/: foo (200; 35.321295ms)
Jun 28 19:08:03.352: INFO: (17) /api/v1/namespaces/proxy-5083/services/proxy-service-cnhxj:portname1/proxy/: foo (200; 35.386279ms)
Jun 28 19:08:03.352: INFO: (17) /api/v1/namespaces/proxy-5083/services/http:proxy-service-cnhxj:portname2/proxy/: bar (200; 35.791227ms)
Jun 28 19:08:03.370: INFO: (18) /api/v1/namespaces/proxy-5083/pods/proxy-service-cnhxj-x7ztg/proxy/: <a href="/api/v1/namespaces/proxy-5083/pods/proxy-service-cnhxj-x7ztg/proxy/rewriteme">test</a> (200; 16.82016ms)
Jun 28 19:08:03.374: INFO: (18) /api/v1/namespaces/proxy-5083/pods/https:proxy-service-cnhxj-x7ztg:460/proxy/: tls baz (200; 20.581653ms)
Jun 28 19:08:03.375: INFO: (18) /api/v1/namespaces/proxy-5083/pods/proxy-service-cnhxj-x7ztg:162/proxy/: bar (200; 22.234612ms)
Jun 28 19:08:03.376: INFO: (18) /api/v1/namespaces/proxy-5083/pods/http:proxy-service-cnhxj-x7ztg:162/proxy/: bar (200; 22.229503ms)
Jun 28 19:08:03.376: INFO: (18) /api/v1/namespaces/proxy-5083/pods/http:proxy-service-cnhxj-x7ztg:160/proxy/: foo (200; 22.295028ms)
Jun 28 19:08:03.376: INFO: (18) /api/v1/namespaces/proxy-5083/pods/proxy-service-cnhxj-x7ztg:160/proxy/: foo (200; 22.522408ms)
Jun 28 19:08:03.376: INFO: (18) /api/v1/namespaces/proxy-5083/pods/proxy-service-cnhxj-x7ztg:1080/proxy/: <a href="/api/v1/namespaces/proxy-5083/pods/proxy-service-cnhxj-x7ztg:1080/proxy/rewriteme">test<... (200; 23.042045ms)
Jun 28 19:08:03.377: INFO: (18) /api/v1/namespaces/proxy-5083/pods/https:proxy-service-cnhxj-x7ztg:443/proxy/: <a href="/api/v1/namespaces/proxy-5083/pods/https:proxy-service-cnhxj-x7ztg:443/proxy/tlsrewritem... (200; 22.519248ms)
Jun 28 19:08:03.377: INFO: (18) /api/v1/namespaces/proxy-5083/pods/http:proxy-service-cnhxj-x7ztg:1080/proxy/: <a href="/api/v1/namespaces/proxy-5083/pods/http:proxy-service-cnhxj-x7ztg:1080/proxy/rewriteme">... (200; 22.796595ms)
Jun 28 19:08:03.377: INFO: (18) /api/v1/namespaces/proxy-5083/pods/https:proxy-service-cnhxj-x7ztg:462/proxy/: tls qux (200; 24.276345ms)
Jun 28 19:08:03.377: INFO: (18) /api/v1/namespaces/proxy-5083/services/https:proxy-service-cnhxj:tlsportname1/proxy/: tls baz (200; 23.914405ms)
Jun 28 19:08:03.381: INFO: (18) /api/v1/namespaces/proxy-5083/services/https:proxy-service-cnhxj:tlsportname2/proxy/: tls qux (200; 27.148504ms)
Jun 28 19:08:03.382: INFO: (18) /api/v1/namespaces/proxy-5083/services/proxy-service-cnhxj:portname1/proxy/: foo (200; 28.411298ms)
Jun 28 19:08:03.382: INFO: (18) /api/v1/namespaces/proxy-5083/services/http:proxy-service-cnhxj:portname1/proxy/: foo (200; 28.93118ms)
Jun 28 19:08:03.382: INFO: (18) /api/v1/namespaces/proxy-5083/services/http:proxy-service-cnhxj:portname2/proxy/: bar (200; 29.487054ms)
Jun 28 19:08:03.383: INFO: (18) /api/v1/namespaces/proxy-5083/services/proxy-service-cnhxj:portname2/proxy/: bar (200; 28.86636ms)
Jun 28 19:08:03.403: INFO: (19) /api/v1/namespaces/proxy-5083/pods/https:proxy-service-cnhxj-x7ztg:460/proxy/: tls baz (200; 19.787476ms)
Jun 28 19:08:03.404: INFO: (19) /api/v1/namespaces/proxy-5083/pods/https:proxy-service-cnhxj-x7ztg:462/proxy/: tls qux (200; 20.717082ms)
Jun 28 19:08:03.404: INFO: (19) /api/v1/namespaces/proxy-5083/pods/http:proxy-service-cnhxj-x7ztg:160/proxy/: foo (200; 21.396837ms)
Jun 28 19:08:03.406: INFO: (19) /api/v1/namespaces/proxy-5083/pods/proxy-service-cnhxj-x7ztg:160/proxy/: foo (200; 22.438869ms)
Jun 28 19:08:03.407: INFO: (19) /api/v1/namespaces/proxy-5083/pods/proxy-service-cnhxj-x7ztg:162/proxy/: bar (200; 23.015644ms)
Jun 28 19:08:03.407: INFO: (19) /api/v1/namespaces/proxy-5083/pods/http:proxy-service-cnhxj-x7ztg:162/proxy/: bar (200; 23.609187ms)
Jun 28 19:08:03.407: INFO: (19) /api/v1/namespaces/proxy-5083/pods/proxy-service-cnhxj-x7ztg/proxy/: <a href="/api/v1/namespaces/proxy-5083/pods/proxy-service-cnhxj-x7ztg/proxy/rewriteme">test</a> (200; 23.792354ms)
Jun 28 19:08:03.408: INFO: (19) /api/v1/namespaces/proxy-5083/pods/https:proxy-service-cnhxj-x7ztg:443/proxy/: <a href="/api/v1/namespaces/proxy-5083/pods/https:proxy-service-cnhxj-x7ztg:443/proxy/tlsrewritem... (200; 24.140183ms)
Jun 28 19:08:03.408: INFO: (19) /api/v1/namespaces/proxy-5083/pods/proxy-service-cnhxj-x7ztg:1080/proxy/: <a href="/api/v1/namespaces/proxy-5083/pods/proxy-service-cnhxj-x7ztg:1080/proxy/rewriteme">test<... (200; 24.195861ms)
Jun 28 19:08:03.408: INFO: (19) /api/v1/namespaces/proxy-5083/pods/http:proxy-service-cnhxj-x7ztg:1080/proxy/: <a href="/api/v1/namespaces/proxy-5083/pods/http:proxy-service-cnhxj-x7ztg:1080/proxy/rewriteme">... (200; 25.044736ms)
Jun 28 19:08:03.408: INFO: (19) /api/v1/namespaces/proxy-5083/services/https:proxy-service-cnhxj:tlsportname2/proxy/: tls qux (200; 25.351247ms)
Jun 28 19:08:03.410: INFO: (19) /api/v1/namespaces/proxy-5083/services/https:proxy-service-cnhxj:tlsportname1/proxy/: tls baz (200; 26.043918ms)
Jun 28 19:08:03.410: INFO: (19) /api/v1/namespaces/proxy-5083/services/proxy-service-cnhxj:portname1/proxy/: foo (200; 26.507498ms)
Jun 28 19:08:03.411: INFO: (19) /api/v1/namespaces/proxy-5083/services/http:proxy-service-cnhxj:portname1/proxy/: foo (200; 27.022456ms)
Jun 28 19:08:03.412: INFO: (19) /api/v1/namespaces/proxy-5083/services/proxy-service-cnhxj:portname2/proxy/: bar (200; 28.61978ms)
Jun 28 19:08:03.412: INFO: (19) /api/v1/namespaces/proxy-5083/services/http:proxy-service-cnhxj:portname2/proxy/: bar (200; 28.25884ms)
STEP: deleting ReplicationController proxy-service-cnhxj in namespace proxy-5083, will wait for the garbage collector to delete the pods
Jun 28 19:08:03.492: INFO: Deleting ReplicationController proxy-service-cnhxj took: 19.63644ms
Jun 28 19:08:03.593: INFO: Terminating ReplicationController proxy-service-cnhxj pods took: 100.43726ms
[AfterEach] version v1
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 19:08:05.894: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-5083" for this suite.

• [SLOW TEST:8.008 seconds]
[sig-network] Proxy
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  version v1
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:59
    should proxy through a service and a pod  [Conformance]
    /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Proxy version v1 should proxy through a service and a pod  [Conformance]","total":305,"completed":171,"skipped":2726,"failed":0}
[sig-network] Services 
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 19:08:05.942: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating service in namespace services-1789
STEP: creating service affinity-nodeport-transition in namespace services-1789
STEP: creating replication controller affinity-nodeport-transition in namespace services-1789
I0628 19:08:06.157693      22 runners.go:190] Created replication controller with name: affinity-nodeport-transition, namespace: services-1789, replica count: 3
I0628 19:08:09.208672      22 runners.go:190] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jun 28 19:08:09.249: INFO: Creating new exec pod
Jun 28 19:08:12.353: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-108209140 exec --namespace=services-1789 execpod-affinityrlbt5 -- /bin/sh -x -c nc -zv -t -w 2 affinity-nodeport-transition 80'
Jun 28 19:08:12.775: INFO: stderr: "+ nc -zv -t -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
Jun 28 19:08:12.775: INFO: stdout: ""
Jun 28 19:08:12.776: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-108209140 exec --namespace=services-1789 execpod-affinityrlbt5 -- /bin/sh -x -c nc -zv -t -w 2 172.21.222.225 80'
Jun 28 19:08:13.437: INFO: stderr: "+ nc -zv -t -w 2 172.21.222.225 80\nConnection to 172.21.222.225 80 port [tcp/http] succeeded!\n"
Jun 28 19:08:13.437: INFO: stdout: ""
Jun 28 19:08:13.437: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-108209140 exec --namespace=services-1789 execpod-affinityrlbt5 -- /bin/sh -x -c nc -zv -t -w 2 10.13.107.37 31759'
Jun 28 19:08:13.990: INFO: stderr: "+ nc -zv -t -w 2 10.13.107.37 31759\nConnection to 10.13.107.37 31759 port [tcp/31759] succeeded!\n"
Jun 28 19:08:13.990: INFO: stdout: ""
Jun 28 19:08:13.991: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-108209140 exec --namespace=services-1789 execpod-affinityrlbt5 -- /bin/sh -x -c nc -zv -t -w 2 10.13.107.57 31759'
Jun 28 19:08:14.407: INFO: stderr: "+ nc -zv -t -w 2 10.13.107.57 31759\nConnection to 10.13.107.57 31759 port [tcp/31759] succeeded!\n"
Jun 28 19:08:14.407: INFO: stdout: ""
Jun 28 19:08:14.408: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-108209140 exec --namespace=services-1789 execpod-affinityrlbt5 -- /bin/sh -x -c nc -zv -t -w 2 149.81.178.52 31759'
Jun 28 19:08:14.868: INFO: stderr: "+ nc -zv -t -w 2 149.81.178.52 31759\nConnection to 149.81.178.52 31759 port [tcp/31759] succeeded!\n"
Jun 28 19:08:14.868: INFO: stdout: ""
Jun 28 19:08:14.868: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-108209140 exec --namespace=services-1789 execpod-affinityrlbt5 -- /bin/sh -x -c nc -zv -t -w 2 149.81.178.54 31759'
Jun 28 19:08:15.284: INFO: stderr: "+ nc -zv -t -w 2 149.81.178.54 31759\nConnection to 149.81.178.54 31759 port [tcp/31759] succeeded!\n"
Jun 28 19:08:15.284: INFO: stdout: ""
Jun 28 19:08:15.325: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-108209140 exec --namespace=services-1789 execpod-affinityrlbt5 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.13.107.37:31759/ ; done'
Jun 28 19:08:15.909: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.13.107.37:31759/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.13.107.37:31759/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.13.107.37:31759/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.13.107.37:31759/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.13.107.37:31759/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.13.107.37:31759/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.13.107.37:31759/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.13.107.37:31759/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.13.107.37:31759/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.13.107.37:31759/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.13.107.37:31759/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.13.107.37:31759/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.13.107.37:31759/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.13.107.37:31759/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.13.107.37:31759/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.13.107.37:31759/\n"
Jun 28 19:08:15.910: INFO: stdout: "\naffinity-nodeport-transition-pp66j\naffinity-nodeport-transition-xp5r6\naffinity-nodeport-transition-lmx8f\naffinity-nodeport-transition-xp5r6\naffinity-nodeport-transition-pp66j\naffinity-nodeport-transition-pp66j\naffinity-nodeport-transition-xp5r6\naffinity-nodeport-transition-xp5r6\naffinity-nodeport-transition-lmx8f\naffinity-nodeport-transition-pp66j\naffinity-nodeport-transition-lmx8f\naffinity-nodeport-transition-xp5r6\naffinity-nodeport-transition-xp5r6\naffinity-nodeport-transition-pp66j\naffinity-nodeport-transition-pp66j\naffinity-nodeport-transition-lmx8f"
Jun 28 19:08:15.910: INFO: Received response from host: affinity-nodeport-transition-pp66j
Jun 28 19:08:15.910: INFO: Received response from host: affinity-nodeport-transition-xp5r6
Jun 28 19:08:15.910: INFO: Received response from host: affinity-nodeport-transition-lmx8f
Jun 28 19:08:15.910: INFO: Received response from host: affinity-nodeport-transition-xp5r6
Jun 28 19:08:15.910: INFO: Received response from host: affinity-nodeport-transition-pp66j
Jun 28 19:08:15.910: INFO: Received response from host: affinity-nodeport-transition-pp66j
Jun 28 19:08:15.910: INFO: Received response from host: affinity-nodeport-transition-xp5r6
Jun 28 19:08:15.910: INFO: Received response from host: affinity-nodeport-transition-xp5r6
Jun 28 19:08:15.910: INFO: Received response from host: affinity-nodeport-transition-lmx8f
Jun 28 19:08:15.910: INFO: Received response from host: affinity-nodeport-transition-pp66j
Jun 28 19:08:15.910: INFO: Received response from host: affinity-nodeport-transition-lmx8f
Jun 28 19:08:15.910: INFO: Received response from host: affinity-nodeport-transition-xp5r6
Jun 28 19:08:15.910: INFO: Received response from host: affinity-nodeport-transition-xp5r6
Jun 28 19:08:15.910: INFO: Received response from host: affinity-nodeport-transition-pp66j
Jun 28 19:08:15.910: INFO: Received response from host: affinity-nodeport-transition-pp66j
Jun 28 19:08:15.910: INFO: Received response from host: affinity-nodeport-transition-lmx8f
Jun 28 19:08:15.947: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-108209140 exec --namespace=services-1789 execpod-affinityrlbt5 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.13.107.37:31759/ ; done'
Jun 28 19:08:16.511: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.13.107.37:31759/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.13.107.37:31759/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.13.107.37:31759/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.13.107.37:31759/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.13.107.37:31759/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.13.107.37:31759/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.13.107.37:31759/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.13.107.37:31759/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.13.107.37:31759/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.13.107.37:31759/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.13.107.37:31759/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.13.107.37:31759/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.13.107.37:31759/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.13.107.37:31759/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.13.107.37:31759/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.13.107.37:31759/\n"
Jun 28 19:08:16.512: INFO: stdout: "\naffinity-nodeport-transition-lmx8f\naffinity-nodeport-transition-lmx8f\naffinity-nodeport-transition-lmx8f\naffinity-nodeport-transition-lmx8f\naffinity-nodeport-transition-lmx8f\naffinity-nodeport-transition-lmx8f\naffinity-nodeport-transition-lmx8f\naffinity-nodeport-transition-lmx8f\naffinity-nodeport-transition-lmx8f\naffinity-nodeport-transition-lmx8f\naffinity-nodeport-transition-lmx8f\naffinity-nodeport-transition-lmx8f\naffinity-nodeport-transition-lmx8f\naffinity-nodeport-transition-lmx8f\naffinity-nodeport-transition-lmx8f\naffinity-nodeport-transition-lmx8f"
Jun 28 19:08:16.512: INFO: Received response from host: affinity-nodeport-transition-lmx8f
Jun 28 19:08:16.512: INFO: Received response from host: affinity-nodeport-transition-lmx8f
Jun 28 19:08:16.513: INFO: Received response from host: affinity-nodeport-transition-lmx8f
Jun 28 19:08:16.513: INFO: Received response from host: affinity-nodeport-transition-lmx8f
Jun 28 19:08:16.513: INFO: Received response from host: affinity-nodeport-transition-lmx8f
Jun 28 19:08:16.513: INFO: Received response from host: affinity-nodeport-transition-lmx8f
Jun 28 19:08:16.513: INFO: Received response from host: affinity-nodeport-transition-lmx8f
Jun 28 19:08:16.513: INFO: Received response from host: affinity-nodeport-transition-lmx8f
Jun 28 19:08:16.513: INFO: Received response from host: affinity-nodeport-transition-lmx8f
Jun 28 19:08:16.514: INFO: Received response from host: affinity-nodeport-transition-lmx8f
Jun 28 19:08:16.514: INFO: Received response from host: affinity-nodeport-transition-lmx8f
Jun 28 19:08:16.514: INFO: Received response from host: affinity-nodeport-transition-lmx8f
Jun 28 19:08:16.514: INFO: Received response from host: affinity-nodeport-transition-lmx8f
Jun 28 19:08:16.514: INFO: Received response from host: affinity-nodeport-transition-lmx8f
Jun 28 19:08:16.514: INFO: Received response from host: affinity-nodeport-transition-lmx8f
Jun 28 19:08:16.514: INFO: Received response from host: affinity-nodeport-transition-lmx8f
Jun 28 19:08:16.515: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-1789, will wait for the garbage collector to delete the pods
Jun 28 19:08:16.634: INFO: Deleting ReplicationController affinity-nodeport-transition took: 19.61669ms
Jun 28 19:08:16.734: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 100.33569ms
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 19:08:28.006: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1789" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

• [SLOW TEST:22.098 seconds]
[sig-network] Services
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]","total":305,"completed":172,"skipped":2726,"failed":0}
[sig-storage] EmptyDir volumes 
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 19:08:28.043: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0644 on tmpfs
Jun 28 19:08:28.251: INFO: Waiting up to 5m0s for pod "pod-f3481d90-a5c7-4d91-9f19-142344d97755" in namespace "emptydir-9873" to be "Succeeded or Failed"
Jun 28 19:08:28.262: INFO: Pod "pod-f3481d90-a5c7-4d91-9f19-142344d97755": Phase="Pending", Reason="", readiness=false. Elapsed: 11.221369ms
Jun 28 19:08:30.280: INFO: Pod "pod-f3481d90-a5c7-4d91-9f19-142344d97755": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028891322s
Jun 28 19:08:32.300: INFO: Pod "pod-f3481d90-a5c7-4d91-9f19-142344d97755": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.048683373s
STEP: Saw pod success
Jun 28 19:08:32.300: INFO: Pod "pod-f3481d90-a5c7-4d91-9f19-142344d97755" satisfied condition "Succeeded or Failed"
Jun 28 19:08:32.309: INFO: Trying to get logs from node 10.13.107.37 pod pod-f3481d90-a5c7-4d91-9f19-142344d97755 container test-container: <nil>
STEP: delete the pod
Jun 28 19:08:32.371: INFO: Waiting for pod pod-f3481d90-a5c7-4d91-9f19-142344d97755 to disappear
Jun 28 19:08:32.380: INFO: Pod pod-f3481d90-a5c7-4d91-9f19-142344d97755 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 19:08:32.380: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9873" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":173,"skipped":2726,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 19:08:32.418: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating secret with name projected-secret-test-70067a48-28c1-40da-b1e7-b81994a727a5
STEP: Creating a pod to test consume secrets
Jun 28 19:08:32.622: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-1889acf5-fa93-48c2-99de-ff435b8d00f2" in namespace "projected-1766" to be "Succeeded or Failed"
Jun 28 19:08:32.633: INFO: Pod "pod-projected-secrets-1889acf5-fa93-48c2-99de-ff435b8d00f2": Phase="Pending", Reason="", readiness=false. Elapsed: 10.977946ms
Jun 28 19:08:34.647: INFO: Pod "pod-projected-secrets-1889acf5-fa93-48c2-99de-ff435b8d00f2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024693431s
Jun 28 19:08:36.667: INFO: Pod "pod-projected-secrets-1889acf5-fa93-48c2-99de-ff435b8d00f2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.044879953s
STEP: Saw pod success
Jun 28 19:08:36.667: INFO: Pod "pod-projected-secrets-1889acf5-fa93-48c2-99de-ff435b8d00f2" satisfied condition "Succeeded or Failed"
Jun 28 19:08:36.678: INFO: Trying to get logs from node 10.13.107.37 pod pod-projected-secrets-1889acf5-fa93-48c2-99de-ff435b8d00f2 container secret-volume-test: <nil>
STEP: delete the pod
Jun 28 19:08:36.744: INFO: Waiting for pod pod-projected-secrets-1889acf5-fa93-48c2-99de-ff435b8d00f2 to disappear
Jun 28 19:08:36.757: INFO: Pod pod-projected-secrets-1889acf5-fa93-48c2-99de-ff435b8d00f2 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 19:08:36.757: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1766" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","total":305,"completed":174,"skipped":2742,"failed":0}

------------------------------
[sig-api-machinery] Discovery 
  should validate PreferredVersion for each APIGroup [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Discovery
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 19:08:36.789: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename discovery
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Discovery
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/discovery.go:39
STEP: Setting up server cert
[It] should validate PreferredVersion for each APIGroup [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jun 28 19:08:37.330: INFO: Checking APIGroup: apiregistration.k8s.io
Jun 28 19:08:37.334: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
Jun 28 19:08:37.334: INFO: Versions found [{apiregistration.k8s.io/v1 v1} {apiregistration.k8s.io/v1beta1 v1beta1}]
Jun 28 19:08:37.334: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
Jun 28 19:08:37.334: INFO: Checking APIGroup: extensions
Jun 28 19:08:37.338: INFO: PreferredVersion.GroupVersion: extensions/v1beta1
Jun 28 19:08:37.338: INFO: Versions found [{extensions/v1beta1 v1beta1}]
Jun 28 19:08:37.338: INFO: extensions/v1beta1 matches extensions/v1beta1
Jun 28 19:08:37.338: INFO: Checking APIGroup: apps
Jun 28 19:08:37.342: INFO: PreferredVersion.GroupVersion: apps/v1
Jun 28 19:08:37.342: INFO: Versions found [{apps/v1 v1}]
Jun 28 19:08:37.342: INFO: apps/v1 matches apps/v1
Jun 28 19:08:37.342: INFO: Checking APIGroup: events.k8s.io
Jun 28 19:08:37.345: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
Jun 28 19:08:37.346: INFO: Versions found [{events.k8s.io/v1 v1} {events.k8s.io/v1beta1 v1beta1}]
Jun 28 19:08:37.346: INFO: events.k8s.io/v1 matches events.k8s.io/v1
Jun 28 19:08:37.346: INFO: Checking APIGroup: authentication.k8s.io
Jun 28 19:08:37.349: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
Jun 28 19:08:37.349: INFO: Versions found [{authentication.k8s.io/v1 v1} {authentication.k8s.io/v1beta1 v1beta1}]
Jun 28 19:08:37.349: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
Jun 28 19:08:37.349: INFO: Checking APIGroup: authorization.k8s.io
Jun 28 19:08:37.353: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
Jun 28 19:08:37.353: INFO: Versions found [{authorization.k8s.io/v1 v1} {authorization.k8s.io/v1beta1 v1beta1}]
Jun 28 19:08:37.353: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
Jun 28 19:08:37.353: INFO: Checking APIGroup: autoscaling
Jun 28 19:08:37.357: INFO: PreferredVersion.GroupVersion: autoscaling/v1
Jun 28 19:08:37.357: INFO: Versions found [{autoscaling/v1 v1} {autoscaling/v2beta1 v2beta1} {autoscaling/v2beta2 v2beta2}]
Jun 28 19:08:37.357: INFO: autoscaling/v1 matches autoscaling/v1
Jun 28 19:08:37.357: INFO: Checking APIGroup: batch
Jun 28 19:08:37.361: INFO: PreferredVersion.GroupVersion: batch/v1
Jun 28 19:08:37.361: INFO: Versions found [{batch/v1 v1} {batch/v1beta1 v1beta1}]
Jun 28 19:08:37.361: INFO: batch/v1 matches batch/v1
Jun 28 19:08:37.361: INFO: Checking APIGroup: certificates.k8s.io
Jun 28 19:08:37.367: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
Jun 28 19:08:37.367: INFO: Versions found [{certificates.k8s.io/v1 v1} {certificates.k8s.io/v1beta1 v1beta1}]
Jun 28 19:08:37.367: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
Jun 28 19:08:37.367: INFO: Checking APIGroup: networking.k8s.io
Jun 28 19:08:37.377: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
Jun 28 19:08:37.377: INFO: Versions found [{networking.k8s.io/v1 v1} {networking.k8s.io/v1beta1 v1beta1}]
Jun 28 19:08:37.377: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
Jun 28 19:08:37.377: INFO: Checking APIGroup: policy
Jun 28 19:08:37.381: INFO: PreferredVersion.GroupVersion: policy/v1beta1
Jun 28 19:08:37.381: INFO: Versions found [{policy/v1beta1 v1beta1}]
Jun 28 19:08:37.381: INFO: policy/v1beta1 matches policy/v1beta1
Jun 28 19:08:37.382: INFO: Checking APIGroup: rbac.authorization.k8s.io
Jun 28 19:08:37.388: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
Jun 28 19:08:37.388: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1} {rbac.authorization.k8s.io/v1beta1 v1beta1}]
Jun 28 19:08:37.388: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
Jun 28 19:08:37.388: INFO: Checking APIGroup: storage.k8s.io
Jun 28 19:08:37.393: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
Jun 28 19:08:37.393: INFO: Versions found [{storage.k8s.io/v1 v1} {storage.k8s.io/v1beta1 v1beta1}]
Jun 28 19:08:37.393: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
Jun 28 19:08:37.393: INFO: Checking APIGroup: admissionregistration.k8s.io
Jun 28 19:08:37.400: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
Jun 28 19:08:37.401: INFO: Versions found [{admissionregistration.k8s.io/v1 v1} {admissionregistration.k8s.io/v1beta1 v1beta1}]
Jun 28 19:08:37.401: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
Jun 28 19:08:37.402: INFO: Checking APIGroup: apiextensions.k8s.io
Jun 28 19:08:37.413: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
Jun 28 19:08:37.413: INFO: Versions found [{apiextensions.k8s.io/v1 v1} {apiextensions.k8s.io/v1beta1 v1beta1}]
Jun 28 19:08:37.413: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
Jun 28 19:08:37.414: INFO: Checking APIGroup: scheduling.k8s.io
Jun 28 19:08:37.417: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
Jun 28 19:08:37.417: INFO: Versions found [{scheduling.k8s.io/v1 v1} {scheduling.k8s.io/v1beta1 v1beta1}]
Jun 28 19:08:37.417: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
Jun 28 19:08:37.417: INFO: Checking APIGroup: coordination.k8s.io
Jun 28 19:08:37.421: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
Jun 28 19:08:37.421: INFO: Versions found [{coordination.k8s.io/v1 v1} {coordination.k8s.io/v1beta1 v1beta1}]
Jun 28 19:08:37.421: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
Jun 28 19:08:37.421: INFO: Checking APIGroup: node.k8s.io
Jun 28 19:08:37.425: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1beta1
Jun 28 19:08:37.425: INFO: Versions found [{node.k8s.io/v1beta1 v1beta1}]
Jun 28 19:08:37.425: INFO: node.k8s.io/v1beta1 matches node.k8s.io/v1beta1
Jun 28 19:08:37.425: INFO: Checking APIGroup: discovery.k8s.io
Jun 28 19:08:37.428: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1beta1
Jun 28 19:08:37.429: INFO: Versions found [{discovery.k8s.io/v1beta1 v1beta1}]
Jun 28 19:08:37.429: INFO: discovery.k8s.io/v1beta1 matches discovery.k8s.io/v1beta1
Jun 28 19:08:37.429: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
Jun 28 19:08:37.434: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1alpha1
Jun 28 19:08:37.434: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1alpha1 v1alpha1}]
Jun 28 19:08:37.434: INFO: flowcontrol.apiserver.k8s.io/v1alpha1 matches flowcontrol.apiserver.k8s.io/v1alpha1
Jun 28 19:08:37.434: INFO: Checking APIGroup: apps.openshift.io
Jun 28 19:08:37.438: INFO: PreferredVersion.GroupVersion: apps.openshift.io/v1
Jun 28 19:08:37.438: INFO: Versions found [{apps.openshift.io/v1 v1}]
Jun 28 19:08:37.439: INFO: apps.openshift.io/v1 matches apps.openshift.io/v1
Jun 28 19:08:37.439: INFO: Checking APIGroup: authorization.openshift.io
Jun 28 19:08:37.444: INFO: PreferredVersion.GroupVersion: authorization.openshift.io/v1
Jun 28 19:08:37.444: INFO: Versions found [{authorization.openshift.io/v1 v1}]
Jun 28 19:08:37.444: INFO: authorization.openshift.io/v1 matches authorization.openshift.io/v1
Jun 28 19:08:37.444: INFO: Checking APIGroup: build.openshift.io
Jun 28 19:08:37.448: INFO: PreferredVersion.GroupVersion: build.openshift.io/v1
Jun 28 19:08:37.448: INFO: Versions found [{build.openshift.io/v1 v1}]
Jun 28 19:08:37.449: INFO: build.openshift.io/v1 matches build.openshift.io/v1
Jun 28 19:08:37.449: INFO: Checking APIGroup: image.openshift.io
Jun 28 19:08:37.452: INFO: PreferredVersion.GroupVersion: image.openshift.io/v1
Jun 28 19:08:37.452: INFO: Versions found [{image.openshift.io/v1 v1}]
Jun 28 19:08:37.453: INFO: image.openshift.io/v1 matches image.openshift.io/v1
Jun 28 19:08:37.453: INFO: Checking APIGroup: oauth.openshift.io
Jun 28 19:08:37.457: INFO: PreferredVersion.GroupVersion: oauth.openshift.io/v1
Jun 28 19:08:37.457: INFO: Versions found [{oauth.openshift.io/v1 v1}]
Jun 28 19:08:37.457: INFO: oauth.openshift.io/v1 matches oauth.openshift.io/v1
Jun 28 19:08:37.457: INFO: Checking APIGroup: project.openshift.io
Jun 28 19:08:37.461: INFO: PreferredVersion.GroupVersion: project.openshift.io/v1
Jun 28 19:08:37.462: INFO: Versions found [{project.openshift.io/v1 v1}]
Jun 28 19:08:37.462: INFO: project.openshift.io/v1 matches project.openshift.io/v1
Jun 28 19:08:37.462: INFO: Checking APIGroup: quota.openshift.io
Jun 28 19:08:37.466: INFO: PreferredVersion.GroupVersion: quota.openshift.io/v1
Jun 28 19:08:37.466: INFO: Versions found [{quota.openshift.io/v1 v1}]
Jun 28 19:08:37.466: INFO: quota.openshift.io/v1 matches quota.openshift.io/v1
Jun 28 19:08:37.466: INFO: Checking APIGroup: route.openshift.io
Jun 28 19:08:37.470: INFO: PreferredVersion.GroupVersion: route.openshift.io/v1
Jun 28 19:08:37.470: INFO: Versions found [{route.openshift.io/v1 v1}]
Jun 28 19:08:37.470: INFO: route.openshift.io/v1 matches route.openshift.io/v1
Jun 28 19:08:37.470: INFO: Checking APIGroup: security.openshift.io
Jun 28 19:08:37.474: INFO: PreferredVersion.GroupVersion: security.openshift.io/v1
Jun 28 19:08:37.474: INFO: Versions found [{security.openshift.io/v1 v1}]
Jun 28 19:08:37.474: INFO: security.openshift.io/v1 matches security.openshift.io/v1
Jun 28 19:08:37.474: INFO: Checking APIGroup: template.openshift.io
Jun 28 19:08:37.478: INFO: PreferredVersion.GroupVersion: template.openshift.io/v1
Jun 28 19:08:37.478: INFO: Versions found [{template.openshift.io/v1 v1}]
Jun 28 19:08:37.478: INFO: template.openshift.io/v1 matches template.openshift.io/v1
Jun 28 19:08:37.478: INFO: Checking APIGroup: user.openshift.io
Jun 28 19:08:37.483: INFO: PreferredVersion.GroupVersion: user.openshift.io/v1
Jun 28 19:08:37.483: INFO: Versions found [{user.openshift.io/v1 v1}]
Jun 28 19:08:37.483: INFO: user.openshift.io/v1 matches user.openshift.io/v1
Jun 28 19:08:37.483: INFO: Checking APIGroup: packages.operators.coreos.com
Jun 28 19:08:37.487: INFO: PreferredVersion.GroupVersion: packages.operators.coreos.com/v1
Jun 28 19:08:37.487: INFO: Versions found [{packages.operators.coreos.com/v1 v1}]
Jun 28 19:08:37.487: INFO: packages.operators.coreos.com/v1 matches packages.operators.coreos.com/v1
Jun 28 19:08:37.487: INFO: Checking APIGroup: config.openshift.io
Jun 28 19:08:37.491: INFO: PreferredVersion.GroupVersion: config.openshift.io/v1
Jun 28 19:08:37.491: INFO: Versions found [{config.openshift.io/v1 v1}]
Jun 28 19:08:37.491: INFO: config.openshift.io/v1 matches config.openshift.io/v1
Jun 28 19:08:37.491: INFO: Checking APIGroup: operator.openshift.io
Jun 28 19:08:37.495: INFO: PreferredVersion.GroupVersion: operator.openshift.io/v1
Jun 28 19:08:37.495: INFO: Versions found [{operator.openshift.io/v1 v1} {operator.openshift.io/v1alpha1 v1alpha1}]
Jun 28 19:08:37.495: INFO: operator.openshift.io/v1 matches operator.openshift.io/v1
Jun 28 19:08:37.495: INFO: Checking APIGroup: cloudcredential.openshift.io
Jun 28 19:08:37.500: INFO: PreferredVersion.GroupVersion: cloudcredential.openshift.io/v1
Jun 28 19:08:37.500: INFO: Versions found [{cloudcredential.openshift.io/v1 v1}]
Jun 28 19:08:37.500: INFO: cloudcredential.openshift.io/v1 matches cloudcredential.openshift.io/v1
Jun 28 19:08:37.500: INFO: Checking APIGroup: console.openshift.io
Jun 28 19:08:37.504: INFO: PreferredVersion.GroupVersion: console.openshift.io/v1
Jun 28 19:08:37.504: INFO: Versions found [{console.openshift.io/v1 v1}]
Jun 28 19:08:37.504: INFO: console.openshift.io/v1 matches console.openshift.io/v1
Jun 28 19:08:37.504: INFO: Checking APIGroup: crd.projectcalico.org
Jun 28 19:08:37.508: INFO: PreferredVersion.GroupVersion: crd.projectcalico.org/v1
Jun 28 19:08:37.508: INFO: Versions found [{crd.projectcalico.org/v1 v1}]
Jun 28 19:08:37.508: INFO: crd.projectcalico.org/v1 matches crd.projectcalico.org/v1
Jun 28 19:08:37.508: INFO: Checking APIGroup: imageregistry.operator.openshift.io
Jun 28 19:08:37.512: INFO: PreferredVersion.GroupVersion: imageregistry.operator.openshift.io/v1
Jun 28 19:08:37.512: INFO: Versions found [{imageregistry.operator.openshift.io/v1 v1}]
Jun 28 19:08:37.512: INFO: imageregistry.operator.openshift.io/v1 matches imageregistry.operator.openshift.io/v1
Jun 28 19:08:37.512: INFO: Checking APIGroup: ingress.operator.openshift.io
Jun 28 19:08:37.517: INFO: PreferredVersion.GroupVersion: ingress.operator.openshift.io/v1
Jun 28 19:08:37.517: INFO: Versions found [{ingress.operator.openshift.io/v1 v1}]
Jun 28 19:08:37.517: INFO: ingress.operator.openshift.io/v1 matches ingress.operator.openshift.io/v1
Jun 28 19:08:37.517: INFO: Checking APIGroup: k8s.cni.cncf.io
Jun 28 19:08:37.523: INFO: PreferredVersion.GroupVersion: k8s.cni.cncf.io/v1
Jun 28 19:08:37.524: INFO: Versions found [{k8s.cni.cncf.io/v1 v1}]
Jun 28 19:08:37.524: INFO: k8s.cni.cncf.io/v1 matches k8s.cni.cncf.io/v1
Jun 28 19:08:37.524: INFO: Checking APIGroup: machineconfiguration.openshift.io
Jun 28 19:08:37.529: INFO: PreferredVersion.GroupVersion: machineconfiguration.openshift.io/v1
Jun 28 19:08:37.529: INFO: Versions found [{machineconfiguration.openshift.io/v1 v1}]
Jun 28 19:08:37.529: INFO: machineconfiguration.openshift.io/v1 matches machineconfiguration.openshift.io/v1
Jun 28 19:08:37.529: INFO: Checking APIGroup: monitoring.coreos.com
Jun 28 19:08:37.533: INFO: PreferredVersion.GroupVersion: monitoring.coreos.com/v1
Jun 28 19:08:37.533: INFO: Versions found [{monitoring.coreos.com/v1 v1}]
Jun 28 19:08:37.533: INFO: monitoring.coreos.com/v1 matches monitoring.coreos.com/v1
Jun 28 19:08:37.533: INFO: Checking APIGroup: network.operator.openshift.io
Jun 28 19:08:37.537: INFO: PreferredVersion.GroupVersion: network.operator.openshift.io/v1
Jun 28 19:08:37.537: INFO: Versions found [{network.operator.openshift.io/v1 v1}]
Jun 28 19:08:37.537: INFO: network.operator.openshift.io/v1 matches network.operator.openshift.io/v1
Jun 28 19:08:37.537: INFO: Checking APIGroup: operator.tigera.io
Jun 28 19:08:37.542: INFO: PreferredVersion.GroupVersion: operator.tigera.io/v1
Jun 28 19:08:37.542: INFO: Versions found [{operator.tigera.io/v1 v1}]
Jun 28 19:08:37.542: INFO: operator.tigera.io/v1 matches operator.tigera.io/v1
Jun 28 19:08:37.542: INFO: Checking APIGroup: operators.coreos.com
Jun 28 19:08:37.546: INFO: PreferredVersion.GroupVersion: operators.coreos.com/v1
Jun 28 19:08:37.546: INFO: Versions found [{operators.coreos.com/v1 v1} {operators.coreos.com/v1alpha2 v1alpha2} {operators.coreos.com/v1alpha1 v1alpha1}]
Jun 28 19:08:37.546: INFO: operators.coreos.com/v1 matches operators.coreos.com/v1
Jun 28 19:08:37.546: INFO: Checking APIGroup: samples.operator.openshift.io
Jun 28 19:08:37.551: INFO: PreferredVersion.GroupVersion: samples.operator.openshift.io/v1
Jun 28 19:08:37.551: INFO: Versions found [{samples.operator.openshift.io/v1 v1}]
Jun 28 19:08:37.551: INFO: samples.operator.openshift.io/v1 matches samples.operator.openshift.io/v1
Jun 28 19:08:37.551: INFO: Checking APIGroup: security.internal.openshift.io
Jun 28 19:08:37.555: INFO: PreferredVersion.GroupVersion: security.internal.openshift.io/v1
Jun 28 19:08:37.556: INFO: Versions found [{security.internal.openshift.io/v1 v1}]
Jun 28 19:08:37.556: INFO: security.internal.openshift.io/v1 matches security.internal.openshift.io/v1
Jun 28 19:08:37.556: INFO: Checking APIGroup: tuned.openshift.io
Jun 28 19:08:37.561: INFO: PreferredVersion.GroupVersion: tuned.openshift.io/v1
Jun 28 19:08:37.561: INFO: Versions found [{tuned.openshift.io/v1 v1}]
Jun 28 19:08:37.561: INFO: tuned.openshift.io/v1 matches tuned.openshift.io/v1
Jun 28 19:08:37.561: INFO: Checking APIGroup: ibm.com
Jun 28 19:08:37.565: INFO: PreferredVersion.GroupVersion: ibm.com/v1alpha1
Jun 28 19:08:37.566: INFO: Versions found [{ibm.com/v1alpha1 v1alpha1}]
Jun 28 19:08:37.566: INFO: ibm.com/v1alpha1 matches ibm.com/v1alpha1
Jun 28 19:08:37.566: INFO: Checking APIGroup: metal3.io
Jun 28 19:08:37.570: INFO: PreferredVersion.GroupVersion: metal3.io/v1alpha1
Jun 28 19:08:37.570: INFO: Versions found [{metal3.io/v1alpha1 v1alpha1}]
Jun 28 19:08:37.570: INFO: metal3.io/v1alpha1 matches metal3.io/v1alpha1
Jun 28 19:08:37.570: INFO: Checking APIGroup: migration.k8s.io
Jun 28 19:08:37.574: INFO: PreferredVersion.GroupVersion: migration.k8s.io/v1alpha1
Jun 28 19:08:37.574: INFO: Versions found [{migration.k8s.io/v1alpha1 v1alpha1}]
Jun 28 19:08:37.574: INFO: migration.k8s.io/v1alpha1 matches migration.k8s.io/v1alpha1
Jun 28 19:08:37.574: INFO: Checking APIGroup: whereabouts.cni.cncf.io
Jun 28 19:08:37.578: INFO: PreferredVersion.GroupVersion: whereabouts.cni.cncf.io/v1alpha1
Jun 28 19:08:37.578: INFO: Versions found [{whereabouts.cni.cncf.io/v1alpha1 v1alpha1}]
Jun 28 19:08:37.578: INFO: whereabouts.cni.cncf.io/v1alpha1 matches whereabouts.cni.cncf.io/v1alpha1
Jun 28 19:08:37.578: INFO: Checking APIGroup: helm.openshift.io
Jun 28 19:08:37.582: INFO: PreferredVersion.GroupVersion: helm.openshift.io/v1beta1
Jun 28 19:08:37.582: INFO: Versions found [{helm.openshift.io/v1beta1 v1beta1}]
Jun 28 19:08:37.582: INFO: helm.openshift.io/v1beta1 matches helm.openshift.io/v1beta1
Jun 28 19:08:37.582: INFO: Checking APIGroup: snapshot.storage.k8s.io
Jun 28 19:08:37.586: INFO: PreferredVersion.GroupVersion: snapshot.storage.k8s.io/v1beta1
Jun 28 19:08:37.586: INFO: Versions found [{snapshot.storage.k8s.io/v1beta1 v1beta1}]
Jun 28 19:08:37.586: INFO: snapshot.storage.k8s.io/v1beta1 matches snapshot.storage.k8s.io/v1beta1
Jun 28 19:08:37.586: INFO: Checking APIGroup: metrics.k8s.io
Jun 28 19:08:37.624: INFO: PreferredVersion.GroupVersion: metrics.k8s.io/v1beta1
Jun 28 19:08:37.624: INFO: Versions found [{metrics.k8s.io/v1beta1 v1beta1}]
Jun 28 19:08:37.624: INFO: metrics.k8s.io/v1beta1 matches metrics.k8s.io/v1beta1
[AfterEach] [sig-api-machinery] Discovery
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 19:08:37.625: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "discovery-2535" for this suite.
•{"msg":"PASSED [sig-api-machinery] Discovery should validate PreferredVersion for each APIGroup [Conformance]","total":305,"completed":175,"skipped":2742,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 19:08:37.744: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:181
[It] should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod
STEP: setting up watch
STEP: submitting the pod to kubernetes
Jun 28 19:08:37.880: INFO: observed the pod list
STEP: verifying the pod is in kubernetes
STEP: verifying pod creation was observed
STEP: deleting the pod gracefully
STEP: verifying pod deletion was observed
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 19:08:47.230: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-8593" for this suite.

• [SLOW TEST:9.521 seconds]
[k8s.io] Pods
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Pods should be submitted and removed [NodeConformance] [Conformance]","total":305,"completed":176,"skipped":2770,"failed":0}
SSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 19:08:47.265: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test override all
Jun 28 19:08:47.437: INFO: Waiting up to 5m0s for pod "client-containers-2c5bad15-5434-43fd-90c7-94079b3d910b" in namespace "containers-7320" to be "Succeeded or Failed"
Jun 28 19:08:47.452: INFO: Pod "client-containers-2c5bad15-5434-43fd-90c7-94079b3d910b": Phase="Pending", Reason="", readiness=false. Elapsed: 15.035441ms
Jun 28 19:08:49.467: INFO: Pod "client-containers-2c5bad15-5434-43fd-90c7-94079b3d910b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.030040391s
STEP: Saw pod success
Jun 28 19:08:49.467: INFO: Pod "client-containers-2c5bad15-5434-43fd-90c7-94079b3d910b" satisfied condition "Succeeded or Failed"
Jun 28 19:08:49.481: INFO: Trying to get logs from node 10.13.107.37 pod client-containers-2c5bad15-5434-43fd-90c7-94079b3d910b container test-container: <nil>
STEP: delete the pod
Jun 28 19:08:49.606: INFO: Waiting for pod client-containers-2c5bad15-5434-43fd-90c7-94079b3d910b to disappear
Jun 28 19:08:49.638: INFO: Pod client-containers-2c5bad15-5434-43fd-90c7-94079b3d910b no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 19:08:49.639: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-7320" for this suite.
•{"msg":"PASSED [k8s.io] Docker Containers should be able to override the image's default command and arguments [NodeConformance] [Conformance]","total":305,"completed":177,"skipped":2775,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 19:08:49.682: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test env composition
Jun 28 19:08:49.890: INFO: Waiting up to 5m0s for pod "var-expansion-145ef393-c45f-46c9-9653-640403aae96b" in namespace "var-expansion-8861" to be "Succeeded or Failed"
Jun 28 19:08:49.908: INFO: Pod "var-expansion-145ef393-c45f-46c9-9653-640403aae96b": Phase="Pending", Reason="", readiness=false. Elapsed: 15.425517ms
Jun 28 19:08:51.948: INFO: Pod "var-expansion-145ef393-c45f-46c9-9653-640403aae96b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.055690977s
Jun 28 19:08:53.962: INFO: Pod "var-expansion-145ef393-c45f-46c9-9653-640403aae96b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.069707859s
STEP: Saw pod success
Jun 28 19:08:53.962: INFO: Pod "var-expansion-145ef393-c45f-46c9-9653-640403aae96b" satisfied condition "Succeeded or Failed"
Jun 28 19:08:53.977: INFO: Trying to get logs from node 10.13.107.37 pod var-expansion-145ef393-c45f-46c9-9653-640403aae96b container dapi-container: <nil>
STEP: delete the pod
Jun 28 19:08:54.043: INFO: Waiting for pod var-expansion-145ef393-c45f-46c9-9653-640403aae96b to disappear
Jun 28 19:08:54.055: INFO: Pod var-expansion-145ef393-c45f-46c9-9653-640403aae96b no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 19:08:54.056: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-8861" for this suite.
•{"msg":"PASSED [k8s.io] Variable Expansion should allow composing env vars into new env vars [NodeConformance] [Conformance]","total":305,"completed":178,"skipped":2811,"failed":0}
S
------------------------------
[sig-storage] EmptyDir volumes 
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 19:08:54.096: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir volume type on tmpfs
Jun 28 19:08:54.284: INFO: Waiting up to 5m0s for pod "pod-43f00f72-21b2-4630-8ff1-1a23fd3dcc53" in namespace "emptydir-5346" to be "Succeeded or Failed"
Jun 28 19:08:54.299: INFO: Pod "pod-43f00f72-21b2-4630-8ff1-1a23fd3dcc53": Phase="Pending", Reason="", readiness=false. Elapsed: 15.408539ms
Jun 28 19:08:56.312: INFO: Pod "pod-43f00f72-21b2-4630-8ff1-1a23fd3dcc53": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028108344s
Jun 28 19:08:58.343: INFO: Pod "pod-43f00f72-21b2-4630-8ff1-1a23fd3dcc53": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.05918788s
STEP: Saw pod success
Jun 28 19:08:58.356: INFO: Pod "pod-43f00f72-21b2-4630-8ff1-1a23fd3dcc53" satisfied condition "Succeeded or Failed"
Jun 28 19:08:58.373: INFO: Trying to get logs from node 10.13.107.37 pod pod-43f00f72-21b2-4630-8ff1-1a23fd3dcc53 container test-container: <nil>
STEP: delete the pod
Jun 28 19:08:58.447: INFO: Waiting for pod pod-43f00f72-21b2-4630-8ff1-1a23fd3dcc53 to disappear
Jun 28 19:08:58.459: INFO: Pod pod-43f00f72-21b2-4630-8ff1-1a23fd3dcc53 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 19:08:58.460: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5346" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":179,"skipped":2812,"failed":0}
SS
------------------------------
[sig-network] Services 
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 19:08:58.498: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating service in namespace services-9439
STEP: creating service affinity-clusterip in namespace services-9439
STEP: creating replication controller affinity-clusterip in namespace services-9439
I0628 19:08:58.679127      22 runners.go:190] Created replication controller with name: affinity-clusterip, namespace: services-9439, replica count: 3
I0628 19:09:01.729748      22 runners.go:190] affinity-clusterip Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0628 19:09:04.730766      22 runners.go:190] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jun 28 19:09:04.751: INFO: Creating new exec pod
Jun 28 19:09:09.843: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-108209140 exec --namespace=services-9439 execpod-affinitycnvdp -- /bin/sh -x -c nc -zv -t -w 2 affinity-clusterip 80'
Jun 28 19:09:10.874: INFO: stderr: "+ nc -zv -t -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
Jun 28 19:09:10.874: INFO: stdout: ""
Jun 28 19:09:10.877: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-108209140 exec --namespace=services-9439 execpod-affinitycnvdp -- /bin/sh -x -c nc -zv -t -w 2 172.21.57.205 80'
Jun 28 19:09:11.522: INFO: stderr: "+ nc -zv -t -w 2 172.21.57.205 80\nConnection to 172.21.57.205 80 port [tcp/http] succeeded!\n"
Jun 28 19:09:11.522: INFO: stdout: ""
Jun 28 19:09:11.529: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-108209140 exec --namespace=services-9439 execpod-affinitycnvdp -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.21.57.205:80/ ; done'
Jun 28 19:09:12.097: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.57.205:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.57.205:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.57.205:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.57.205:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.57.205:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.57.205:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.57.205:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.57.205:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.57.205:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.57.205:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.57.205:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.57.205:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.57.205:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.57.205:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.57.205:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.57.205:80/\n"
Jun 28 19:09:12.097: INFO: stdout: "\naffinity-clusterip-dwj4x\naffinity-clusterip-dwj4x\naffinity-clusterip-dwj4x\naffinity-clusterip-dwj4x\naffinity-clusterip-dwj4x\naffinity-clusterip-dwj4x\naffinity-clusterip-dwj4x\naffinity-clusterip-dwj4x\naffinity-clusterip-dwj4x\naffinity-clusterip-dwj4x\naffinity-clusterip-dwj4x\naffinity-clusterip-dwj4x\naffinity-clusterip-dwj4x\naffinity-clusterip-dwj4x\naffinity-clusterip-dwj4x\naffinity-clusterip-dwj4x"
Jun 28 19:09:12.097: INFO: Received response from host: affinity-clusterip-dwj4x
Jun 28 19:09:12.097: INFO: Received response from host: affinity-clusterip-dwj4x
Jun 28 19:09:12.097: INFO: Received response from host: affinity-clusterip-dwj4x
Jun 28 19:09:12.097: INFO: Received response from host: affinity-clusterip-dwj4x
Jun 28 19:09:12.097: INFO: Received response from host: affinity-clusterip-dwj4x
Jun 28 19:09:12.097: INFO: Received response from host: affinity-clusterip-dwj4x
Jun 28 19:09:12.097: INFO: Received response from host: affinity-clusterip-dwj4x
Jun 28 19:09:12.097: INFO: Received response from host: affinity-clusterip-dwj4x
Jun 28 19:09:12.097: INFO: Received response from host: affinity-clusterip-dwj4x
Jun 28 19:09:12.097: INFO: Received response from host: affinity-clusterip-dwj4x
Jun 28 19:09:12.097: INFO: Received response from host: affinity-clusterip-dwj4x
Jun 28 19:09:12.097: INFO: Received response from host: affinity-clusterip-dwj4x
Jun 28 19:09:12.097: INFO: Received response from host: affinity-clusterip-dwj4x
Jun 28 19:09:12.097: INFO: Received response from host: affinity-clusterip-dwj4x
Jun 28 19:09:12.097: INFO: Received response from host: affinity-clusterip-dwj4x
Jun 28 19:09:12.097: INFO: Received response from host: affinity-clusterip-dwj4x
Jun 28 19:09:12.097: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip in namespace services-9439, will wait for the garbage collector to delete the pods
Jun 28 19:09:12.235: INFO: Deleting ReplicationController affinity-clusterip took: 20.254275ms
Jun 28 19:09:12.336: INFO: Terminating ReplicationController affinity-clusterip pods took: 100.768668ms
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 19:09:27.986: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-9439" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

• [SLOW TEST:29.526 seconds]
[sig-network] Services
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]","total":305,"completed":180,"skipped":2814,"failed":0}
SSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 19:09:28.031: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-737.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-737.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-737.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-737.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-737.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-737.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jun 28 19:09:30.477: INFO: DNS probes using dns-737/dns-test-60e4174b-65da-4b89-9058-ab54dbb427e7 succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 19:09:30.584: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-737" for this suite.
•{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Hostname [LinuxOnly] [Conformance]","total":305,"completed":181,"skipped":2824,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should be able to update and delete ResourceQuota. [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 19:09:30.618: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to update and delete ResourceQuota. [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a ResourceQuota
STEP: Getting a ResourceQuota
STEP: Updating a ResourceQuota
STEP: Verifying a ResourceQuota was modified
STEP: Deleting a ResourceQuota
STEP: Verifying the deleted ResourceQuota
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 19:09:30.823: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-9116" for this suite.
•{"msg":"PASSED [sig-api-machinery] ResourceQuota should be able to update and delete ResourceQuota. [Conformance]","total":305,"completed":182,"skipped":2837,"failed":0}
SS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should patch a Namespace [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 19:09:30.857: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should patch a Namespace [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a Namespace
STEP: patching the Namespace
STEP: get the Namespace and ensuring it has the label
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 19:09:31.120: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-6233" for this suite.
STEP: Destroying namespace "nspatchtest-da1814c4-2570-47b0-8265-9df2b04c3e9a-2062" for this suite.
•{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should patch a Namespace [Conformance]","total":305,"completed":183,"skipped":2839,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl logs 
  should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 19:09:31.171: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[BeforeEach] Kubectl logs
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1415
STEP: creating an pod
Jun 28 19:09:31.311: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-108209140 run logs-generator --image=k8s.gcr.io/e2e-test-images/agnhost:2.20 --namespace=kubectl-2218 --restart=Never -- logs-generator --log-lines-total 100 --run-duration 20s'
Jun 28 19:09:31.536: INFO: stderr: ""
Jun 28 19:09:31.536: INFO: stdout: "pod/logs-generator created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Waiting for log generator to start.
Jun 28 19:09:31.536: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
Jun 28 19:09:31.536: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-2218" to be "running and ready, or succeeded"
Jun 28 19:09:31.545: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 9.06872ms
Jun 28 19:09:33.560: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 2.024033918s
Jun 28 19:09:33.560: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
Jun 28 19:09:33.560: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
STEP: checking for a matching strings
Jun 28 19:09:33.560: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-108209140 logs logs-generator logs-generator --namespace=kubectl-2218'
Jun 28 19:09:33.786: INFO: stderr: ""
Jun 28 19:09:33.786: INFO: stdout: "I0628 19:09:32.610264       1 logs_generator.go:76] 0 PUT /api/v1/namespaces/default/pods/2dl2 271\nI0628 19:09:32.810450       1 logs_generator.go:76] 1 PUT /api/v1/namespaces/ns/pods/ff88 402\nI0628 19:09:33.010477       1 logs_generator.go:76] 2 PUT /api/v1/namespaces/ns/pods/r7jq 472\nI0628 19:09:33.210364       1 logs_generator.go:76] 3 GET /api/v1/namespaces/default/pods/kvgm 479\nI0628 19:09:33.410359       1 logs_generator.go:76] 4 PUT /api/v1/namespaces/ns/pods/gcg9 462\nI0628 19:09:33.610366       1 logs_generator.go:76] 5 POST /api/v1/namespaces/ns/pods/6twc 437\n"
Jun 28 19:09:35.787: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-108209140 logs logs-generator logs-generator --namespace=kubectl-2218'
Jun 28 19:09:36.052: INFO: stderr: ""
Jun 28 19:09:36.052: INFO: stdout: "I0628 19:09:32.610264       1 logs_generator.go:76] 0 PUT /api/v1/namespaces/default/pods/2dl2 271\nI0628 19:09:32.810450       1 logs_generator.go:76] 1 PUT /api/v1/namespaces/ns/pods/ff88 402\nI0628 19:09:33.010477       1 logs_generator.go:76] 2 PUT /api/v1/namespaces/ns/pods/r7jq 472\nI0628 19:09:33.210364       1 logs_generator.go:76] 3 GET /api/v1/namespaces/default/pods/kvgm 479\nI0628 19:09:33.410359       1 logs_generator.go:76] 4 PUT /api/v1/namespaces/ns/pods/gcg9 462\nI0628 19:09:33.610366       1 logs_generator.go:76] 5 POST /api/v1/namespaces/ns/pods/6twc 437\nI0628 19:09:33.810360       1 logs_generator.go:76] 6 PUT /api/v1/namespaces/default/pods/knq8 260\nI0628 19:09:34.010391       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/kube-system/pods/k2t 203\nI0628 19:09:34.210437       1 logs_generator.go:76] 8 POST /api/v1/namespaces/default/pods/dcjt 411\nI0628 19:09:34.410370       1 logs_generator.go:76] 9 PUT /api/v1/namespaces/kube-system/pods/p7c 204\nI0628 19:09:34.610370       1 logs_generator.go:76] 10 PUT /api/v1/namespaces/default/pods/2lq 313\nI0628 19:09:34.810465       1 logs_generator.go:76] 11 PUT /api/v1/namespaces/kube-system/pods/6qdk 230\nI0628 19:09:35.010369       1 logs_generator.go:76] 12 PUT /api/v1/namespaces/default/pods/qjz 492\nI0628 19:09:35.210370       1 logs_generator.go:76] 13 GET /api/v1/namespaces/default/pods/6jc 516\nI0628 19:09:35.410374       1 logs_generator.go:76] 14 POST /api/v1/namespaces/ns/pods/6nn 253\nI0628 19:09:35.610476       1 logs_generator.go:76] 15 PUT /api/v1/namespaces/ns/pods/ms7 215\nI0628 19:09:35.810379       1 logs_generator.go:76] 16 POST /api/v1/namespaces/default/pods/9lx 402\nI0628 19:09:36.010369       1 logs_generator.go:76] 17 POST /api/v1/namespaces/default/pods/5js 359\n"
STEP: limiting log lines
Jun 28 19:09:36.052: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-108209140 logs logs-generator logs-generator --namespace=kubectl-2218 --tail=1'
Jun 28 19:09:36.260: INFO: stderr: ""
Jun 28 19:09:36.260: INFO: stdout: "I0628 19:09:36.210374       1 logs_generator.go:76] 18 PUT /api/v1/namespaces/ns/pods/rv5 392\n"
Jun 28 19:09:36.260: INFO: got output "I0628 19:09:36.210374       1 logs_generator.go:76] 18 PUT /api/v1/namespaces/ns/pods/rv5 392\n"
STEP: limiting log bytes
Jun 28 19:09:36.260: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-108209140 logs logs-generator logs-generator --namespace=kubectl-2218 --limit-bytes=1'
Jun 28 19:09:36.516: INFO: stderr: ""
Jun 28 19:09:36.517: INFO: stdout: "I"
Jun 28 19:09:36.517: INFO: got output "I"
STEP: exposing timestamps
Jun 28 19:09:36.517: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-108209140 logs logs-generator logs-generator --namespace=kubectl-2218 --tail=1 --timestamps'
Jun 28 19:09:36.760: INFO: stderr: ""
Jun 28 19:09:36.760: INFO: stdout: "2021-06-28T14:09:36.610452151-05:00 I0628 19:09:36.610383       1 logs_generator.go:76] 20 GET /api/v1/namespaces/kube-system/pods/4bw 282\n"
Jun 28 19:09:36.760: INFO: got output "2021-06-28T14:09:36.610452151-05:00 I0628 19:09:36.610383       1 logs_generator.go:76] 20 GET /api/v1/namespaces/kube-system/pods/4bw 282\n"
STEP: restricting to a time range
Jun 28 19:09:39.261: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-108209140 logs logs-generator logs-generator --namespace=kubectl-2218 --since=1s'
Jun 28 19:09:39.503: INFO: stderr: ""
Jun 28 19:09:39.503: INFO: stdout: "I0628 19:09:38.610364       1 logs_generator.go:76] 30 PUT /api/v1/namespaces/default/pods/dgf9 337\nI0628 19:09:38.810469       1 logs_generator.go:76] 31 GET /api/v1/namespaces/ns/pods/cjvn 358\nI0628 19:09:39.010481       1 logs_generator.go:76] 32 POST /api/v1/namespaces/kube-system/pods/x2dg 257\nI0628 19:09:39.210457       1 logs_generator.go:76] 33 PUT /api/v1/namespaces/ns/pods/wqj 216\nI0628 19:09:39.410371       1 logs_generator.go:76] 34 POST /api/v1/namespaces/kube-system/pods/chjk 374\n"
Jun 28 19:09:39.503: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-108209140 logs logs-generator logs-generator --namespace=kubectl-2218 --since=24h'
Jun 28 19:09:39.799: INFO: stderr: ""
Jun 28 19:09:39.799: INFO: stdout: "I0628 19:09:32.610264       1 logs_generator.go:76] 0 PUT /api/v1/namespaces/default/pods/2dl2 271\nI0628 19:09:32.810450       1 logs_generator.go:76] 1 PUT /api/v1/namespaces/ns/pods/ff88 402\nI0628 19:09:33.010477       1 logs_generator.go:76] 2 PUT /api/v1/namespaces/ns/pods/r7jq 472\nI0628 19:09:33.210364       1 logs_generator.go:76] 3 GET /api/v1/namespaces/default/pods/kvgm 479\nI0628 19:09:33.410359       1 logs_generator.go:76] 4 PUT /api/v1/namespaces/ns/pods/gcg9 462\nI0628 19:09:33.610366       1 logs_generator.go:76] 5 POST /api/v1/namespaces/ns/pods/6twc 437\nI0628 19:09:33.810360       1 logs_generator.go:76] 6 PUT /api/v1/namespaces/default/pods/knq8 260\nI0628 19:09:34.010391       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/kube-system/pods/k2t 203\nI0628 19:09:34.210437       1 logs_generator.go:76] 8 POST /api/v1/namespaces/default/pods/dcjt 411\nI0628 19:09:34.410370       1 logs_generator.go:76] 9 PUT /api/v1/namespaces/kube-system/pods/p7c 204\nI0628 19:09:34.610370       1 logs_generator.go:76] 10 PUT /api/v1/namespaces/default/pods/2lq 313\nI0628 19:09:34.810465       1 logs_generator.go:76] 11 PUT /api/v1/namespaces/kube-system/pods/6qdk 230\nI0628 19:09:35.010369       1 logs_generator.go:76] 12 PUT /api/v1/namespaces/default/pods/qjz 492\nI0628 19:09:35.210370       1 logs_generator.go:76] 13 GET /api/v1/namespaces/default/pods/6jc 516\nI0628 19:09:35.410374       1 logs_generator.go:76] 14 POST /api/v1/namespaces/ns/pods/6nn 253\nI0628 19:09:35.610476       1 logs_generator.go:76] 15 PUT /api/v1/namespaces/ns/pods/ms7 215\nI0628 19:09:35.810379       1 logs_generator.go:76] 16 POST /api/v1/namespaces/default/pods/9lx 402\nI0628 19:09:36.010369       1 logs_generator.go:76] 17 POST /api/v1/namespaces/default/pods/5js 359\nI0628 19:09:36.210374       1 logs_generator.go:76] 18 PUT /api/v1/namespaces/ns/pods/rv5 392\nI0628 19:09:36.410461       1 logs_generator.go:76] 19 PUT /api/v1/namespaces/default/pods/wkh 210\nI0628 19:09:36.610383       1 logs_generator.go:76] 20 GET /api/v1/namespaces/kube-system/pods/4bw 282\nI0628 19:09:36.810393       1 logs_generator.go:76] 21 GET /api/v1/namespaces/default/pods/mvs 429\nI0628 19:09:37.010460       1 logs_generator.go:76] 22 POST /api/v1/namespaces/default/pods/97x 321\nI0628 19:09:37.210409       1 logs_generator.go:76] 23 PUT /api/v1/namespaces/ns/pods/8hh 377\nI0628 19:09:37.410342       1 logs_generator.go:76] 24 POST /api/v1/namespaces/kube-system/pods/4cf 403\nI0628 19:09:37.610367       1 logs_generator.go:76] 25 GET /api/v1/namespaces/kube-system/pods/bls 480\nI0628 19:09:37.810376       1 logs_generator.go:76] 26 GET /api/v1/namespaces/ns/pods/hpr 544\nI0628 19:09:38.010483       1 logs_generator.go:76] 27 GET /api/v1/namespaces/kube-system/pods/zhpb 373\nI0628 19:09:38.210469       1 logs_generator.go:76] 28 POST /api/v1/namespaces/default/pods/djz 493\nI0628 19:09:38.410367       1 logs_generator.go:76] 29 GET /api/v1/namespaces/ns/pods/2w8 580\nI0628 19:09:38.610364       1 logs_generator.go:76] 30 PUT /api/v1/namespaces/default/pods/dgf9 337\nI0628 19:09:38.810469       1 logs_generator.go:76] 31 GET /api/v1/namespaces/ns/pods/cjvn 358\nI0628 19:09:39.010481       1 logs_generator.go:76] 32 POST /api/v1/namespaces/kube-system/pods/x2dg 257\nI0628 19:09:39.210457       1 logs_generator.go:76] 33 PUT /api/v1/namespaces/ns/pods/wqj 216\nI0628 19:09:39.410371       1 logs_generator.go:76] 34 POST /api/v1/namespaces/kube-system/pods/chjk 374\nI0628 19:09:39.610455       1 logs_generator.go:76] 35 PUT /api/v1/namespaces/ns/pods/gsbq 203\n"
[AfterEach] Kubectl logs
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1421
Jun 28 19:09:39.799: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-108209140 delete pod logs-generator --namespace=kubectl-2218'
Jun 28 19:09:47.190: INFO: stderr: ""
Jun 28 19:09:47.190: INFO: stdout: "pod \"logs-generator\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 19:09:47.190: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2218" for this suite.

• [SLOW TEST:16.055 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl logs
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1411
    should be able to retrieve and filter logs  [Conformance]
    /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl logs should be able to retrieve and filter logs  [Conformance]","total":305,"completed":184,"skipped":2867,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 19:09:47.227: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jun 28 19:09:47.372: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
STEP: Creating projection with configMap that has name projected-configmap-test-upd-92f8c5c2-42f9-4f09-a58f-8d77ba3d3728
STEP: Creating the pod
STEP: Updating configmap projected-configmap-test-upd-92f8c5c2-42f9-4f09-a58f-8d77ba3d3728
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 19:09:51.567: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9027" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap updates should be reflected in volume [NodeConformance] [Conformance]","total":305,"completed":185,"skipped":2902,"failed":0}
SSSSSS
------------------------------
[sig-apps] Deployment 
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 19:09:51.613: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:78
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jun 28 19:09:51.748: INFO: Creating deployment "test-recreate-deployment"
Jun 28 19:09:51.763: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Jun 28 19:09:51.783: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
Jun 28 19:09:53.802: INFO: Waiting deployment "test-recreate-deployment" to complete
Jun 28 19:09:53.811: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Jun 28 19:09:53.842: INFO: Updating deployment test-recreate-deployment
Jun 28 19:09:53.842: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:72
Jun 28 19:09:53.980: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:{test-recreate-deployment  deployment-3692 /apis/apps/v1/namespaces/deployment-3692/deployments/test-recreate-deployment e3a20714-3919-452c-9c60-b2dd2a7a70e9 87873 2 2021-06-28 19:09:51 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2021-06-28 19:09:53 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{}}},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-06-28 19:09:53 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}}}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc006092788 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2021-06-28 19:09:53 +0000 UTC,LastTransitionTime:2021-06-28 19:09:53 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-f79dd4667" is progressing.,LastUpdateTime:2021-06-28 19:09:53 +0000 UTC,LastTransitionTime:2021-06-28 19:09:51 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

Jun 28 19:09:53.993: INFO: New ReplicaSet "test-recreate-deployment-f79dd4667" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:{test-recreate-deployment-f79dd4667  deployment-3692 /apis/apps/v1/namespaces/deployment-3692/replicasets/test-recreate-deployment-f79dd4667 e2a93826-5807-480e-b5bd-0d43c80ac233 87872 1 2021-06-28 19:09:53 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:f79dd4667] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment e3a20714-3919-452c-9c60-b2dd2a7a70e9 0xc00aa7e5f0 0xc00aa7e5f1}] []  [{kube-controller-manager Update apps/v1 2021-06-28 19:09:53 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e3a20714-3919-452c-9c60-b2dd2a7a70e9\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: f79dd4667,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:f79dd4667] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00aa7e668 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jun 28 19:09:53.993: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Jun 28 19:09:53.993: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-c96cf48f  deployment-3692 /apis/apps/v1/namespaces/deployment-3692/replicasets/test-recreate-deployment-c96cf48f 8a5fd8ce-b01d-4ac4-bc12-0be05a09ddd0 87859 2 2021-06-28 19:09:51 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:c96cf48f] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment e3a20714-3919-452c-9c60-b2dd2a7a70e9 0xc00aa7e45f 0xc00aa7e470}] []  [{kube-controller-manager Update apps/v1 2021-06-28 19:09:53 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e3a20714-3919-452c-9c60-b2dd2a7a70e9\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: c96cf48f,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:c96cf48f] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.20 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00aa7e588 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jun 28 19:09:54.004: INFO: Pod "test-recreate-deployment-f79dd4667-mdcwg" is not available:
&Pod{ObjectMeta:{test-recreate-deployment-f79dd4667-mdcwg test-recreate-deployment-f79dd4667- deployment-3692 /api/v1/namespaces/deployment-3692/pods/test-recreate-deployment-f79dd4667-mdcwg b0297ab0-8c63-4a22-85cf-54ef2adef616 87875 0 2021-06-28 19:09:53 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:f79dd4667] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet test-recreate-deployment-f79dd4667 e2a93826-5807-480e-b5bd-0d43c80ac233 0xc00aa7ec77 0xc00aa7ec78}] []  [{kube-controller-manager Update v1 2021-06-28 19:09:53 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e2a93826-5807-480e-b5bd-0d43c80ac233\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-06-28 19:09:53 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-dsvsj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-dsvsj,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-dsvsj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.13.107.37,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c53,c17,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-k758p,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-28 19:09:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-28 19:09:53 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-28 19:09:53 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-28 19:09:53 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.13.107.37,PodIP:,StartTime:2021-06-28 19:09:53 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 19:09:54.004: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-3692" for this suite.
•{"msg":"PASSED [sig-apps] Deployment RecreateDeployment should delete old pods and create new ones [Conformance]","total":305,"completed":186,"skipped":2908,"failed":0}

------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 19:09:54.035: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a service externalname-service with the type=ExternalName in namespace services-4261
STEP: changing the ExternalName service to type=ClusterIP
STEP: creating replication controller externalname-service in namespace services-4261
I0628 19:09:54.222185      22 runners.go:190] Created replication controller with name: externalname-service, namespace: services-4261, replica count: 2
Jun 28 19:09:57.272: INFO: Creating new exec pod
I0628 19:09:57.272742      22 runners.go:190] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jun 28 19:10:00.348: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-108209140 exec --namespace=services-4261 execpodzq7x6 -- /bin/sh -x -c nc -zv -t -w 2 externalname-service 80'
Jun 28 19:10:00.842: INFO: stderr: "+ nc -zv -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Jun 28 19:10:00.842: INFO: stdout: ""
Jun 28 19:10:00.843: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-108209140 exec --namespace=services-4261 execpodzq7x6 -- /bin/sh -x -c nc -zv -t -w 2 172.21.157.180 80'
Jun 28 19:10:01.349: INFO: stderr: "+ nc -zv -t -w 2 172.21.157.180 80\nConnection to 172.21.157.180 80 port [tcp/http] succeeded!\n"
Jun 28 19:10:01.349: INFO: stdout: ""
Jun 28 19:10:01.349: INFO: Cleaning up the ExternalName to ClusterIP test service
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 19:10:01.399: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4261" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

• [SLOW TEST:7.413 seconds]
[sig-network] Services
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to ClusterIP [Conformance]","total":305,"completed":187,"skipped":2908,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Security Context when creating containers with AllowPrivilegeEscalation 
  should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 19:10:01.448: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:41
[It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jun 28 19:10:01.629: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-a36858ae-2570-4c86-b2f2-78c82a8c25cc" in namespace "security-context-test-3283" to be "Succeeded or Failed"
Jun 28 19:10:01.644: INFO: Pod "alpine-nnp-false-a36858ae-2570-4c86-b2f2-78c82a8c25cc": Phase="Pending", Reason="", readiness=false. Elapsed: 14.248985ms
Jun 28 19:10:03.656: INFO: Pod "alpine-nnp-false-a36858ae-2570-4c86-b2f2-78c82a8c25cc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026011852s
Jun 28 19:10:05.669: INFO: Pod "alpine-nnp-false-a36858ae-2570-4c86-b2f2-78c82a8c25cc": Phase="Pending", Reason="", readiness=false. Elapsed: 4.039608972s
Jun 28 19:10:07.681: INFO: Pod "alpine-nnp-false-a36858ae-2570-4c86-b2f2-78c82a8c25cc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.051318657s
Jun 28 19:10:07.681: INFO: Pod "alpine-nnp-false-a36858ae-2570-4c86-b2f2-78c82a8c25cc" satisfied condition "Succeeded or Failed"
[AfterEach] [k8s.io] Security Context
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 19:10:07.729: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-3283" for this suite.

• [SLOW TEST:6.313 seconds]
[k8s.io] Security Context
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  when creating containers with AllowPrivilegeEscalation
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:291
    should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Security Context when creating containers with AllowPrivilegeEscalation should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":188,"skipped":2924,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 19:10:07.762: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating secret with name secret-test-6ab5fb3d-2fbc-4d57-80f9-dafd4d984d81
STEP: Creating a pod to test consume secrets
Jun 28 19:10:07.956: INFO: Waiting up to 5m0s for pod "pod-secrets-008b8cc7-13a9-4aca-b13a-4497094255ff" in namespace "secrets-4378" to be "Succeeded or Failed"
Jun 28 19:10:07.969: INFO: Pod "pod-secrets-008b8cc7-13a9-4aca-b13a-4497094255ff": Phase="Pending", Reason="", readiness=false. Elapsed: 12.648024ms
Jun 28 19:10:09.983: INFO: Pod "pod-secrets-008b8cc7-13a9-4aca-b13a-4497094255ff": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.026643578s
STEP: Saw pod success
Jun 28 19:10:09.983: INFO: Pod "pod-secrets-008b8cc7-13a9-4aca-b13a-4497094255ff" satisfied condition "Succeeded or Failed"
Jun 28 19:10:09.993: INFO: Trying to get logs from node 10.13.107.37 pod pod-secrets-008b8cc7-13a9-4aca-b13a-4497094255ff container secret-volume-test: <nil>
STEP: delete the pod
Jun 28 19:10:10.066: INFO: Waiting for pod pod-secrets-008b8cc7-13a9-4aca-b13a-4497094255ff to disappear
Jun 28 19:10:10.076: INFO: Pod pod-secrets-008b8cc7-13a9-4aca-b13a-4497094255ff no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 19:10:10.076: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4378" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":189,"skipped":2942,"failed":0}
SSSSSSSS
------------------------------
[sig-network] Services 
  should find a service from listing all namespaces [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 19:10:10.110: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should find a service from listing all namespaces [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: fetching services
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 19:10:10.249: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8654" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786
•{"msg":"PASSED [sig-network] Services should find a service from listing all namespaces [Conformance]","total":305,"completed":190,"skipped":2950,"failed":0}
S
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 19:10:10.295: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Jun 28 19:10:12.479: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 19:10:12.523: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-9320" for this suite.
•{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]","total":305,"completed":191,"skipped":2951,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl describe 
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 19:10:12.556: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jun 28 19:10:12.684: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-108209140 create -f - --namespace=kubectl-7864'
Jun 28 19:10:13.658: INFO: stderr: ""
Jun 28 19:10:13.658: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
Jun 28 19:10:13.658: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-108209140 create -f - --namespace=kubectl-7864'
Jun 28 19:10:14.553: INFO: stderr: ""
Jun 28 19:10:14.553: INFO: stdout: "service/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
Jun 28 19:10:15.568: INFO: Selector matched 1 pods for map[app:agnhost]
Jun 28 19:10:15.568: INFO: Found 1 / 1
Jun 28 19:10:15.568: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Jun 28 19:10:15.581: INFO: Selector matched 1 pods for map[app:agnhost]
Jun 28 19:10:15.581: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jun 28 19:10:15.581: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-108209140 describe pod agnhost-primary-p8vrs --namespace=kubectl-7864'
Jun 28 19:10:15.802: INFO: stderr: ""
Jun 28 19:10:15.802: INFO: stdout: "Name:         agnhost-primary-p8vrs\nNamespace:    kubectl-7864\nPriority:     0\nNode:         10.13.107.37/10.13.107.37\nStart Time:   Mon, 28 Jun 2021 19:10:13 +0000\nLabels:       app=agnhost\n              role=primary\nAnnotations:  cni.projectcalico.org/podIP: 172.30.221.183/32\n              cni.projectcalico.org/podIPs: 172.30.221.183/32\n              k8s.v1.cni.cncf.io/network-status:\n                [{\n                    \"name\": \"\",\n                    \"ips\": [\n                        \"172.30.221.183\"\n                    ],\n                    \"default\": true,\n                    \"dns\": {}\n                }]\n              k8s.v1.cni.cncf.io/networks-status:\n                [{\n                    \"name\": \"\",\n                    \"ips\": [\n                        \"172.30.221.183\"\n                    ],\n                    \"default\": true,\n                    \"dns\": {}\n                }]\n              openshift.io/scc: anyuid\nStatus:       Running\nIP:           172.30.221.183\nIPs:\n  IP:           172.30.221.183\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   cri-o://bef77e2dc35c4c5dd573fb7cac13229967741a58ee3a25d1fb9b7902595c2898\n    Image:          k8s.gcr.io/e2e-test-images/agnhost:2.20\n    Image ID:       k8s.gcr.io/e2e-test-images/agnhost@sha256:17e61a0b9e498b6c73ed97670906be3d5a3ae394739c1bd5b619e1a004885cf0\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Mon, 28 Jun 2021 19:10:14 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from default-token-bktgl (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  default-token-bktgl:\n    Type:        Secret (a volume populated by a Secret)\n    SecretName:  default-token-bktgl\n    Optional:    false\nQoS Class:       BestEffort\nNode-Selectors:  <none>\nTolerations:     node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                 node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason          Age   From                   Message\n  ----    ------          ----  ----                   -------\n  Normal  Scheduled       2s    default-scheduler      Successfully assigned kubectl-7864/agnhost-primary-p8vrs to 10.13.107.37\n  Normal  AddedInterface  1s    multus                 Add eth0 [172.30.221.183/32]\n  Normal  Pulled          1s    kubelet, 10.13.107.37  Container image \"k8s.gcr.io/e2e-test-images/agnhost:2.20\" already present on machine\n  Normal  Created         1s    kubelet, 10.13.107.37  Created container agnhost-primary\n  Normal  Started         1s    kubelet, 10.13.107.37  Started container agnhost-primary\n"
Jun 28 19:10:15.802: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-108209140 describe rc agnhost-primary --namespace=kubectl-7864'
Jun 28 19:10:16.109: INFO: stderr: ""
Jun 28 19:10:16.109: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-7864\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        k8s.gcr.io/e2e-test-images/agnhost:2.20\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  3s    replication-controller  Created pod: agnhost-primary-p8vrs\n"
Jun 28 19:10:16.109: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-108209140 describe service agnhost-primary --namespace=kubectl-7864'
Jun 28 19:10:16.309: INFO: stderr: ""
Jun 28 19:10:16.309: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-7864\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP:                172.21.247.158\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         172.30.221.183:6379\nSession Affinity:  None\nEvents:            <none>\n"
Jun 28 19:10:16.328: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-108209140 describe node 10.13.107.37'
Jun 28 19:10:16.730: INFO: stderr: ""
Jun 28 19:10:16.730: INFO: stdout: "Name:               10.13.107.37\nRoles:              master,worker\nLabels:             arch=amd64\n                    beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/instance-type=b3c.4x16.encrypted\n                    beta.kubernetes.io/os=linux\n                    failure-domain.beta.kubernetes.io/region=eu-de\n                    failure-domain.beta.kubernetes.io/zone=fra05\n                    ibm-cloud.kubernetes.io/encrypted-docker-data=true\n                    ibm-cloud.kubernetes.io/external-ip=149.81.178.52\n                    ibm-cloud.kubernetes.io/iaas-provider=softlayer\n                    ibm-cloud.kubernetes.io/internal-ip=10.13.107.37\n                    ibm-cloud.kubernetes.io/machine-type=b3c.4x16.encrypted\n                    ibm-cloud.kubernetes.io/os=REDHAT_7_64\n                    ibm-cloud.kubernetes.io/region=eu-de\n                    ibm-cloud.kubernetes.io/sgx-enabled=false\n                    ibm-cloud.kubernetes.io/worker-id=kube-c3cvbqkf0fncrrosfn30-kubee2epvgo-default-000002ec\n                    ibm-cloud.kubernetes.io/worker-pool-id=c3cvbqkf0fncrrosfn30-fdf3b90\n                    ibm-cloud.kubernetes.io/worker-pool-name=default\n                    ibm-cloud.kubernetes.io/worker-version=4.6.34_1546_openshift\n                    ibm-cloud.kubernetes.io/zone=fra05\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=10.13.107.37\n                    kubernetes.io/os=linux\n                    node-role.kubernetes.io/master=\n                    node-role.kubernetes.io/worker=\n                    node.kubernetes.io/instance-type=b3c.4x16.encrypted\n                    node.openshift.io/os_id=rhel\n                    privateVLAN=2723042\n                    publicVLAN=2723040\n                    topology.kubernetes.io/region=eu-de\n                    topology.kubernetes.io/zone=fra05\nAnnotations:        projectcalico.org/IPv4Address: 10.13.107.37/26\n                    projectcalico.org/IPv4IPIPTunnelAddr: 172.30.221.128\nCreationTimestamp:  Mon, 28 Jun 2021 16:41:04 +0000\nTaints:             <none>\nUnschedulable:      false\nLease:\n  HolderIdentity:  10.13.107.37\n  AcquireTime:     <unset>\n  RenewTime:       Mon, 28 Jun 2021 19:10:14 +0000\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable   False   Mon, 28 Jun 2021 16:43:13 +0000   Mon, 28 Jun 2021 16:43:13 +0000   CalicoIsUp                   Calico is running on this node\n  MemoryPressure       False   Mon, 28 Jun 2021 19:07:42 +0000   Mon, 28 Jun 2021 16:41:04 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Mon, 28 Jun 2021 19:07:42 +0000   Mon, 28 Jun 2021 16:41:04 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Mon, 28 Jun 2021 19:07:42 +0000   Mon, 28 Jun 2021 16:41:04 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Mon, 28 Jun 2021 19:07:42 +0000   Mon, 28 Jun 2021 16:43:15 +0000   KubeletReady                 kubelet is posting ready status\nAddresses:\n  InternalIP:  10.13.107.37\n  ExternalIP:  149.81.178.52\n  Hostname:    10.13.107.37\nCapacity:\n  cpu:                  4\n  ephemeral-storage:    103078840Ki\n  example.com/fakecpu:  1k\n  hugepages-1Gi:        0\n  hugepages-2Mi:        0\n  memory:               16260852Ki\n  pods:                 110\nAllocatable:\n  cpu:                  3910m\n  ephemeral-storage:    94369515442\n  example.com/fakecpu:  1k\n  hugepages-1Gi:        0\n  hugepages-2Mi:        0\n  memory:               13484788Ki\n  pods:                 110\nSystem Info:\n  Machine ID:                             86b1e632da2c4d8eb6df8fd7bbd2c7c8\n  System UUID:                            453694ED-63E3-9781-A555-1E67C6197822\n  Boot ID:                                30e6e2f2-6aad-4e16-9851-4774b3854269\n  Kernel Version:                         3.10.0-1160.31.1.el7.x86_64\n  OS Image:                               Red Hat\n  Operating System:                       linux\n  Architecture:                           amd64\n  Container Runtime Version:              cri-o://1.19.2-2.rhaos4.6.git9575c69.el7\n  Kubelet Version:                        v1.19.0+c3e2e69\n  Kube-Proxy Version:                     v1.19.0+c3e2e69\nPodCIDR:                                  172.30.0.0/24\nPodCIDRs:                                 172.30.0.0/24\nProviderID:                               ibm://fee034388aa6435883a1f720010ab3a2///c3cvbqkf0fncrrosfn30/kube-c3cvbqkf0fncrrosfn30-kubee2epvgo-default-000002ec\nNon-terminated Pods:                      (15 in total)\n  Namespace                               Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  AGE\n  ---------                               ----                                                       ------------  ----------  ---------------  -------------  ---\n  calico-system                           calico-node-7ntd9                                          250m (6%)     0 (0%)      80Mi (0%)        0 (0%)         147m\n  calico-system                           calico-typha-649969bb55-mj7tk                              250m (6%)     0 (0%)      80Mi (0%)        0 (0%)         145m\n  kube-system                             ibm-keepalived-watcher-f9nwv                               5m (0%)       0 (0%)      10Mi (0%)        0 (0%)         149m\n  kube-system                             ibm-master-proxy-static-10.13.107.37                       25m (0%)      300m (7%)   32M (0%)         512M (3%)      148m\n  kube-system                             ibmcloud-block-storage-driver-kbnst                        50m (1%)      300m (7%)   100Mi (0%)       300Mi (2%)     149m\n  kubectl-7864                            agnhost-primary-p8vrs                                      0 (0%)        0 (0%)      0 (0%)           0 (0%)         3s\n  openshift-cluster-node-tuning-operator  tuned-2tw2w                                                10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         146m\n  openshift-dns                           dns-default-crxtq                                          65m (1%)      0 (0%)      110Mi (0%)       512Mi (3%)     146m\n  openshift-image-registry                node-ca-z64gj                                              10m (0%)      0 (0%)      10Mi (0%)        0 (0%)         146m\n  openshift-kube-proxy                    openshift-kube-proxy-8l7d5                                 100m (2%)     0 (0%)      200Mi (1%)       0 (0%)         148m\n  openshift-monitoring                    node-exporter-qj7hs                                        9m (0%)       0 (0%)      210Mi (1%)       0 (0%)         145m\n  openshift-multus                        multus-admission-controller-qz22l                          20m (0%)      0 (0%)      20Mi (0%)        0 (0%)         40m\n  openshift-multus                        multus-brfr7                                               10m (0%)      0 (0%)      150Mi (1%)       0 (0%)         148m\n  openshift-multus                        network-metrics-daemon-5rjvj                               20m (0%)      0 (0%)      120Mi (0%)       0 (0%)         148m\n  sonobuoy                                sonobuoy-systemd-logs-daemon-set-bc3c64b0befe49cb-gp9nl    0 (0%)        0 (0%)      0 (0%)           0 (0%)         58m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource             Requests        Limits\n  --------             --------        ------\n  cpu                  824m (21%)      600m (15%)\n  memory               1198610Ki (8%)  1363443712 (9%)\n  ephemeral-storage    0 (0%)          0 (0%)\n  hugepages-1Gi        0 (0%)          0 (0%)\n  hugepages-2Mi        0 (0%)          0 (0%)\n  example.com/fakecpu  0               0\nEvents:\n  Type    Reason                   Age                  From                      Message\n  ----    ------                   ----                 ----                      -------\n  Normal  Starting                 149m                 kubelet, 10.13.107.37     Starting kubelet.\n  Normal  NodeHasSufficientPID     149m (x7 over 149m)  kubelet, 10.13.107.37     Node 10.13.107.37 status is now: NodeHasSufficientPID\n  Normal  NodeAllocatableEnforced  149m                 kubelet, 10.13.107.37     Updated Node Allocatable limit across pods\n  Normal  NodeHasSufficientMemory  149m (x8 over 149m)  kubelet, 10.13.107.37     Node 10.13.107.37 status is now: NodeHasSufficientMemory\n  Normal  NodeHasNoDiskPressure    149m (x8 over 149m)  kubelet, 10.13.107.37     Node 10.13.107.37 status is now: NodeHasNoDiskPressure\n  Normal  Starting                 147m                 kube-proxy, 10.13.107.37  Starting kube-proxy.\n"
Jun 28 19:10:16.732: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-108209140 describe namespace kubectl-7864'
Jun 28 19:10:16.979: INFO: stderr: ""
Jun 28 19:10:16.979: INFO: stdout: "Name:         kubectl-7864\nLabels:       e2e-framework=kubectl\n              e2e-run=b5c16fbf-adda-4cec-91a4-4348765d9b5b\nAnnotations:  openshift.io/sa.scc.mcs: s0:c53,c47\n              openshift.io/sa.scc.supplemental-groups: 1002850000/10000\n              openshift.io/sa.scc.uid-range: 1002850000/10000\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 19:10:16.980: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7864" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl describe should check if kubectl describe prints relevant information for rc and pods  [Conformance]","total":305,"completed":192,"skipped":2962,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 19:10:17.019: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Service
STEP: Ensuring resource quota status captures service creation
STEP: Deleting a Service
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 19:10:28.292: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-1193" for this suite.

• [SLOW TEST:11.312 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a service. [Conformance]","total":305,"completed":193,"skipped":3006,"failed":0}
SSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 19:10:28.331: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test substitution in container's command
Jun 28 19:10:28.502: INFO: Waiting up to 5m0s for pod "var-expansion-de911c32-264d-42af-8101-899b8346b5c0" in namespace "var-expansion-3551" to be "Succeeded or Failed"
Jun 28 19:10:28.512: INFO: Pod "var-expansion-de911c32-264d-42af-8101-899b8346b5c0": Phase="Pending", Reason="", readiness=false. Elapsed: 9.673536ms
Jun 28 19:10:30.529: INFO: Pod "var-expansion-de911c32-264d-42af-8101-899b8346b5c0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.026922384s
STEP: Saw pod success
Jun 28 19:10:30.530: INFO: Pod "var-expansion-de911c32-264d-42af-8101-899b8346b5c0" satisfied condition "Succeeded or Failed"
Jun 28 19:10:30.553: INFO: Trying to get logs from node 10.13.107.37 pod var-expansion-de911c32-264d-42af-8101-899b8346b5c0 container dapi-container: <nil>
STEP: delete the pod
Jun 28 19:10:30.621: INFO: Waiting for pod var-expansion-de911c32-264d-42af-8101-899b8346b5c0 to disappear
Jun 28 19:10:30.633: INFO: Pod var-expansion-de911c32-264d-42af-8101-899b8346b5c0 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 19:10:30.634: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-3551" for this suite.
•{"msg":"PASSED [k8s.io] Variable Expansion should allow substituting values in a container's command [NodeConformance] [Conformance]","total":305,"completed":194,"skipped":3009,"failed":0}
SSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 19:10:30.665: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Jun 28 19:10:34.959: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jun 28 19:10:34.970: INFO: Pod pod-with-poststart-exec-hook still exists
Jun 28 19:10:36.970: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jun 28 19:10:36.982: INFO: Pod pod-with-poststart-exec-hook still exists
Jun 28 19:10:38.970: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jun 28 19:10:38.994: INFO: Pod pod-with-poststart-exec-hook still exists
Jun 28 19:10:40.970: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jun 28 19:10:40.986: INFO: Pod pod-with-poststart-exec-hook still exists
Jun 28 19:10:42.970: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jun 28 19:10:43.012: INFO: Pod pod-with-poststart-exec-hook still exists
Jun 28 19:10:44.970: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jun 28 19:10:45.024: INFO: Pod pod-with-poststart-exec-hook still exists
Jun 28 19:10:46.970: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jun 28 19:10:46.984: INFO: Pod pod-with-poststart-exec-hook still exists
Jun 28 19:10:48.971: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jun 28 19:10:48.992: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 19:10:48.993: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-7587" for this suite.

• [SLOW TEST:18.372 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  when create a pod with lifecycle hook
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart exec hook properly [NodeConformance] [Conformance]","total":305,"completed":195,"skipped":3012,"failed":0}
SSSSSSS
------------------------------
[k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial] 
  removing taint cancels eviction [Disruptive] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 19:10:49.037: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename taint-single-pod
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/taints.go:164
Jun 28 19:10:49.183: INFO: Waiting up to 1m0s for all nodes to be ready
Jun 28 19:11:49.410: INFO: Waiting for terminating namespaces to be deleted...
[It] removing taint cancels eviction [Disruptive] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jun 28 19:11:49.431: INFO: Starting informer...
STEP: Starting pod...
Jun 28 19:11:49.700: INFO: Pod is running on 10.13.107.37. Tainting Node
STEP: Trying to apply a taint on the Node
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting short time to make sure Pod is queued for deletion
Jun 28 19:11:49.756: INFO: Pod wasn't evicted. Proceeding
Jun 28 19:11:49.756: INFO: Removing taint from Node
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting some time to make sure that toleration time passed.
Jun 28 19:13:04.820: INFO: Pod wasn't evicted. Test successful
[AfterEach] [k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 19:13:04.820: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-single-pod-4532" for this suite.

• [SLOW TEST:135.823 seconds]
[k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial]
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  removing taint cancels eviction [Disruptive] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial] removing taint cancels eviction [Disruptive] [Conformance]","total":305,"completed":196,"skipped":3019,"failed":0}
SS
------------------------------
[sig-storage] Secrets 
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 19:13:04.861: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating secret with name secret-test-7fb28a99-07e9-453e-ae69-58d390db3c94
STEP: Creating a pod to test consume secrets
Jun 28 19:13:05.145: INFO: Waiting up to 5m0s for pod "pod-secrets-a1d36afa-b457-48e6-9d4e-75285aeea73d" in namespace "secrets-7549" to be "Succeeded or Failed"
Jun 28 19:13:05.156: INFO: Pod "pod-secrets-a1d36afa-b457-48e6-9d4e-75285aeea73d": Phase="Pending", Reason="", readiness=false. Elapsed: 10.481308ms
Jun 28 19:13:07.170: INFO: Pod "pod-secrets-a1d36afa-b457-48e6-9d4e-75285aeea73d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.024387372s
STEP: Saw pod success
Jun 28 19:13:07.171: INFO: Pod "pod-secrets-a1d36afa-b457-48e6-9d4e-75285aeea73d" satisfied condition "Succeeded or Failed"
Jun 28 19:13:07.182: INFO: Trying to get logs from node 10.13.107.37 pod pod-secrets-a1d36afa-b457-48e6-9d4e-75285aeea73d container secret-volume-test: <nil>
STEP: delete the pod
Jun 28 19:13:07.322: INFO: Waiting for pod pod-secrets-a1d36afa-b457-48e6-9d4e-75285aeea73d to disappear
Jun 28 19:13:07.333: INFO: Pod pod-secrets-a1d36afa-b457-48e6-9d4e-75285aeea73d no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 19:13:07.333: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-7549" for this suite.
STEP: Destroying namespace "secret-namespace-6200" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]","total":305,"completed":197,"skipped":3021,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should delete a collection of pods [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 19:13:07.400: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:181
[It] should delete a collection of pods [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Create set of pods
Jun 28 19:13:07.569: INFO: created test-pod-1
Jun 28 19:13:07.607: INFO: created test-pod-2
Jun 28 19:13:07.650: INFO: created test-pod-3
STEP: waiting for all 3 pods to be located
STEP: waiting for all pods to be deleted
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 19:13:07.774: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-1625" for this suite.
•{"msg":"PASSED [k8s.io] Pods should delete a collection of pods [Conformance]","total":305,"completed":198,"skipped":3080,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 19:13:07.806: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jun 28 19:13:07.948: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 19:13:09.030: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-4565" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition creating/deleting custom resource definition objects works  [Conformance]","total":305,"completed":199,"skipped":3092,"failed":0}
SSSSSS
------------------------------
[sig-storage] ConfigMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 19:13:09.074: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jun 28 19:13:09.233: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
STEP: Creating configMap with name configmap-test-upd-0c61c63b-f5c8-44e8-a4b6-4ebab33c5dca
STEP: Creating the pod
STEP: Updating configmap configmap-test-upd-0c61c63b-f5c8-44e8-a4b6-4ebab33c5dca
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 19:13:13.430: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6540" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap updates should be reflected in volume [NodeConformance] [Conformance]","total":305,"completed":200,"skipped":3098,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 19:13:13.477: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename crd-webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:126
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
Jun 28 19:13:14.573: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
Jun 28 19:13:16.602: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63760504394, loc:(*time.Location)(0x7702840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63760504394, loc:(*time.Location)(0x7702840)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63760504394, loc:(*time.Location)(0x7702840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63760504394, loc:(*time.Location)(0x7702840)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-85d57b96d6\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun 28 19:13:19.658: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jun 28 19:13:19.672: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Creating a v1 custom resource
STEP: Create a v2 custom resource
STEP: List CRs in v1
STEP: List CRs in v2
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 19:13:21.304: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-7186" for this suite.
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:137

• [SLOW TEST:7.978 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert a non homogeneous list of CRs [Conformance]","total":305,"completed":201,"skipped":3135,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 19:13:21.452: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:78
[It] deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jun 28 19:13:21.609: INFO: Creating deployment "webserver-deployment"
Jun 28 19:13:21.626: INFO: Waiting for observed generation 1
Jun 28 19:13:23.654: INFO: Waiting for all required pods to come up
Jun 28 19:13:23.667: INFO: Pod name httpd: Found 10 pods out of 10
STEP: ensuring each pod is running
Jun 28 19:13:25.702: INFO: Waiting for deployment "webserver-deployment" to complete
Jun 28 19:13:25.719: INFO: Updating deployment "webserver-deployment" with a non-existent image
Jun 28 19:13:25.784: INFO: Updating deployment webserver-deployment
Jun 28 19:13:25.784: INFO: Waiting for observed generation 2
Jun 28 19:13:27.808: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Jun 28 19:13:27.817: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Jun 28 19:13:27.825: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Jun 28 19:13:27.851: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Jun 28 19:13:27.854: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Jun 28 19:13:27.864: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Jun 28 19:13:27.880: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
Jun 28 19:13:27.880: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
Jun 28 19:13:27.906: INFO: Updating deployment webserver-deployment
Jun 28 19:13:27.906: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
Jun 28 19:13:27.923: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Jun 28 19:13:27.931: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:72
Jun 28 19:13:27.949: INFO: Deployment "webserver-deployment":
&Deployment{ObjectMeta:{webserver-deployment  deployment-5382 /apis/apps/v1/namespaces/deployment-5382/deployments/webserver-deployment b39f3c30-8f42-4634-aa2e-055b5b7729d2 90843 3 2021-06-28 19:13:21 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2021-06-28 19:13:27 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{}}},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-06-28 19:13:27 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}}}]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002e90728 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:13,UpdatedReplicas:5,AvailableReplicas:8,UnavailableReplicas:5,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-795d758f88" is progressing.,LastUpdateTime:2021-06-28 19:13:26 +0000 UTC,LastTransitionTime:2021-06-28 19:13:21 +0000 UTC,},DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2021-06-28 19:13:27 +0000 UTC,LastTransitionTime:2021-06-28 19:13:27 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

Jun 28 19:13:27.958: INFO: New ReplicaSet "webserver-deployment-795d758f88" of Deployment "webserver-deployment":
&ReplicaSet{ObjectMeta:{webserver-deployment-795d758f88  deployment-5382 /apis/apps/v1/namespaces/deployment-5382/replicasets/webserver-deployment-795d758f88 c564dfbd-f56e-4c3a-aef3-7c1a527540c8 90842 3 2021-06-28 19:13:25 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment b39f3c30-8f42-4634-aa2e-055b5b7729d2 0xc002e90bc7 0xc002e90bc8}] []  [{kube-controller-manager Update apps/v1 2021-06-28 19:13:27 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b39f3c30-8f42-4634-aa2e-055b5b7729d2\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 795d758f88,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002e90c48 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:5,FullyLabeledReplicas:5,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jun 28 19:13:27.959: INFO: All old ReplicaSets of Deployment "webserver-deployment":
Jun 28 19:13:27.960: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-dd94f59b7  deployment-5382 /apis/apps/v1/namespaces/deployment-5382/replicasets/webserver-deployment-dd94f59b7 35311c72-ea07-40a4-845f-57b18b549bfe 90840 3 2021-06-28 19:13:21 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment b39f3c30-8f42-4634-aa2e-055b5b7729d2 0xc002e90ca7 0xc002e90ca8}] []  [{kube-controller-manager Update apps/v1 2021-06-28 19:13:27 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b39f3c30-8f42-4634-aa2e-055b5b7729d2\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: dd94f59b7,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002e90d18 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:8,FullyLabeledReplicas:8,ObservedGeneration:2,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
Jun 28 19:13:27.980: INFO: Pod "webserver-deployment-795d758f88-2p558" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-2p558 webserver-deployment-795d758f88- deployment-5382 /api/v1/namespaces/deployment-5382/pods/webserver-deployment-795d758f88-2p558 846123d6-9a82-4db6-a0f2-513d64f4f836 90837 0 2021-06-28 19:13:25 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[cni.projectcalico.org/podIP:172.30.196.99/32 cni.projectcalico.org/podIPs:172.30.196.99/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "",
    "ips": [
        "172.30.196.99"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "",
    "ips": [
        "172.30.196.99"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 c564dfbd-f56e-4c3a-aef3-7c1a527540c8 0xc00a6c3227 0xc00a6c3228}] []  [{kube-controller-manager Update v1 2021-06-28 19:13:25 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c564dfbd-f56e-4c3a-aef3-7c1a527540c8\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-06-28 19:13:27 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2021-06-28 19:13:27 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}} {multus Update v1 2021-06-28 19:13:27 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-dql95,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-dql95,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-dql95,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.13.107.60,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c54,c49,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-jc987,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-28 19:13:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-28 19:13:25 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-28 19:13:25 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-28 19:13:25 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.13.107.60,PodIP:,StartTime:2021-06-28 19:13:25 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 28 19:13:27.981: INFO: Pod "webserver-deployment-795d758f88-6hcvr" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-6hcvr webserver-deployment-795d758f88- deployment-5382 /api/v1/namespaces/deployment-5382/pods/webserver-deployment-795d758f88-6hcvr 93e285c2-86ab-45fe-9dda-ac17c56f61bd 90822 0 2021-06-28 19:13:26 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[cni.projectcalico.org/podIP:172.30.54.6/32 cni.projectcalico.org/podIPs:172.30.54.6/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "",
    "ips": [
        "172.30.54.6"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "",
    "ips": [
        "172.30.54.6"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 c564dfbd-f56e-4c3a-aef3-7c1a527540c8 0xc00a6c3457 0xc00a6c3458}] []  [{kube-controller-manager Update v1 2021-06-28 19:13:26 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c564dfbd-f56e-4c3a-aef3-7c1a527540c8\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-06-28 19:13:26 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:hostIP":{},"f:startTime":{}}}} {calico Update v1 2021-06-28 19:13:27 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {multus Update v1 2021-06-28 19:13:27 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}},"f:status":{"f:containerStatuses":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-dql95,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-dql95,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-dql95,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.13.107.57,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c54,c49,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-jc987,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-28 19:13:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-28 19:13:26 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-28 19:13:26 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-28 19:13:26 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.13.107.57,PodIP:,StartTime:2021-06-28 19:13:26 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 28 19:13:27.982: INFO: Pod "webserver-deployment-795d758f88-cvk79" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-cvk79 webserver-deployment-795d758f88- deployment-5382 /api/v1/namespaces/deployment-5382/pods/webserver-deployment-795d758f88-cvk79 bf58dd22-53c3-4539-8e06-4ff1b7f539a6 90827 0 2021-06-28 19:13:26 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[cni.projectcalico.org/podIP:172.30.221.145/32 cni.projectcalico.org/podIPs:172.30.221.145/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "",
    "ips": [
        "172.30.221.145"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "",
    "ips": [
        "172.30.221.145"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 c564dfbd-f56e-4c3a-aef3-7c1a527540c8 0xc00a6c3657 0xc00a6c3658}] []  [{kube-controller-manager Update v1 2021-06-28 19:13:25 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c564dfbd-f56e-4c3a-aef3-7c1a527540c8\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-06-28 19:13:26 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:hostIP":{},"f:startTime":{}}}} {calico Update v1 2021-06-28 19:13:27 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {multus Update v1 2021-06-28 19:13:27 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}},"f:status":{"f:containerStatuses":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-dql95,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-dql95,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-dql95,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.13.107.37,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c54,c49,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-jc987,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-28 19:13:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-28 19:13:26 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-28 19:13:26 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-28 19:13:26 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.13.107.37,PodIP:,StartTime:2021-06-28 19:13:26 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 28 19:13:27.983: INFO: Pod "webserver-deployment-795d758f88-hf94z" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-hf94z webserver-deployment-795d758f88- deployment-5382 /api/v1/namespaces/deployment-5382/pods/webserver-deployment-795d758f88-hf94z 5ba9da2e-e63f-4369-acf0-bdec8c8e8b57 90823 0 2021-06-28 19:13:25 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[cni.projectcalico.org/podIP:172.30.221.191/32 cni.projectcalico.org/podIPs:172.30.221.191/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "",
    "ips": [
        "172.30.221.191"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "",
    "ips": [
        "172.30.221.191"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 c564dfbd-f56e-4c3a-aef3-7c1a527540c8 0xc00a6c3857 0xc00a6c3858}] []  [{kube-controller-manager Update v1 2021-06-28 19:13:25 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c564dfbd-f56e-4c3a-aef3-7c1a527540c8\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-06-28 19:13:26 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {multus Update v1 2021-06-28 19:13:26 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}}} {kubelet Update v1 2021-06-28 19:13:27 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-dql95,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-dql95,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-dql95,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.13.107.37,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c54,c49,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-jc987,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-28 19:13:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-28 19:13:25 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-28 19:13:25 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-28 19:13:25 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.13.107.37,PodIP:,StartTime:2021-06-28 19:13:25 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 28 19:13:27.984: INFO: Pod "webserver-deployment-795d758f88-kgpvg" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-kgpvg webserver-deployment-795d758f88- deployment-5382 /api/v1/namespaces/deployment-5382/pods/webserver-deployment-795d758f88-kgpvg 68e5cd98-55f8-410b-b243-be6dcd28c10d 90799 0 2021-06-28 19:13:25 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[cni.projectcalico.org/podIP:172.30.54.62/32 cni.projectcalico.org/podIPs:172.30.54.62/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "",
    "ips": [
        "172.30.54.62"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "",
    "ips": [
        "172.30.54.62"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 c564dfbd-f56e-4c3a-aef3-7c1a527540c8 0xc00a6c3a77 0xc00a6c3a78}] []  [{kube-controller-manager Update v1 2021-06-28 19:13:25 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c564dfbd-f56e-4c3a-aef3-7c1a527540c8\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-06-28 19:13:25 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:hostIP":{},"f:startTime":{}}}} {calico Update v1 2021-06-28 19:13:26 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {multus Update v1 2021-06-28 19:13:26 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}},"f:status":{"f:containerStatuses":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-dql95,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-dql95,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-dql95,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.13.107.57,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c54,c49,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-jc987,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-28 19:13:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-28 19:13:25 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-28 19:13:25 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-28 19:13:25 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.13.107.57,PodIP:,StartTime:2021-06-28 19:13:25 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 28 19:13:27.985: INFO: Pod "webserver-deployment-dd94f59b7-46qmp" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-46qmp webserver-deployment-dd94f59b7- deployment-5382 /api/v1/namespaces/deployment-5382/pods/webserver-deployment-dd94f59b7-46qmp 2a36f1d9-8026-4dad-868d-f848834a8c6e 90688 0 2021-06-28 19:13:21 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[cni.projectcalico.org/podIP:172.30.196.97/32 cni.projectcalico.org/podIPs:172.30.196.97/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "",
    "ips": [
        "172.30.196.97"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "",
    "ips": [
        "172.30.196.97"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 35311c72-ea07-40a4-845f-57b18b549bfe 0xc00a6c3c97 0xc00a6c3c98}] []  [{kube-controller-manager Update v1 2021-06-28 19:13:21 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"35311c72-ea07-40a4-845f-57b18b549bfe\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-06-28 19:13:23 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {multus Update v1 2021-06-28 19:13:23 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}}} {kubelet Update v1 2021-06-28 19:13:24 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.196.97\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-dql95,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-dql95,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-dql95,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.13.107.60,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c54,c49,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-jc987,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-28 19:13:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-28 19:13:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-28 19:13:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-28 19:13:21 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.13.107.60,PodIP:172.30.196.97,StartTime:2021-06-28 19:13:21 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-06-28 19:13:23 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:6feb0ea7b0967367da66e8d58ba813fde32bdb92f63bfc21a9e170d211539db4,ContainerID:cri-o://3acd08b4e0336065edb0a26d4f7f51e9083613537f2f929c959e2048a0527947,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.196.97,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 28 19:13:27.986: INFO: Pod "webserver-deployment-dd94f59b7-4swc4" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-4swc4 webserver-deployment-dd94f59b7- deployment-5382 /api/v1/namespaces/deployment-5382/pods/webserver-deployment-dd94f59b7-4swc4 c0500436-c1b5-4a57-9eb2-648829dce23b 90669 0 2021-06-28 19:13:21 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[cni.projectcalico.org/podIP:172.30.221.143/32 cni.projectcalico.org/podIPs:172.30.221.143/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "",
    "ips": [
        "172.30.221.143"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "",
    "ips": [
        "172.30.221.143"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 35311c72-ea07-40a4-845f-57b18b549bfe 0xc00a6c3e97 0xc00a6c3e98}] []  [{kube-controller-manager Update v1 2021-06-28 19:13:21 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"35311c72-ea07-40a4-845f-57b18b549bfe\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-06-28 19:13:22 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {multus Update v1 2021-06-28 19:13:22 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}}} {kubelet Update v1 2021-06-28 19:13:23 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.221.143\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-dql95,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-dql95,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-dql95,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.13.107.37,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c54,c49,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-28 19:13:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-28 19:13:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-28 19:13:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-28 19:13:21 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.13.107.37,PodIP:172.30.221.143,StartTime:2021-06-28 19:13:21 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-06-28 19:13:23 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:6feb0ea7b0967367da66e8d58ba813fde32bdb92f63bfc21a9e170d211539db4,ContainerID:cri-o://88f806c30bf9cc92c48714012e12e13eba09a34943e1395ab80c50dbe02859f4,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.221.143,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 28 19:13:27.987: INFO: Pod "webserver-deployment-dd94f59b7-6xj5t" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-6xj5t webserver-deployment-dd94f59b7- deployment-5382 /api/v1/namespaces/deployment-5382/pods/webserver-deployment-dd94f59b7-6xj5t e37bc1ad-4c53-4a5e-bec2-d7da7ab13258 90674 0 2021-06-28 19:13:21 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[cni.projectcalico.org/podIP:172.30.221.147/32 cni.projectcalico.org/podIPs:172.30.221.147/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "",
    "ips": [
        "172.30.221.147"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "",
    "ips": [
        "172.30.221.147"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 35311c72-ea07-40a4-845f-57b18b549bfe 0xc002c06bc7 0xc002c06bc8}] []  [{kube-controller-manager Update v1 2021-06-28 19:13:21 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"35311c72-ea07-40a4-845f-57b18b549bfe\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-06-28 19:13:23 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2021-06-28 19:13:23 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.221.147\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}} {multus Update v1 2021-06-28 19:13:23 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-dql95,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-dql95,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-dql95,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.13.107.37,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c54,c49,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-jc987,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-28 19:13:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-28 19:13:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-28 19:13:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-28 19:13:21 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.13.107.37,PodIP:172.30.221.147,StartTime:2021-06-28 19:13:21 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-06-28 19:13:23 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:6feb0ea7b0967367da66e8d58ba813fde32bdb92f63bfc21a9e170d211539db4,ContainerID:cri-o://a96f25db0cc74a9ff33d43dd63270314835c75ef8af9217a3ac5f992587340fc,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.221.147,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 28 19:13:27.988: INFO: Pod "webserver-deployment-dd94f59b7-874hw" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-874hw webserver-deployment-dd94f59b7- deployment-5382 /api/v1/namespaces/deployment-5382/pods/webserver-deployment-dd94f59b7-874hw 9c068055-1b62-4dfb-b3fd-c3c9c03c72c4 90685 0 2021-06-28 19:13:21 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[cni.projectcalico.org/podIP:172.30.196.90/32 cni.projectcalico.org/podIPs:172.30.196.90/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "",
    "ips": [
        "172.30.196.90"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "",
    "ips": [
        "172.30.196.90"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 35311c72-ea07-40a4-845f-57b18b549bfe 0xc002c06fe7 0xc002c06fe8}] []  [{kube-controller-manager Update v1 2021-06-28 19:13:21 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"35311c72-ea07-40a4-845f-57b18b549bfe\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-06-28 19:13:23 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {multus Update v1 2021-06-28 19:13:23 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}}} {kubelet Update v1 2021-06-28 19:13:24 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.196.90\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-dql95,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-dql95,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-dql95,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.13.107.60,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c54,c49,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-28 19:13:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-28 19:13:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-28 19:13:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-28 19:13:21 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.13.107.60,PodIP:172.30.196.90,StartTime:2021-06-28 19:13:21 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-06-28 19:13:23 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:6feb0ea7b0967367da66e8d58ba813fde32bdb92f63bfc21a9e170d211539db4,ContainerID:cri-o://194c0c3531aecd1da35cf70f4639e76e2652f8330397db4915dbe4fb8bcfbe3d,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.196.90,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 28 19:13:27.990: INFO: Pod "webserver-deployment-dd94f59b7-9686z" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-9686z webserver-deployment-dd94f59b7- deployment-5382 /api/v1/namespaces/deployment-5382/pods/webserver-deployment-dd94f59b7-9686z d763f90d-5203-41cc-a52d-fa76055cc7bd 90694 0 2021-06-28 19:13:21 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[cni.projectcalico.org/podIP:172.30.196.91/32 cni.projectcalico.org/podIPs:172.30.196.91/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "",
    "ips": [
        "172.30.196.91"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "",
    "ips": [
        "172.30.196.91"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 35311c72-ea07-40a4-845f-57b18b549bfe 0xc002c07207 0xc002c07208}] []  [{kube-controller-manager Update v1 2021-06-28 19:13:21 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"35311c72-ea07-40a4-845f-57b18b549bfe\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-06-28 19:13:23 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {multus Update v1 2021-06-28 19:13:23 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}}} {kubelet Update v1 2021-06-28 19:13:24 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.196.91\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-dql95,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-dql95,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-dql95,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.13.107.60,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c54,c49,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-jc987,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-28 19:13:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-28 19:13:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-28 19:13:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-28 19:13:21 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.13.107.60,PodIP:172.30.196.91,StartTime:2021-06-28 19:13:21 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-06-28 19:13:23 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:6feb0ea7b0967367da66e8d58ba813fde32bdb92f63bfc21a9e170d211539db4,ContainerID:cri-o://5d6fd20f9b339d856238be6643ad0c232e5274ecb00b6d1afbc80388d6907660,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.196.91,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 28 19:13:27.991: INFO: Pod "webserver-deployment-dd94f59b7-gm2kw" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-gm2kw webserver-deployment-dd94f59b7- deployment-5382 /api/v1/namespaces/deployment-5382/pods/webserver-deployment-dd94f59b7-gm2kw af2b68a5-5104-41d7-b27c-bab9ba665588 90659 0 2021-06-28 19:13:21 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[cni.projectcalico.org/podIP:172.30.54.56/32 cni.projectcalico.org/podIPs:172.30.54.56/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "",
    "ips": [
        "172.30.54.56"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "",
    "ips": [
        "172.30.54.56"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 35311c72-ea07-40a4-845f-57b18b549bfe 0xc002c07437 0xc002c07438}] []  [{kube-controller-manager Update v1 2021-06-28 19:13:21 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"35311c72-ea07-40a4-845f-57b18b549bfe\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-06-28 19:13:22 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {multus Update v1 2021-06-28 19:13:22 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}}} {kubelet Update v1 2021-06-28 19:13:23 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.54.56\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-dql95,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-dql95,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-dql95,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.13.107.57,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c54,c49,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-jc987,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-28 19:13:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-28 19:13:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-28 19:13:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-28 19:13:21 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.13.107.57,PodIP:172.30.54.56,StartTime:2021-06-28 19:13:21 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-06-28 19:13:23 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:6feb0ea7b0967367da66e8d58ba813fde32bdb92f63bfc21a9e170d211539db4,ContainerID:cri-o://5a14bbf6c83c568c8c6812426b8d57fea979b28ed4a72610a78089df3a370cc5,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.54.56,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 28 19:13:27.998: INFO: Pod "webserver-deployment-dd94f59b7-h6whk" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-h6whk webserver-deployment-dd94f59b7- deployment-5382 /api/v1/namespaces/deployment-5382/pods/webserver-deployment-dd94f59b7-h6whk df97cc95-e8a2-47e2-8012-e9bc0f772116 90689 0 2021-06-28 19:13:21 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[cni.projectcalico.org/podIP:172.30.54.50/32 cni.projectcalico.org/podIPs:172.30.54.50/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "",
    "ips": [
        "172.30.54.50"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "",
    "ips": [
        "172.30.54.50"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 35311c72-ea07-40a4-845f-57b18b549bfe 0xc002c07657 0xc002c07658}] []  [{kube-controller-manager Update v1 2021-06-28 19:13:21 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"35311c72-ea07-40a4-845f-57b18b549bfe\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-06-28 19:13:23 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {multus Update v1 2021-06-28 19:13:23 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}}} {kubelet Update v1 2021-06-28 19:13:24 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.54.50\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-dql95,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-dql95,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-dql95,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.13.107.57,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c54,c49,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-jc987,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-28 19:13:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-28 19:13:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-28 19:13:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-28 19:13:21 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.13.107.57,PodIP:172.30.54.50,StartTime:2021-06-28 19:13:21 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-06-28 19:13:23 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:6feb0ea7b0967367da66e8d58ba813fde32bdb92f63bfc21a9e170d211539db4,ContainerID:cri-o://7327d97bfc86273706ac1f51bed74429008e7a33fe85e91b1ac43ecc3afb41ce,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.54.50,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 28 19:13:27.999: INFO: Pod "webserver-deployment-dd94f59b7-pv97c" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-pv97c webserver-deployment-dd94f59b7- deployment-5382 /api/v1/namespaces/deployment-5382/pods/webserver-deployment-dd94f59b7-pv97c 76c0f6a8-03f3-4500-8aeb-1d96b6bf9932 90847 0 2021-06-28 19:13:27 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 35311c72-ea07-40a4-845f-57b18b549bfe 0xc002c079c7 0xc002c079c8}] []  [{kube-controller-manager Update v1 2021-06-28 19:13:27 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"35311c72-ea07-40a4-845f-57b18b549bfe\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-dql95,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-dql95,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-dql95,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.13.107.37,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c54,c49,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-jc987,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-28 19:13:27 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 28 19:13:27.999: INFO: Pod "webserver-deployment-dd94f59b7-w5x2s" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-w5x2s webserver-deployment-dd94f59b7- deployment-5382 /api/v1/namespaces/deployment-5382/pods/webserver-deployment-dd94f59b7-w5x2s cf8b8b75-5de6-4bb3-820e-663f883190fe 90657 0 2021-06-28 19:13:21 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[cni.projectcalico.org/podIP:172.30.54.55/32 cni.projectcalico.org/podIPs:172.30.54.55/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "",
    "ips": [
        "172.30.54.55"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "",
    "ips": [
        "172.30.54.55"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 35311c72-ea07-40a4-845f-57b18b549bfe 0xc0005b3187 0xc0005b3188}] []  [{kube-controller-manager Update v1 2021-06-28 19:13:21 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"35311c72-ea07-40a4-845f-57b18b549bfe\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-06-28 19:13:22 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {multus Update v1 2021-06-28 19:13:22 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}}} {kubelet Update v1 2021-06-28 19:13:23 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.54.55\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-dql95,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-dql95,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-dql95,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.13.107.57,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c54,c49,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-28 19:13:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-28 19:13:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-28 19:13:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-28 19:13:21 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.13.107.57,PodIP:172.30.54.55,StartTime:2021-06-28 19:13:21 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-06-28 19:13:23 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:6feb0ea7b0967367da66e8d58ba813fde32bdb92f63bfc21a9e170d211539db4,ContainerID:cri-o://27f3092fb5012a6c3acdcf86945bdb5fce39e2642df3cd209c19376392d6afd0,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.54.55,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 19:13:27.999: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-5382" for this suite.

• [SLOW TEST:6.588 seconds]
[sig-apps] Deployment
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should support proportional scaling [Conformance]","total":305,"completed":202,"skipped":3170,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[k8s.io] Lease 
  lease API should be available [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Lease
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 19:13:28.043: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename lease-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] lease API should be available [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [k8s.io] Lease
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 19:13:28.327: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "lease-test-7888" for this suite.
•{"msg":"PASSED [k8s.io] Lease lease API should be available [Conformance]","total":305,"completed":203,"skipped":3184,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 19:13:28.363: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0666 on tmpfs
Jun 28 19:13:28.560: INFO: Waiting up to 5m0s for pod "pod-1a60228e-effe-4837-a12a-5318908971d4" in namespace "emptydir-2938" to be "Succeeded or Failed"
Jun 28 19:13:28.585: INFO: Pod "pod-1a60228e-effe-4837-a12a-5318908971d4": Phase="Pending", Reason="", readiness=false. Elapsed: 24.698081ms
Jun 28 19:13:30.603: INFO: Pod "pod-1a60228e-effe-4837-a12a-5318908971d4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.04274304s
Jun 28 19:13:32.621: INFO: Pod "pod-1a60228e-effe-4837-a12a-5318908971d4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.060006643s
STEP: Saw pod success
Jun 28 19:13:32.621: INFO: Pod "pod-1a60228e-effe-4837-a12a-5318908971d4" satisfied condition "Succeeded or Failed"
Jun 28 19:13:32.632: INFO: Trying to get logs from node 10.13.107.37 pod pod-1a60228e-effe-4837-a12a-5318908971d4 container test-container: <nil>
STEP: delete the pod
Jun 28 19:13:32.696: INFO: Waiting for pod pod-1a60228e-effe-4837-a12a-5318908971d4 to disappear
Jun 28 19:13:32.706: INFO: Pod pod-1a60228e-effe-4837-a12a-5318908971d4 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 19:13:32.707: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2938" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":204,"skipped":3194,"failed":0}
SSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 19:13:32.740: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Jun 28 19:13:37.060: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jun 28 19:13:37.072: INFO: Pod pod-with-prestop-exec-hook still exists
Jun 28 19:13:39.073: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jun 28 19:13:39.086: INFO: Pod pod-with-prestop-exec-hook still exists
Jun 28 19:13:41.073: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jun 28 19:13:41.090: INFO: Pod pod-with-prestop-exec-hook still exists
Jun 28 19:13:43.072: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jun 28 19:13:43.085: INFO: Pod pod-with-prestop-exec-hook still exists
Jun 28 19:13:45.073: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jun 28 19:13:45.085: INFO: Pod pod-with-prestop-exec-hook still exists
Jun 28 19:13:47.073: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jun 28 19:13:47.085: INFO: Pod pod-with-prestop-exec-hook still exists
Jun 28 19:13:49.072: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jun 28 19:13:49.091: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 19:13:49.115: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-1169" for this suite.

• [SLOW TEST:16.407 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  when create a pod with lifecycle hook
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop exec hook properly [NodeConformance] [Conformance]","total":305,"completed":205,"skipped":3204,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 19:13:49.148: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:82
[It] should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 19:13:49.355: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-7226" for this suite.
•{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox command that always fails in a pod should be possible to delete [NodeConformance] [Conformance]","total":305,"completed":206,"skipped":3222,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates 
  should delete a collection of pod templates [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-node] PodTemplates
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 19:13:49.395: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename podtemplate
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a collection of pod templates [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Create set of pod templates
Jun 28 19:13:49.543: INFO: created test-podtemplate-1
Jun 28 19:13:49.557: INFO: created test-podtemplate-2
Jun 28 19:13:49.572: INFO: created test-podtemplate-3
STEP: get a list of pod templates with a label in the current namespace
STEP: delete collection of pod templates
Jun 28 19:13:49.581: INFO: requesting DeleteCollection of pod templates
STEP: check that the list of pod templates matches the requested quantity
Jun 28 19:13:49.630: INFO: requesting list of pod templates to confirm quantity
[AfterEach] [sig-node] PodTemplates
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 19:13:49.639: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-4114" for this suite.
•{"msg":"PASSED [sig-node] PodTemplates should delete a collection of pod templates [Conformance]","total":305,"completed":207,"skipped":3249,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 19:13:49.673: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name projected-configmap-test-volume-map-f73e2b66-b3a2-46a7-9d48-80a949a6bf4e
STEP: Creating a pod to test consume configMaps
Jun 28 19:13:49.857: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-c4999943-abb4-4fd6-bbfe-21d49ef2c7d2" in namespace "projected-3157" to be "Succeeded or Failed"
Jun 28 19:13:49.983: INFO: Pod "pod-projected-configmaps-c4999943-abb4-4fd6-bbfe-21d49ef2c7d2": Phase="Pending", Reason="", readiness=false. Elapsed: 125.934915ms
Jun 28 19:13:52.006: INFO: Pod "pod-projected-configmaps-c4999943-abb4-4fd6-bbfe-21d49ef2c7d2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.14937845s
STEP: Saw pod success
Jun 28 19:13:52.006: INFO: Pod "pod-projected-configmaps-c4999943-abb4-4fd6-bbfe-21d49ef2c7d2" satisfied condition "Succeeded or Failed"
Jun 28 19:13:52.017: INFO: Trying to get logs from node 10.13.107.37 pod pod-projected-configmaps-c4999943-abb4-4fd6-bbfe-21d49ef2c7d2 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jun 28 19:13:52.111: INFO: Waiting for pod pod-projected-configmaps-c4999943-abb4-4fd6-bbfe-21d49ef2c7d2 to disappear
Jun 28 19:13:52.121: INFO: Pod pod-projected-configmaps-c4999943-abb4-4fd6-bbfe-21d49ef2c7d2 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 19:13:52.121: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3157" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","total":305,"completed":208,"skipped":3260,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 19:13:52.155: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating the pod
Jun 28 19:13:54.953: INFO: Successfully updated pod "annotationupdate5143f545-0915-4a35-9137-35372044c960"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 19:13:59.042: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6195" for this suite.

• [SLOW TEST:6.921 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:37
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should update annotations on modification [NodeConformance] [Conformance]","total":305,"completed":209,"skipped":3292,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 19:13:59.077: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:82
[It] should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 19:14:03.301: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-7773" for this suite.
•{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox command that always fails in a pod should have an terminated reason [NodeConformance] [Conformance]","total":305,"completed":210,"skipped":3335,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 19:14:03.346: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test override command
Jun 28 19:14:03.524: INFO: Waiting up to 5m0s for pod "client-containers-435c65d5-6540-43c7-82cb-e095d2370ec6" in namespace "containers-8589" to be "Succeeded or Failed"
Jun 28 19:14:03.536: INFO: Pod "client-containers-435c65d5-6540-43c7-82cb-e095d2370ec6": Phase="Pending", Reason="", readiness=false. Elapsed: 11.415319ms
Jun 28 19:14:05.550: INFO: Pod "client-containers-435c65d5-6540-43c7-82cb-e095d2370ec6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.026098159s
STEP: Saw pod success
Jun 28 19:14:05.550: INFO: Pod "client-containers-435c65d5-6540-43c7-82cb-e095d2370ec6" satisfied condition "Succeeded or Failed"
Jun 28 19:14:05.562: INFO: Trying to get logs from node 10.13.107.37 pod client-containers-435c65d5-6540-43c7-82cb-e095d2370ec6 container test-container: <nil>
STEP: delete the pod
Jun 28 19:14:05.635: INFO: Waiting for pod client-containers-435c65d5-6540-43c7-82cb-e095d2370ec6 to disappear
Jun 28 19:14:05.646: INFO: Pod client-containers-435c65d5-6540-43c7-82cb-e095d2370ec6 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 19:14:05.646: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-8589" for this suite.
•{"msg":"PASSED [k8s.io] Docker Containers should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]","total":305,"completed":211,"skipped":3363,"failed":0}
SSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 19:14:05.685: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name configmap-test-volume-c3e5e356-ac18-4cef-8996-007d993e4cda
STEP: Creating a pod to test consume configMaps
Jun 28 19:14:05.886: INFO: Waiting up to 5m0s for pod "pod-configmaps-02b631b0-c404-48da-9f9e-d4d211cd23c1" in namespace "configmap-6801" to be "Succeeded or Failed"
Jun 28 19:14:05.898: INFO: Pod "pod-configmaps-02b631b0-c404-48da-9f9e-d4d211cd23c1": Phase="Pending", Reason="", readiness=false. Elapsed: 11.90766ms
Jun 28 19:14:07.916: INFO: Pod "pod-configmaps-02b631b0-c404-48da-9f9e-d4d211cd23c1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029791841s
Jun 28 19:14:09.931: INFO: Pod "pod-configmaps-02b631b0-c404-48da-9f9e-d4d211cd23c1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.044662735s
STEP: Saw pod success
Jun 28 19:14:09.931: INFO: Pod "pod-configmaps-02b631b0-c404-48da-9f9e-d4d211cd23c1" satisfied condition "Succeeded or Failed"
Jun 28 19:14:09.942: INFO: Trying to get logs from node 10.13.107.37 pod pod-configmaps-02b631b0-c404-48da-9f9e-d4d211cd23c1 container configmap-volume-test: <nil>
STEP: delete the pod
Jun 28 19:14:10.010: INFO: Waiting for pod pod-configmaps-02b631b0-c404-48da-9f9e-d4d211cd23c1 to disappear
Jun 28 19:14:10.057: INFO: Pod pod-configmaps-02b631b0-c404-48da-9f9e-d4d211cd23c1 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 19:14:10.058: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6801" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume [NodeConformance] [Conformance]","total":305,"completed":212,"skipped":3366,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 19:14:10.096: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a pod in the namespace
STEP: Waiting for the pod to have running status
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there are no pods in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 19:14:28.888: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-2830" for this suite.
STEP: Destroying namespace "nsdeletetest-1031" for this suite.
Jun 28 19:14:28.928: INFO: Namespace nsdeletetest-1031 was already deleted
STEP: Destroying namespace "nsdeletetest-431" for this suite.

• [SLOW TEST:18.847 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all pods are removed when a namespace is deleted [Conformance]","total":305,"completed":213,"skipped":3397,"failed":0}
[sig-storage] Downward API volume 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 19:14:28.947: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating the pod
Jun 28 19:14:31.737: INFO: Successfully updated pod "labelsupdated657fece-4abc-4a90-a194-6abda193c1a6"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 19:14:33.817: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6498" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should update labels on modification [NodeConformance] [Conformance]","total":305,"completed":214,"skipped":3397,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 19:14:33.866: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Jun 28 19:14:34.075: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ee201aee-71ee-489f-a70b-89bf0625be25" in namespace "projected-9137" to be "Succeeded or Failed"
Jun 28 19:14:34.089: INFO: Pod "downwardapi-volume-ee201aee-71ee-489f-a70b-89bf0625be25": Phase="Pending", Reason="", readiness=false. Elapsed: 13.900731ms
Jun 28 19:14:36.106: INFO: Pod "downwardapi-volume-ee201aee-71ee-489f-a70b-89bf0625be25": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.0301531s
STEP: Saw pod success
Jun 28 19:14:36.106: INFO: Pod "downwardapi-volume-ee201aee-71ee-489f-a70b-89bf0625be25" satisfied condition "Succeeded or Failed"
Jun 28 19:14:36.118: INFO: Trying to get logs from node 10.13.107.37 pod downwardapi-volume-ee201aee-71ee-489f-a70b-89bf0625be25 container client-container: <nil>
STEP: delete the pod
Jun 28 19:14:36.177: INFO: Waiting for pod downwardapi-volume-ee201aee-71ee-489f-a70b-89bf0625be25 to disappear
Jun 28 19:14:36.199: INFO: Pod downwardapi-volume-ee201aee-71ee-489f-a70b-89bf0625be25 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 19:14:36.199: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9137" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":215,"skipped":3423,"failed":0}
SSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Subdomain [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 19:14:36.234: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for pods for Subdomain [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-2692.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-2692.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-2692.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-2692.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-2692.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-2692.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-2692.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-2692.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-2692.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-2692.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-2692.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-2692.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-2692.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-2692.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-2692.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-2692.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-2692.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-2692.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jun 28 19:14:40.520: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-2692.svc.cluster.local from pod dns-2692/dns-test-4b17af09-e07e-41dc-9c1d-de0132e23fdd: the server could not find the requested resource (get pods dns-test-4b17af09-e07e-41dc-9c1d-de0132e23fdd)
Jun 28 19:14:40.567: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-2692.svc.cluster.local from pod dns-2692/dns-test-4b17af09-e07e-41dc-9c1d-de0132e23fdd: the server could not find the requested resource (get pods dns-test-4b17af09-e07e-41dc-9c1d-de0132e23fdd)
Jun 28 19:14:40.584: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-2692.svc.cluster.local from pod dns-2692/dns-test-4b17af09-e07e-41dc-9c1d-de0132e23fdd: the server could not find the requested resource (get pods dns-test-4b17af09-e07e-41dc-9c1d-de0132e23fdd)
Jun 28 19:14:40.643: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-2692.svc.cluster.local from pod dns-2692/dns-test-4b17af09-e07e-41dc-9c1d-de0132e23fdd: the server could not find the requested resource (get pods dns-test-4b17af09-e07e-41dc-9c1d-de0132e23fdd)
Jun 28 19:14:40.659: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-2692.svc.cluster.local from pod dns-2692/dns-test-4b17af09-e07e-41dc-9c1d-de0132e23fdd: the server could not find the requested resource (get pods dns-test-4b17af09-e07e-41dc-9c1d-de0132e23fdd)
Jun 28 19:14:40.676: INFO: Unable to read jessie_udp@dns-test-service-2.dns-2692.svc.cluster.local from pod dns-2692/dns-test-4b17af09-e07e-41dc-9c1d-de0132e23fdd: the server could not find the requested resource (get pods dns-test-4b17af09-e07e-41dc-9c1d-de0132e23fdd)
Jun 28 19:14:40.695: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-2692.svc.cluster.local from pod dns-2692/dns-test-4b17af09-e07e-41dc-9c1d-de0132e23fdd: the server could not find the requested resource (get pods dns-test-4b17af09-e07e-41dc-9c1d-de0132e23fdd)
Jun 28 19:14:40.749: INFO: Lookups using dns-2692/dns-test-4b17af09-e07e-41dc-9c1d-de0132e23fdd failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-2692.svc.cluster.local wheezy_udp@dns-test-service-2.dns-2692.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-2692.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-2692.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-2692.svc.cluster.local jessie_udp@dns-test-service-2.dns-2692.svc.cluster.local jessie_tcp@dns-test-service-2.dns-2692.svc.cluster.local]

Jun 28 19:14:46.011: INFO: DNS probes using dns-2692/dns-test-4b17af09-e07e-41dc-9c1d-de0132e23fdd succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 19:14:46.095: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-2692" for this suite.

• [SLOW TEST:9.894 seconds]
[sig-network] DNS
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for pods for Subdomain [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Subdomain [Conformance]","total":305,"completed":216,"skipped":3428,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 19:14:46.129: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jun 28 19:14:46.378: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"5a0548a9-e794-4fcb-b83f-72f70e15e20a", Controller:(*bool)(0xc002a495b2), BlockOwnerDeletion:(*bool)(0xc002a495b3)}}
Jun 28 19:14:46.398: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"3244e2d9-8e01-445d-95ff-e238979f68ae", Controller:(*bool)(0xc00303ec02), BlockOwnerDeletion:(*bool)(0xc00303ec03)}}
Jun 28 19:14:46.429: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"00bb8549-1eed-466c-9b2c-6d16012fb13f", Controller:(*bool)(0xc002a49fa6), BlockOwnerDeletion:(*bool)(0xc002a49fa7)}}
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 19:14:51.471: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-5285" for this suite.

• [SLOW TEST:5.380 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should not be blocked by dependency circle [Conformance]","total":305,"completed":217,"skipped":3443,"failed":0}
SSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 19:14:51.512: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-8161
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Initializing watcher for selector baz=blah,foo=bar
STEP: Creating stateful set ss in namespace statefulset-8161
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-8161
Jun 28 19:14:51.711: INFO: Found 0 stateful pods, waiting for 1
Jun 28 19:15:01.736: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod
Jun 28 19:15:01.747: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-108209140 exec --namespace=statefulset-8161 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jun 28 19:15:02.301: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jun 28 19:15:02.301: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jun 28 19:15:02.301: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jun 28 19:15:02.314: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Jun 28 19:15:12.334: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jun 28 19:15:12.334: INFO: Waiting for statefulset status.replicas updated to 0
Jun 28 19:15:12.387: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999995142s
Jun 28 19:15:13.399: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.989035291s
Jun 28 19:15:14.423: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.976790693s
Jun 28 19:15:15.438: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.953257017s
Jun 28 19:15:16.453: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.937977103s
Jun 28 19:15:17.469: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.922537022s
Jun 28 19:15:18.480: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.907347253s
Jun 28 19:15:19.504: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.89625694s
Jun 28 19:15:20.515: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.872491449s
Jun 28 19:15:21.525: INFO: Verifying statefulset ss doesn't scale past 1 for another 861.20456ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-8161
Jun 28 19:15:22.539: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-108209140 exec --namespace=statefulset-8161 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 28 19:15:22.952: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jun 28 19:15:22.952: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jun 28 19:15:22.952: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jun 28 19:15:22.962: INFO: Found 1 stateful pods, waiting for 3
Jun 28 19:15:32.996: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Jun 28 19:15:32.996: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Jun 28 19:15:32.996: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order
STEP: Scale down will halt with unhealthy stateful pod
Jun 28 19:15:33.017: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-108209140 exec --namespace=statefulset-8161 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jun 28 19:15:33.416: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jun 28 19:15:33.416: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jun 28 19:15:33.416: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jun 28 19:15:33.416: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-108209140 exec --namespace=statefulset-8161 ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jun 28 19:15:33.960: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jun 28 19:15:33.960: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jun 28 19:15:33.960: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jun 28 19:15:33.960: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-108209140 exec --namespace=statefulset-8161 ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jun 28 19:15:34.390: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jun 28 19:15:34.390: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jun 28 19:15:34.390: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jun 28 19:15:34.390: INFO: Waiting for statefulset status.replicas updated to 0
Jun 28 19:15:34.404: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
Jun 28 19:15:44.438: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jun 28 19:15:44.438: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Jun 28 19:15:44.438: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Jun 28 19:15:44.482: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999994392s
Jun 28 19:15:45.498: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.98726243s
Jun 28 19:15:46.513: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.970958502s
Jun 28 19:15:47.528: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.956065162s
Jun 28 19:15:48.545: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.941522536s
Jun 28 19:15:49.562: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.924725492s
Jun 28 19:15:50.581: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.90721681s
Jun 28 19:15:51.599: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.888861556s
Jun 28 19:15:52.615: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.870681053s
Jun 28 19:15:53.629: INFO: Verifying statefulset ss doesn't scale past 3 for another 854.184199ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-8161
Jun 28 19:15:54.643: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-108209140 exec --namespace=statefulset-8161 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 28 19:15:55.107: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jun 28 19:15:55.107: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jun 28 19:15:55.107: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jun 28 19:15:55.107: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-108209140 exec --namespace=statefulset-8161 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 28 19:15:55.669: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jun 28 19:15:55.669: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jun 28 19:15:55.669: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jun 28 19:15:55.669: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-108209140 exec --namespace=statefulset-8161 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 28 19:15:56.226: INFO: rc: 1
Jun 28 19:15:56.226: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-108209140 exec --namespace=statefulset-8161 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
time="2021-06-28T14:15:56-05:00" level=error msg="exec failed: container_linux.go:366: starting container process caused: process_linux.go:111: executing setns process caused: signal: broken pipe"
command terminated with exit code 1

error:
exit status 1
Jun 28 19:16:06.227: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-108209140 exec --namespace=statefulset-8161 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 28 19:16:06.438: INFO: rc: 1
Jun 28 19:16:06.438: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-108209140 exec --namespace=statefulset-8161 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jun 28 19:16:16.439: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-108209140 exec --namespace=statefulset-8161 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 28 19:16:16.679: INFO: rc: 1
Jun 28 19:16:16.679: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-108209140 exec --namespace=statefulset-8161 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jun 28 19:16:26.680: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-108209140 exec --namespace=statefulset-8161 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 28 19:16:26.954: INFO: rc: 1
Jun 28 19:16:26.954: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-108209140 exec --namespace=statefulset-8161 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jun 28 19:16:36.955: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-108209140 exec --namespace=statefulset-8161 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 28 19:16:37.125: INFO: rc: 1
Jun 28 19:16:37.125: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-108209140 exec --namespace=statefulset-8161 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jun 28 19:16:47.126: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-108209140 exec --namespace=statefulset-8161 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 28 19:16:47.386: INFO: rc: 1
Jun 28 19:16:47.386: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-108209140 exec --namespace=statefulset-8161 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jun 28 19:16:57.386: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-108209140 exec --namespace=statefulset-8161 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 28 19:16:57.629: INFO: rc: 1
Jun 28 19:16:57.629: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-108209140 exec --namespace=statefulset-8161 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jun 28 19:17:07.630: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-108209140 exec --namespace=statefulset-8161 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 28 19:17:07.884: INFO: rc: 1
Jun 28 19:17:07.884: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-108209140 exec --namespace=statefulset-8161 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jun 28 19:17:17.884: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-108209140 exec --namespace=statefulset-8161 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 28 19:17:18.161: INFO: rc: 1
Jun 28 19:17:18.161: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-108209140 exec --namespace=statefulset-8161 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jun 28 19:17:28.161: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-108209140 exec --namespace=statefulset-8161 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 28 19:17:28.391: INFO: rc: 1
Jun 28 19:17:28.391: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-108209140 exec --namespace=statefulset-8161 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jun 28 19:17:38.393: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-108209140 exec --namespace=statefulset-8161 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 28 19:17:38.569: INFO: rc: 1
Jun 28 19:17:38.569: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-108209140 exec --namespace=statefulset-8161 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jun 28 19:17:48.569: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-108209140 exec --namespace=statefulset-8161 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 28 19:17:49.041: INFO: rc: 1
Jun 28 19:17:49.041: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-108209140 exec --namespace=statefulset-8161 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jun 28 19:17:59.042: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-108209140 exec --namespace=statefulset-8161 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 28 19:17:59.265: INFO: rc: 1
Jun 28 19:17:59.265: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-108209140 exec --namespace=statefulset-8161 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jun 28 19:18:09.266: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-108209140 exec --namespace=statefulset-8161 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 28 19:18:09.472: INFO: rc: 1
Jun 28 19:18:09.472: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-108209140 exec --namespace=statefulset-8161 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jun 28 19:18:19.473: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-108209140 exec --namespace=statefulset-8161 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 28 19:18:19.684: INFO: rc: 1
Jun 28 19:18:19.684: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-108209140 exec --namespace=statefulset-8161 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jun 28 19:18:29.684: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-108209140 exec --namespace=statefulset-8161 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 28 19:18:29.879: INFO: rc: 1
Jun 28 19:18:29.879: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-108209140 exec --namespace=statefulset-8161 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jun 28 19:18:39.880: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-108209140 exec --namespace=statefulset-8161 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 28 19:18:40.130: INFO: rc: 1
Jun 28 19:18:40.131: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-108209140 exec --namespace=statefulset-8161 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jun 28 19:18:50.131: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-108209140 exec --namespace=statefulset-8161 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 28 19:18:50.745: INFO: rc: 1
Jun 28 19:18:50.745: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-108209140 exec --namespace=statefulset-8161 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jun 28 19:19:00.746: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-108209140 exec --namespace=statefulset-8161 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 28 19:19:00.950: INFO: rc: 1
Jun 28 19:19:00.950: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-108209140 exec --namespace=statefulset-8161 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jun 28 19:19:10.952: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-108209140 exec --namespace=statefulset-8161 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 28 19:19:11.465: INFO: rc: 1
Jun 28 19:19:11.465: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-108209140 exec --namespace=statefulset-8161 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jun 28 19:19:21.467: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-108209140 exec --namespace=statefulset-8161 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 28 19:19:21.749: INFO: rc: 1
Jun 28 19:19:21.749: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-108209140 exec --namespace=statefulset-8161 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jun 28 19:19:31.749: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-108209140 exec --namespace=statefulset-8161 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 28 19:19:31.980: INFO: rc: 1
Jun 28 19:19:31.980: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-108209140 exec --namespace=statefulset-8161 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jun 28 19:19:41.980: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-108209140 exec --namespace=statefulset-8161 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 28 19:19:42.200: INFO: rc: 1
Jun 28 19:19:42.200: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-108209140 exec --namespace=statefulset-8161 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jun 28 19:19:52.201: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-108209140 exec --namespace=statefulset-8161 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 28 19:19:52.451: INFO: rc: 1
Jun 28 19:19:52.451: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-108209140 exec --namespace=statefulset-8161 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jun 28 19:20:02.452: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-108209140 exec --namespace=statefulset-8161 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 28 19:20:02.643: INFO: rc: 1
Jun 28 19:20:02.643: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-108209140 exec --namespace=statefulset-8161 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jun 28 19:20:12.643: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-108209140 exec --namespace=statefulset-8161 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 28 19:20:12.810: INFO: rc: 1
Jun 28 19:20:12.810: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-108209140 exec --namespace=statefulset-8161 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jun 28 19:20:22.811: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-108209140 exec --namespace=statefulset-8161 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 28 19:20:23.195: INFO: rc: 1
Jun 28 19:20:23.195: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-108209140 exec --namespace=statefulset-8161 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jun 28 19:20:33.195: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-108209140 exec --namespace=statefulset-8161 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 28 19:20:33.390: INFO: rc: 1
Jun 28 19:20:33.390: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-108209140 exec --namespace=statefulset-8161 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jun 28 19:20:43.390: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-108209140 exec --namespace=statefulset-8161 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 28 19:20:43.638: INFO: rc: 1
Jun 28 19:20:43.638: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-108209140 exec --namespace=statefulset-8161 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jun 28 19:20:53.638: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-108209140 exec --namespace=statefulset-8161 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 28 19:20:53.852: INFO: rc: 1
Jun 28 19:20:53.852: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-108209140 exec --namespace=statefulset-8161 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jun 28 19:21:03.853: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-108209140 exec --namespace=statefulset-8161 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 28 19:21:04.070: INFO: rc: 1
Jun 28 19:21:04.071: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: 
Jun 28 19:21:04.071: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
Jun 28 19:21:04.114: INFO: Deleting all statefulset in ns statefulset-8161
Jun 28 19:21:04.123: INFO: Scaling statefulset ss to 0
Jun 28 19:21:04.163: INFO: Waiting for statefulset status.replicas updated to 0
Jun 28 19:21:04.172: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 19:21:04.216: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-8161" for this suite.

• [SLOW TEST:372.737 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
    /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]","total":305,"completed":218,"skipped":3447,"failed":0}
S
------------------------------
[sig-network] Services 
  should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 19:21:04.255: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating service in namespace services-6805
Jun 28 19:21:06.458: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-108209140 exec --namespace=services-6805 kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
Jun 28 19:21:06.902: INFO: rc: 7
Jun 28 19:21:06.920: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Jun 28 19:21:06.935: INFO: Pod kube-proxy-mode-detector still exists
Jun 28 19:21:08.936: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Jun 28 19:21:08.951: INFO: Pod kube-proxy-mode-detector still exists
Jun 28 19:21:10.936: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Jun 28 19:21:10.951: INFO: Pod kube-proxy-mode-detector still exists
Jun 28 19:21:12.936: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Jun 28 19:21:12.953: INFO: Pod kube-proxy-mode-detector still exists
Jun 28 19:21:14.935: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Jun 28 19:21:14.950: INFO: Pod kube-proxy-mode-detector still exists
Jun 28 19:21:16.936: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Jun 28 19:21:16.955: INFO: Pod kube-proxy-mode-detector still exists
Jun 28 19:21:18.935: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Jun 28 19:21:18.953: INFO: Pod kube-proxy-mode-detector no longer exists
Jun 28 19:21:18.953: INFO: Couldn't detect KubeProxy mode - test failure may be expected: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-108209140 exec --namespace=services-6805 kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode:
Command stdout:

stderr:
+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode
command terminated with exit code 7

error:
exit status 7
STEP: creating service affinity-nodeport-timeout in namespace services-6805
STEP: creating replication controller affinity-nodeport-timeout in namespace services-6805
I0628 19:21:19.014589      22 runners.go:190] Created replication controller with name: affinity-nodeport-timeout, namespace: services-6805, replica count: 3
I0628 19:21:22.069858      22 runners.go:190] affinity-nodeport-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jun 28 19:21:22.111: INFO: Creating new exec pod
Jun 28 19:21:25.202: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-108209140 exec --namespace=services-6805 execpod-affinitywbdzs -- /bin/sh -x -c nc -zv -t -w 2 affinity-nodeport-timeout 80'
Jun 28 19:21:25.663: INFO: stderr: "+ nc -zv -t -w 2 affinity-nodeport-timeout 80\nConnection to affinity-nodeport-timeout 80 port [tcp/http] succeeded!\n"
Jun 28 19:21:25.663: INFO: stdout: ""
Jun 28 19:21:25.665: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-108209140 exec --namespace=services-6805 execpod-affinitywbdzs -- /bin/sh -x -c nc -zv -t -w 2 172.21.177.253 80'
Jun 28 19:21:26.019: INFO: stderr: "+ nc -zv -t -w 2 172.21.177.253 80\nConnection to 172.21.177.253 80 port [tcp/http] succeeded!\n"
Jun 28 19:21:26.019: INFO: stdout: ""
Jun 28 19:21:26.019: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-108209140 exec --namespace=services-6805 execpod-affinitywbdzs -- /bin/sh -x -c nc -zv -t -w 2 10.13.107.37 30768'
Jun 28 19:21:26.412: INFO: stderr: "+ nc -zv -t -w 2 10.13.107.37 30768\nConnection to 10.13.107.37 30768 port [tcp/30768] succeeded!\n"
Jun 28 19:21:26.412: INFO: stdout: ""
Jun 28 19:21:26.412: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-108209140 exec --namespace=services-6805 execpod-affinitywbdzs -- /bin/sh -x -c nc -zv -t -w 2 10.13.107.60 30768'
Jun 28 19:21:26.868: INFO: stderr: "+ nc -zv -t -w 2 10.13.107.60 30768\nConnection to 10.13.107.60 30768 port [tcp/30768] succeeded!\n"
Jun 28 19:21:26.868: INFO: stdout: ""
Jun 28 19:21:26.868: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-108209140 exec --namespace=services-6805 execpod-affinitywbdzs -- /bin/sh -x -c nc -zv -t -w 2 149.81.178.52 30768'
Jun 28 19:21:27.270: INFO: stderr: "+ nc -zv -t -w 2 149.81.178.52 30768\nConnection to 149.81.178.52 30768 port [tcp/30768] succeeded!\n"
Jun 28 19:21:27.270: INFO: stdout: ""
Jun 28 19:21:27.270: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-108209140 exec --namespace=services-6805 execpod-affinitywbdzs -- /bin/sh -x -c nc -zv -t -w 2 149.81.178.59 30768'
Jun 28 19:21:27.787: INFO: stderr: "+ nc -zv -t -w 2 149.81.178.59 30768\nConnection to 149.81.178.59 30768 port [tcp/30768] succeeded!\n"
Jun 28 19:21:27.787: INFO: stdout: ""
Jun 28 19:21:27.787: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-108209140 exec --namespace=services-6805 execpod-affinitywbdzs -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.13.107.37:30768/ ; done'
Jun 28 19:21:28.387: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.13.107.37:30768/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.13.107.37:30768/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.13.107.37:30768/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.13.107.37:30768/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.13.107.37:30768/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.13.107.37:30768/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.13.107.37:30768/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.13.107.37:30768/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.13.107.37:30768/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.13.107.37:30768/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.13.107.37:30768/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.13.107.37:30768/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.13.107.37:30768/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.13.107.37:30768/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.13.107.37:30768/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.13.107.37:30768/\n"
Jun 28 19:21:28.387: INFO: stdout: "\naffinity-nodeport-timeout-g6fr8\naffinity-nodeport-timeout-g6fr8\naffinity-nodeport-timeout-g6fr8\naffinity-nodeport-timeout-g6fr8\naffinity-nodeport-timeout-g6fr8\naffinity-nodeport-timeout-g6fr8\naffinity-nodeport-timeout-g6fr8\naffinity-nodeport-timeout-g6fr8\naffinity-nodeport-timeout-g6fr8\naffinity-nodeport-timeout-g6fr8\naffinity-nodeport-timeout-g6fr8\naffinity-nodeport-timeout-g6fr8\naffinity-nodeport-timeout-g6fr8\naffinity-nodeport-timeout-g6fr8\naffinity-nodeport-timeout-g6fr8\naffinity-nodeport-timeout-g6fr8"
Jun 28 19:21:28.387: INFO: Received response from host: affinity-nodeport-timeout-g6fr8
Jun 28 19:21:28.387: INFO: Received response from host: affinity-nodeport-timeout-g6fr8
Jun 28 19:21:28.387: INFO: Received response from host: affinity-nodeport-timeout-g6fr8
Jun 28 19:21:28.387: INFO: Received response from host: affinity-nodeport-timeout-g6fr8
Jun 28 19:21:28.387: INFO: Received response from host: affinity-nodeport-timeout-g6fr8
Jun 28 19:21:28.387: INFO: Received response from host: affinity-nodeport-timeout-g6fr8
Jun 28 19:21:28.387: INFO: Received response from host: affinity-nodeport-timeout-g6fr8
Jun 28 19:21:28.387: INFO: Received response from host: affinity-nodeport-timeout-g6fr8
Jun 28 19:21:28.387: INFO: Received response from host: affinity-nodeport-timeout-g6fr8
Jun 28 19:21:28.387: INFO: Received response from host: affinity-nodeport-timeout-g6fr8
Jun 28 19:21:28.387: INFO: Received response from host: affinity-nodeport-timeout-g6fr8
Jun 28 19:21:28.387: INFO: Received response from host: affinity-nodeport-timeout-g6fr8
Jun 28 19:21:28.387: INFO: Received response from host: affinity-nodeport-timeout-g6fr8
Jun 28 19:21:28.387: INFO: Received response from host: affinity-nodeport-timeout-g6fr8
Jun 28 19:21:28.387: INFO: Received response from host: affinity-nodeport-timeout-g6fr8
Jun 28 19:21:28.387: INFO: Received response from host: affinity-nodeport-timeout-g6fr8
Jun 28 19:21:28.387: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-108209140 exec --namespace=services-6805 execpod-affinitywbdzs -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.13.107.37:30768/'
Jun 28 19:21:28.793: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.13.107.37:30768/\n"
Jun 28 19:21:28.793: INFO: stdout: "affinity-nodeport-timeout-g6fr8"
Jun 28 19:21:43.793: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-108209140 exec --namespace=services-6805 execpod-affinitywbdzs -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.13.107.37:30768/'
Jun 28 19:21:44.254: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.13.107.37:30768/\n"
Jun 28 19:21:44.255: INFO: stdout: "affinity-nodeport-timeout-z4vdf"
Jun 28 19:21:44.255: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-timeout in namespace services-6805, will wait for the garbage collector to delete the pods
Jun 28 19:21:44.368: INFO: Deleting ReplicationController affinity-nodeport-timeout took: 19.286264ms
Jun 28 19:21:44.469: INFO: Terminating ReplicationController affinity-nodeport-timeout pods took: 100.343931ms
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 19:21:57.924: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6805" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

• [SLOW TEST:53.717 seconds]
[sig-network] Services
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]","total":305,"completed":219,"skipped":3448,"failed":0}
SSSSS
------------------------------
[sig-apps] Job 
  should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Job
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 19:21:57.977: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: Orphaning one of the Job's Pods
Jun 28 19:22:00.716: INFO: Successfully updated pod "adopt-release-7pqf8"
STEP: Checking that the Job readopts the Pod
Jun 28 19:22:00.716: INFO: Waiting up to 15m0s for pod "adopt-release-7pqf8" in namespace "job-5095" to be "adopted"
Jun 28 19:22:00.726: INFO: Pod "adopt-release-7pqf8": Phase="Running", Reason="", readiness=true. Elapsed: 9.872962ms
Jun 28 19:22:02.750: INFO: Pod "adopt-release-7pqf8": Phase="Running", Reason="", readiness=true. Elapsed: 2.033730313s
Jun 28 19:22:02.751: INFO: Pod "adopt-release-7pqf8" satisfied condition "adopted"
STEP: Removing the labels from the Job's Pod
Jun 28 19:22:03.300: INFO: Successfully updated pod "adopt-release-7pqf8"
STEP: Checking that the Job releases the Pod
Jun 28 19:22:03.301: INFO: Waiting up to 15m0s for pod "adopt-release-7pqf8" in namespace "job-5095" to be "released"
Jun 28 19:22:03.317: INFO: Pod "adopt-release-7pqf8": Phase="Running", Reason="", readiness=true. Elapsed: 16.045402ms
Jun 28 19:22:05.329: INFO: Pod "adopt-release-7pqf8": Phase="Running", Reason="", readiness=true. Elapsed: 2.028129058s
Jun 28 19:22:05.329: INFO: Pod "adopt-release-7pqf8" satisfied condition "released"
[AfterEach] [sig-apps] Job
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 19:22:05.330: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-5095" for this suite.

• [SLOW TEST:7.387 seconds]
[sig-apps] Job
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Job should adopt matching orphans and release non-matching pods [Conformance]","total":305,"completed":220,"skipped":3453,"failed":0}
SSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 19:22:05.370: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Jun 28 19:22:08.649: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 19:22:08.696: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-3871" for this suite.
•{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":305,"completed":221,"skipped":3461,"failed":0}
SSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 19:22:08.731: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:90
Jun 28 19:22:08.862: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jun 28 19:22:08.912: INFO: Waiting for terminating namespaces to be deleted...
Jun 28 19:22:08.929: INFO: 
Logging pods the apiserver thinks is on node 10.13.107.37 before test
Jun 28 19:22:08.963: INFO: calico-node-7ntd9 from calico-system started at 2021-06-28 16:43:04 +0000 UTC (1 container statuses recorded)
Jun 28 19:22:08.964: INFO: 	Container calico-node ready: true, restart count 0
Jun 28 19:22:08.964: INFO: calico-typha-649969bb55-mj7tk from calico-system started at 2021-06-28 16:45:03 +0000 UTC (1 container statuses recorded)
Jun 28 19:22:08.964: INFO: 	Container calico-typha ready: true, restart count 0
Jun 28 19:22:08.964: INFO: adopt-release-7pqf8 from job-5095 started at 2021-06-28 19:21:58 +0000 UTC (1 container statuses recorded)
Jun 28 19:22:08.964: INFO: 	Container c ready: true, restart count 0
Jun 28 19:22:08.964: INFO: adopt-release-9dbnq from job-5095 started at 2021-06-28 19:21:58 +0000 UTC (1 container statuses recorded)
Jun 28 19:22:08.964: INFO: 	Container c ready: true, restart count 0
Jun 28 19:22:08.965: INFO: adopt-release-zdxf4 from job-5095 started at 2021-06-28 19:22:03 +0000 UTC (1 container statuses recorded)
Jun 28 19:22:08.965: INFO: 	Container c ready: true, restart count 0
Jun 28 19:22:08.965: INFO: ibm-keepalived-watcher-f9nwv from kube-system started at 2021-06-28 16:41:30 +0000 UTC (1 container statuses recorded)
Jun 28 19:22:08.965: INFO: 	Container keepalived-watcher ready: true, restart count 0
Jun 28 19:22:08.965: INFO: ibm-master-proxy-static-10.13.107.37 from kube-system started at 2021-06-28 16:40:48 +0000 UTC (2 container statuses recorded)
Jun 28 19:22:08.965: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Jun 28 19:22:08.965: INFO: 	Container pause ready: true, restart count 0
Jun 28 19:22:08.965: INFO: ibmcloud-block-storage-driver-kbnst from kube-system started at 2021-06-28 16:41:30 +0000 UTC (1 container statuses recorded)
Jun 28 19:22:08.965: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Jun 28 19:22:08.965: INFO: tuned-2tw2w from openshift-cluster-node-tuning-operator started at 2021-06-28 16:43:24 +0000 UTC (1 container statuses recorded)
Jun 28 19:22:08.965: INFO: 	Container tuned ready: true, restart count 0
Jun 28 19:22:08.965: INFO: dns-default-crxtq from openshift-dns started at 2021-06-28 16:44:04 +0000 UTC (3 container statuses recorded)
Jun 28 19:22:08.965: INFO: 	Container dns ready: true, restart count 0
Jun 28 19:22:08.965: INFO: 	Container dns-node-resolver ready: true, restart count 0
Jun 28 19:22:08.965: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 28 19:22:08.965: INFO: node-ca-z64gj from openshift-image-registry started at 2021-06-28 16:44:14 +0000 UTC (1 container statuses recorded)
Jun 28 19:22:08.965: INFO: 	Container node-ca ready: true, restart count 0
Jun 28 19:22:08.965: INFO: registry-pvc-permissions-lfjks from openshift-image-registry started at 2021-06-28 16:46:50 +0000 UTC (1 container statuses recorded)
Jun 28 19:22:08.965: INFO: 	Container pvc-permissions ready: false, restart count 0
Jun 28 19:22:08.965: INFO: openshift-kube-proxy-8l7d5 from openshift-kube-proxy started at 2021-06-28 16:42:08 +0000 UTC (1 container statuses recorded)
Jun 28 19:22:08.965: INFO: 	Container kube-proxy ready: true, restart count 0
Jun 28 19:22:08.965: INFO: node-exporter-qj7hs from openshift-monitoring started at 2021-06-28 16:44:29 +0000 UTC (2 container statuses recorded)
Jun 28 19:22:08.965: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 28 19:22:08.965: INFO: 	Container node-exporter ready: true, restart count 0
Jun 28 19:22:08.965: INFO: multus-admission-controller-nfv4l from openshift-multus started at 2021-06-28 19:12:27 +0000 UTC (2 container statuses recorded)
Jun 28 19:22:08.965: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 28 19:22:08.965: INFO: 	Container multus-admission-controller ready: true, restart count 0
Jun 28 19:22:08.965: INFO: multus-brfr7 from openshift-multus started at 2021-06-28 16:42:07 +0000 UTC (1 container statuses recorded)
Jun 28 19:22:08.965: INFO: 	Container kube-multus ready: true, restart count 0
Jun 28 19:22:08.965: INFO: network-metrics-daemon-5rjvj from openshift-multus started at 2021-06-28 16:42:08 +0000 UTC (2 container statuses recorded)
Jun 28 19:22:08.965: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 28 19:22:08.965: INFO: 	Container network-metrics-daemon ready: true, restart count 0
Jun 28 19:22:08.965: INFO: sonobuoy-systemd-logs-daemon-set-bc3c64b0befe49cb-gp9nl from sonobuoy started at 2021-06-28 18:11:47 +0000 UTC (2 container statuses recorded)
Jun 28 19:22:08.965: INFO: 	Container sonobuoy-worker ready: false, restart count 6
Jun 28 19:22:08.965: INFO: 	Container systemd-logs ready: true, restart count 0
Jun 28 19:22:08.965: INFO: 
Logging pods the apiserver thinks is on node 10.13.107.57 before test
Jun 28 19:22:09.017: INFO: calico-node-tr9lp from calico-system started at 2021-06-28 16:43:04 +0000 UTC (1 container statuses recorded)
Jun 28 19:22:09.017: INFO: 	Container calico-node ready: true, restart count 0
Jun 28 19:22:09.017: INFO: calico-typha-649969bb55-tpj68 from calico-system started at 2021-06-28 16:43:04 +0000 UTC (1 container statuses recorded)
Jun 28 19:22:09.017: INFO: 	Container calico-typha ready: true, restart count 0
Jun 28 19:22:09.017: INFO: ibm-cloud-provider-ip-149-81-71-226-77554565fb-clmlm from ibm-system started at 2021-06-28 16:58:09 +0000 UTC (1 container statuses recorded)
Jun 28 19:22:09.017: INFO: 	Container ibm-cloud-provider-ip-149-81-71-226 ready: true, restart count 0
Jun 28 19:22:09.017: INFO: ibm-file-plugin-7d974bf47c-bw8qh from kube-system started at 2021-06-28 16:43:14 +0000 UTC (1 container statuses recorded)
Jun 28 19:22:09.017: INFO: 	Container ibm-file-plugin-container ready: true, restart count 0
Jun 28 19:22:09.017: INFO: ibm-keepalived-watcher-rqj9s from kube-system started at 2021-06-28 16:41:35 +0000 UTC (1 container statuses recorded)
Jun 28 19:22:09.017: INFO: 	Container keepalived-watcher ready: true, restart count 0
Jun 28 19:22:09.018: INFO: ibm-master-proxy-static-10.13.107.57 from kube-system started at 2021-06-28 16:40:56 +0000 UTC (2 container statuses recorded)
Jun 28 19:22:09.018: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Jun 28 19:22:09.018: INFO: 	Container pause ready: true, restart count 0
Jun 28 19:22:09.018: INFO: ibm-storage-watcher-869677c8b-q8rh5 from kube-system started at 2021-06-28 16:43:14 +0000 UTC (1 container statuses recorded)
Jun 28 19:22:09.018: INFO: 	Container ibm-storage-watcher-container ready: true, restart count 0
Jun 28 19:22:09.018: INFO: ibmcloud-block-storage-driver-p9gg7 from kube-system started at 2021-06-28 16:41:35 +0000 UTC (1 container statuses recorded)
Jun 28 19:22:09.018: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Jun 28 19:22:09.018: INFO: ibmcloud-block-storage-plugin-665488684b-h5fhz from kube-system started at 2021-06-28 16:43:14 +0000 UTC (1 container statuses recorded)
Jun 28 19:22:09.018: INFO: 	Container ibmcloud-block-storage-plugin-container ready: true, restart count 0
Jun 28 19:22:09.018: INFO: vpn-6c58c4756c-rw748 from kube-system started at 2021-06-28 16:50:23 +0000 UTC (1 container statuses recorded)
Jun 28 19:22:09.018: INFO: 	Container vpn ready: true, restart count 0
Jun 28 19:22:09.018: INFO: cluster-node-tuning-operator-77dfdd89b8-chjmh from openshift-cluster-node-tuning-operator started at 2021-06-28 16:43:14 +0000 UTC (1 container statuses recorded)
Jun 28 19:22:09.018: INFO: 	Container cluster-node-tuning-operator ready: true, restart count 0
Jun 28 19:22:09.018: INFO: tuned-r5kts from openshift-cluster-node-tuning-operator started at 2021-06-28 16:43:24 +0000 UTC (1 container statuses recorded)
Jun 28 19:22:09.018: INFO: 	Container tuned ready: true, restart count 0
Jun 28 19:22:09.018: INFO: cluster-samples-operator-56dd9bbcb-8mjfl from openshift-cluster-samples-operator started at 2021-06-28 16:43:14 +0000 UTC (2 container statuses recorded)
Jun 28 19:22:09.018: INFO: 	Container cluster-samples-operator ready: true, restart count 0
Jun 28 19:22:09.018: INFO: 	Container cluster-samples-operator-watch ready: true, restart count 0
Jun 28 19:22:09.018: INFO: cluster-storage-operator-56664f46b8-kjjbs from openshift-cluster-storage-operator started at 2021-06-28 16:43:14 +0000 UTC (1 container statuses recorded)
Jun 28 19:22:09.018: INFO: 	Container cluster-storage-operator ready: true, restart count 0
Jun 28 19:22:09.018: INFO: console-operator-cd789fcfb-4j2b8 from openshift-console-operator started at 2021-06-28 16:43:14 +0000 UTC (1 container statuses recorded)
Jun 28 19:22:09.018: INFO: 	Container console-operator ready: true, restart count 1
Jun 28 19:22:09.018: INFO: console-74dd6cb4c6-vnhx8 from openshift-console started at 2021-06-28 18:29:06 +0000 UTC (1 container statuses recorded)
Jun 28 19:22:09.018: INFO: 	Container console ready: true, restart count 0
Jun 28 19:22:09.018: INFO: downloads-65dcc64f75-2bzn8 from openshift-console started at 2021-06-28 16:43:14 +0000 UTC (1 container statuses recorded)
Jun 28 19:22:09.018: INFO: 	Container download-server ready: true, restart count 0
Jun 28 19:22:09.018: INFO: downloads-65dcc64f75-5hm2s from openshift-console started at 2021-06-28 16:43:14 +0000 UTC (1 container statuses recorded)
Jun 28 19:22:09.018: INFO: 	Container download-server ready: true, restart count 0
Jun 28 19:22:09.018: INFO: dns-operator-7d8cb9bb6d-kkfvf from openshift-dns-operator started at 2021-06-28 16:43:14 +0000 UTC (2 container statuses recorded)
Jun 28 19:22:09.018: INFO: 	Container dns-operator ready: true, restart count 0
Jun 28 19:22:09.018: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 28 19:22:09.018: INFO: dns-default-48b6x from openshift-dns started at 2021-06-28 16:44:04 +0000 UTC (3 container statuses recorded)
Jun 28 19:22:09.018: INFO: 	Container dns ready: true, restart count 0
Jun 28 19:22:09.018: INFO: 	Container dns-node-resolver ready: true, restart count 0
Jun 28 19:22:09.018: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 28 19:22:09.018: INFO: cluster-image-registry-operator-999d7b49c-d458n from openshift-image-registry started at 2021-06-28 16:43:14 +0000 UTC (1 container statuses recorded)
Jun 28 19:22:09.018: INFO: 	Container cluster-image-registry-operator ready: true, restart count 0
Jun 28 19:22:09.018: INFO: node-ca-49d6t from openshift-image-registry started at 2021-06-28 16:44:14 +0000 UTC (1 container statuses recorded)
Jun 28 19:22:09.018: INFO: 	Container node-ca ready: true, restart count 0
Jun 28 19:22:09.018: INFO: ingress-operator-795fb4477f-sgnmp from openshift-ingress-operator started at 2021-06-28 16:43:14 +0000 UTC (2 container statuses recorded)
Jun 28 19:22:09.019: INFO: 	Container ingress-operator ready: true, restart count 0
Jun 28 19:22:09.019: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 28 19:22:09.019: INFO: router-default-8b99f5968-b84k7 from openshift-ingress started at 2021-06-28 18:29:06 +0000 UTC (1 container statuses recorded)
Jun 28 19:22:09.019: INFO: 	Container router ready: true, restart count 0
Jun 28 19:22:09.019: INFO: openshift-kube-proxy-xq2t8 from openshift-kube-proxy started at 2021-06-28 16:42:08 +0000 UTC (1 container statuses recorded)
Jun 28 19:22:09.019: INFO: 	Container kube-proxy ready: true, restart count 0
Jun 28 19:22:09.019: INFO: kube-storage-version-migrator-operator-77d5dd5f6c-z9m4j from openshift-kube-storage-version-migrator-operator started at 2021-06-28 16:43:14 +0000 UTC (1 container statuses recorded)
Jun 28 19:22:09.019: INFO: 	Container kube-storage-version-migrator-operator ready: true, restart count 1
Jun 28 19:22:09.019: INFO: marketplace-operator-7b9d5dcb99-nzgf8 from openshift-marketplace started at 2021-06-28 16:43:14 +0000 UTC (1 container statuses recorded)
Jun 28 19:22:09.019: INFO: 	Container marketplace-operator ready: true, restart count 0
Jun 28 19:22:09.019: INFO: alertmanager-main-1 from openshift-monitoring started at 2021-06-28 16:45:25 +0000 UTC (5 container statuses recorded)
Jun 28 19:22:09.019: INFO: 	Container alertmanager ready: true, restart count 0
Jun 28 19:22:09.019: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Jun 28 19:22:09.019: INFO: 	Container config-reloader ready: true, restart count 0
Jun 28 19:22:09.019: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 28 19:22:09.019: INFO: 	Container prom-label-proxy ready: true, restart count 0
Jun 28 19:22:09.019: INFO: cluster-monitoring-operator-6964464f67-jbfjk from openshift-monitoring started at 2021-06-28 16:43:14 +0000 UTC (2 container statuses recorded)
Jun 28 19:22:09.020: INFO: 	Container cluster-monitoring-operator ready: true, restart count 0
Jun 28 19:22:09.020: INFO: 	Container kube-rbac-proxy ready: true, restart count 2
Jun 28 19:22:09.020: INFO: node-exporter-ld8qc from openshift-monitoring started at 2021-06-28 16:44:29 +0000 UTC (2 container statuses recorded)
Jun 28 19:22:09.020: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 28 19:22:09.020: INFO: 	Container node-exporter ready: true, restart count 0
Jun 28 19:22:09.020: INFO: prometheus-adapter-8646bbc64d-stfnh from openshift-monitoring started at 2021-06-28 18:29:07 +0000 UTC (1 container statuses recorded)
Jun 28 19:22:09.020: INFO: 	Container prometheus-adapter ready: true, restart count 0
Jun 28 19:22:09.020: INFO: prometheus-k8s-0 from openshift-monitoring started at 2021-06-28 18:29:17 +0000 UTC (6 container statuses recorded)
Jun 28 19:22:09.020: INFO: 	Container config-reloader ready: true, restart count 0
Jun 28 19:22:09.021: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 28 19:22:09.021: INFO: 	Container prom-label-proxy ready: true, restart count 0
Jun 28 19:22:09.021: INFO: 	Container prometheus ready: true, restart count 1
Jun 28 19:22:09.021: INFO: 	Container prometheus-proxy ready: true, restart count 0
Jun 28 19:22:09.021: INFO: 	Container thanos-sidecar ready: true, restart count 0
Jun 28 19:22:09.021: INFO: thanos-querier-b46bb8497-dcbxw from openshift-monitoring started at 2021-06-28 18:29:07 +0000 UTC (5 container statuses recorded)
Jun 28 19:22:09.021: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 28 19:22:09.021: INFO: 	Container kube-rbac-proxy-rules ready: true, restart count 0
Jun 28 19:22:09.021: INFO: 	Container oauth-proxy ready: true, restart count 0
Jun 28 19:22:09.022: INFO: 	Container prom-label-proxy ready: true, restart count 0
Jun 28 19:22:09.022: INFO: 	Container thanos-query ready: true, restart count 0
Jun 28 19:22:09.022: INFO: multus-admission-controller-58k5w from openshift-multus started at 2021-06-28 16:43:13 +0000 UTC (2 container statuses recorded)
Jun 28 19:22:09.022: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 28 19:22:09.022: INFO: 	Container multus-admission-controller ready: true, restart count 0
Jun 28 19:22:09.022: INFO: multus-hvm6b from openshift-multus started at 2021-06-28 16:42:08 +0000 UTC (1 container statuses recorded)
Jun 28 19:22:09.022: INFO: 	Container kube-multus ready: true, restart count 0
Jun 28 19:22:09.022: INFO: network-metrics-daemon-78dfb from openshift-multus started at 2021-06-28 16:42:08 +0000 UTC (2 container statuses recorded)
Jun 28 19:22:09.022: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 28 19:22:09.022: INFO: 	Container network-metrics-daemon ready: true, restart count 0
Jun 28 19:22:09.022: INFO: network-operator-5b78d575b8-gvzvg from openshift-network-operator started at 2021-06-28 16:41:35 +0000 UTC (1 container statuses recorded)
Jun 28 19:22:09.023: INFO: 	Container network-operator ready: true, restart count 0
Jun 28 19:22:09.023: INFO: catalog-operator-67c774db7d-5ps99 from openshift-operator-lifecycle-manager started at 2021-06-28 16:43:14 +0000 UTC (1 container statuses recorded)
Jun 28 19:22:09.023: INFO: 	Container catalog-operator ready: true, restart count 0
Jun 28 19:22:09.023: INFO: olm-operator-68fcf6954f-nfsgq from openshift-operator-lifecycle-manager started at 2021-06-28 16:43:14 +0000 UTC (1 container statuses recorded)
Jun 28 19:22:09.023: INFO: 	Container olm-operator ready: true, restart count 0
Jun 28 19:22:09.023: INFO: packageserver-65544748bd-hhnld from openshift-operator-lifecycle-manager started at 2021-06-28 18:29:07 +0000 UTC (1 container statuses recorded)
Jun 28 19:22:09.023: INFO: 	Container packageserver ready: true, restart count 0
Jun 28 19:22:09.023: INFO: metrics-7df79584fc-f6g2g from openshift-roks-metrics started at 2021-06-28 16:43:14 +0000 UTC (1 container statuses recorded)
Jun 28 19:22:09.023: INFO: 	Container metrics ready: true, restart count 1
Jun 28 19:22:09.023: INFO: push-gateway-6dc86bc94f-dz7nt from openshift-roks-metrics started at 2021-06-28 16:43:14 +0000 UTC (1 container statuses recorded)
Jun 28 19:22:09.023: INFO: 	Container push-gateway ready: true, restart count 0
Jun 28 19:22:09.023: INFO: service-ca-operator-d4bfd498b-4xhnv from openshift-service-ca-operator started at 2021-06-28 16:43:14 +0000 UTC (1 container statuses recorded)
Jun 28 19:22:09.023: INFO: 	Container service-ca-operator ready: true, restart count 1
Jun 28 19:22:09.023: INFO: service-ca-665fb97685-mn77q from openshift-service-ca started at 2021-06-28 18:29:06 +0000 UTC (1 container statuses recorded)
Jun 28 19:22:09.024: INFO: 	Container service-ca-controller ready: true, restart count 0
Jun 28 19:22:09.024: INFO: sonobuoy from sonobuoy started at 2021-06-28 18:11:40 +0000 UTC (1 container statuses recorded)
Jun 28 19:22:09.024: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jun 28 19:22:09.024: INFO: sonobuoy-systemd-logs-daemon-set-bc3c64b0befe49cb-s48t5 from sonobuoy started at 2021-06-28 18:11:47 +0000 UTC (2 container statuses recorded)
Jun 28 19:22:09.024: INFO: 	Container sonobuoy-worker ready: false, restart count 6
Jun 28 19:22:09.024: INFO: 	Container systemd-logs ready: true, restart count 0
Jun 28 19:22:09.024: INFO: tigera-operator-64c8f4c7d7-hzwkg from tigera-operator started at 2021-06-28 18:29:06 +0000 UTC (1 container statuses recorded)
Jun 28 19:22:09.024: INFO: 	Container tigera-operator ready: true, restart count 0
Jun 28 19:22:09.024: INFO: 
Logging pods the apiserver thinks is on node 10.13.107.60 before test
Jun 28 19:22:09.074: INFO: calico-kube-controllers-57df785794-xp76v from calico-system started at 2021-06-28 18:29:06 +0000 UTC (1 container statuses recorded)
Jun 28 19:22:09.075: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Jun 28 19:22:09.075: INFO: calico-node-vr9g2 from calico-system started at 2021-06-28 16:43:04 +0000 UTC (1 container statuses recorded)
Jun 28 19:22:09.075: INFO: 	Container calico-node ready: true, restart count 0
Jun 28 19:22:09.075: INFO: calico-typha-649969bb55-mc6s8 from calico-system started at 2021-06-28 16:45:02 +0000 UTC (1 container statuses recorded)
Jun 28 19:22:09.075: INFO: 	Container calico-typha ready: true, restart count 0
Jun 28 19:22:09.075: INFO: test-k8s-e2e-pvg-master-verification from default started at 2021-06-28 16:51:21 +0000 UTC (1 container statuses recorded)
Jun 28 19:22:09.075: INFO: 	Container test-k8s-e2e-pvg-master-verification ready: true, restart count 0
Jun 28 19:22:09.075: INFO: ibm-cloud-provider-ip-149-81-71-226-77554565fb-djwh7 from ibm-system started at 2021-06-28 18:29:06 +0000 UTC (1 container statuses recorded)
Jun 28 19:22:09.075: INFO: 	Container ibm-cloud-provider-ip-149-81-71-226 ready: true, restart count 0
Jun 28 19:22:09.075: INFO: ibm-keepalived-watcher-w6fhq from kube-system started at 2021-06-28 16:42:37 +0000 UTC (1 container statuses recorded)
Jun 28 19:22:09.075: INFO: 	Container keepalived-watcher ready: true, restart count 0
Jun 28 19:22:09.075: INFO: ibm-master-proxy-static-10.13.107.60 from kube-system started at 2021-06-28 16:41:49 +0000 UTC (2 container statuses recorded)
Jun 28 19:22:09.075: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Jun 28 19:22:09.075: INFO: 	Container pause ready: true, restart count 0
Jun 28 19:22:09.075: INFO: ibmcloud-block-storage-driver-kwxt2 from kube-system started at 2021-06-28 16:42:37 +0000 UTC (1 container statuses recorded)
Jun 28 19:22:09.075: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Jun 28 19:22:09.075: INFO: tuned-djhn5 from openshift-cluster-node-tuning-operator started at 2021-06-28 16:43:24 +0000 UTC (1 container statuses recorded)
Jun 28 19:22:09.075: INFO: 	Container tuned ready: true, restart count 0
Jun 28 19:22:09.075: INFO: console-74dd6cb4c6-gbthm from openshift-console started at 2021-06-28 17:20:10 +0000 UTC (1 container statuses recorded)
Jun 28 19:22:09.075: INFO: 	Container console ready: true, restart count 0
Jun 28 19:22:09.075: INFO: dns-default-84l5j from openshift-dns started at 2021-06-28 16:44:04 +0000 UTC (3 container statuses recorded)
Jun 28 19:22:09.075: INFO: 	Container dns ready: true, restart count 0
Jun 28 19:22:09.075: INFO: 	Container dns-node-resolver ready: true, restart count 0
Jun 28 19:22:09.075: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 28 19:22:09.075: INFO: image-registry-7f9574d5c6-tmmns from openshift-image-registry started at 2021-06-28 16:46:50 +0000 UTC (1 container statuses recorded)
Jun 28 19:22:09.075: INFO: 	Container registry ready: true, restart count 0
Jun 28 19:22:09.075: INFO: node-ca-42txp from openshift-image-registry started at 2021-06-28 16:44:14 +0000 UTC (1 container statuses recorded)
Jun 28 19:22:09.075: INFO: 	Container node-ca ready: true, restart count 0
Jun 28 19:22:09.075: INFO: router-default-8b99f5968-8nzw5 from openshift-ingress started at 2021-06-28 17:18:25 +0000 UTC (1 container statuses recorded)
Jun 28 19:22:09.075: INFO: 	Container router ready: true, restart count 0
Jun 28 19:22:09.075: INFO: openshift-kube-proxy-n6pzf from openshift-kube-proxy started at 2021-06-28 16:42:37 +0000 UTC (1 container statuses recorded)
Jun 28 19:22:09.075: INFO: 	Container kube-proxy ready: true, restart count 0
Jun 28 19:22:09.075: INFO: migrator-858cfc6f4c-tvj6h from openshift-kube-storage-version-migrator started at 2021-06-28 18:29:06 +0000 UTC (1 container statuses recorded)
Jun 28 19:22:09.075: INFO: 	Container migrator ready: true, restart count 0
Jun 28 19:22:09.075: INFO: certified-operators-d5d5w from openshift-marketplace started at 2021-06-28 16:44:19 +0000 UTC (1 container statuses recorded)
Jun 28 19:22:09.075: INFO: 	Container registry-server ready: true, restart count 0
Jun 28 19:22:09.075: INFO: community-operators-dbmgh from openshift-marketplace started at 2021-06-28 17:01:52 +0000 UTC (1 container statuses recorded)
Jun 28 19:22:09.075: INFO: 	Container registry-server ready: true, restart count 0
Jun 28 19:22:09.075: INFO: redhat-marketplace-4t4vz from openshift-marketplace started at 2021-06-28 16:44:21 +0000 UTC (1 container statuses recorded)
Jun 28 19:22:09.076: INFO: 	Container registry-server ready: true, restart count 0
Jun 28 19:22:09.076: INFO: redhat-operators-dvpnz from openshift-marketplace started at 2021-06-28 16:44:19 +0000 UTC (1 container statuses recorded)
Jun 28 19:22:09.076: INFO: 	Container registry-server ready: true, restart count 0
Jun 28 19:22:09.076: INFO: alertmanager-main-0 from openshift-monitoring started at 2021-06-28 16:45:25 +0000 UTC (5 container statuses recorded)
Jun 28 19:22:09.076: INFO: 	Container alertmanager ready: true, restart count 0
Jun 28 19:22:09.076: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Jun 28 19:22:09.076: INFO: 	Container config-reloader ready: true, restart count 0
Jun 28 19:22:09.076: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 28 19:22:09.076: INFO: 	Container prom-label-proxy ready: true, restart count 0
Jun 28 19:22:09.076: INFO: alertmanager-main-2 from openshift-monitoring started at 2021-06-28 18:29:17 +0000 UTC (5 container statuses recorded)
Jun 28 19:22:09.076: INFO: 	Container alertmanager ready: true, restart count 0
Jun 28 19:22:09.076: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Jun 28 19:22:09.076: INFO: 	Container config-reloader ready: true, restart count 0
Jun 28 19:22:09.076: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 28 19:22:09.076: INFO: 	Container prom-label-proxy ready: true, restart count 0
Jun 28 19:22:09.076: INFO: grafana-85d4b8dbd-jn67b from openshift-monitoring started at 2021-06-28 16:44:32 +0000 UTC (2 container statuses recorded)
Jun 28 19:22:09.076: INFO: 	Container grafana ready: true, restart count 0
Jun 28 19:22:09.076: INFO: 	Container grafana-proxy ready: true, restart count 0
Jun 28 19:22:09.076: INFO: kube-state-metrics-659c7b865d-974cs from openshift-monitoring started at 2021-06-28 16:44:29 +0000 UTC (3 container statuses recorded)
Jun 28 19:22:09.076: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Jun 28 19:22:09.076: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Jun 28 19:22:09.076: INFO: 	Container kube-state-metrics ready: true, restart count 0
Jun 28 19:22:09.076: INFO: node-exporter-6s54d from openshift-monitoring started at 2021-06-28 16:44:29 +0000 UTC (2 container statuses recorded)
Jun 28 19:22:09.076: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 28 19:22:09.076: INFO: 	Container node-exporter ready: true, restart count 0
Jun 28 19:22:09.076: INFO: openshift-state-metrics-7cf4dc694b-d9w6x from openshift-monitoring started at 2021-06-28 16:44:29 +0000 UTC (3 container statuses recorded)
Jun 28 19:22:09.076: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Jun 28 19:22:09.076: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Jun 28 19:22:09.076: INFO: 	Container openshift-state-metrics ready: true, restart count 0
Jun 28 19:22:09.076: INFO: prometheus-adapter-8646bbc64d-nq9hj from openshift-monitoring started at 2021-06-28 16:44:29 +0000 UTC (1 container statuses recorded)
Jun 28 19:22:09.076: INFO: 	Container prometheus-adapter ready: true, restart count 0
Jun 28 19:22:09.076: INFO: prometheus-k8s-1 from openshift-monitoring started at 2021-06-28 16:45:26 +0000 UTC (6 container statuses recorded)
Jun 28 19:22:09.076: INFO: 	Container config-reloader ready: true, restart count 0
Jun 28 19:22:09.076: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 28 19:22:09.076: INFO: 	Container prom-label-proxy ready: true, restart count 0
Jun 28 19:22:09.076: INFO: 	Container prometheus ready: true, restart count 1
Jun 28 19:22:09.076: INFO: 	Container prometheus-proxy ready: true, restart count 0
Jun 28 19:22:09.076: INFO: 	Container thanos-sidecar ready: true, restart count 0
Jun 28 19:22:09.076: INFO: prometheus-operator-5d48db6d9c-z9sxl from openshift-monitoring started at 2021-06-28 16:44:29 +0000 UTC (2 container statuses recorded)
Jun 28 19:22:09.076: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 28 19:22:09.076: INFO: 	Container prometheus-operator ready: true, restart count 0
Jun 28 19:22:09.076: INFO: telemeter-client-5584d54f85-bhqql from openshift-monitoring started at 2021-06-28 16:44:29 +0000 UTC (3 container statuses recorded)
Jun 28 19:22:09.076: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 28 19:22:09.076: INFO: 	Container reload ready: true, restart count 0
Jun 28 19:22:09.076: INFO: 	Container telemeter-client ready: true, restart count 0
Jun 28 19:22:09.076: INFO: thanos-querier-b46bb8497-wxrxb from openshift-monitoring started at 2021-06-28 16:44:31 +0000 UTC (5 container statuses recorded)
Jun 28 19:22:09.076: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 28 19:22:09.076: INFO: 	Container kube-rbac-proxy-rules ready: true, restart count 0
Jun 28 19:22:09.076: INFO: 	Container oauth-proxy ready: true, restart count 0
Jun 28 19:22:09.076: INFO: 	Container prom-label-proxy ready: true, restart count 0
Jun 28 19:22:09.076: INFO: 	Container thanos-query ready: true, restart count 0
Jun 28 19:22:09.076: INFO: multus-admission-controller-5rwk7 from openshift-multus started at 2021-06-28 16:43:56 +0000 UTC (2 container statuses recorded)
Jun 28 19:22:09.076: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 28 19:22:09.076: INFO: 	Container multus-admission-controller ready: true, restart count 0
Jun 28 19:22:09.076: INFO: multus-tsj42 from openshift-multus started at 2021-06-28 16:42:37 +0000 UTC (1 container statuses recorded)
Jun 28 19:22:09.076: INFO: 	Container kube-multus ready: true, restart count 0
Jun 28 19:22:09.076: INFO: network-metrics-daemon-gzxgq from openshift-multus started at 2021-06-28 16:42:37 +0000 UTC (2 container statuses recorded)
Jun 28 19:22:09.076: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 28 19:22:09.076: INFO: 	Container network-metrics-daemon ready: true, restart count 0
Jun 28 19:22:09.076: INFO: packageserver-65544748bd-h6vfz from openshift-operator-lifecycle-manager started at 2021-06-28 16:44:29 +0000 UTC (1 container statuses recorded)
Jun 28 19:22:09.076: INFO: 	Container packageserver ready: true, restart count 0
Jun 28 19:22:09.076: INFO: sonobuoy-e2e-job-3182b6be48fa4a64 from sonobuoy started at 2021-06-28 18:11:47 +0000 UTC (2 container statuses recorded)
Jun 28 19:22:09.076: INFO: 	Container e2e ready: true, restart count 0
Jun 28 19:22:09.076: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 28 19:22:09.076: INFO: sonobuoy-systemd-logs-daemon-set-bc3c64b0befe49cb-47fm7 from sonobuoy started at 2021-06-28 18:11:47 +0000 UTC (2 container statuses recorded)
Jun 28 19:22:09.076: INFO: 	Container sonobuoy-worker ready: false, restart count 6
Jun 28 19:22:09.076: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-ac494216-e910-4f7c-8579-82472da227aa 90
STEP: Trying to create a pod(pod1) with hostport 54321 and hostIP 127.0.0.1 and expect scheduled
STEP: Trying to create another pod(pod2) with hostport 54321 but hostIP 127.0.0.2 on the node which pod1 resides and expect scheduled
STEP: Trying to create a third pod(pod3) with hostport 54321, hostIP 127.0.0.2 but use UDP protocol on the node which pod2 resides
STEP: removing the label kubernetes.io/e2e-ac494216-e910-4f7c-8579-82472da227aa off the node 10.13.107.37
STEP: verifying the node doesn't have the label kubernetes.io/e2e-ac494216-e910-4f7c-8579-82472da227aa
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 19:22:17.475: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-8679" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81

• [SLOW TEST:8.776 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]","total":305,"completed":222,"skipped":3464,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 19:22:17.511: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name configmap-test-volume-map-a954de99-1714-4a80-b78a-f8aead3909e9
STEP: Creating a pod to test consume configMaps
Jun 28 19:22:17.691: INFO: Waiting up to 5m0s for pod "pod-configmaps-0ab8ebe4-9e01-43ec-b8bf-0ef18f66aee8" in namespace "configmap-6071" to be "Succeeded or Failed"
Jun 28 19:22:17.704: INFO: Pod "pod-configmaps-0ab8ebe4-9e01-43ec-b8bf-0ef18f66aee8": Phase="Pending", Reason="", readiness=false. Elapsed: 13.170974ms
Jun 28 19:22:19.717: INFO: Pod "pod-configmaps-0ab8ebe4-9e01-43ec-b8bf-0ef18f66aee8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026689395s
Jun 28 19:22:21.734: INFO: Pod "pod-configmaps-0ab8ebe4-9e01-43ec-b8bf-0ef18f66aee8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.043305234s
STEP: Saw pod success
Jun 28 19:22:21.734: INFO: Pod "pod-configmaps-0ab8ebe4-9e01-43ec-b8bf-0ef18f66aee8" satisfied condition "Succeeded or Failed"
Jun 28 19:22:21.743: INFO: Trying to get logs from node 10.13.107.60 pod pod-configmaps-0ab8ebe4-9e01-43ec-b8bf-0ef18f66aee8 container configmap-volume-test: <nil>
STEP: delete the pod
Jun 28 19:22:21.870: INFO: Waiting for pod pod-configmaps-0ab8ebe4-9e01-43ec-b8bf-0ef18f66aee8 to disappear
Jun 28 19:22:21.879: INFO: Pod pod-configmaps-0ab8ebe4-9e01-43ec-b8bf-0ef18f66aee8 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 19:22:21.879: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6071" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":305,"completed":223,"skipped":3474,"failed":0}
SSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 19:22:21.909: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:90
Jun 28 19:22:22.043: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jun 28 19:22:22.079: INFO: Waiting for terminating namespaces to be deleted...
Jun 28 19:22:22.104: INFO: 
Logging pods the apiserver thinks is on node 10.13.107.37 before test
Jun 28 19:22:22.139: INFO: calico-node-7ntd9 from calico-system started at 2021-06-28 16:43:04 +0000 UTC (1 container statuses recorded)
Jun 28 19:22:22.141: INFO: 	Container calico-node ready: true, restart count 0
Jun 28 19:22:22.141: INFO: calico-typha-649969bb55-mj7tk from calico-system started at 2021-06-28 16:45:03 +0000 UTC (1 container statuses recorded)
Jun 28 19:22:22.141: INFO: 	Container calico-typha ready: true, restart count 0
Jun 28 19:22:22.141: INFO: adopt-release-7pqf8 from job-5095 started at 2021-06-28 19:21:58 +0000 UTC (1 container statuses recorded)
Jun 28 19:22:22.142: INFO: 	Container c ready: true, restart count 0
Jun 28 19:22:22.142: INFO: adopt-release-9dbnq from job-5095 started at 2021-06-28 19:21:58 +0000 UTC (1 container statuses recorded)
Jun 28 19:22:22.143: INFO: 	Container c ready: true, restart count 0
Jun 28 19:22:22.144: INFO: adopt-release-zdxf4 from job-5095 started at 2021-06-28 19:22:03 +0000 UTC (1 container statuses recorded)
Jun 28 19:22:22.144: INFO: 	Container c ready: true, restart count 0
Jun 28 19:22:22.144: INFO: ibm-keepalived-watcher-f9nwv from kube-system started at 2021-06-28 16:41:30 +0000 UTC (1 container statuses recorded)
Jun 28 19:22:22.144: INFO: 	Container keepalived-watcher ready: true, restart count 0
Jun 28 19:22:22.144: INFO: ibm-master-proxy-static-10.13.107.37 from kube-system started at 2021-06-28 16:40:48 +0000 UTC (2 container statuses recorded)
Jun 28 19:22:22.144: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Jun 28 19:22:22.144: INFO: 	Container pause ready: true, restart count 0
Jun 28 19:22:22.144: INFO: ibmcloud-block-storage-driver-kbnst from kube-system started at 2021-06-28 16:41:30 +0000 UTC (1 container statuses recorded)
Jun 28 19:22:22.144: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Jun 28 19:22:22.145: INFO: tuned-2tw2w from openshift-cluster-node-tuning-operator started at 2021-06-28 16:43:24 +0000 UTC (1 container statuses recorded)
Jun 28 19:22:22.145: INFO: 	Container tuned ready: true, restart count 0
Jun 28 19:22:22.145: INFO: dns-default-crxtq from openshift-dns started at 2021-06-28 16:44:04 +0000 UTC (3 container statuses recorded)
Jun 28 19:22:22.145: INFO: 	Container dns ready: true, restart count 0
Jun 28 19:22:22.145: INFO: 	Container dns-node-resolver ready: true, restart count 0
Jun 28 19:22:22.145: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 28 19:22:22.146: INFO: node-ca-z64gj from openshift-image-registry started at 2021-06-28 16:44:14 +0000 UTC (1 container statuses recorded)
Jun 28 19:22:22.146: INFO: 	Container node-ca ready: true, restart count 0
Jun 28 19:22:22.146: INFO: registry-pvc-permissions-lfjks from openshift-image-registry started at 2021-06-28 16:46:50 +0000 UTC (1 container statuses recorded)
Jun 28 19:22:22.146: INFO: 	Container pvc-permissions ready: false, restart count 0
Jun 28 19:22:22.146: INFO: openshift-kube-proxy-8l7d5 from openshift-kube-proxy started at 2021-06-28 16:42:08 +0000 UTC (1 container statuses recorded)
Jun 28 19:22:22.146: INFO: 	Container kube-proxy ready: true, restart count 0
Jun 28 19:22:22.146: INFO: node-exporter-qj7hs from openshift-monitoring started at 2021-06-28 16:44:29 +0000 UTC (2 container statuses recorded)
Jun 28 19:22:22.146: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 28 19:22:22.146: INFO: 	Container node-exporter ready: true, restart count 0
Jun 28 19:22:22.146: INFO: multus-admission-controller-nfv4l from openshift-multus started at 2021-06-28 19:12:27 +0000 UTC (2 container statuses recorded)
Jun 28 19:22:22.146: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 28 19:22:22.146: INFO: 	Container multus-admission-controller ready: true, restart count 0
Jun 28 19:22:22.146: INFO: multus-brfr7 from openshift-multus started at 2021-06-28 16:42:07 +0000 UTC (1 container statuses recorded)
Jun 28 19:22:22.146: INFO: 	Container kube-multus ready: true, restart count 0
Jun 28 19:22:22.147: INFO: network-metrics-daemon-5rjvj from openshift-multus started at 2021-06-28 16:42:08 +0000 UTC (2 container statuses recorded)
Jun 28 19:22:22.147: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 28 19:22:22.147: INFO: 	Container network-metrics-daemon ready: true, restart count 0
Jun 28 19:22:22.147: INFO: pod1 from sched-pred-8679 started at 2021-06-28 19:22:11 +0000 UTC (1 container statuses recorded)
Jun 28 19:22:22.147: INFO: 	Container pod1 ready: true, restart count 0
Jun 28 19:22:22.147: INFO: pod2 from sched-pred-8679 started at 2021-06-28 19:22:13 +0000 UTC (1 container statuses recorded)
Jun 28 19:22:22.147: INFO: 	Container pod2 ready: true, restart count 0
Jun 28 19:22:22.147: INFO: pod3 from sched-pred-8679 started at 2021-06-28 19:22:15 +0000 UTC (1 container statuses recorded)
Jun 28 19:22:22.147: INFO: 	Container pod3 ready: true, restart count 0
Jun 28 19:22:22.147: INFO: sonobuoy-systemd-logs-daemon-set-bc3c64b0befe49cb-gp9nl from sonobuoy started at 2021-06-28 18:11:47 +0000 UTC (2 container statuses recorded)
Jun 28 19:22:22.147: INFO: 	Container sonobuoy-worker ready: false, restart count 6
Jun 28 19:22:22.147: INFO: 	Container systemd-logs ready: true, restart count 0
Jun 28 19:22:22.147: INFO: 
Logging pods the apiserver thinks is on node 10.13.107.57 before test
Jun 28 19:22:22.194: INFO: calico-node-tr9lp from calico-system started at 2021-06-28 16:43:04 +0000 UTC (1 container statuses recorded)
Jun 28 19:22:22.194: INFO: 	Container calico-node ready: true, restart count 0
Jun 28 19:22:22.194: INFO: calico-typha-649969bb55-tpj68 from calico-system started at 2021-06-28 16:43:04 +0000 UTC (1 container statuses recorded)
Jun 28 19:22:22.194: INFO: 	Container calico-typha ready: true, restart count 0
Jun 28 19:22:22.194: INFO: ibm-cloud-provider-ip-149-81-71-226-77554565fb-clmlm from ibm-system started at 2021-06-28 16:58:09 +0000 UTC (1 container statuses recorded)
Jun 28 19:22:22.194: INFO: 	Container ibm-cloud-provider-ip-149-81-71-226 ready: true, restart count 0
Jun 28 19:22:22.194: INFO: ibm-file-plugin-7d974bf47c-bw8qh from kube-system started at 2021-06-28 16:43:14 +0000 UTC (1 container statuses recorded)
Jun 28 19:22:22.194: INFO: 	Container ibm-file-plugin-container ready: true, restart count 0
Jun 28 19:22:22.194: INFO: ibm-keepalived-watcher-rqj9s from kube-system started at 2021-06-28 16:41:35 +0000 UTC (1 container statuses recorded)
Jun 28 19:22:22.194: INFO: 	Container keepalived-watcher ready: true, restart count 0
Jun 28 19:22:22.194: INFO: ibm-master-proxy-static-10.13.107.57 from kube-system started at 2021-06-28 16:40:56 +0000 UTC (2 container statuses recorded)
Jun 28 19:22:22.194: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Jun 28 19:22:22.194: INFO: 	Container pause ready: true, restart count 0
Jun 28 19:22:22.194: INFO: ibm-storage-watcher-869677c8b-q8rh5 from kube-system started at 2021-06-28 16:43:14 +0000 UTC (1 container statuses recorded)
Jun 28 19:22:22.194: INFO: 	Container ibm-storage-watcher-container ready: true, restart count 0
Jun 28 19:22:22.194: INFO: ibmcloud-block-storage-driver-p9gg7 from kube-system started at 2021-06-28 16:41:35 +0000 UTC (1 container statuses recorded)
Jun 28 19:22:22.194: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Jun 28 19:22:22.194: INFO: ibmcloud-block-storage-plugin-665488684b-h5fhz from kube-system started at 2021-06-28 16:43:14 +0000 UTC (1 container statuses recorded)
Jun 28 19:22:22.194: INFO: 	Container ibmcloud-block-storage-plugin-container ready: true, restart count 0
Jun 28 19:22:22.194: INFO: vpn-6c58c4756c-rw748 from kube-system started at 2021-06-28 16:50:23 +0000 UTC (1 container statuses recorded)
Jun 28 19:22:22.194: INFO: 	Container vpn ready: true, restart count 0
Jun 28 19:22:22.194: INFO: cluster-node-tuning-operator-77dfdd89b8-chjmh from openshift-cluster-node-tuning-operator started at 2021-06-28 16:43:14 +0000 UTC (1 container statuses recorded)
Jun 28 19:22:22.194: INFO: 	Container cluster-node-tuning-operator ready: true, restart count 0
Jun 28 19:22:22.194: INFO: tuned-r5kts from openshift-cluster-node-tuning-operator started at 2021-06-28 16:43:24 +0000 UTC (1 container statuses recorded)
Jun 28 19:22:22.194: INFO: 	Container tuned ready: true, restart count 0
Jun 28 19:22:22.194: INFO: cluster-samples-operator-56dd9bbcb-8mjfl from openshift-cluster-samples-operator started at 2021-06-28 16:43:14 +0000 UTC (2 container statuses recorded)
Jun 28 19:22:22.194: INFO: 	Container cluster-samples-operator ready: true, restart count 0
Jun 28 19:22:22.194: INFO: 	Container cluster-samples-operator-watch ready: true, restart count 0
Jun 28 19:22:22.194: INFO: cluster-storage-operator-56664f46b8-kjjbs from openshift-cluster-storage-operator started at 2021-06-28 16:43:14 +0000 UTC (1 container statuses recorded)
Jun 28 19:22:22.194: INFO: 	Container cluster-storage-operator ready: true, restart count 0
Jun 28 19:22:22.194: INFO: console-operator-cd789fcfb-4j2b8 from openshift-console-operator started at 2021-06-28 16:43:14 +0000 UTC (1 container statuses recorded)
Jun 28 19:22:22.194: INFO: 	Container console-operator ready: true, restart count 1
Jun 28 19:22:22.194: INFO: console-74dd6cb4c6-vnhx8 from openshift-console started at 2021-06-28 18:29:06 +0000 UTC (1 container statuses recorded)
Jun 28 19:22:22.194: INFO: 	Container console ready: true, restart count 0
Jun 28 19:22:22.194: INFO: downloads-65dcc64f75-2bzn8 from openshift-console started at 2021-06-28 16:43:14 +0000 UTC (1 container statuses recorded)
Jun 28 19:22:22.194: INFO: 	Container download-server ready: true, restart count 0
Jun 28 19:22:22.194: INFO: downloads-65dcc64f75-5hm2s from openshift-console started at 2021-06-28 16:43:14 +0000 UTC (1 container statuses recorded)
Jun 28 19:22:22.194: INFO: 	Container download-server ready: true, restart count 0
Jun 28 19:22:22.194: INFO: dns-operator-7d8cb9bb6d-kkfvf from openshift-dns-operator started at 2021-06-28 16:43:14 +0000 UTC (2 container statuses recorded)
Jun 28 19:22:22.194: INFO: 	Container dns-operator ready: true, restart count 0
Jun 28 19:22:22.194: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 28 19:22:22.194: INFO: dns-default-48b6x from openshift-dns started at 2021-06-28 16:44:04 +0000 UTC (3 container statuses recorded)
Jun 28 19:22:22.194: INFO: 	Container dns ready: true, restart count 0
Jun 28 19:22:22.194: INFO: 	Container dns-node-resolver ready: true, restart count 0
Jun 28 19:22:22.194: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 28 19:22:22.194: INFO: cluster-image-registry-operator-999d7b49c-d458n from openshift-image-registry started at 2021-06-28 16:43:14 +0000 UTC (1 container statuses recorded)
Jun 28 19:22:22.194: INFO: 	Container cluster-image-registry-operator ready: true, restart count 0
Jun 28 19:22:22.194: INFO: node-ca-49d6t from openshift-image-registry started at 2021-06-28 16:44:14 +0000 UTC (1 container statuses recorded)
Jun 28 19:22:22.194: INFO: 	Container node-ca ready: true, restart count 0
Jun 28 19:22:22.194: INFO: ingress-operator-795fb4477f-sgnmp from openshift-ingress-operator started at 2021-06-28 16:43:14 +0000 UTC (2 container statuses recorded)
Jun 28 19:22:22.194: INFO: 	Container ingress-operator ready: true, restart count 0
Jun 28 19:22:22.194: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 28 19:22:22.194: INFO: router-default-8b99f5968-b84k7 from openshift-ingress started at 2021-06-28 18:29:06 +0000 UTC (1 container statuses recorded)
Jun 28 19:22:22.194: INFO: 	Container router ready: true, restart count 0
Jun 28 19:22:22.194: INFO: openshift-kube-proxy-xq2t8 from openshift-kube-proxy started at 2021-06-28 16:42:08 +0000 UTC (1 container statuses recorded)
Jun 28 19:22:22.194: INFO: 	Container kube-proxy ready: true, restart count 0
Jun 28 19:22:22.194: INFO: kube-storage-version-migrator-operator-77d5dd5f6c-z9m4j from openshift-kube-storage-version-migrator-operator started at 2021-06-28 16:43:14 +0000 UTC (1 container statuses recorded)
Jun 28 19:22:22.194: INFO: 	Container kube-storage-version-migrator-operator ready: true, restart count 1
Jun 28 19:22:22.194: INFO: marketplace-operator-7b9d5dcb99-nzgf8 from openshift-marketplace started at 2021-06-28 16:43:14 +0000 UTC (1 container statuses recorded)
Jun 28 19:22:22.194: INFO: 	Container marketplace-operator ready: true, restart count 0
Jun 28 19:22:22.194: INFO: alertmanager-main-1 from openshift-monitoring started at 2021-06-28 16:45:25 +0000 UTC (5 container statuses recorded)
Jun 28 19:22:22.194: INFO: 	Container alertmanager ready: true, restart count 0
Jun 28 19:22:22.194: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Jun 28 19:22:22.194: INFO: 	Container config-reloader ready: true, restart count 0
Jun 28 19:22:22.194: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 28 19:22:22.194: INFO: 	Container prom-label-proxy ready: true, restart count 0
Jun 28 19:22:22.194: INFO: cluster-monitoring-operator-6964464f67-jbfjk from openshift-monitoring started at 2021-06-28 16:43:14 +0000 UTC (2 container statuses recorded)
Jun 28 19:22:22.194: INFO: 	Container cluster-monitoring-operator ready: true, restart count 0
Jun 28 19:22:22.194: INFO: 	Container kube-rbac-proxy ready: true, restart count 2
Jun 28 19:22:22.195: INFO: node-exporter-ld8qc from openshift-monitoring started at 2021-06-28 16:44:29 +0000 UTC (2 container statuses recorded)
Jun 28 19:22:22.195: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 28 19:22:22.195: INFO: 	Container node-exporter ready: true, restart count 0
Jun 28 19:22:22.195: INFO: prometheus-adapter-8646bbc64d-stfnh from openshift-monitoring started at 2021-06-28 18:29:07 +0000 UTC (1 container statuses recorded)
Jun 28 19:22:22.195: INFO: 	Container prometheus-adapter ready: true, restart count 0
Jun 28 19:22:22.195: INFO: prometheus-k8s-0 from openshift-monitoring started at 2021-06-28 18:29:17 +0000 UTC (6 container statuses recorded)
Jun 28 19:22:22.195: INFO: 	Container config-reloader ready: true, restart count 0
Jun 28 19:22:22.195: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 28 19:22:22.195: INFO: 	Container prom-label-proxy ready: true, restart count 0
Jun 28 19:22:22.195: INFO: 	Container prometheus ready: true, restart count 1
Jun 28 19:22:22.195: INFO: 	Container prometheus-proxy ready: true, restart count 0
Jun 28 19:22:22.195: INFO: 	Container thanos-sidecar ready: true, restart count 0
Jun 28 19:22:22.195: INFO: thanos-querier-b46bb8497-dcbxw from openshift-monitoring started at 2021-06-28 18:29:07 +0000 UTC (5 container statuses recorded)
Jun 28 19:22:22.195: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 28 19:22:22.195: INFO: 	Container kube-rbac-proxy-rules ready: true, restart count 0
Jun 28 19:22:22.195: INFO: 	Container oauth-proxy ready: true, restart count 0
Jun 28 19:22:22.195: INFO: 	Container prom-label-proxy ready: true, restart count 0
Jun 28 19:22:22.195: INFO: 	Container thanos-query ready: true, restart count 0
Jun 28 19:22:22.195: INFO: multus-admission-controller-58k5w from openshift-multus started at 2021-06-28 16:43:13 +0000 UTC (2 container statuses recorded)
Jun 28 19:22:22.195: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 28 19:22:22.195: INFO: 	Container multus-admission-controller ready: true, restart count 0
Jun 28 19:22:22.195: INFO: multus-hvm6b from openshift-multus started at 2021-06-28 16:42:08 +0000 UTC (1 container statuses recorded)
Jun 28 19:22:22.195: INFO: 	Container kube-multus ready: true, restart count 0
Jun 28 19:22:22.195: INFO: network-metrics-daemon-78dfb from openshift-multus started at 2021-06-28 16:42:08 +0000 UTC (2 container statuses recorded)
Jun 28 19:22:22.195: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 28 19:22:22.195: INFO: 	Container network-metrics-daemon ready: true, restart count 0
Jun 28 19:22:22.195: INFO: network-operator-5b78d575b8-gvzvg from openshift-network-operator started at 2021-06-28 16:41:35 +0000 UTC (1 container statuses recorded)
Jun 28 19:22:22.195: INFO: 	Container network-operator ready: true, restart count 0
Jun 28 19:22:22.195: INFO: catalog-operator-67c774db7d-5ps99 from openshift-operator-lifecycle-manager started at 2021-06-28 16:43:14 +0000 UTC (1 container statuses recorded)
Jun 28 19:22:22.195: INFO: 	Container catalog-operator ready: true, restart count 0
Jun 28 19:22:22.195: INFO: olm-operator-68fcf6954f-nfsgq from openshift-operator-lifecycle-manager started at 2021-06-28 16:43:14 +0000 UTC (1 container statuses recorded)
Jun 28 19:22:22.195: INFO: 	Container olm-operator ready: true, restart count 0
Jun 28 19:22:22.195: INFO: packageserver-65544748bd-hhnld from openshift-operator-lifecycle-manager started at 2021-06-28 18:29:07 +0000 UTC (1 container statuses recorded)
Jun 28 19:22:22.195: INFO: 	Container packageserver ready: true, restart count 0
Jun 28 19:22:22.195: INFO: metrics-7df79584fc-f6g2g from openshift-roks-metrics started at 2021-06-28 16:43:14 +0000 UTC (1 container statuses recorded)
Jun 28 19:22:22.195: INFO: 	Container metrics ready: true, restart count 1
Jun 28 19:22:22.195: INFO: push-gateway-6dc86bc94f-dz7nt from openshift-roks-metrics started at 2021-06-28 16:43:14 +0000 UTC (1 container statuses recorded)
Jun 28 19:22:22.195: INFO: 	Container push-gateway ready: true, restart count 0
Jun 28 19:22:22.195: INFO: service-ca-operator-d4bfd498b-4xhnv from openshift-service-ca-operator started at 2021-06-28 16:43:14 +0000 UTC (1 container statuses recorded)
Jun 28 19:22:22.195: INFO: 	Container service-ca-operator ready: true, restart count 1
Jun 28 19:22:22.195: INFO: service-ca-665fb97685-mn77q from openshift-service-ca started at 2021-06-28 18:29:06 +0000 UTC (1 container statuses recorded)
Jun 28 19:22:22.195: INFO: 	Container service-ca-controller ready: true, restart count 0
Jun 28 19:22:22.195: INFO: sonobuoy from sonobuoy started at 2021-06-28 18:11:40 +0000 UTC (1 container statuses recorded)
Jun 28 19:22:22.195: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jun 28 19:22:22.195: INFO: sonobuoy-systemd-logs-daemon-set-bc3c64b0befe49cb-s48t5 from sonobuoy started at 2021-06-28 18:11:47 +0000 UTC (2 container statuses recorded)
Jun 28 19:22:22.195: INFO: 	Container sonobuoy-worker ready: false, restart count 6
Jun 28 19:22:22.195: INFO: 	Container systemd-logs ready: true, restart count 0
Jun 28 19:22:22.195: INFO: tigera-operator-64c8f4c7d7-hzwkg from tigera-operator started at 2021-06-28 18:29:06 +0000 UTC (1 container statuses recorded)
Jun 28 19:22:22.195: INFO: 	Container tigera-operator ready: true, restart count 0
Jun 28 19:22:22.195: INFO: 
Logging pods the apiserver thinks is on node 10.13.107.60 before test
Jun 28 19:22:22.245: INFO: calico-kube-controllers-57df785794-xp76v from calico-system started at 2021-06-28 18:29:06 +0000 UTC (1 container statuses recorded)
Jun 28 19:22:22.245: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Jun 28 19:22:22.245: INFO: calico-node-vr9g2 from calico-system started at 2021-06-28 16:43:04 +0000 UTC (1 container statuses recorded)
Jun 28 19:22:22.245: INFO: 	Container calico-node ready: true, restart count 0
Jun 28 19:22:22.245: INFO: calico-typha-649969bb55-mc6s8 from calico-system started at 2021-06-28 16:45:02 +0000 UTC (1 container statuses recorded)
Jun 28 19:22:22.245: INFO: 	Container calico-typha ready: true, restart count 0
Jun 28 19:22:22.245: INFO: test-k8s-e2e-pvg-master-verification from default started at 2021-06-28 16:51:21 +0000 UTC (1 container statuses recorded)
Jun 28 19:22:22.245: INFO: 	Container test-k8s-e2e-pvg-master-verification ready: true, restart count 0
Jun 28 19:22:22.245: INFO: ibm-cloud-provider-ip-149-81-71-226-77554565fb-djwh7 from ibm-system started at 2021-06-28 18:29:06 +0000 UTC (1 container statuses recorded)
Jun 28 19:22:22.245: INFO: 	Container ibm-cloud-provider-ip-149-81-71-226 ready: true, restart count 0
Jun 28 19:22:22.245: INFO: ibm-keepalived-watcher-w6fhq from kube-system started at 2021-06-28 16:42:37 +0000 UTC (1 container statuses recorded)
Jun 28 19:22:22.245: INFO: 	Container keepalived-watcher ready: true, restart count 0
Jun 28 19:22:22.246: INFO: ibm-master-proxy-static-10.13.107.60 from kube-system started at 2021-06-28 16:41:49 +0000 UTC (2 container statuses recorded)
Jun 28 19:22:22.246: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Jun 28 19:22:22.246: INFO: 	Container pause ready: true, restart count 0
Jun 28 19:22:22.246: INFO: ibmcloud-block-storage-driver-kwxt2 from kube-system started at 2021-06-28 16:42:37 +0000 UTC (1 container statuses recorded)
Jun 28 19:22:22.246: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Jun 28 19:22:22.246: INFO: tuned-djhn5 from openshift-cluster-node-tuning-operator started at 2021-06-28 16:43:24 +0000 UTC (1 container statuses recorded)
Jun 28 19:22:22.246: INFO: 	Container tuned ready: true, restart count 0
Jun 28 19:22:22.246: INFO: console-74dd6cb4c6-gbthm from openshift-console started at 2021-06-28 17:20:10 +0000 UTC (1 container statuses recorded)
Jun 28 19:22:22.246: INFO: 	Container console ready: true, restart count 0
Jun 28 19:22:22.246: INFO: dns-default-84l5j from openshift-dns started at 2021-06-28 16:44:04 +0000 UTC (3 container statuses recorded)
Jun 28 19:22:22.246: INFO: 	Container dns ready: true, restart count 0
Jun 28 19:22:22.246: INFO: 	Container dns-node-resolver ready: true, restart count 0
Jun 28 19:22:22.246: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 28 19:22:22.246: INFO: image-registry-7f9574d5c6-tmmns from openshift-image-registry started at 2021-06-28 16:46:50 +0000 UTC (1 container statuses recorded)
Jun 28 19:22:22.246: INFO: 	Container registry ready: true, restart count 0
Jun 28 19:22:22.246: INFO: node-ca-42txp from openshift-image-registry started at 2021-06-28 16:44:14 +0000 UTC (1 container statuses recorded)
Jun 28 19:22:22.246: INFO: 	Container node-ca ready: true, restart count 0
Jun 28 19:22:22.246: INFO: router-default-8b99f5968-8nzw5 from openshift-ingress started at 2021-06-28 17:18:25 +0000 UTC (1 container statuses recorded)
Jun 28 19:22:22.246: INFO: 	Container router ready: true, restart count 0
Jun 28 19:22:22.246: INFO: openshift-kube-proxy-n6pzf from openshift-kube-proxy started at 2021-06-28 16:42:37 +0000 UTC (1 container statuses recorded)
Jun 28 19:22:22.246: INFO: 	Container kube-proxy ready: true, restart count 0
Jun 28 19:22:22.246: INFO: migrator-858cfc6f4c-tvj6h from openshift-kube-storage-version-migrator started at 2021-06-28 18:29:06 +0000 UTC (1 container statuses recorded)
Jun 28 19:22:22.246: INFO: 	Container migrator ready: true, restart count 0
Jun 28 19:22:22.246: INFO: certified-operators-d5d5w from openshift-marketplace started at 2021-06-28 16:44:19 +0000 UTC (1 container statuses recorded)
Jun 28 19:22:22.246: INFO: 	Container registry-server ready: true, restart count 0
Jun 28 19:22:22.246: INFO: community-operators-dbmgh from openshift-marketplace started at 2021-06-28 17:01:52 +0000 UTC (1 container statuses recorded)
Jun 28 19:22:22.246: INFO: 	Container registry-server ready: true, restart count 0
Jun 28 19:22:22.246: INFO: redhat-marketplace-4t4vz from openshift-marketplace started at 2021-06-28 16:44:21 +0000 UTC (1 container statuses recorded)
Jun 28 19:22:22.246: INFO: 	Container registry-server ready: true, restart count 0
Jun 28 19:22:22.246: INFO: redhat-operators-dvpnz from openshift-marketplace started at 2021-06-28 16:44:19 +0000 UTC (1 container statuses recorded)
Jun 28 19:22:22.246: INFO: 	Container registry-server ready: true, restart count 0
Jun 28 19:22:22.246: INFO: alertmanager-main-0 from openshift-monitoring started at 2021-06-28 16:45:25 +0000 UTC (5 container statuses recorded)
Jun 28 19:22:22.246: INFO: 	Container alertmanager ready: true, restart count 0
Jun 28 19:22:22.246: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Jun 28 19:22:22.246: INFO: 	Container config-reloader ready: true, restart count 0
Jun 28 19:22:22.246: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 28 19:22:22.246: INFO: 	Container prom-label-proxy ready: true, restart count 0
Jun 28 19:22:22.246: INFO: alertmanager-main-2 from openshift-monitoring started at 2021-06-28 18:29:17 +0000 UTC (5 container statuses recorded)
Jun 28 19:22:22.246: INFO: 	Container alertmanager ready: true, restart count 0
Jun 28 19:22:22.246: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Jun 28 19:22:22.246: INFO: 	Container config-reloader ready: true, restart count 0
Jun 28 19:22:22.246: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 28 19:22:22.246: INFO: 	Container prom-label-proxy ready: true, restart count 0
Jun 28 19:22:22.246: INFO: grafana-85d4b8dbd-jn67b from openshift-monitoring started at 2021-06-28 16:44:32 +0000 UTC (2 container statuses recorded)
Jun 28 19:22:22.246: INFO: 	Container grafana ready: true, restart count 0
Jun 28 19:22:22.246: INFO: 	Container grafana-proxy ready: true, restart count 0
Jun 28 19:22:22.246: INFO: kube-state-metrics-659c7b865d-974cs from openshift-monitoring started at 2021-06-28 16:44:29 +0000 UTC (3 container statuses recorded)
Jun 28 19:22:22.246: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Jun 28 19:22:22.246: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Jun 28 19:22:22.246: INFO: 	Container kube-state-metrics ready: true, restart count 0
Jun 28 19:22:22.246: INFO: node-exporter-6s54d from openshift-monitoring started at 2021-06-28 16:44:29 +0000 UTC (2 container statuses recorded)
Jun 28 19:22:22.246: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 28 19:22:22.246: INFO: 	Container node-exporter ready: true, restart count 0
Jun 28 19:22:22.246: INFO: openshift-state-metrics-7cf4dc694b-d9w6x from openshift-monitoring started at 2021-06-28 16:44:29 +0000 UTC (3 container statuses recorded)
Jun 28 19:22:22.246: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Jun 28 19:22:22.246: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Jun 28 19:22:22.246: INFO: 	Container openshift-state-metrics ready: true, restart count 0
Jun 28 19:22:22.246: INFO: prometheus-adapter-8646bbc64d-nq9hj from openshift-monitoring started at 2021-06-28 16:44:29 +0000 UTC (1 container statuses recorded)
Jun 28 19:22:22.246: INFO: 	Container prometheus-adapter ready: true, restart count 0
Jun 28 19:22:22.246: INFO: prometheus-k8s-1 from openshift-monitoring started at 2021-06-28 16:45:26 +0000 UTC (6 container statuses recorded)
Jun 28 19:22:22.246: INFO: 	Container config-reloader ready: true, restart count 0
Jun 28 19:22:22.246: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 28 19:22:22.246: INFO: 	Container prom-label-proxy ready: true, restart count 0
Jun 28 19:22:22.246: INFO: 	Container prometheus ready: true, restart count 1
Jun 28 19:22:22.246: INFO: 	Container prometheus-proxy ready: true, restart count 0
Jun 28 19:22:22.246: INFO: 	Container thanos-sidecar ready: true, restart count 0
Jun 28 19:22:22.246: INFO: prometheus-operator-5d48db6d9c-z9sxl from openshift-monitoring started at 2021-06-28 16:44:29 +0000 UTC (2 container statuses recorded)
Jun 28 19:22:22.246: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 28 19:22:22.246: INFO: 	Container prometheus-operator ready: true, restart count 0
Jun 28 19:22:22.246: INFO: telemeter-client-5584d54f85-bhqql from openshift-monitoring started at 2021-06-28 16:44:29 +0000 UTC (3 container statuses recorded)
Jun 28 19:22:22.247: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 28 19:22:22.247: INFO: 	Container reload ready: true, restart count 0
Jun 28 19:22:22.247: INFO: 	Container telemeter-client ready: true, restart count 0
Jun 28 19:22:22.247: INFO: thanos-querier-b46bb8497-wxrxb from openshift-monitoring started at 2021-06-28 16:44:31 +0000 UTC (5 container statuses recorded)
Jun 28 19:22:22.247: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 28 19:22:22.247: INFO: 	Container kube-rbac-proxy-rules ready: true, restart count 0
Jun 28 19:22:22.247: INFO: 	Container oauth-proxy ready: true, restart count 0
Jun 28 19:22:22.247: INFO: 	Container prom-label-proxy ready: true, restart count 0
Jun 28 19:22:22.247: INFO: 	Container thanos-query ready: true, restart count 0
Jun 28 19:22:22.247: INFO: multus-admission-controller-5rwk7 from openshift-multus started at 2021-06-28 16:43:56 +0000 UTC (2 container statuses recorded)
Jun 28 19:22:22.247: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 28 19:22:22.247: INFO: 	Container multus-admission-controller ready: true, restart count 0
Jun 28 19:22:22.247: INFO: multus-tsj42 from openshift-multus started at 2021-06-28 16:42:37 +0000 UTC (1 container statuses recorded)
Jun 28 19:22:22.247: INFO: 	Container kube-multus ready: true, restart count 0
Jun 28 19:22:22.247: INFO: network-metrics-daemon-gzxgq from openshift-multus started at 2021-06-28 16:42:37 +0000 UTC (2 container statuses recorded)
Jun 28 19:22:22.247: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 28 19:22:22.247: INFO: 	Container network-metrics-daemon ready: true, restart count 0
Jun 28 19:22:22.247: INFO: packageserver-65544748bd-h6vfz from openshift-operator-lifecycle-manager started at 2021-06-28 16:44:29 +0000 UTC (1 container statuses recorded)
Jun 28 19:22:22.247: INFO: 	Container packageserver ready: true, restart count 0
Jun 28 19:22:22.247: INFO: sonobuoy-e2e-job-3182b6be48fa4a64 from sonobuoy started at 2021-06-28 18:11:47 +0000 UTC (2 container statuses recorded)
Jun 28 19:22:22.247: INFO: 	Container e2e ready: true, restart count 0
Jun 28 19:22:22.247: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 28 19:22:22.247: INFO: sonobuoy-systemd-logs-daemon-set-bc3c64b0befe49cb-47fm7 from sonobuoy started at 2021-06-28 18:11:47 +0000 UTC (2 container statuses recorded)
Jun 28 19:22:22.247: INFO: 	Container sonobuoy-worker ready: false, restart count 6
Jun 28 19:22:22.247: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-8f3706fd-6b90-46ee-98e0-22560dae865b 95
STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled
STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 127.0.0.1 on the node which pod4 resides and expect not scheduled
STEP: removing the label kubernetes.io/e2e-8f3706fd-6b90-46ee-98e0-22560dae865b off the node 10.13.107.57
STEP: verifying the node doesn't have the label kubernetes.io/e2e-8f3706fd-6b90-46ee-98e0-22560dae865b
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 19:27:26.601: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-9676" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81

• [SLOW TEST:304.723 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]","total":305,"completed":224,"skipped":3483,"failed":0}
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 19:27:26.632: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:90
Jun 28 19:27:26.767: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jun 28 19:27:26.802: INFO: Waiting for terminating namespaces to be deleted...
Jun 28 19:27:26.819: INFO: 
Logging pods the apiserver thinks is on node 10.13.107.37 before test
Jun 28 19:27:26.862: INFO: calico-node-7ntd9 from calico-system started at 2021-06-28 16:43:04 +0000 UTC (1 container statuses recorded)
Jun 28 19:27:26.862: INFO: 	Container calico-node ready: true, restart count 0
Jun 28 19:27:26.862: INFO: calico-typha-649969bb55-mj7tk from calico-system started at 2021-06-28 16:45:03 +0000 UTC (1 container statuses recorded)
Jun 28 19:27:26.862: INFO: 	Container calico-typha ready: true, restart count 0
Jun 28 19:27:26.862: INFO: ibm-keepalived-watcher-f9nwv from kube-system started at 2021-06-28 16:41:30 +0000 UTC (1 container statuses recorded)
Jun 28 19:27:26.862: INFO: 	Container keepalived-watcher ready: true, restart count 0
Jun 28 19:27:26.862: INFO: ibm-master-proxy-static-10.13.107.37 from kube-system started at 2021-06-28 16:40:48 +0000 UTC (2 container statuses recorded)
Jun 28 19:27:26.862: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Jun 28 19:27:26.862: INFO: 	Container pause ready: true, restart count 0
Jun 28 19:27:26.862: INFO: ibmcloud-block-storage-driver-kbnst from kube-system started at 2021-06-28 16:41:30 +0000 UTC (1 container statuses recorded)
Jun 28 19:27:26.862: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Jun 28 19:27:26.862: INFO: tuned-2tw2w from openshift-cluster-node-tuning-operator started at 2021-06-28 16:43:24 +0000 UTC (1 container statuses recorded)
Jun 28 19:27:26.862: INFO: 	Container tuned ready: true, restart count 0
Jun 28 19:27:26.862: INFO: dns-default-crxtq from openshift-dns started at 2021-06-28 16:44:04 +0000 UTC (3 container statuses recorded)
Jun 28 19:27:26.862: INFO: 	Container dns ready: true, restart count 0
Jun 28 19:27:26.862: INFO: 	Container dns-node-resolver ready: true, restart count 0
Jun 28 19:27:26.862: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 28 19:27:26.863: INFO: node-ca-z64gj from openshift-image-registry started at 2021-06-28 16:44:14 +0000 UTC (1 container statuses recorded)
Jun 28 19:27:26.863: INFO: 	Container node-ca ready: true, restart count 0
Jun 28 19:27:26.863: INFO: registry-pvc-permissions-lfjks from openshift-image-registry started at 2021-06-28 16:46:50 +0000 UTC (1 container statuses recorded)
Jun 28 19:27:26.863: INFO: 	Container pvc-permissions ready: false, restart count 0
Jun 28 19:27:26.863: INFO: openshift-kube-proxy-8l7d5 from openshift-kube-proxy started at 2021-06-28 16:42:08 +0000 UTC (1 container statuses recorded)
Jun 28 19:27:26.863: INFO: 	Container kube-proxy ready: true, restart count 0
Jun 28 19:27:26.863: INFO: node-exporter-qj7hs from openshift-monitoring started at 2021-06-28 16:44:29 +0000 UTC (2 container statuses recorded)
Jun 28 19:27:26.863: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 28 19:27:26.863: INFO: 	Container node-exporter ready: true, restart count 0
Jun 28 19:27:26.863: INFO: multus-admission-controller-nfv4l from openshift-multus started at 2021-06-28 19:12:27 +0000 UTC (2 container statuses recorded)
Jun 28 19:27:26.863: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 28 19:27:26.863: INFO: 	Container multus-admission-controller ready: true, restart count 0
Jun 28 19:27:26.863: INFO: multus-brfr7 from openshift-multus started at 2021-06-28 16:42:07 +0000 UTC (1 container statuses recorded)
Jun 28 19:27:26.863: INFO: 	Container kube-multus ready: true, restart count 0
Jun 28 19:27:26.863: INFO: network-metrics-daemon-5rjvj from openshift-multus started at 2021-06-28 16:42:08 +0000 UTC (2 container statuses recorded)
Jun 28 19:27:26.863: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 28 19:27:26.863: INFO: 	Container network-metrics-daemon ready: true, restart count 0
Jun 28 19:27:26.863: INFO: sonobuoy-systemd-logs-daemon-set-bc3c64b0befe49cb-gp9nl from sonobuoy started at 2021-06-28 18:11:47 +0000 UTC (2 container statuses recorded)
Jun 28 19:27:26.863: INFO: 	Container sonobuoy-worker ready: false, restart count 7
Jun 28 19:27:26.863: INFO: 	Container systemd-logs ready: true, restart count 0
Jun 28 19:27:26.863: INFO: 
Logging pods the apiserver thinks is on node 10.13.107.57 before test
Jun 28 19:27:26.919: INFO: calico-node-tr9lp from calico-system started at 2021-06-28 16:43:04 +0000 UTC (1 container statuses recorded)
Jun 28 19:27:26.919: INFO: 	Container calico-node ready: true, restart count 0
Jun 28 19:27:26.920: INFO: calico-typha-649969bb55-tpj68 from calico-system started at 2021-06-28 16:43:04 +0000 UTC (1 container statuses recorded)
Jun 28 19:27:26.920: INFO: 	Container calico-typha ready: true, restart count 0
Jun 28 19:27:26.920: INFO: ibm-cloud-provider-ip-149-81-71-226-77554565fb-clmlm from ibm-system started at 2021-06-28 16:58:09 +0000 UTC (1 container statuses recorded)
Jun 28 19:27:26.920: INFO: 	Container ibm-cloud-provider-ip-149-81-71-226 ready: true, restart count 0
Jun 28 19:27:26.920: INFO: ibm-file-plugin-7d974bf47c-bw8qh from kube-system started at 2021-06-28 16:43:14 +0000 UTC (1 container statuses recorded)
Jun 28 19:27:26.920: INFO: 	Container ibm-file-plugin-container ready: true, restart count 0
Jun 28 19:27:26.920: INFO: ibm-keepalived-watcher-rqj9s from kube-system started at 2021-06-28 16:41:35 +0000 UTC (1 container statuses recorded)
Jun 28 19:27:26.920: INFO: 	Container keepalived-watcher ready: true, restart count 0
Jun 28 19:27:26.920: INFO: ibm-master-proxy-static-10.13.107.57 from kube-system started at 2021-06-28 16:40:56 +0000 UTC (2 container statuses recorded)
Jun 28 19:27:26.920: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Jun 28 19:27:26.920: INFO: 	Container pause ready: true, restart count 0
Jun 28 19:27:26.920: INFO: ibm-storage-watcher-869677c8b-q8rh5 from kube-system started at 2021-06-28 16:43:14 +0000 UTC (1 container statuses recorded)
Jun 28 19:27:26.920: INFO: 	Container ibm-storage-watcher-container ready: true, restart count 0
Jun 28 19:27:26.920: INFO: ibmcloud-block-storage-driver-p9gg7 from kube-system started at 2021-06-28 16:41:35 +0000 UTC (1 container statuses recorded)
Jun 28 19:27:26.920: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Jun 28 19:27:26.920: INFO: ibmcloud-block-storage-plugin-665488684b-h5fhz from kube-system started at 2021-06-28 16:43:14 +0000 UTC (1 container statuses recorded)
Jun 28 19:27:26.920: INFO: 	Container ibmcloud-block-storage-plugin-container ready: true, restart count 0
Jun 28 19:27:26.920: INFO: vpn-6c58c4756c-rw748 from kube-system started at 2021-06-28 16:50:23 +0000 UTC (1 container statuses recorded)
Jun 28 19:27:26.920: INFO: 	Container vpn ready: true, restart count 0
Jun 28 19:27:26.920: INFO: cluster-node-tuning-operator-77dfdd89b8-chjmh from openshift-cluster-node-tuning-operator started at 2021-06-28 16:43:14 +0000 UTC (1 container statuses recorded)
Jun 28 19:27:26.920: INFO: 	Container cluster-node-tuning-operator ready: true, restart count 0
Jun 28 19:27:26.920: INFO: tuned-r5kts from openshift-cluster-node-tuning-operator started at 2021-06-28 16:43:24 +0000 UTC (1 container statuses recorded)
Jun 28 19:27:26.920: INFO: 	Container tuned ready: true, restart count 0
Jun 28 19:27:26.920: INFO: cluster-samples-operator-56dd9bbcb-8mjfl from openshift-cluster-samples-operator started at 2021-06-28 16:43:14 +0000 UTC (2 container statuses recorded)
Jun 28 19:27:26.920: INFO: 	Container cluster-samples-operator ready: true, restart count 0
Jun 28 19:27:26.920: INFO: 	Container cluster-samples-operator-watch ready: true, restart count 0
Jun 28 19:27:26.920: INFO: cluster-storage-operator-56664f46b8-kjjbs from openshift-cluster-storage-operator started at 2021-06-28 16:43:14 +0000 UTC (1 container statuses recorded)
Jun 28 19:27:26.920: INFO: 	Container cluster-storage-operator ready: true, restart count 0
Jun 28 19:27:26.920: INFO: console-operator-cd789fcfb-4j2b8 from openshift-console-operator started at 2021-06-28 16:43:14 +0000 UTC (1 container statuses recorded)
Jun 28 19:27:26.920: INFO: 	Container console-operator ready: true, restart count 1
Jun 28 19:27:26.920: INFO: console-74dd6cb4c6-vnhx8 from openshift-console started at 2021-06-28 18:29:06 +0000 UTC (1 container statuses recorded)
Jun 28 19:27:26.920: INFO: 	Container console ready: true, restart count 0
Jun 28 19:27:26.920: INFO: downloads-65dcc64f75-2bzn8 from openshift-console started at 2021-06-28 16:43:14 +0000 UTC (1 container statuses recorded)
Jun 28 19:27:26.920: INFO: 	Container download-server ready: true, restart count 0
Jun 28 19:27:26.920: INFO: downloads-65dcc64f75-5hm2s from openshift-console started at 2021-06-28 16:43:14 +0000 UTC (1 container statuses recorded)
Jun 28 19:27:26.920: INFO: 	Container download-server ready: true, restart count 0
Jun 28 19:27:26.920: INFO: dns-operator-7d8cb9bb6d-kkfvf from openshift-dns-operator started at 2021-06-28 16:43:14 +0000 UTC (2 container statuses recorded)
Jun 28 19:27:26.920: INFO: 	Container dns-operator ready: true, restart count 0
Jun 28 19:27:26.920: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 28 19:27:26.920: INFO: dns-default-48b6x from openshift-dns started at 2021-06-28 16:44:04 +0000 UTC (3 container statuses recorded)
Jun 28 19:27:26.920: INFO: 	Container dns ready: true, restart count 0
Jun 28 19:27:26.920: INFO: 	Container dns-node-resolver ready: true, restart count 0
Jun 28 19:27:26.920: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 28 19:27:26.920: INFO: cluster-image-registry-operator-999d7b49c-d458n from openshift-image-registry started at 2021-06-28 16:43:14 +0000 UTC (1 container statuses recorded)
Jun 28 19:27:26.921: INFO: 	Container cluster-image-registry-operator ready: true, restart count 0
Jun 28 19:27:26.921: INFO: node-ca-49d6t from openshift-image-registry started at 2021-06-28 16:44:14 +0000 UTC (1 container statuses recorded)
Jun 28 19:27:26.921: INFO: 	Container node-ca ready: true, restart count 0
Jun 28 19:27:26.921: INFO: ingress-operator-795fb4477f-sgnmp from openshift-ingress-operator started at 2021-06-28 16:43:14 +0000 UTC (2 container statuses recorded)
Jun 28 19:27:26.921: INFO: 	Container ingress-operator ready: true, restart count 0
Jun 28 19:27:26.921: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 28 19:27:26.921: INFO: router-default-8b99f5968-b84k7 from openshift-ingress started at 2021-06-28 18:29:06 +0000 UTC (1 container statuses recorded)
Jun 28 19:27:26.921: INFO: 	Container router ready: true, restart count 0
Jun 28 19:27:26.921: INFO: openshift-kube-proxy-xq2t8 from openshift-kube-proxy started at 2021-06-28 16:42:08 +0000 UTC (1 container statuses recorded)
Jun 28 19:27:26.921: INFO: 	Container kube-proxy ready: true, restart count 0
Jun 28 19:27:26.921: INFO: kube-storage-version-migrator-operator-77d5dd5f6c-z9m4j from openshift-kube-storage-version-migrator-operator started at 2021-06-28 16:43:14 +0000 UTC (1 container statuses recorded)
Jun 28 19:27:26.921: INFO: 	Container kube-storage-version-migrator-operator ready: true, restart count 1
Jun 28 19:27:26.921: INFO: marketplace-operator-7b9d5dcb99-nzgf8 from openshift-marketplace started at 2021-06-28 16:43:14 +0000 UTC (1 container statuses recorded)
Jun 28 19:27:26.921: INFO: 	Container marketplace-operator ready: true, restart count 0
Jun 28 19:27:26.921: INFO: alertmanager-main-1 from openshift-monitoring started at 2021-06-28 16:45:25 +0000 UTC (5 container statuses recorded)
Jun 28 19:27:26.921: INFO: 	Container alertmanager ready: true, restart count 0
Jun 28 19:27:26.921: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Jun 28 19:27:26.921: INFO: 	Container config-reloader ready: true, restart count 0
Jun 28 19:27:26.921: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 28 19:27:26.921: INFO: 	Container prom-label-proxy ready: true, restart count 0
Jun 28 19:27:26.921: INFO: cluster-monitoring-operator-6964464f67-jbfjk from openshift-monitoring started at 2021-06-28 16:43:14 +0000 UTC (2 container statuses recorded)
Jun 28 19:27:26.921: INFO: 	Container cluster-monitoring-operator ready: true, restart count 0
Jun 28 19:27:26.921: INFO: 	Container kube-rbac-proxy ready: true, restart count 2
Jun 28 19:27:26.921: INFO: node-exporter-ld8qc from openshift-monitoring started at 2021-06-28 16:44:29 +0000 UTC (2 container statuses recorded)
Jun 28 19:27:26.921: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 28 19:27:26.921: INFO: 	Container node-exporter ready: true, restart count 0
Jun 28 19:27:26.921: INFO: prometheus-adapter-8646bbc64d-stfnh from openshift-monitoring started at 2021-06-28 18:29:07 +0000 UTC (1 container statuses recorded)
Jun 28 19:27:26.921: INFO: 	Container prometheus-adapter ready: true, restart count 0
Jun 28 19:27:26.921: INFO: prometheus-k8s-0 from openshift-monitoring started at 2021-06-28 18:29:17 +0000 UTC (6 container statuses recorded)
Jun 28 19:27:26.921: INFO: 	Container config-reloader ready: true, restart count 0
Jun 28 19:27:26.921: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 28 19:27:26.921: INFO: 	Container prom-label-proxy ready: true, restart count 0
Jun 28 19:27:26.921: INFO: 	Container prometheus ready: true, restart count 1
Jun 28 19:27:26.921: INFO: 	Container prometheus-proxy ready: true, restart count 0
Jun 28 19:27:26.921: INFO: 	Container thanos-sidecar ready: true, restart count 0
Jun 28 19:27:26.921: INFO: thanos-querier-b46bb8497-dcbxw from openshift-monitoring started at 2021-06-28 18:29:07 +0000 UTC (5 container statuses recorded)
Jun 28 19:27:26.921: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 28 19:27:26.921: INFO: 	Container kube-rbac-proxy-rules ready: true, restart count 0
Jun 28 19:27:26.921: INFO: 	Container oauth-proxy ready: true, restart count 0
Jun 28 19:27:26.921: INFO: 	Container prom-label-proxy ready: true, restart count 0
Jun 28 19:27:26.921: INFO: 	Container thanos-query ready: true, restart count 0
Jun 28 19:27:26.921: INFO: multus-admission-controller-58k5w from openshift-multus started at 2021-06-28 16:43:13 +0000 UTC (2 container statuses recorded)
Jun 28 19:27:26.921: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 28 19:27:26.921: INFO: 	Container multus-admission-controller ready: true, restart count 0
Jun 28 19:27:26.921: INFO: multus-hvm6b from openshift-multus started at 2021-06-28 16:42:08 +0000 UTC (1 container statuses recorded)
Jun 28 19:27:26.921: INFO: 	Container kube-multus ready: true, restart count 0
Jun 28 19:27:26.921: INFO: network-metrics-daemon-78dfb from openshift-multus started at 2021-06-28 16:42:08 +0000 UTC (2 container statuses recorded)
Jun 28 19:27:26.921: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 28 19:27:26.921: INFO: 	Container network-metrics-daemon ready: true, restart count 0
Jun 28 19:27:26.921: INFO: network-operator-5b78d575b8-gvzvg from openshift-network-operator started at 2021-06-28 16:41:35 +0000 UTC (1 container statuses recorded)
Jun 28 19:27:26.921: INFO: 	Container network-operator ready: true, restart count 0
Jun 28 19:27:26.921: INFO: catalog-operator-67c774db7d-5ps99 from openshift-operator-lifecycle-manager started at 2021-06-28 16:43:14 +0000 UTC (1 container statuses recorded)
Jun 28 19:27:26.921: INFO: 	Container catalog-operator ready: true, restart count 0
Jun 28 19:27:26.921: INFO: olm-operator-68fcf6954f-nfsgq from openshift-operator-lifecycle-manager started at 2021-06-28 16:43:14 +0000 UTC (1 container statuses recorded)
Jun 28 19:27:26.921: INFO: 	Container olm-operator ready: true, restart count 0
Jun 28 19:27:26.921: INFO: packageserver-65544748bd-hhnld from openshift-operator-lifecycle-manager started at 2021-06-28 18:29:07 +0000 UTC (1 container statuses recorded)
Jun 28 19:27:26.921: INFO: 	Container packageserver ready: true, restart count 0
Jun 28 19:27:26.921: INFO: metrics-7df79584fc-f6g2g from openshift-roks-metrics started at 2021-06-28 16:43:14 +0000 UTC (1 container statuses recorded)
Jun 28 19:27:26.921: INFO: 	Container metrics ready: true, restart count 1
Jun 28 19:27:26.921: INFO: push-gateway-6dc86bc94f-dz7nt from openshift-roks-metrics started at 2021-06-28 16:43:14 +0000 UTC (1 container statuses recorded)
Jun 28 19:27:26.921: INFO: 	Container push-gateway ready: true, restart count 0
Jun 28 19:27:26.921: INFO: service-ca-operator-d4bfd498b-4xhnv from openshift-service-ca-operator started at 2021-06-28 16:43:14 +0000 UTC (1 container statuses recorded)
Jun 28 19:27:26.921: INFO: 	Container service-ca-operator ready: true, restart count 1
Jun 28 19:27:26.921: INFO: service-ca-665fb97685-mn77q from openshift-service-ca started at 2021-06-28 18:29:06 +0000 UTC (1 container statuses recorded)
Jun 28 19:27:26.921: INFO: 	Container service-ca-controller ready: true, restart count 0
Jun 28 19:27:26.921: INFO: pod4 from sched-pred-9676 started at 2021-06-28 19:22:24 +0000 UTC (1 container statuses recorded)
Jun 28 19:27:26.921: INFO: 	Container pod4 ready: true, restart count 0
Jun 28 19:27:26.921: INFO: sonobuoy from sonobuoy started at 2021-06-28 18:11:40 +0000 UTC (1 container statuses recorded)
Jun 28 19:27:26.921: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jun 28 19:27:26.921: INFO: sonobuoy-systemd-logs-daemon-set-bc3c64b0befe49cb-s48t5 from sonobuoy started at 2021-06-28 18:11:47 +0000 UTC (2 container statuses recorded)
Jun 28 19:27:26.921: INFO: 	Container sonobuoy-worker ready: false, restart count 7
Jun 28 19:27:26.921: INFO: 	Container systemd-logs ready: true, restart count 0
Jun 28 19:27:26.921: INFO: tigera-operator-64c8f4c7d7-hzwkg from tigera-operator started at 2021-06-28 18:29:06 +0000 UTC (1 container statuses recorded)
Jun 28 19:27:26.921: INFO: 	Container tigera-operator ready: true, restart count 0
Jun 28 19:27:26.921: INFO: 
Logging pods the apiserver thinks is on node 10.13.107.60 before test
Jun 28 19:27:26.971: INFO: calico-kube-controllers-57df785794-xp76v from calico-system started at 2021-06-28 18:29:06 +0000 UTC (1 container statuses recorded)
Jun 28 19:27:26.971: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Jun 28 19:27:26.971: INFO: calico-node-vr9g2 from calico-system started at 2021-06-28 16:43:04 +0000 UTC (1 container statuses recorded)
Jun 28 19:27:26.971: INFO: 	Container calico-node ready: true, restart count 0
Jun 28 19:27:26.971: INFO: calico-typha-649969bb55-mc6s8 from calico-system started at 2021-06-28 16:45:02 +0000 UTC (1 container statuses recorded)
Jun 28 19:27:26.971: INFO: 	Container calico-typha ready: true, restart count 0
Jun 28 19:27:26.971: INFO: test-k8s-e2e-pvg-master-verification from default started at 2021-06-28 16:51:21 +0000 UTC (1 container statuses recorded)
Jun 28 19:27:26.971: INFO: 	Container test-k8s-e2e-pvg-master-verification ready: true, restart count 0
Jun 28 19:27:26.971: INFO: ibm-cloud-provider-ip-149-81-71-226-77554565fb-djwh7 from ibm-system started at 2021-06-28 18:29:06 +0000 UTC (1 container statuses recorded)
Jun 28 19:27:26.971: INFO: 	Container ibm-cloud-provider-ip-149-81-71-226 ready: true, restart count 0
Jun 28 19:27:26.971: INFO: ibm-keepalived-watcher-w6fhq from kube-system started at 2021-06-28 16:42:37 +0000 UTC (1 container statuses recorded)
Jun 28 19:27:26.971: INFO: 	Container keepalived-watcher ready: true, restart count 0
Jun 28 19:27:26.971: INFO: ibm-master-proxy-static-10.13.107.60 from kube-system started at 2021-06-28 16:41:49 +0000 UTC (2 container statuses recorded)
Jun 28 19:27:26.971: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Jun 28 19:27:26.971: INFO: 	Container pause ready: true, restart count 0
Jun 28 19:27:26.971: INFO: ibmcloud-block-storage-driver-kwxt2 from kube-system started at 2021-06-28 16:42:37 +0000 UTC (1 container statuses recorded)
Jun 28 19:27:26.971: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Jun 28 19:27:26.971: INFO: tuned-djhn5 from openshift-cluster-node-tuning-operator started at 2021-06-28 16:43:24 +0000 UTC (1 container statuses recorded)
Jun 28 19:27:26.971: INFO: 	Container tuned ready: true, restart count 0
Jun 28 19:27:26.971: INFO: console-74dd6cb4c6-gbthm from openshift-console started at 2021-06-28 17:20:10 +0000 UTC (1 container statuses recorded)
Jun 28 19:27:26.971: INFO: 	Container console ready: true, restart count 0
Jun 28 19:27:26.971: INFO: dns-default-84l5j from openshift-dns started at 2021-06-28 16:44:04 +0000 UTC (3 container statuses recorded)
Jun 28 19:27:26.971: INFO: 	Container dns ready: true, restart count 0
Jun 28 19:27:26.971: INFO: 	Container dns-node-resolver ready: true, restart count 0
Jun 28 19:27:26.971: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 28 19:27:26.971: INFO: image-registry-7f9574d5c6-tmmns from openshift-image-registry started at 2021-06-28 16:46:50 +0000 UTC (1 container statuses recorded)
Jun 28 19:27:26.971: INFO: 	Container registry ready: true, restart count 0
Jun 28 19:27:26.971: INFO: node-ca-42txp from openshift-image-registry started at 2021-06-28 16:44:14 +0000 UTC (1 container statuses recorded)
Jun 28 19:27:26.971: INFO: 	Container node-ca ready: true, restart count 0
Jun 28 19:27:26.971: INFO: router-default-8b99f5968-8nzw5 from openshift-ingress started at 2021-06-28 17:18:25 +0000 UTC (1 container statuses recorded)
Jun 28 19:27:26.971: INFO: 	Container router ready: true, restart count 0
Jun 28 19:27:26.971: INFO: openshift-kube-proxy-n6pzf from openshift-kube-proxy started at 2021-06-28 16:42:37 +0000 UTC (1 container statuses recorded)
Jun 28 19:27:26.971: INFO: 	Container kube-proxy ready: true, restart count 0
Jun 28 19:27:26.971: INFO: migrator-858cfc6f4c-tvj6h from openshift-kube-storage-version-migrator started at 2021-06-28 18:29:06 +0000 UTC (1 container statuses recorded)
Jun 28 19:27:26.971: INFO: 	Container migrator ready: true, restart count 0
Jun 28 19:27:26.971: INFO: certified-operators-d5d5w from openshift-marketplace started at 2021-06-28 16:44:19 +0000 UTC (1 container statuses recorded)
Jun 28 19:27:26.971: INFO: 	Container registry-server ready: true, restart count 0
Jun 28 19:27:26.971: INFO: community-operators-dbmgh from openshift-marketplace started at 2021-06-28 17:01:52 +0000 UTC (1 container statuses recorded)
Jun 28 19:27:26.971: INFO: 	Container registry-server ready: true, restart count 0
Jun 28 19:27:26.971: INFO: redhat-marketplace-4t4vz from openshift-marketplace started at 2021-06-28 16:44:21 +0000 UTC (1 container statuses recorded)
Jun 28 19:27:26.971: INFO: 	Container registry-server ready: true, restart count 0
Jun 28 19:27:26.971: INFO: redhat-operators-dvpnz from openshift-marketplace started at 2021-06-28 16:44:19 +0000 UTC (1 container statuses recorded)
Jun 28 19:27:26.971: INFO: 	Container registry-server ready: true, restart count 0
Jun 28 19:27:26.971: INFO: alertmanager-main-0 from openshift-monitoring started at 2021-06-28 16:45:25 +0000 UTC (5 container statuses recorded)
Jun 28 19:27:26.971: INFO: 	Container alertmanager ready: true, restart count 0
Jun 28 19:27:26.971: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Jun 28 19:27:26.971: INFO: 	Container config-reloader ready: true, restart count 0
Jun 28 19:27:26.971: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 28 19:27:26.971: INFO: 	Container prom-label-proxy ready: true, restart count 0
Jun 28 19:27:26.972: INFO: alertmanager-main-2 from openshift-monitoring started at 2021-06-28 18:29:17 +0000 UTC (5 container statuses recorded)
Jun 28 19:27:26.972: INFO: 	Container alertmanager ready: true, restart count 0
Jun 28 19:27:26.972: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Jun 28 19:27:26.972: INFO: 	Container config-reloader ready: true, restart count 0
Jun 28 19:27:26.972: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 28 19:27:26.972: INFO: 	Container prom-label-proxy ready: true, restart count 0
Jun 28 19:27:26.972: INFO: grafana-85d4b8dbd-jn67b from openshift-monitoring started at 2021-06-28 16:44:32 +0000 UTC (2 container statuses recorded)
Jun 28 19:27:26.972: INFO: 	Container grafana ready: true, restart count 0
Jun 28 19:27:26.972: INFO: 	Container grafana-proxy ready: true, restart count 0
Jun 28 19:27:26.972: INFO: kube-state-metrics-659c7b865d-974cs from openshift-monitoring started at 2021-06-28 16:44:29 +0000 UTC (3 container statuses recorded)
Jun 28 19:27:26.972: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Jun 28 19:27:26.972: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Jun 28 19:27:26.972: INFO: 	Container kube-state-metrics ready: true, restart count 0
Jun 28 19:27:26.972: INFO: node-exporter-6s54d from openshift-monitoring started at 2021-06-28 16:44:29 +0000 UTC (2 container statuses recorded)
Jun 28 19:27:26.972: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 28 19:27:26.972: INFO: 	Container node-exporter ready: true, restart count 0
Jun 28 19:27:26.972: INFO: openshift-state-metrics-7cf4dc694b-d9w6x from openshift-monitoring started at 2021-06-28 16:44:29 +0000 UTC (3 container statuses recorded)
Jun 28 19:27:26.972: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Jun 28 19:27:26.972: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Jun 28 19:27:26.972: INFO: 	Container openshift-state-metrics ready: true, restart count 0
Jun 28 19:27:26.972: INFO: prometheus-adapter-8646bbc64d-nq9hj from openshift-monitoring started at 2021-06-28 16:44:29 +0000 UTC (1 container statuses recorded)
Jun 28 19:27:26.972: INFO: 	Container prometheus-adapter ready: true, restart count 0
Jun 28 19:27:26.972: INFO: prometheus-k8s-1 from openshift-monitoring started at 2021-06-28 16:45:26 +0000 UTC (6 container statuses recorded)
Jun 28 19:27:26.972: INFO: 	Container config-reloader ready: true, restart count 0
Jun 28 19:27:26.972: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 28 19:27:26.972: INFO: 	Container prom-label-proxy ready: true, restart count 0
Jun 28 19:27:26.972: INFO: 	Container prometheus ready: true, restart count 1
Jun 28 19:27:26.972: INFO: 	Container prometheus-proxy ready: true, restart count 0
Jun 28 19:27:26.972: INFO: 	Container thanos-sidecar ready: true, restart count 0
Jun 28 19:27:26.972: INFO: prometheus-operator-5d48db6d9c-z9sxl from openshift-monitoring started at 2021-06-28 16:44:29 +0000 UTC (2 container statuses recorded)
Jun 28 19:27:26.972: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 28 19:27:26.972: INFO: 	Container prometheus-operator ready: true, restart count 0
Jun 28 19:27:26.972: INFO: telemeter-client-5584d54f85-bhqql from openshift-monitoring started at 2021-06-28 16:44:29 +0000 UTC (3 container statuses recorded)
Jun 28 19:27:26.972: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 28 19:27:26.972: INFO: 	Container reload ready: true, restart count 0
Jun 28 19:27:26.972: INFO: 	Container telemeter-client ready: true, restart count 0
Jun 28 19:27:26.972: INFO: thanos-querier-b46bb8497-wxrxb from openshift-monitoring started at 2021-06-28 16:44:31 +0000 UTC (5 container statuses recorded)
Jun 28 19:27:26.972: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 28 19:27:26.972: INFO: 	Container kube-rbac-proxy-rules ready: true, restart count 0
Jun 28 19:27:26.972: INFO: 	Container oauth-proxy ready: true, restart count 0
Jun 28 19:27:26.972: INFO: 	Container prom-label-proxy ready: true, restart count 0
Jun 28 19:27:26.972: INFO: 	Container thanos-query ready: true, restart count 0
Jun 28 19:27:26.972: INFO: multus-admission-controller-5rwk7 from openshift-multus started at 2021-06-28 16:43:56 +0000 UTC (2 container statuses recorded)
Jun 28 19:27:26.972: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 28 19:27:26.972: INFO: 	Container multus-admission-controller ready: true, restart count 0
Jun 28 19:27:26.972: INFO: multus-tsj42 from openshift-multus started at 2021-06-28 16:42:37 +0000 UTC (1 container statuses recorded)
Jun 28 19:27:26.972: INFO: 	Container kube-multus ready: true, restart count 0
Jun 28 19:27:26.972: INFO: network-metrics-daemon-gzxgq from openshift-multus started at 2021-06-28 16:42:37 +0000 UTC (2 container statuses recorded)
Jun 28 19:27:26.972: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 28 19:27:26.972: INFO: 	Container network-metrics-daemon ready: true, restart count 0
Jun 28 19:27:26.972: INFO: packageserver-65544748bd-h6vfz from openshift-operator-lifecycle-manager started at 2021-06-28 16:44:29 +0000 UTC (1 container statuses recorded)
Jun 28 19:27:26.972: INFO: 	Container packageserver ready: true, restart count 0
Jun 28 19:27:26.972: INFO: sonobuoy-e2e-job-3182b6be48fa4a64 from sonobuoy started at 2021-06-28 18:11:47 +0000 UTC (2 container statuses recorded)
Jun 28 19:27:26.972: INFO: 	Container e2e ready: true, restart count 0
Jun 28 19:27:26.972: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 28 19:27:26.972: INFO: sonobuoy-systemd-logs-daemon-set-bc3c64b0befe49cb-47fm7 from sonobuoy started at 2021-06-28 18:11:47 +0000 UTC (2 container statuses recorded)
Jun 28 19:27:26.972: INFO: 	Container sonobuoy-worker ready: false, restart count 7
Jun 28 19:27:26.972: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Trying to schedule Pod with nonempty NodeSelector.
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.168cd5a112e72336], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 node(s) didn't match node selector.]
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 19:27:34.362: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-6518" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81

• [SLOW TEST:7.764 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if not matching  [Conformance]","total":305,"completed":225,"skipped":3483,"failed":0}
SSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command in a pod 
  should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 19:27:34.406: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[It] should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 19:27:38.754: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-9941" for this suite.
•{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox command in a pod should print the output to logs [NodeConformance] [Conformance]","total":305,"completed":226,"skipped":3488,"failed":0}
SSSSSSS
------------------------------
[sig-storage] ConfigMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 19:27:38.794: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jun 28 19:27:38.933: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
STEP: Creating configMap with name cm-test-opt-del-4747ab7b-b1b6-4b96-9081-026486a9a463
STEP: Creating configMap with name cm-test-opt-upd-b0cd0e5e-217b-45ff-b629-e250a363097a
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-4747ab7b-b1b6-4b96-9081-026486a9a463
STEP: Updating configmap cm-test-opt-upd-b0cd0e5e-217b-45ff-b629-e250a363097a
STEP: Creating configMap with name cm-test-opt-create-e72e937b-c671-4fb0-adf1-5f66bf59f0f4
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 19:27:43.382: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7769" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap optional updates should be reflected in volume [NodeConformance] [Conformance]","total":305,"completed":227,"skipped":3495,"failed":0}
SSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 19:27:43.426: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Jun 28 19:27:49.750: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jun 28 19:27:49.761: INFO: Pod pod-with-poststart-http-hook still exists
Jun 28 19:27:51.762: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jun 28 19:27:51.778: INFO: Pod pod-with-poststart-http-hook still exists
Jun 28 19:27:53.762: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jun 28 19:27:53.777: INFO: Pod pod-with-poststart-http-hook still exists
Jun 28 19:27:55.765: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jun 28 19:27:55.777: INFO: Pod pod-with-poststart-http-hook still exists
Jun 28 19:27:57.762: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jun 28 19:27:57.773: INFO: Pod pod-with-poststart-http-hook still exists
Jun 28 19:27:59.762: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jun 28 19:27:59.777: INFO: Pod pod-with-poststart-http-hook still exists
Jun 28 19:28:01.762: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jun 28 19:28:01.780: INFO: Pod pod-with-poststart-http-hook still exists
Jun 28 19:28:03.762: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jun 28 19:28:03.778: INFO: Pod pod-with-poststart-http-hook still exists
Jun 28 19:28:05.762: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jun 28 19:28:05.776: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 19:28:05.781: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-4081" for this suite.

• [SLOW TEST:22.395 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  when create a pod with lifecycle hook
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute poststart http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart http hook properly [NodeConformance] [Conformance]","total":305,"completed":228,"skipped":3498,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group but different versions [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 19:28:05.831: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of same group but different versions [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation
Jun 28 19:28:05.968: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation
Jun 28 19:28:38.848: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
Jun 28 19:28:49.078: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 19:29:27.599: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-2369" for this suite.

• [SLOW TEST:81.808 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group but different versions [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group but different versions [Conformance]","total":305,"completed":229,"skipped":3515,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Events 
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 19:29:27.643: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: retrieving the pod
Jun 28 19:29:31.862: INFO: &Pod{ObjectMeta:{send-events-7c668bca-e42c-4a3c-b251-8d41dd11629e  events-2382 /api/v1/namespaces/events-2382/pods/send-events-7c668bca-e42c-4a3c-b251-8d41dd11629e 2ecfcc0b-af9c-4f41-b2da-46007de3eb93 99113 0 2021-06-28 19:29:27 +0000 UTC <nil> <nil> map[name:foo time:764408830] map[cni.projectcalico.org/podIP:172.30.221.168/32 cni.projectcalico.org/podIPs:172.30.221.168/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "",
    "ips": [
        "172.30.221.168"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "",
    "ips": [
        "172.30.221.168"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [] []  [{e2e.test Update v1 2021-06-28 19:29:27 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:time":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"p\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":80,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-06-28 19:29:28 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {multus Update v1 2021-06-28 19:29:28 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}}} {kubelet Update v1 2021-06-28 19:29:30 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.221.168\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-x2d4q,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-x2d4q,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:p,Image:k8s.gcr.io/e2e-test-images/agnhost:2.20,Command:[],Args:[serve-hostname],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:80,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-x2d4q,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.13.107.37,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c57,c34,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-28 19:29:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-28 19:29:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-28 19:29:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-28 19:29:27 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.13.107.37,PodIP:172.30.221.168,StartTime:2021-06-28 19:29:27 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:p,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-06-28 19:29:28 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.20,ImageID:k8s.gcr.io/e2e-test-images/agnhost@sha256:17e61a0b9e498b6c73ed97670906be3d5a3ae394739c1bd5b619e1a004885cf0,ContainerID:cri-o://4b974fe9381e80b33d36ad04a08bc024d27a5ef7a103cbc74be7b7cd971b8161,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.221.168,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

STEP: checking for scheduler event about the pod
Jun 28 19:29:33.875: INFO: Saw scheduler event for our pod.
STEP: checking for kubelet event about the pod
Jun 28 19:29:35.885: INFO: Saw kubelet event for our pod.
STEP: deleting the pod
[AfterEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 19:29:35.910: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-2382" for this suite.

• [SLOW TEST:8.319 seconds]
[k8s.io] [sig-node] Events
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] [sig-node] Events should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]","total":305,"completed":230,"skipped":3559,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 19:29:35.964: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating the pod
Jun 28 19:29:38.775: INFO: Successfully updated pod "labelsupdatee1eb413a-6b66-41cd-9450-1909e3cb5ff0"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 19:29:40.843: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7154" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should update labels on modification [NodeConformance] [Conformance]","total":305,"completed":231,"skipped":3571,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 19:29:40.882: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating the pod
Jun 28 19:29:43.683: INFO: Successfully updated pod "annotationupdate3dc65457-d333-42c7-8372-d81f5fed3267"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 19:29:45.739: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7956" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should update annotations on modification [NodeConformance] [Conformance]","total":305,"completed":232,"skipped":3588,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 19:29:45.783: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jun 28 19:29:45.950: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
STEP: Creating configMap with name configmap-test-upd-239431d1-026a-4037-b53d-e9782b9e2b7a
STEP: Creating the pod
STEP: Waiting for pod with text data
STEP: Waiting for pod with binary data
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 19:29:48.114: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3208" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap binary data should be reflected in volume [NodeConformance] [Conformance]","total":305,"completed":233,"skipped":3609,"failed":0}
SSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 19:29:48.148: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating secret with name secret-test-map-aa805af6-9fd1-4619-b36a-705ad08b2bad
STEP: Creating a pod to test consume secrets
Jun 28 19:29:48.333: INFO: Waiting up to 5m0s for pod "pod-secrets-b0084c74-7e6a-4737-82fd-b562607211d4" in namespace "secrets-179" to be "Succeeded or Failed"
Jun 28 19:29:48.344: INFO: Pod "pod-secrets-b0084c74-7e6a-4737-82fd-b562607211d4": Phase="Pending", Reason="", readiness=false. Elapsed: 10.919778ms
Jun 28 19:29:50.354: INFO: Pod "pod-secrets-b0084c74-7e6a-4737-82fd-b562607211d4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.021353315s
STEP: Saw pod success
Jun 28 19:29:50.357: INFO: Pod "pod-secrets-b0084c74-7e6a-4737-82fd-b562607211d4" satisfied condition "Succeeded or Failed"
Jun 28 19:29:50.370: INFO: Trying to get logs from node 10.13.107.60 pod pod-secrets-b0084c74-7e6a-4737-82fd-b562607211d4 container secret-volume-test: <nil>
STEP: delete the pod
Jun 28 19:29:50.462: INFO: Waiting for pod pod-secrets-b0084c74-7e6a-4737-82fd-b562607211d4 to disappear
Jun 28 19:29:50.473: INFO: Pod pod-secrets-b0084c74-7e6a-4737-82fd-b562607211d4 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 19:29:50.473: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-179" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":305,"completed":234,"skipped":3614,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 19:29:50.517: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod pod-subpath-test-projected-wnfj
STEP: Creating a pod to test atomic-volume-subpath
Jun 28 19:29:50.713: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-wnfj" in namespace "subpath-5099" to be "Succeeded or Failed"
Jun 28 19:29:50.724: INFO: Pod "pod-subpath-test-projected-wnfj": Phase="Pending", Reason="", readiness=false. Elapsed: 10.936463ms
Jun 28 19:29:52.737: INFO: Pod "pod-subpath-test-projected-wnfj": Phase="Running", Reason="", readiness=true. Elapsed: 2.023656325s
Jun 28 19:29:54.749: INFO: Pod "pod-subpath-test-projected-wnfj": Phase="Running", Reason="", readiness=true. Elapsed: 4.03569694s
Jun 28 19:29:56.762: INFO: Pod "pod-subpath-test-projected-wnfj": Phase="Running", Reason="", readiness=true. Elapsed: 6.0489128s
Jun 28 19:29:58.774: INFO: Pod "pod-subpath-test-projected-wnfj": Phase="Running", Reason="", readiness=true. Elapsed: 8.060838073s
Jun 28 19:30:00.788: INFO: Pod "pod-subpath-test-projected-wnfj": Phase="Running", Reason="", readiness=true. Elapsed: 10.074137565s
Jun 28 19:30:02.805: INFO: Pod "pod-subpath-test-projected-wnfj": Phase="Running", Reason="", readiness=true. Elapsed: 12.091881702s
Jun 28 19:30:04.823: INFO: Pod "pod-subpath-test-projected-wnfj": Phase="Running", Reason="", readiness=true. Elapsed: 14.109296086s
Jun 28 19:30:06.834: INFO: Pod "pod-subpath-test-projected-wnfj": Phase="Running", Reason="", readiness=true. Elapsed: 16.120787767s
Jun 28 19:30:08.846: INFO: Pod "pod-subpath-test-projected-wnfj": Phase="Running", Reason="", readiness=true. Elapsed: 18.132888853s
Jun 28 19:30:10.859: INFO: Pod "pod-subpath-test-projected-wnfj": Phase="Running", Reason="", readiness=true. Elapsed: 20.145788834s
Jun 28 19:30:12.871: INFO: Pod "pod-subpath-test-projected-wnfj": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.157811661s
STEP: Saw pod success
Jun 28 19:30:12.871: INFO: Pod "pod-subpath-test-projected-wnfj" satisfied condition "Succeeded or Failed"
Jun 28 19:30:12.883: INFO: Trying to get logs from node 10.13.107.37 pod pod-subpath-test-projected-wnfj container test-container-subpath-projected-wnfj: <nil>
STEP: delete the pod
Jun 28 19:30:12.943: INFO: Waiting for pod pod-subpath-test-projected-wnfj to disappear
Jun 28 19:30:12.953: INFO: Pod pod-subpath-test-projected-wnfj no longer exists
STEP: Deleting pod pod-subpath-test-projected-wnfj
Jun 28 19:30:12.953: INFO: Deleting pod "pod-subpath-test-projected-wnfj" in namespace "subpath-5099"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 19:30:12.964: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-5099" for this suite.

• [SLOW TEST:22.482 seconds]
[sig-storage] Subpath
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with projected pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with projected pod [LinuxOnly] [Conformance]","total":305,"completed":235,"skipped":3636,"failed":0}
SS
------------------------------
[sig-apps] Deployment 
  deployment should support rollover [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 19:30:13.002: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:78
[It] deployment should support rollover [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jun 28 19:30:13.155: INFO: Pod name rollover-pod: Found 0 pods out of 1
Jun 28 19:30:18.176: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Jun 28 19:30:18.176: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Jun 28 19:30:20.189: INFO: Creating deployment "test-rollover-deployment"
Jun 28 19:30:20.216: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Jun 28 19:30:22.243: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Jun 28 19:30:22.263: INFO: Ensure that both replica sets have 1 created replica
Jun 28 19:30:22.285: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Jun 28 19:30:22.323: INFO: Updating deployment test-rollover-deployment
Jun 28 19:30:22.325: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Jun 28 19:30:24.351: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Jun 28 19:30:24.369: INFO: Make sure deployment "test-rollover-deployment" is complete
Jun 28 19:30:24.387: INFO: all replica sets need to contain the pod-template-hash label
Jun 28 19:30:24.388: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63760505420, loc:(*time.Location)(0x7702840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63760505420, loc:(*time.Location)(0x7702840)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63760505424, loc:(*time.Location)(0x7702840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63760505420, loc:(*time.Location)(0x7702840)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-5797c7764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 28 19:30:26.594: INFO: all replica sets need to contain the pod-template-hash label
Jun 28 19:30:26.594: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63760505420, loc:(*time.Location)(0x7702840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63760505420, loc:(*time.Location)(0x7702840)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63760505424, loc:(*time.Location)(0x7702840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63760505420, loc:(*time.Location)(0x7702840)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-5797c7764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 28 19:30:28.410: INFO: all replica sets need to contain the pod-template-hash label
Jun 28 19:30:28.410: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63760505420, loc:(*time.Location)(0x7702840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63760505420, loc:(*time.Location)(0x7702840)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63760505424, loc:(*time.Location)(0x7702840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63760505420, loc:(*time.Location)(0x7702840)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-5797c7764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 28 19:30:30.415: INFO: all replica sets need to contain the pod-template-hash label
Jun 28 19:30:30.416: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63760505420, loc:(*time.Location)(0x7702840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63760505420, loc:(*time.Location)(0x7702840)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63760505424, loc:(*time.Location)(0x7702840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63760505420, loc:(*time.Location)(0x7702840)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-5797c7764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 28 19:30:32.417: INFO: all replica sets need to contain the pod-template-hash label
Jun 28 19:30:32.417: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63760505420, loc:(*time.Location)(0x7702840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63760505420, loc:(*time.Location)(0x7702840)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63760505424, loc:(*time.Location)(0x7702840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63760505420, loc:(*time.Location)(0x7702840)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-5797c7764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 28 19:30:34.413: INFO: 
Jun 28 19:30:34.413: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:72
Jun 28 19:30:34.443: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:{test-rollover-deployment  deployment-8098 /apis/apps/v1/namespaces/deployment-8098/deployments/test-rollover-deployment 1d297555-6cc9-417f-b8d5-8a26f055fa46 100136 2 2021-06-28 19:30:20 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2021-06-28 19:30:22 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{}}},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-06-28 19:30:34 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}}}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.20 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0092b0d28 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2021-06-28 19:30:20 +0000 UTC,LastTransitionTime:2021-06-28 19:30:20 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-5797c7764" has successfully progressed.,LastUpdateTime:2021-06-28 19:30:34 +0000 UTC,LastTransitionTime:2021-06-28 19:30:20 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Jun 28 19:30:34.454: INFO: New ReplicaSet "test-rollover-deployment-5797c7764" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:{test-rollover-deployment-5797c7764  deployment-8098 /apis/apps/v1/namespaces/deployment-8098/replicasets/test-rollover-deployment-5797c7764 5ee2a98f-4d46-4f34-8fb0-d1132e13dc71 100125 2 2021-06-28 19:30:22 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:5797c7764] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment 1d297555-6cc9-417f-b8d5-8a26f055fa46 0xc0092b1400 0xc0092b1401}] []  [{kube-controller-manager Update apps/v1 2021-06-28 19:30:34 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1d297555-6cc9-417f-b8d5-8a26f055fa46\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 5797c7764,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:5797c7764] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.20 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0092b1488 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jun 28 19:30:34.454: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Jun 28 19:30:34.454: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-8098 /apis/apps/v1/namespaces/deployment-8098/replicasets/test-rollover-controller c752072e-bf48-47c3-95a0-bfa4600c75dd 100134 2 2021-06-28 19:30:13 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment 1d297555-6cc9-417f-b8d5-8a26f055fa46 0xc0092b1297 0xc0092b1298}] []  [{e2e.test Update apps/v1 2021-06-28 19:30:13 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{"f:matchLabels":{".":{},"f:name":{},"f:pod":{}}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-06-28 19:30:34 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1d297555-6cc9-417f-b8d5-8a26f055fa46\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{}},"f:status":{"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0092b1398 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jun 28 19:30:34.454: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-78bc8b888c  deployment-8098 /apis/apps/v1/namespaces/deployment-8098/replicasets/test-rollover-deployment-78bc8b888c 94bcbfb7-6c0c-4c82-8cd6-3dce225b8168 100048 2 2021-06-28 19:30:20 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:78bc8b888c] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment 1d297555-6cc9-417f-b8d5-8a26f055fa46 0xc0092b1547 0xc0092b1548}] []  [{kube-controller-manager Update apps/v1 2021-06-28 19:30:22 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1d297555-6cc9-417f-b8d5-8a26f055fa46\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 78bc8b888c,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:78bc8b888c] map[] [] []  []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0092b15e8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jun 28 19:30:34.465: INFO: Pod "test-rollover-deployment-5797c7764-mnqcz" is available:
&Pod{ObjectMeta:{test-rollover-deployment-5797c7764-mnqcz test-rollover-deployment-5797c7764- deployment-8098 /api/v1/namespaces/deployment-8098/pods/test-rollover-deployment-5797c7764-mnqcz 8c1cbf84-6662-4769-8112-3b3203384e92 100078 0 2021-06-28 19:30:22 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:5797c7764] map[cni.projectcalico.org/podIP:172.30.221.179/32 cni.projectcalico.org/podIPs:172.30.221.179/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "",
    "ips": [
        "172.30.221.179"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "",
    "ips": [
        "172.30.221.179"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet test-rollover-deployment-5797c7764 5ee2a98f-4d46-4f34-8fb0-d1132e13dc71 0xc0092b1d07 0xc0092b1d08}] []  [{kube-controller-manager Update v1 2021-06-28 19:30:22 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5ee2a98f-4d46-4f34-8fb0-d1132e13dc71\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-06-28 19:30:23 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {multus Update v1 2021-06-28 19:30:23 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}}} {kubelet Update v1 2021-06-28 19:30:24 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.221.179\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-s8lz5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-s8lz5,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:k8s.gcr.io/e2e-test-images/agnhost:2.20,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-s8lz5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.13.107.37,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c58,c7,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-8pdl5,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-28 19:30:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-28 19:30:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-28 19:30:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-28 19:30:22 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.13.107.37,PodIP:172.30.221.179,StartTime:2021-06-28 19:30:22 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-06-28 19:30:23 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.20,ImageID:k8s.gcr.io/e2e-test-images/agnhost@sha256:17e61a0b9e498b6c73ed97670906be3d5a3ae394739c1bd5b619e1a004885cf0,ContainerID:cri-o://0d5af26ac24bee2b7e2a5379c99ad0beb52c69085b5d86620307d269d9868e81,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.221.179,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 19:30:34.465: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-8098" for this suite.

• [SLOW TEST:21.498 seconds]
[sig-apps] Deployment
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support rollover [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should support rollover [Conformance]","total":305,"completed":236,"skipped":3638,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 19:30:34.500: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Jun 28 19:30:34.687: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e7b727ba-84a8-4723-b6b6-45a1d9065fe1" in namespace "downward-api-1546" to be "Succeeded or Failed"
Jun 28 19:30:34.701: INFO: Pod "downwardapi-volume-e7b727ba-84a8-4723-b6b6-45a1d9065fe1": Phase="Pending", Reason="", readiness=false. Elapsed: 13.69398ms
Jun 28 19:30:36.727: INFO: Pod "downwardapi-volume-e7b727ba-84a8-4723-b6b6-45a1d9065fe1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0398823s
Jun 28 19:30:38.746: INFO: Pod "downwardapi-volume-e7b727ba-84a8-4723-b6b6-45a1d9065fe1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.05863033s
STEP: Saw pod success
Jun 28 19:30:38.747: INFO: Pod "downwardapi-volume-e7b727ba-84a8-4723-b6b6-45a1d9065fe1" satisfied condition "Succeeded or Failed"
Jun 28 19:30:38.757: INFO: Trying to get logs from node 10.13.107.37 pod downwardapi-volume-e7b727ba-84a8-4723-b6b6-45a1d9065fe1 container client-container: <nil>
STEP: delete the pod
Jun 28 19:30:38.821: INFO: Waiting for pod downwardapi-volume-e7b727ba-84a8-4723-b6b6-45a1d9065fe1 to disappear
Jun 28 19:30:38.831: INFO: Pod downwardapi-volume-e7b727ba-84a8-4723-b6b6-45a1d9065fe1 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 19:30:38.831: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1546" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":237,"skipped":3646,"failed":0}
SSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 19:30:38.864: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jun 28 19:30:39.561: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jun 28 19:30:41.590: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63760505439, loc:(*time.Location)(0x7702840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63760505439, loc:(*time.Location)(0x7702840)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63760505439, loc:(*time.Location)(0x7702840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63760505439, loc:(*time.Location)(0x7702840)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun 28 19:30:44.639: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jun 28 19:30:44.656: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-5157-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 19:30:45.935: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9697" for this suite.
STEP: Destroying namespace "webhook-9697-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:7.308 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource [Conformance]","total":305,"completed":238,"skipped":3653,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 19:30:46.190: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-5834 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-5834;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-5834 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-5834;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-5834.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-5834.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-5834.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-5834.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-5834.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-5834.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-5834.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-5834.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-5834.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-5834.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-5834.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-5834.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-5834.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 250.44.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.44.250_udp@PTR;check="$$(dig +tcp +noall +answer +search 250.44.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.44.250_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-5834 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-5834;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-5834 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-5834;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-5834.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-5834.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-5834.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-5834.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-5834.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-5834.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-5834.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-5834.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-5834.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-5834.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-5834.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-5834.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-5834.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 250.44.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.44.250_udp@PTR;check="$$(dig +tcp +noall +answer +search 250.44.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.44.250_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jun 28 19:30:48.521: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-5834/dns-test-f4d1fa9b-6bc2-4517-b3a0-ac36b4afd263: the server could not find the requested resource (get pods dns-test-f4d1fa9b-6bc2-4517-b3a0-ac36b4afd263)
Jun 28 19:30:48.550: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-5834/dns-test-f4d1fa9b-6bc2-4517-b3a0-ac36b4afd263: the server could not find the requested resource (get pods dns-test-f4d1fa9b-6bc2-4517-b3a0-ac36b4afd263)
Jun 28 19:30:48.580: INFO: Unable to read wheezy_udp@dns-test-service.dns-5834 from pod dns-5834/dns-test-f4d1fa9b-6bc2-4517-b3a0-ac36b4afd263: the server could not find the requested resource (get pods dns-test-f4d1fa9b-6bc2-4517-b3a0-ac36b4afd263)
Jun 28 19:30:48.605: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5834 from pod dns-5834/dns-test-f4d1fa9b-6bc2-4517-b3a0-ac36b4afd263: the server could not find the requested resource (get pods dns-test-f4d1fa9b-6bc2-4517-b3a0-ac36b4afd263)
Jun 28 19:30:48.640: INFO: Unable to read wheezy_udp@dns-test-service.dns-5834.svc from pod dns-5834/dns-test-f4d1fa9b-6bc2-4517-b3a0-ac36b4afd263: the server could not find the requested resource (get pods dns-test-f4d1fa9b-6bc2-4517-b3a0-ac36b4afd263)
Jun 28 19:30:48.677: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5834.svc from pod dns-5834/dns-test-f4d1fa9b-6bc2-4517-b3a0-ac36b4afd263: the server could not find the requested resource (get pods dns-test-f4d1fa9b-6bc2-4517-b3a0-ac36b4afd263)
Jun 28 19:30:48.708: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-5834.svc from pod dns-5834/dns-test-f4d1fa9b-6bc2-4517-b3a0-ac36b4afd263: the server could not find the requested resource (get pods dns-test-f4d1fa9b-6bc2-4517-b3a0-ac36b4afd263)
Jun 28 19:30:48.738: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-5834.svc from pod dns-5834/dns-test-f4d1fa9b-6bc2-4517-b3a0-ac36b4afd263: the server could not find the requested resource (get pods dns-test-f4d1fa9b-6bc2-4517-b3a0-ac36b4afd263)
Jun 28 19:30:48.938: INFO: Unable to read jessie_udp@dns-test-service from pod dns-5834/dns-test-f4d1fa9b-6bc2-4517-b3a0-ac36b4afd263: the server could not find the requested resource (get pods dns-test-f4d1fa9b-6bc2-4517-b3a0-ac36b4afd263)
Jun 28 19:30:48.959: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-5834/dns-test-f4d1fa9b-6bc2-4517-b3a0-ac36b4afd263: the server could not find the requested resource (get pods dns-test-f4d1fa9b-6bc2-4517-b3a0-ac36b4afd263)
Jun 28 19:30:48.993: INFO: Unable to read jessie_udp@dns-test-service.dns-5834 from pod dns-5834/dns-test-f4d1fa9b-6bc2-4517-b3a0-ac36b4afd263: the server could not find the requested resource (get pods dns-test-f4d1fa9b-6bc2-4517-b3a0-ac36b4afd263)
Jun 28 19:30:49.024: INFO: Unable to read jessie_tcp@dns-test-service.dns-5834 from pod dns-5834/dns-test-f4d1fa9b-6bc2-4517-b3a0-ac36b4afd263: the server could not find the requested resource (get pods dns-test-f4d1fa9b-6bc2-4517-b3a0-ac36b4afd263)
Jun 28 19:30:49.059: INFO: Unable to read jessie_udp@dns-test-service.dns-5834.svc from pod dns-5834/dns-test-f4d1fa9b-6bc2-4517-b3a0-ac36b4afd263: the server could not find the requested resource (get pods dns-test-f4d1fa9b-6bc2-4517-b3a0-ac36b4afd263)
Jun 28 19:30:49.082: INFO: Unable to read jessie_tcp@dns-test-service.dns-5834.svc from pod dns-5834/dns-test-f4d1fa9b-6bc2-4517-b3a0-ac36b4afd263: the server could not find the requested resource (get pods dns-test-f4d1fa9b-6bc2-4517-b3a0-ac36b4afd263)
Jun 28 19:30:49.115: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-5834.svc from pod dns-5834/dns-test-f4d1fa9b-6bc2-4517-b3a0-ac36b4afd263: the server could not find the requested resource (get pods dns-test-f4d1fa9b-6bc2-4517-b3a0-ac36b4afd263)
Jun 28 19:30:49.161: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-5834.svc from pod dns-5834/dns-test-f4d1fa9b-6bc2-4517-b3a0-ac36b4afd263: the server could not find the requested resource (get pods dns-test-f4d1fa9b-6bc2-4517-b3a0-ac36b4afd263)
Jun 28 19:30:49.311: INFO: Lookups using dns-5834/dns-test-f4d1fa9b-6bc2-4517-b3a0-ac36b4afd263 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-5834 wheezy_tcp@dns-test-service.dns-5834 wheezy_udp@dns-test-service.dns-5834.svc wheezy_tcp@dns-test-service.dns-5834.svc wheezy_udp@_http._tcp.dns-test-service.dns-5834.svc wheezy_tcp@_http._tcp.dns-test-service.dns-5834.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-5834 jessie_tcp@dns-test-service.dns-5834 jessie_udp@dns-test-service.dns-5834.svc jessie_tcp@dns-test-service.dns-5834.svc jessie_udp@_http._tcp.dns-test-service.dns-5834.svc jessie_tcp@_http._tcp.dns-test-service.dns-5834.svc]

Jun 28 19:30:55.218: INFO: DNS probes using dns-5834/dns-test-f4d1fa9b-6bc2-4517-b3a0-ac36b4afd263 succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 19:30:55.370: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-5834" for this suite.

• [SLOW TEST:9.216 seconds]
[sig-network] DNS
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] DNS should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]","total":305,"completed":239,"skipped":3693,"failed":0}
SSSSSS
------------------------------
[sig-apps] ReplicationController 
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 19:30:55.407: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should release no longer matching pods [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Given a ReplicationController is created
STEP: When the matched label of one of its pods change
Jun 28 19:30:55.590: INFO: Pod name pod-release: Found 0 pods out of 1
Jun 28 19:31:00.622: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 19:31:00.694: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-7058" for this suite.

• [SLOW TEST:5.321 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should release no longer matching pods [Conformance]","total":305,"completed":240,"skipped":3699,"failed":0}
SSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 19:31:00.732: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods
STEP: Gathering metrics
W0628 19:31:41.021914      22 metrics_grabber.go:98] Can't find kube-scheduler pod. Grabbing metrics from kube-scheduler is disabled.
W0628 19:31:41.022221      22 metrics_grabber.go:102] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
W0628 19:31:41.022420      22 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
Jun 28 19:31:41.022: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

Jun 28 19:31:41.022: INFO: Deleting pod "simpletest.rc-4c924" in namespace "gc-4540"
Jun 28 19:31:41.064: INFO: Deleting pod "simpletest.rc-6nrbt" in namespace "gc-4540"
Jun 28 19:31:41.111: INFO: Deleting pod "simpletest.rc-f7zxc" in namespace "gc-4540"
Jun 28 19:31:41.145: INFO: Deleting pod "simpletest.rc-h8ft9" in namespace "gc-4540"
Jun 28 19:31:41.183: INFO: Deleting pod "simpletest.rc-j2429" in namespace "gc-4540"
Jun 28 19:31:41.216: INFO: Deleting pod "simpletest.rc-j2f28" in namespace "gc-4540"
Jun 28 19:31:41.258: INFO: Deleting pod "simpletest.rc-jvs42" in namespace "gc-4540"
Jun 28 19:31:41.304: INFO: Deleting pod "simpletest.rc-s6bcc" in namespace "gc-4540"
Jun 28 19:31:41.370: INFO: Deleting pod "simpletest.rc-t5qlz" in namespace "gc-4540"
Jun 28 19:31:41.413: INFO: Deleting pod "simpletest.rc-zqzkh" in namespace "gc-4540"
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 19:31:41.448: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-4540" for this suite.

• [SLOW TEST:40.746 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan pods created by rc if delete options say so [Conformance]","total":305,"completed":241,"skipped":3703,"failed":0}
SSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a volume subpath [sig-storage] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 19:31:41.479: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a volume subpath [sig-storage] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test substitution in volume subpath
Jun 28 19:31:41.788: INFO: Waiting up to 5m0s for pod "var-expansion-00bfbdb5-35da-4e46-b409-0aab3f4e85e5" in namespace "var-expansion-2473" to be "Succeeded or Failed"
Jun 28 19:31:41.821: INFO: Pod "var-expansion-00bfbdb5-35da-4e46-b409-0aab3f4e85e5": Phase="Pending", Reason="", readiness=false. Elapsed: 32.766695ms
Jun 28 19:31:43.834: INFO: Pod "var-expansion-00bfbdb5-35da-4e46-b409-0aab3f4e85e5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.045804226s
Jun 28 19:31:45.851: INFO: Pod "var-expansion-00bfbdb5-35da-4e46-b409-0aab3f4e85e5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.062889596s
STEP: Saw pod success
Jun 28 19:31:45.851: INFO: Pod "var-expansion-00bfbdb5-35da-4e46-b409-0aab3f4e85e5" satisfied condition "Succeeded or Failed"
Jun 28 19:31:45.867: INFO: Trying to get logs from node 10.13.107.37 pod var-expansion-00bfbdb5-35da-4e46-b409-0aab3f4e85e5 container dapi-container: <nil>
STEP: delete the pod
Jun 28 19:31:45.945: INFO: Waiting for pod var-expansion-00bfbdb5-35da-4e46-b409-0aab3f4e85e5 to disappear
Jun 28 19:31:45.958: INFO: Pod var-expansion-00bfbdb5-35da-4e46-b409-0aab3f4e85e5 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 19:31:45.958: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-2473" for this suite.
•{"msg":"PASSED [k8s.io] Variable Expansion should allow substituting values in a volume subpath [sig-storage] [Conformance]","total":305,"completed":242,"skipped":3708,"failed":0}
SSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 19:31:45.998: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod pod-subpath-test-configmap-75kx
STEP: Creating a pod to test atomic-volume-subpath
Jun 28 19:31:46.226: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-75kx" in namespace "subpath-4778" to be "Succeeded or Failed"
Jun 28 19:31:46.240: INFO: Pod "pod-subpath-test-configmap-75kx": Phase="Pending", Reason="", readiness=false. Elapsed: 13.313737ms
Jun 28 19:31:48.252: INFO: Pod "pod-subpath-test-configmap-75kx": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025492137s
Jun 28 19:31:50.268: INFO: Pod "pod-subpath-test-configmap-75kx": Phase="Running", Reason="", readiness=true. Elapsed: 4.041199357s
Jun 28 19:31:52.281: INFO: Pod "pod-subpath-test-configmap-75kx": Phase="Running", Reason="", readiness=true. Elapsed: 6.054952474s
Jun 28 19:31:54.303: INFO: Pod "pod-subpath-test-configmap-75kx": Phase="Running", Reason="", readiness=true. Elapsed: 8.07665849s
Jun 28 19:31:56.318: INFO: Pod "pod-subpath-test-configmap-75kx": Phase="Running", Reason="", readiness=true. Elapsed: 10.091828528s
Jun 28 19:31:58.341: INFO: Pod "pod-subpath-test-configmap-75kx": Phase="Running", Reason="", readiness=true. Elapsed: 12.114829288s
Jun 28 19:32:00.360: INFO: Pod "pod-subpath-test-configmap-75kx": Phase="Running", Reason="", readiness=true. Elapsed: 14.133449094s
Jun 28 19:32:02.374: INFO: Pod "pod-subpath-test-configmap-75kx": Phase="Running", Reason="", readiness=true. Elapsed: 16.147805732s
Jun 28 19:32:04.391: INFO: Pod "pod-subpath-test-configmap-75kx": Phase="Running", Reason="", readiness=true. Elapsed: 18.165049746s
Jun 28 19:32:06.414: INFO: Pod "pod-subpath-test-configmap-75kx": Phase="Running", Reason="", readiness=true. Elapsed: 20.187402394s
Jun 28 19:32:08.450: INFO: Pod "pod-subpath-test-configmap-75kx": Phase="Running", Reason="", readiness=true. Elapsed: 22.223244s
Jun 28 19:32:10.484: INFO: Pod "pod-subpath-test-configmap-75kx": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.257965398s
STEP: Saw pod success
Jun 28 19:32:10.484: INFO: Pod "pod-subpath-test-configmap-75kx" satisfied condition "Succeeded or Failed"
Jun 28 19:32:10.494: INFO: Trying to get logs from node 10.13.107.37 pod pod-subpath-test-configmap-75kx container test-container-subpath-configmap-75kx: <nil>
STEP: delete the pod
Jun 28 19:32:10.551: INFO: Waiting for pod pod-subpath-test-configmap-75kx to disappear
Jun 28 19:32:10.562: INFO: Pod pod-subpath-test-configmap-75kx no longer exists
STEP: Deleting pod pod-subpath-test-configmap-75kx
Jun 28 19:32:10.562: INFO: Deleting pod "pod-subpath-test-configmap-75kx" in namespace "subpath-4778"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 19:32:10.572: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-4778" for this suite.

• [SLOW TEST:24.605 seconds]
[sig-storage] Subpath
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
    /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]","total":305,"completed":243,"skipped":3713,"failed":0}
SSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl label 
  should update the label on a resource  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 19:32:10.604: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[BeforeEach] Kubectl label
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1333
STEP: creating the pod
Jun 28 19:32:10.738: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-108209140 create -f - --namespace=kubectl-7054'
Jun 28 19:32:11.744: INFO: stderr: ""
Jun 28 19:32:11.744: INFO: stdout: "pod/pause created\n"
Jun 28 19:32:11.744: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Jun 28 19:32:11.744: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-7054" to be "running and ready"
Jun 28 19:32:11.758: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 13.628516ms
Jun 28 19:32:13.774: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.02964607s
Jun 28 19:32:13.774: INFO: Pod "pause" satisfied condition "running and ready"
Jun 28 19:32:13.774: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: adding the label testing-label with value testing-label-value to a pod
Jun 28 19:32:13.774: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-108209140 label pods pause testing-label=testing-label-value --namespace=kubectl-7054'
Jun 28 19:32:14.052: INFO: stderr: ""
Jun 28 19:32:14.052: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value
Jun 28 19:32:14.052: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-108209140 get pod pause -L testing-label --namespace=kubectl-7054'
Jun 28 19:32:14.233: INFO: stderr: ""
Jun 28 19:32:14.233: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          3s    testing-label-value\n"
STEP: removing the label testing-label of a pod
Jun 28 19:32:14.233: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-108209140 label pods pause testing-label- --namespace=kubectl-7054'
Jun 28 19:32:14.534: INFO: stderr: ""
Jun 28 19:32:14.534: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod doesn't have the label testing-label
Jun 28 19:32:14.534: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-108209140 get pod pause -L testing-label --namespace=kubectl-7054'
Jun 28 19:32:14.742: INFO: stderr: ""
Jun 28 19:32:14.742: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          3s    \n"
[AfterEach] Kubectl label
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1340
STEP: using delete to clean up resources
Jun 28 19:32:14.742: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-108209140 delete --grace-period=0 --force -f - --namespace=kubectl-7054'
Jun 28 19:32:14.945: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jun 28 19:32:14.945: INFO: stdout: "pod \"pause\" force deleted\n"
Jun 28 19:32:14.945: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-108209140 get rc,svc -l name=pause --no-headers --namespace=kubectl-7054'
Jun 28 19:32:15.180: INFO: stderr: "No resources found in kubectl-7054 namespace.\n"
Jun 28 19:32:15.180: INFO: stdout: ""
Jun 28 19:32:15.180: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-108209140 get pods -l name=pause --namespace=kubectl-7054 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jun 28 19:32:15.391: INFO: stderr: ""
Jun 28 19:32:15.391: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 19:32:15.391: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7054" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl label should update the label on a resource  [Conformance]","total":305,"completed":244,"skipped":3721,"failed":0}
SS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 19:32:15.428: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Jun 28 19:32:15.605: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ea9fc6ea-a806-44f2-b1c9-3ea3c0b318d9" in namespace "projected-2186" to be "Succeeded or Failed"
Jun 28 19:32:15.624: INFO: Pod "downwardapi-volume-ea9fc6ea-a806-44f2-b1c9-3ea3c0b318d9": Phase="Pending", Reason="", readiness=false. Elapsed: 19.066121ms
Jun 28 19:32:17.636: INFO: Pod "downwardapi-volume-ea9fc6ea-a806-44f2-b1c9-3ea3c0b318d9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.030362167s
STEP: Saw pod success
Jun 28 19:32:17.636: INFO: Pod "downwardapi-volume-ea9fc6ea-a806-44f2-b1c9-3ea3c0b318d9" satisfied condition "Succeeded or Failed"
Jun 28 19:32:17.646: INFO: Trying to get logs from node 10.13.107.37 pod downwardapi-volume-ea9fc6ea-a806-44f2-b1c9-3ea3c0b318d9 container client-container: <nil>
STEP: delete the pod
Jun 28 19:32:17.731: INFO: Waiting for pod downwardapi-volume-ea9fc6ea-a806-44f2-b1c9-3ea3c0b318d9 to disappear
Jun 28 19:32:17.741: INFO: Pod downwardapi-volume-ea9fc6ea-a806-44f2-b1c9-3ea3c0b318d9 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 19:32:17.741: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2186" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory limit [NodeConformance] [Conformance]","total":305,"completed":245,"skipped":3723,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl version 
  should check is all data is printed  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 19:32:17.774: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[It] should check is all data is printed  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jun 28 19:32:17.941: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-108209140 version'
Jun 28 19:32:18.097: INFO: stderr: ""
Jun 28 19:32:18.097: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"19\", GitVersion:\"v1.19.0\", GitCommit:\"e19964183377d0ec2052d1f1fa930c4d7575bd50\", GitTreeState:\"clean\", BuildDate:\"2020-08-26T14:30:33Z\", GoVersion:\"go1.15\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"19\", GitVersion:\"v1.19.0+c3e2e69\", GitCommit:\"c3e2e696c23f981fd8a80d66ccab8ccd2fb0ab7e\", GitTreeState:\"clean\", BuildDate:\"2021-06-03T21:43:45Z\", GoVersion:\"go1.15.7\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 19:32:18.098: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2439" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl version should check is all data is printed  [Conformance]","total":305,"completed":246,"skipped":3753,"failed":0}
SSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] 
  custom resource defaulting for requests and from storage works  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 19:32:18.131: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] custom resource defaulting for requests and from storage works  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jun 28 19:32:18.248: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 19:32:19.604: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-7031" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] custom resource defaulting for requests and from storage works  [Conformance]","total":305,"completed":247,"skipped":3757,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 19:32:19.637: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating 50 configmaps
STEP: Creating RC which spawns configmap-volume pods
Jun 28 19:32:20.702: INFO: Pod name wrapped-volume-race-eb2f88f0-76c9-4d24-a790-8de87dc2c19e: Found 0 pods out of 5
Jun 28 19:32:25.723: INFO: Pod name wrapped-volume-race-eb2f88f0-76c9-4d24-a790-8de87dc2c19e: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-eb2f88f0-76c9-4d24-a790-8de87dc2c19e in namespace emptydir-wrapper-8271, will wait for the garbage collector to delete the pods
Jun 28 19:32:25.881: INFO: Deleting ReplicationController wrapped-volume-race-eb2f88f0-76c9-4d24-a790-8de87dc2c19e took: 25.098417ms
Jun 28 19:32:25.981: INFO: Terminating ReplicationController wrapped-volume-race-eb2f88f0-76c9-4d24-a790-8de87dc2c19e pods took: 100.224983ms
STEP: Creating RC which spawns configmap-volume pods
Jun 28 19:32:37.553: INFO: Pod name wrapped-volume-race-33e70cdc-ffcf-4e31-b324-3d89279222bf: Found 0 pods out of 5
Jun 28 19:32:42.571: INFO: Pod name wrapped-volume-race-33e70cdc-ffcf-4e31-b324-3d89279222bf: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-33e70cdc-ffcf-4e31-b324-3d89279222bf in namespace emptydir-wrapper-8271, will wait for the garbage collector to delete the pods
Jun 28 19:32:42.726: INFO: Deleting ReplicationController wrapped-volume-race-33e70cdc-ffcf-4e31-b324-3d89279222bf took: 21.45336ms
Jun 28 19:32:42.827: INFO: Terminating ReplicationController wrapped-volume-race-33e70cdc-ffcf-4e31-b324-3d89279222bf pods took: 100.323321ms
STEP: Creating RC which spawns configmap-volume pods
Jun 28 19:32:57.398: INFO: Pod name wrapped-volume-race-cd4798cb-c8e7-43eb-ab97-81de37473151: Found 0 pods out of 5
Jun 28 19:33:02.424: INFO: Pod name wrapped-volume-race-cd4798cb-c8e7-43eb-ab97-81de37473151: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-cd4798cb-c8e7-43eb-ab97-81de37473151 in namespace emptydir-wrapper-8271, will wait for the garbage collector to delete the pods
Jun 28 19:33:02.581: INFO: Deleting ReplicationController wrapped-volume-race-cd4798cb-c8e7-43eb-ab97-81de37473151 took: 38.407283ms
Jun 28 19:33:02.785: INFO: Terminating ReplicationController wrapped-volume-race-cd4798cb-c8e7-43eb-ab97-81de37473151 pods took: 203.902861ms
STEP: Cleaning up the configMaps
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 19:33:19.241: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-8271" for this suite.

• [SLOW TEST:59.641 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not cause race condition when used for configmaps [Serial] [Conformance]","total":305,"completed":248,"skipped":3812,"failed":0}
SSSS
------------------------------
[sig-apps] ReplicationController 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 19:33:19.279: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating replication controller my-hostname-basic-a0809a20-e6e2-438c-8629-715653522435
Jun 28 19:33:19.430: INFO: Pod name my-hostname-basic-a0809a20-e6e2-438c-8629-715653522435: Found 0 pods out of 1
Jun 28 19:33:24.442: INFO: Pod name my-hostname-basic-a0809a20-e6e2-438c-8629-715653522435: Found 1 pods out of 1
Jun 28 19:33:24.442: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-a0809a20-e6e2-438c-8629-715653522435" are running
Jun 28 19:33:24.451: INFO: Pod "my-hostname-basic-a0809a20-e6e2-438c-8629-715653522435-zhj2v" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-06-28 19:33:19 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-06-28 19:33:20 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-06-28 19:33:20 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-06-28 19:33:19 +0000 UTC Reason: Message:}])
Jun 28 19:33:24.452: INFO: Trying to dial the pod
Jun 28 19:33:29.543: INFO: Controller my-hostname-basic-a0809a20-e6e2-438c-8629-715653522435: Got expected result from replica 1 [my-hostname-basic-a0809a20-e6e2-438c-8629-715653522435-zhj2v]: "my-hostname-basic-a0809a20-e6e2-438c-8629-715653522435-zhj2v", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 19:33:29.543: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-655" for this suite.

• [SLOW TEST:10.306 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should serve a basic image on each replica with a public image  [Conformance]","total":305,"completed":249,"skipped":3816,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox Pod with hostAliases 
  should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 19:33:29.585: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[It] should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 19:33:33.899: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-4487" for this suite.
•{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox Pod with hostAliases should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":250,"skipped":3831,"failed":0}
S
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing mutating webhooks should work [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 19:33:33.934: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jun 28 19:33:34.600: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun 28 19:33:37.673: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing mutating webhooks should work [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that should be mutated
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that should not be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 19:33:38.273: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1486" for this suite.
STEP: Destroying namespace "webhook-1486-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing mutating webhooks should work [Conformance]","total":305,"completed":251,"skipped":3832,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-apps] Job 
  should delete a job [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Job
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 19:33:38.464: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a job [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: delete a job
STEP: deleting Job.batch foo in namespace job-4277, will wait for the garbage collector to delete the pods
Jun 28 19:33:42.767: INFO: Deleting Job.batch foo took: 24.868384ms
Jun 28 19:33:42.867: INFO: Terminating Job.batch foo pods took: 100.354516ms
STEP: Ensuring job was deleted
[AfterEach] [sig-apps] Job
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 19:34:17.289: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-4277" for this suite.

• [SLOW TEST:38.872 seconds]
[sig-apps] Job
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should delete a job [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Job should delete a job [Conformance]","total":305,"completed":252,"skipped":3843,"failed":0}
SS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 19:34:17.350: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Jun 28 19:34:17.508: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f5149e1e-4a65-43e4-9d03-5d70c822a11c" in namespace "downward-api-7905" to be "Succeeded or Failed"
Jun 28 19:34:17.517: INFO: Pod "downwardapi-volume-f5149e1e-4a65-43e4-9d03-5d70c822a11c": Phase="Pending", Reason="", readiness=false. Elapsed: 9.181512ms
Jun 28 19:34:19.534: INFO: Pod "downwardapi-volume-f5149e1e-4a65-43e4-9d03-5d70c822a11c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.02549298s
STEP: Saw pod success
Jun 28 19:34:19.534: INFO: Pod "downwardapi-volume-f5149e1e-4a65-43e4-9d03-5d70c822a11c" satisfied condition "Succeeded or Failed"
Jun 28 19:34:19.545: INFO: Trying to get logs from node 10.13.107.37 pod downwardapi-volume-f5149e1e-4a65-43e4-9d03-5d70c822a11c container client-container: <nil>
STEP: delete the pod
Jun 28 19:34:19.606: INFO: Waiting for pod downwardapi-volume-f5149e1e-4a65-43e4-9d03-5d70c822a11c to disappear
Jun 28 19:34:19.616: INFO: Pod downwardapi-volume-f5149e1e-4a65-43e4-9d03-5d70c822a11c no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 19:34:19.616: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7905" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu request [NodeConformance] [Conformance]","total":305,"completed":253,"skipped":3845,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 19:34:19.651: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0777 on tmpfs
Jun 28 19:34:19.832: INFO: Waiting up to 5m0s for pod "pod-acc85ad4-a79c-4ae9-bc2c-45a17f049f66" in namespace "emptydir-8865" to be "Succeeded or Failed"
Jun 28 19:34:19.842: INFO: Pod "pod-acc85ad4-a79c-4ae9-bc2c-45a17f049f66": Phase="Pending", Reason="", readiness=false. Elapsed: 9.509663ms
Jun 28 19:34:21.859: INFO: Pod "pod-acc85ad4-a79c-4ae9-bc2c-45a17f049f66": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02696875s
Jun 28 19:34:23.874: INFO: Pod "pod-acc85ad4-a79c-4ae9-bc2c-45a17f049f66": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.041698144s
STEP: Saw pod success
Jun 28 19:34:23.889: INFO: Pod "pod-acc85ad4-a79c-4ae9-bc2c-45a17f049f66" satisfied condition "Succeeded or Failed"
Jun 28 19:34:23.899: INFO: Trying to get logs from node 10.13.107.37 pod pod-acc85ad4-a79c-4ae9-bc2c-45a17f049f66 container test-container: <nil>
STEP: delete the pod
Jun 28 19:34:23.957: INFO: Waiting for pod pod-acc85ad4-a79c-4ae9-bc2c-45a17f049f66 to disappear
Jun 28 19:34:23.968: INFO: Pod pod-acc85ad4-a79c-4ae9-bc2c-45a17f049f66 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 19:34:23.968: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8865" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":254,"skipped":3906,"failed":0}

------------------------------
[sig-network] Service endpoints latency 
  should not be very high  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 19:34:23.999: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename svc-latency
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be very high  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jun 28 19:34:24.119: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: creating replication controller svc-latency-rc in namespace svc-latency-4318
I0628 19:34:24.153661      22 runners.go:190] Created replication controller with name: svc-latency-rc, namespace: svc-latency-4318, replica count: 1
I0628 19:34:25.204855      22 runners.go:190] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0628 19:34:26.205571      22 runners.go:190] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jun 28 19:34:26.339: INFO: Created: latency-svc-7stz5
Jun 28 19:34:26.364: INFO: Got endpoints: latency-svc-7stz5 [58.080774ms]
Jun 28 19:34:26.395: INFO: Created: latency-svc-2spbm
Jun 28 19:34:26.407: INFO: Created: latency-svc-5skql
Jun 28 19:34:26.407: INFO: Got endpoints: latency-svc-2spbm [42.911407ms]
Jun 28 19:34:26.427: INFO: Got endpoints: latency-svc-5skql [60.350027ms]
Jun 28 19:34:26.436: INFO: Created: latency-svc-2tm8d
Jun 28 19:34:26.436: INFO: Got endpoints: latency-svc-2tm8d [70.101427ms]
Jun 28 19:34:26.453: INFO: Created: latency-svc-l2ft2
Jun 28 19:34:26.455: INFO: Got endpoints: latency-svc-l2ft2 [89.011563ms]
Jun 28 19:34:26.458: INFO: Created: latency-svc-tcv2n
Jun 28 19:34:26.475: INFO: Got endpoints: latency-svc-tcv2n [110.051977ms]
Jun 28 19:34:26.483: INFO: Created: latency-svc-lvk55
Jun 28 19:34:26.491: INFO: Got endpoints: latency-svc-lvk55 [125.083639ms]
Jun 28 19:34:26.497: INFO: Created: latency-svc-bh4k8
Jun 28 19:34:26.517: INFO: Created: latency-svc-8kvcp
Jun 28 19:34:26.517: INFO: Got endpoints: latency-svc-bh4k8 [151.066435ms]
Jun 28 19:34:26.528: INFO: Got endpoints: latency-svc-8kvcp [161.0787ms]
Jun 28 19:34:26.528: INFO: Created: latency-svc-dbjw5
Jun 28 19:34:26.533: INFO: Created: latency-svc-v56v7
Jun 28 19:34:26.536: INFO: Got endpoints: latency-svc-dbjw5 [169.820981ms]
Jun 28 19:34:26.550: INFO: Got endpoints: latency-svc-v56v7 [182.425549ms]
Jun 28 19:34:26.550: INFO: Created: latency-svc-fpngb
Jun 28 19:34:26.553: INFO: Got endpoints: latency-svc-fpngb [185.42838ms]
Jun 28 19:34:26.554: INFO: Created: latency-svc-zkmlw
Jun 28 19:34:26.565: INFO: Got endpoints: latency-svc-zkmlw [197.451272ms]
Jun 28 19:34:26.566: INFO: Created: latency-svc-9lklk
Jun 28 19:34:26.577: INFO: Created: latency-svc-5c4sw
Jun 28 19:34:26.577: INFO: Got endpoints: latency-svc-9lklk [210.545066ms]
Jun 28 19:34:26.585: INFO: Created: latency-svc-zm5tv
Jun 28 19:34:26.593: INFO: Got endpoints: latency-svc-5c4sw [225.311765ms]
Jun 28 19:34:26.605: INFO: Created: latency-svc-xvsmc
Jun 28 19:34:26.605: INFO: Got endpoints: latency-svc-zm5tv [237.398431ms]
Jun 28 19:34:26.625: INFO: Got endpoints: latency-svc-xvsmc [217.864375ms]
Jun 28 19:34:26.626: INFO: Created: latency-svc-vhthz
Jun 28 19:34:26.630: INFO: Created: latency-svc-jxb54
Jun 28 19:34:26.630: INFO: Got endpoints: latency-svc-vhthz [203.346556ms]
Jun 28 19:34:26.641: INFO: Got endpoints: latency-svc-jxb54 [204.394019ms]
Jun 28 19:34:26.644: INFO: Created: latency-svc-qzp44
Jun 28 19:34:26.653: INFO: Created: latency-svc-ngmkt
Jun 28 19:34:26.654: INFO: Got endpoints: latency-svc-qzp44 [198.873414ms]
Jun 28 19:34:26.664: INFO: Got endpoints: latency-svc-ngmkt [188.357358ms]
Jun 28 19:34:26.665: INFO: Created: latency-svc-nsqkk
Jun 28 19:34:26.674: INFO: Created: latency-svc-bbww9
Jun 28 19:34:26.676: INFO: Got endpoints: latency-svc-nsqkk [184.812223ms]
Jun 28 19:34:26.685: INFO: Created: latency-svc-mr7ph
Jun 28 19:34:26.687: INFO: Got endpoints: latency-svc-bbww9 [169.287341ms]
Jun 28 19:34:26.697: INFO: Created: latency-svc-gztrh
Jun 28 19:34:26.698: INFO: Got endpoints: latency-svc-mr7ph [170.006278ms]
Jun 28 19:34:26.709: INFO: Created: latency-svc-v6rvv
Jun 28 19:34:26.709: INFO: Got endpoints: latency-svc-gztrh [173.089442ms]
Jun 28 19:34:26.716: INFO: Got endpoints: latency-svc-v6rvv [164.850859ms]
Jun 28 19:34:26.720: INFO: Created: latency-svc-lrxrr
Jun 28 19:34:26.727: INFO: Created: latency-svc-77jf5
Jun 28 19:34:26.730: INFO: Got endpoints: latency-svc-lrxrr [176.156617ms]
Jun 28 19:34:26.737: INFO: Created: latency-svc-72n7g
Jun 28 19:34:26.739: INFO: Got endpoints: latency-svc-77jf5 [174.291914ms]
Jun 28 19:34:26.746: INFO: Got endpoints: latency-svc-72n7g [169.304222ms]
Jun 28 19:34:26.748: INFO: Created: latency-svc-mtcz6
Jun 28 19:34:26.756: INFO: Created: latency-svc-jgnvf
Jun 28 19:34:26.760: INFO: Got endpoints: latency-svc-mtcz6 [145.026299ms]
Jun 28 19:34:26.768: INFO: Created: latency-svc-fn9hm
Jun 28 19:34:26.768: INFO: Got endpoints: latency-svc-jgnvf [153.679765ms]
Jun 28 19:34:26.778: INFO: Created: latency-svc-42fhq
Jun 28 19:34:26.778: INFO: Got endpoints: latency-svc-fn9hm [153.264032ms]
Jun 28 19:34:26.789: INFO: Got endpoints: latency-svc-42fhq [158.630048ms]
Jun 28 19:34:26.794: INFO: Created: latency-svc-vr4w5
Jun 28 19:34:26.799: INFO: Created: latency-svc-r9dp7
Jun 28 19:34:26.803: INFO: Got endpoints: latency-svc-vr4w5 [162.437938ms]
Jun 28 19:34:26.817: INFO: Created: latency-svc-xz2bz
Jun 28 19:34:26.819: INFO: Got endpoints: latency-svc-r9dp7 [165.22059ms]
Jun 28 19:34:26.820: INFO: Got endpoints: latency-svc-xz2bz [156.009661ms]
Jun 28 19:34:26.826: INFO: Created: latency-svc-pjxm5
Jun 28 19:34:26.836: INFO: Created: latency-svc-7hhzl
Jun 28 19:34:26.837: INFO: Got endpoints: latency-svc-pjxm5 [161.10121ms]
Jun 28 19:34:26.847: INFO: Created: latency-svc-j4r6q
Jun 28 19:34:26.849: INFO: Got endpoints: latency-svc-7hhzl [161.949272ms]
Jun 28 19:34:26.861: INFO: Created: latency-svc-q6mqf
Jun 28 19:34:26.862: INFO: Got endpoints: latency-svc-j4r6q [163.674553ms]
Jun 28 19:34:26.874: INFO: Got endpoints: latency-svc-q6mqf [164.62289ms]
Jun 28 19:34:26.875: INFO: Created: latency-svc-njw8p
Jun 28 19:34:26.882: INFO: Created: latency-svc-phpct
Jun 28 19:34:26.882: INFO: Got endpoints: latency-svc-njw8p [165.128616ms]
Jun 28 19:34:26.899: INFO: Got endpoints: latency-svc-phpct [169.061394ms]
Jun 28 19:34:26.900: INFO: Created: latency-svc-hsgv6
Jun 28 19:34:26.907: INFO: Created: latency-svc-jpj48
Jun 28 19:34:26.915: INFO: Created: latency-svc-chkct
Jun 28 19:34:26.915: INFO: Got endpoints: latency-svc-hsgv6 [175.819156ms]
Jun 28 19:34:26.918: INFO: Got endpoints: latency-svc-jpj48 [171.467911ms]
Jun 28 19:34:26.923: INFO: Got endpoints: latency-svc-chkct [162.567378ms]
Jun 28 19:34:26.929: INFO: Created: latency-svc-v8ldr
Jun 28 19:34:26.940: INFO: Created: latency-svc-ch24v
Jun 28 19:34:26.940: INFO: Got endpoints: latency-svc-v8ldr [171.842588ms]
Jun 28 19:34:26.950: INFO: Created: latency-svc-mknmp
Jun 28 19:34:26.951: INFO: Got endpoints: latency-svc-ch24v [171.929252ms]
Jun 28 19:34:26.960: INFO: Got endpoints: latency-svc-mknmp [170.864538ms]
Jun 28 19:34:26.961: INFO: Created: latency-svc-bl494
Jun 28 19:34:26.973: INFO: Got endpoints: latency-svc-bl494 [169.116337ms]
Jun 28 19:34:26.973: INFO: Created: latency-svc-gvn86
Jun 28 19:34:26.985: INFO: Got endpoints: latency-svc-gvn86 [165.406679ms]
Jun 28 19:34:26.985: INFO: Created: latency-svc-zx94k
Jun 28 19:34:26.992: INFO: Got endpoints: latency-svc-zx94k [172.06926ms]
Jun 28 19:34:26.993: INFO: Created: latency-svc-5vkh2
Jun 28 19:34:27.007: INFO: Got endpoints: latency-svc-5vkh2 [170.264379ms]
Jun 28 19:34:27.008: INFO: Created: latency-svc-rxsnv
Jun 28 19:34:27.019: INFO: Created: latency-svc-42df7
Jun 28 19:34:27.019: INFO: Got endpoints: latency-svc-rxsnv [169.739356ms]
Jun 28 19:34:27.024: INFO: Created: latency-svc-6gtf9
Jun 28 19:34:27.051: INFO: Created: latency-svc-xrmrj
Jun 28 19:34:27.052: INFO: Got endpoints: latency-svc-6gtf9 [177.251388ms]
Jun 28 19:34:27.052: INFO: Got endpoints: latency-svc-42df7 [190.564088ms]
Jun 28 19:34:27.053: INFO: Got endpoints: latency-svc-xrmrj [170.492702ms]
Jun 28 19:34:27.053: INFO: Created: latency-svc-sbvpk
Jun 28 19:34:27.062: INFO: Got endpoints: latency-svc-sbvpk [162.553865ms]
Jun 28 19:34:27.063: INFO: Created: latency-svc-7m8hj
Jun 28 19:34:27.071: INFO: Created: latency-svc-d458l
Jun 28 19:34:27.076: INFO: Got endpoints: latency-svc-7m8hj [157.641723ms]
Jun 28 19:34:27.084: INFO: Created: latency-svc-w4m5k
Jun 28 19:34:27.085: INFO: Got endpoints: latency-svc-d458l [169.555476ms]
Jun 28 19:34:27.105: INFO: Got endpoints: latency-svc-w4m5k [181.783897ms]
Jun 28 19:34:27.108: INFO: Created: latency-svc-h5q87
Jun 28 19:34:27.115: INFO: Got endpoints: latency-svc-h5q87 [173.260609ms]
Jun 28 19:34:27.116: INFO: Created: latency-svc-5fbzl
Jun 28 19:34:27.119: INFO: Created: latency-svc-wdd7z
Jun 28 19:34:27.125: INFO: Got endpoints: latency-svc-5fbzl [174.421203ms]
Jun 28 19:34:27.136: INFO: Created: latency-svc-r522s
Jun 28 19:34:27.137: INFO: Got endpoints: latency-svc-wdd7z [176.685024ms]
Jun 28 19:34:27.144: INFO: Got endpoints: latency-svc-r522s [170.266313ms]
Jun 28 19:34:27.148: INFO: Created: latency-svc-p5xnf
Jun 28 19:34:27.158: INFO: Created: latency-svc-x8pxf
Jun 28 19:34:27.159: INFO: Got endpoints: latency-svc-p5xnf [174.014356ms]
Jun 28 19:34:27.167: INFO: Created: latency-svc-9htq7
Jun 28 19:34:27.168: INFO: Got endpoints: latency-svc-x8pxf [175.88029ms]
Jun 28 19:34:27.183: INFO: Created: latency-svc-tk62x
Jun 28 19:34:27.184: INFO: Got endpoints: latency-svc-9htq7 [176.380722ms]
Jun 28 19:34:27.191: INFO: Got endpoints: latency-svc-tk62x [172.051027ms]
Jun 28 19:34:27.192: INFO: Created: latency-svc-s9mtg
Jun 28 19:34:27.197: INFO: Got endpoints: latency-svc-s9mtg [145.440994ms]
Jun 28 19:34:27.200: INFO: Created: latency-svc-wzcwb
Jun 28 19:34:27.208: INFO: Created: latency-svc-9jfvj
Jun 28 19:34:27.209: INFO: Got endpoints: latency-svc-wzcwb [155.67171ms]
Jun 28 19:34:27.218: INFO: Got endpoints: latency-svc-9jfvj [165.559864ms]
Jun 28 19:34:27.219: INFO: Created: latency-svc-68tgg
Jun 28 19:34:27.233: INFO: Created: latency-svc-vsg2s
Jun 28 19:34:27.234: INFO: Got endpoints: latency-svc-68tgg [171.731114ms]
Jun 28 19:34:27.247: INFO: Created: latency-svc-6dmct
Jun 28 19:34:27.248: INFO: Got endpoints: latency-svc-vsg2s [172.009348ms]
Jun 28 19:34:27.250: INFO: Got endpoints: latency-svc-6dmct [164.474747ms]
Jun 28 19:34:27.251: INFO: Created: latency-svc-qnqq2
Jun 28 19:34:27.267: INFO: Created: latency-svc-7dmqr
Jun 28 19:34:27.268: INFO: Got endpoints: latency-svc-qnqq2 [163.079344ms]
Jun 28 19:34:27.279: INFO: Created: latency-svc-4tlz6
Jun 28 19:34:27.282: INFO: Got endpoints: latency-svc-7dmqr [167.094685ms]
Jun 28 19:34:27.290: INFO: Created: latency-svc-xmpdk
Jun 28 19:34:27.291: INFO: Got endpoints: latency-svc-4tlz6 [165.926489ms]
Jun 28 19:34:27.306: INFO: Got endpoints: latency-svc-xmpdk [169.358157ms]
Jun 28 19:34:27.307: INFO: Created: latency-svc-l6gkd
Jun 28 19:34:27.309: INFO: Created: latency-svc-xkxl2
Jun 28 19:34:27.314: INFO: Got endpoints: latency-svc-l6gkd [170.678259ms]
Jun 28 19:34:27.321: INFO: Got endpoints: latency-svc-xkxl2 [162.005473ms]
Jun 28 19:34:27.325: INFO: Created: latency-svc-5kwmj
Jun 28 19:34:27.329: INFO: Created: latency-svc-288nv
Jun 28 19:34:27.334: INFO: Got endpoints: latency-svc-5kwmj [164.924216ms]
Jun 28 19:34:27.338: INFO: Created: latency-svc-qdh7b
Jun 28 19:34:27.343: INFO: Got endpoints: latency-svc-288nv [158.400062ms]
Jun 28 19:34:27.349: INFO: Created: latency-svc-xgk9t
Jun 28 19:34:27.350: INFO: Got endpoints: latency-svc-qdh7b [158.297376ms]
Jun 28 19:34:27.359: INFO: Got endpoints: latency-svc-xgk9t [160.46591ms]
Jun 28 19:34:27.361: INFO: Created: latency-svc-rkt7m
Jun 28 19:34:27.375: INFO: Got endpoints: latency-svc-rkt7m [166.544127ms]
Jun 28 19:34:27.376: INFO: Created: latency-svc-q8hhx
Jun 28 19:34:27.386: INFO: Created: latency-svc-tmx6t
Jun 28 19:34:27.386: INFO: Got endpoints: latency-svc-q8hhx [168.397023ms]
Jun 28 19:34:27.395: INFO: Got endpoints: latency-svc-tmx6t [161.333945ms]
Jun 28 19:34:27.402: INFO: Created: latency-svc-7v26p
Jun 28 19:34:27.402: INFO: Got endpoints: latency-svc-7v26p [154.403971ms]
Jun 28 19:34:27.411: INFO: Created: latency-svc-njglb
Jun 28 19:34:27.411: INFO: Created: latency-svc-cflnx
Jun 28 19:34:27.417: INFO: Got endpoints: latency-svc-njglb [167.269937ms]
Jun 28 19:34:27.420: INFO: Got endpoints: latency-svc-cflnx [151.694924ms]
Jun 28 19:34:27.422: INFO: Created: latency-svc-wwff9
Jun 28 19:34:27.431: INFO: Created: latency-svc-djjn7
Jun 28 19:34:27.433: INFO: Got endpoints: latency-svc-wwff9 [150.798044ms]
Jun 28 19:34:27.440: INFO: Created: latency-svc-dplw9
Jun 28 19:34:27.443: INFO: Got endpoints: latency-svc-djjn7 [151.430617ms]
Jun 28 19:34:27.453: INFO: Created: latency-svc-fzbhz
Jun 28 19:34:27.453: INFO: Got endpoints: latency-svc-dplw9 [147.119729ms]
Jun 28 19:34:27.461: INFO: Got endpoints: latency-svc-fzbhz [146.255014ms]
Jun 28 19:34:27.463: INFO: Created: latency-svc-tjrxx
Jun 28 19:34:27.472: INFO: Created: latency-svc-gbs29
Jun 28 19:34:27.474: INFO: Got endpoints: latency-svc-tjrxx [153.011621ms]
Jun 28 19:34:27.487: INFO: Created: latency-svc-k2n98
Jun 28 19:34:27.487: INFO: Got endpoints: latency-svc-gbs29 [153.759928ms]
Jun 28 19:34:27.491: INFO: Created: latency-svc-7lkz7
Jun 28 19:34:27.498: INFO: Got endpoints: latency-svc-k2n98 [155.464178ms]
Jun 28 19:34:27.504: INFO: Created: latency-svc-2ckg4
Jun 28 19:34:27.505: INFO: Got endpoints: latency-svc-7lkz7 [155.497189ms]
Jun 28 19:34:27.511: INFO: Created: latency-svc-kqth4
Jun 28 19:34:27.514: INFO: Got endpoints: latency-svc-2ckg4 [155.508836ms]
Jun 28 19:34:27.524: INFO: Got endpoints: latency-svc-kqth4 [149.044701ms]
Jun 28 19:34:27.525: INFO: Created: latency-svc-sws4f
Jun 28 19:34:27.533: INFO: Created: latency-svc-v7rg9
Jun 28 19:34:27.537: INFO: Got endpoints: latency-svc-sws4f [150.243726ms]
Jun 28 19:34:27.544: INFO: Created: latency-svc-kqhsk
Jun 28 19:34:27.544: INFO: Got endpoints: latency-svc-v7rg9 [149.191048ms]
Jun 28 19:34:27.554: INFO: Got endpoints: latency-svc-kqhsk [151.069488ms]
Jun 28 19:34:27.556: INFO: Created: latency-svc-z2tps
Jun 28 19:34:27.563: INFO: Created: latency-svc-252w4
Jun 28 19:34:27.574: INFO: Created: latency-svc-tdxpg
Jun 28 19:34:27.585: INFO: Created: latency-svc-djltq
Jun 28 19:34:27.600: INFO: Created: latency-svc-5m6ft
Jun 28 19:34:27.603: INFO: Created: latency-svc-v2n8k
Jun 28 19:34:27.607: INFO: Got endpoints: latency-svc-z2tps [189.820703ms]
Jun 28 19:34:27.612: INFO: Got endpoints: latency-svc-252w4 [191.242829ms]
Jun 28 19:34:27.619: INFO: Created: latency-svc-d7gsn
Jun 28 19:34:27.620: INFO: Got endpoints: latency-svc-djltq [176.958511ms]
Jun 28 19:34:27.620: INFO: Got endpoints: latency-svc-tdxpg [186.8944ms]
Jun 28 19:34:27.621: INFO: Got endpoints: latency-svc-v2n8k [160.314995ms]
Jun 28 19:34:27.622: INFO: Got endpoints: latency-svc-5m6ft [168.140625ms]
Jun 28 19:34:27.631: INFO: Created: latency-svc-sfhzd
Jun 28 19:34:27.631: INFO: Got endpoints: latency-svc-d7gsn [156.688254ms]
Jun 28 19:34:27.640: INFO: Got endpoints: latency-svc-sfhzd [152.597079ms]
Jun 28 19:34:27.641: INFO: Created: latency-svc-n56cd
Jun 28 19:34:27.648: INFO: Got endpoints: latency-svc-n56cd [149.889968ms]
Jun 28 19:34:27.655: INFO: Created: latency-svc-x7ck7
Jun 28 19:34:27.656: INFO: Created: latency-svc-bj7q8
Jun 28 19:34:27.659: INFO: Got endpoints: latency-svc-x7ck7 [153.495869ms]
Jun 28 19:34:27.666: INFO: Created: latency-svc-n554f
Jun 28 19:34:27.669: INFO: Got endpoints: latency-svc-bj7q8 [154.334479ms]
Jun 28 19:34:27.679: INFO: Got endpoints: latency-svc-n554f [154.118652ms]
Jun 28 19:34:27.679: INFO: Created: latency-svc-q2xnh
Jun 28 19:34:27.691: INFO: Got endpoints: latency-svc-q2xnh [154.31206ms]
Jun 28 19:34:27.692: INFO: Created: latency-svc-shcfk
Jun 28 19:34:27.701: INFO: Got endpoints: latency-svc-shcfk [156.430125ms]
Jun 28 19:34:27.702: INFO: Created: latency-svc-jsjmf
Jun 28 19:34:27.712: INFO: Created: latency-svc-5llqc
Jun 28 19:34:27.712: INFO: Got endpoints: latency-svc-jsjmf [157.264234ms]
Jun 28 19:34:27.720: INFO: Created: latency-svc-74kb4
Jun 28 19:34:27.724: INFO: Got endpoints: latency-svc-5llqc [116.79273ms]
Jun 28 19:34:27.732: INFO: Created: latency-svc-9j5mf
Jun 28 19:34:27.732: INFO: Got endpoints: latency-svc-74kb4 [120.217909ms]
Jun 28 19:34:27.741: INFO: Got endpoints: latency-svc-9j5mf [120.556948ms]
Jun 28 19:34:27.744: INFO: Created: latency-svc-x9t6q
Jun 28 19:34:27.753: INFO: Created: latency-svc-hxzv2
Jun 28 19:34:27.754: INFO: Got endpoints: latency-svc-x9t6q [131.89094ms]
Jun 28 19:34:27.762: INFO: Got endpoints: latency-svc-hxzv2 [141.019505ms]
Jun 28 19:34:27.765: INFO: Created: latency-svc-sggng
Jun 28 19:34:27.776: INFO: Got endpoints: latency-svc-sggng [155.063808ms]
Jun 28 19:34:27.777: INFO: Created: latency-svc-cff5g
Jun 28 19:34:27.782: INFO: Created: latency-svc-5pg9s
Jun 28 19:34:27.784: INFO: Got endpoints: latency-svc-cff5g [153.302321ms]
Jun 28 19:34:27.791: INFO: Created: latency-svc-jh4s4
Jun 28 19:34:27.793: INFO: Got endpoints: latency-svc-5pg9s [152.26944ms]
Jun 28 19:34:27.801: INFO: Created: latency-svc-zcqkq
Jun 28 19:34:27.802: INFO: Got endpoints: latency-svc-jh4s4 [152.984987ms]
Jun 28 19:34:27.813: INFO: Created: latency-svc-k7l89
Jun 28 19:34:27.813: INFO: Got endpoints: latency-svc-zcqkq [150.577016ms]
Jun 28 19:34:27.832: INFO: Created: latency-svc-q494q
Jun 28 19:34:27.832: INFO: Got endpoints: latency-svc-k7l89 [163.394986ms]
Jun 28 19:34:27.835: INFO: Created: latency-svc-745vd
Jun 28 19:34:27.837: INFO: Got endpoints: latency-svc-q494q [158.575584ms]
Jun 28 19:34:27.844: INFO: Got endpoints: latency-svc-745vd [152.586848ms]
Jun 28 19:34:27.849: INFO: Created: latency-svc-pvc2g
Jun 28 19:34:27.858: INFO: Created: latency-svc-8npjd
Jun 28 19:34:27.861: INFO: Got endpoints: latency-svc-pvc2g [159.618752ms]
Jun 28 19:34:27.873: INFO: Got endpoints: latency-svc-8npjd [160.910003ms]
Jun 28 19:34:27.874: INFO: Created: latency-svc-5stzh
Jun 28 19:34:27.883: INFO: Created: latency-svc-8lhdt
Jun 28 19:34:27.886: INFO: Got endpoints: latency-svc-5stzh [161.938825ms]
Jun 28 19:34:27.894: INFO: Got endpoints: latency-svc-8lhdt [160.817887ms]
Jun 28 19:34:27.895: INFO: Created: latency-svc-v8gs5
Jun 28 19:34:27.905: INFO: Got endpoints: latency-svc-v8gs5 [162.702804ms]
Jun 28 19:34:27.905: INFO: Created: latency-svc-6kcmr
Jun 28 19:34:27.915: INFO: Got endpoints: latency-svc-6kcmr [161.194144ms]
Jun 28 19:34:27.917: INFO: Created: latency-svc-dsv4n
Jun 28 19:34:27.925: INFO: Got endpoints: latency-svc-dsv4n [162.610342ms]
Jun 28 19:34:27.926: INFO: Created: latency-svc-s6t79
Jun 28 19:34:27.934: INFO: Got endpoints: latency-svc-s6t79 [158.405445ms]
Jun 28 19:34:27.935: INFO: Created: latency-svc-cwsn4
Jun 28 19:34:27.943: INFO: Created: latency-svc-6mhwv
Jun 28 19:34:27.945: INFO: Got endpoints: latency-svc-cwsn4 [157.099146ms]
Jun 28 19:34:27.954: INFO: Got endpoints: latency-svc-6mhwv [161.583985ms]
Jun 28 19:34:27.955: INFO: Created: latency-svc-4xbgf
Jun 28 19:34:27.965: INFO: Got endpoints: latency-svc-4xbgf [163.199324ms]
Jun 28 19:34:27.966: INFO: Created: latency-svc-9d7nh
Jun 28 19:34:27.977: INFO: Got endpoints: latency-svc-9d7nh [163.573673ms]
Jun 28 19:34:27.977: INFO: Created: latency-svc-dvszt
Jun 28 19:34:27.987: INFO: Created: latency-svc-8dm9x
Jun 28 19:34:27.987: INFO: Got endpoints: latency-svc-dvszt [153.213699ms]
Jun 28 19:34:27.995: INFO: Created: latency-svc-5kl6m
Jun 28 19:34:27.996: INFO: Got endpoints: latency-svc-8dm9x [158.674448ms]
Jun 28 19:34:28.004: INFO: Created: latency-svc-tvg4t
Jun 28 19:34:28.006: INFO: Got endpoints: latency-svc-5kl6m [161.501145ms]
Jun 28 19:34:28.014: INFO: Created: latency-svc-6bsxx
Jun 28 19:34:28.014: INFO: Got endpoints: latency-svc-tvg4t [153.621077ms]
Jun 28 19:34:28.023: INFO: Created: latency-svc-2mhkz
Jun 28 19:34:28.025: INFO: Got endpoints: latency-svc-6bsxx [151.45682ms]
Jun 28 19:34:28.035: INFO: Created: latency-svc-457qv
Jun 28 19:34:28.035: INFO: Got endpoints: latency-svc-2mhkz [148.894871ms]
Jun 28 19:34:28.043: INFO: Created: latency-svc-48nf8
Jun 28 19:34:28.047: INFO: Got endpoints: latency-svc-457qv [153.160869ms]
Jun 28 19:34:28.054: INFO: Created: latency-svc-h27vn
Jun 28 19:34:28.055: INFO: Got endpoints: latency-svc-48nf8 [150.460723ms]
Jun 28 19:34:28.065: INFO: Got endpoints: latency-svc-h27vn [149.788252ms]
Jun 28 19:34:28.066: INFO: Created: latency-svc-f5bht
Jun 28 19:34:28.074: INFO: Created: latency-svc-q2jrb
Jun 28 19:34:28.080: INFO: Got endpoints: latency-svc-f5bht [154.643935ms]
Jun 28 19:34:28.084: INFO: Created: latency-svc-df5kw
Jun 28 19:34:28.087: INFO: Got endpoints: latency-svc-q2jrb [152.580114ms]
Jun 28 19:34:28.093: INFO: Created: latency-svc-gf2rs
Jun 28 19:34:28.093: INFO: Got endpoints: latency-svc-df5kw [147.852743ms]
Jun 28 19:34:28.102: INFO: Created: latency-svc-skn8k
Jun 28 19:34:28.106: INFO: Got endpoints: latency-svc-gf2rs [151.135957ms]
Jun 28 19:34:28.113: INFO: Got endpoints: latency-svc-skn8k [148.0823ms]
Jun 28 19:34:28.114: INFO: Created: latency-svc-fmszt
Jun 28 19:34:28.120: INFO: Got endpoints: latency-svc-fmszt [143.699931ms]
Jun 28 19:34:28.123: INFO: Created: latency-svc-nwlnt
Jun 28 19:34:28.137: INFO: Created: latency-svc-mdhgw
Jun 28 19:34:28.137: INFO: Got endpoints: latency-svc-nwlnt [150.522141ms]
Jun 28 19:34:28.143: INFO: Created: latency-svc-p88ph
Jun 28 19:34:28.145: INFO: Got endpoints: latency-svc-mdhgw [144.99577ms]
Jun 28 19:34:28.153: INFO: Created: latency-svc-khxrp
Jun 28 19:34:28.157: INFO: Got endpoints: latency-svc-p88ph [151.294378ms]
Jun 28 19:34:28.163: INFO: Created: latency-svc-tbgfl
Jun 28 19:34:28.165: INFO: Got endpoints: latency-svc-khxrp [149.903498ms]
Jun 28 19:34:28.172: INFO: Created: latency-svc-ndzhh
Jun 28 19:34:28.176: INFO: Got endpoints: latency-svc-tbgfl [150.520596ms]
Jun 28 19:34:28.183: INFO: Created: latency-svc-j9r2f
Jun 28 19:34:28.185: INFO: Got endpoints: latency-svc-ndzhh [149.474049ms]
Jun 28 19:34:28.190: INFO: Created: latency-svc-h8r7v
Jun 28 19:34:28.193: INFO: Got endpoints: latency-svc-j9r2f [145.422799ms]
Jun 28 19:34:28.201: INFO: Created: latency-svc-8qswg
Jun 28 19:34:28.203: INFO: Got endpoints: latency-svc-h8r7v [147.70564ms]
Jun 28 19:34:28.212: INFO: Created: latency-svc-8sdgt
Jun 28 19:34:28.212: INFO: Got endpoints: latency-svc-8qswg [146.756751ms]
Jun 28 19:34:28.219: INFO: Created: latency-svc-4q4hx
Jun 28 19:34:28.224: INFO: Got endpoints: latency-svc-8sdgt [144.499797ms]
Jun 28 19:34:28.230: INFO: Got endpoints: latency-svc-4q4hx [143.080336ms]
Jun 28 19:34:28.233: INFO: Created: latency-svc-sj6gf
Jun 28 19:34:28.243: INFO: Got endpoints: latency-svc-sj6gf [149.389682ms]
Jun 28 19:34:28.243: INFO: Created: latency-svc-jcxm2
Jun 28 19:34:28.254: INFO: Got endpoints: latency-svc-jcxm2 [148.482429ms]
Jun 28 19:34:28.255: INFO: Created: latency-svc-dgmdv
Jun 28 19:34:28.262: INFO: Created: latency-svc-5mg6v
Jun 28 19:34:28.268: INFO: Got endpoints: latency-svc-dgmdv [154.991119ms]
Jun 28 19:34:28.271: INFO: Got endpoints: latency-svc-5mg6v [150.008793ms]
Jun 28 19:34:28.280: INFO: Created: latency-svc-h8h4w
Jun 28 19:34:28.291: INFO: Got endpoints: latency-svc-h8h4w [153.382971ms]
Jun 28 19:34:28.293: INFO: Created: latency-svc-p2569
Jun 28 19:34:28.298: INFO: Created: latency-svc-jjkhx
Jun 28 19:34:28.300: INFO: Got endpoints: latency-svc-p2569 [155.044036ms]
Jun 28 19:34:28.304: INFO: Created: latency-svc-8ndc8
Jun 28 19:34:28.306: INFO: Got endpoints: latency-svc-jjkhx [148.724975ms]
Jun 28 19:34:28.317: INFO: Got endpoints: latency-svc-8ndc8 [151.249356ms]
Jun 28 19:34:28.317: INFO: Created: latency-svc-tmtz9
Jun 28 19:34:28.322: INFO: Created: latency-svc-2xv44
Jun 28 19:34:28.329: INFO: Got endpoints: latency-svc-tmtz9 [152.449739ms]
Jun 28 19:34:28.332: INFO: Created: latency-svc-795gn
Jun 28 19:34:28.335: INFO: Got endpoints: latency-svc-2xv44 [149.779872ms]
Jun 28 19:34:28.345: INFO: Got endpoints: latency-svc-795gn [151.823173ms]
Jun 28 19:34:28.349: INFO: Created: latency-svc-865m9
Jun 28 19:34:28.353: INFO: Created: latency-svc-67vpg
Jun 28 19:34:28.353: INFO: Got endpoints: latency-svc-865m9 [149.550659ms]
Jun 28 19:34:28.361: INFO: Created: latency-svc-rvhl5
Jun 28 19:34:28.367: INFO: Got endpoints: latency-svc-67vpg [154.091501ms]
Jun 28 19:34:28.374: INFO: Created: latency-svc-cnx7s
Jun 28 19:34:28.378: INFO: Got endpoints: latency-svc-rvhl5 [152.861435ms]
Jun 28 19:34:28.382: INFO: Created: latency-svc-p8zdp
Jun 28 19:34:28.387: INFO: Got endpoints: latency-svc-cnx7s [155.127642ms]
Jun 28 19:34:28.389: INFO: Created: latency-svc-chxwg
Jun 28 19:34:28.398: INFO: Got endpoints: latency-svc-p8zdp [154.602508ms]
Jun 28 19:34:28.401: INFO: Got endpoints: latency-svc-chxwg [147.169807ms]
Jun 28 19:34:28.405: INFO: Created: latency-svc-xwhvl
Jun 28 19:34:28.417: INFO: Created: latency-svc-4gwx7
Jun 28 19:34:28.418: INFO: Got endpoints: latency-svc-xwhvl [149.210205ms]
Jun 28 19:34:28.427: INFO: Created: latency-svc-ds7gf
Jun 28 19:34:28.434: INFO: Got endpoints: latency-svc-4gwx7 [163.511871ms]
Jun 28 19:34:28.438: INFO: Got endpoints: latency-svc-ds7gf [145.473495ms]
Jun 28 19:34:28.442: INFO: Created: latency-svc-9wm2r
Jun 28 19:34:28.452: INFO: Created: latency-svc-89ft4
Jun 28 19:34:28.454: INFO: Got endpoints: latency-svc-9wm2r [153.580119ms]
Jun 28 19:34:28.465: INFO: Got endpoints: latency-svc-89ft4 [158.935403ms]
Jun 28 19:34:28.471: INFO: Created: latency-svc-glhw2
Jun 28 19:34:28.479: INFO: Got endpoints: latency-svc-glhw2 [162.511692ms]
Jun 28 19:34:28.487: INFO: Created: latency-svc-xc45s
Jun 28 19:34:28.493: INFO: Created: latency-svc-qw9lx
Jun 28 19:34:28.496: INFO: Got endpoints: latency-svc-xc45s [167.132267ms]
Jun 28 19:34:28.504: INFO: Got endpoints: latency-svc-qw9lx [168.53706ms]
Jun 28 19:34:28.505: INFO: Created: latency-svc-8sz5d
Jun 28 19:34:28.515: INFO: Created: latency-svc-hnsfl
Jun 28 19:34:28.518: INFO: Got endpoints: latency-svc-8sz5d [173.603706ms]
Jun 28 19:34:28.524: INFO: Created: latency-svc-x4s5c
Jun 28 19:34:28.526: INFO: Got endpoints: latency-svc-hnsfl [173.002045ms]
Jun 28 19:34:28.535: INFO: Got endpoints: latency-svc-x4s5c [168.453781ms]
Jun 28 19:34:28.536: INFO: Created: latency-svc-cp6kh
Jun 28 19:34:28.545: INFO: Got endpoints: latency-svc-cp6kh [167.030469ms]
Jun 28 19:34:28.546: INFO: Created: latency-svc-4dtfh
Jun 28 19:34:28.553: INFO: Created: latency-svc-4gn5c
Jun 28 19:34:28.557: INFO: Got endpoints: latency-svc-4dtfh [169.786315ms]
Jun 28 19:34:28.565: INFO: Got endpoints: latency-svc-4gn5c [167.500802ms]
Jun 28 19:34:28.566: INFO: Created: latency-svc-mrzt8
Jun 28 19:34:28.571: INFO: Got endpoints: latency-svc-mrzt8 [169.257903ms]
Jun 28 19:34:28.571: INFO: Latencies: [42.911407ms 60.350027ms 70.101427ms 89.011563ms 110.051977ms 116.79273ms 120.217909ms 120.556948ms 125.083639ms 131.89094ms 141.019505ms 143.080336ms 143.699931ms 144.499797ms 144.99577ms 145.026299ms 145.422799ms 145.440994ms 145.473495ms 146.255014ms 146.756751ms 147.119729ms 147.169807ms 147.70564ms 147.852743ms 148.0823ms 148.482429ms 148.724975ms 148.894871ms 149.044701ms 149.191048ms 149.210205ms 149.389682ms 149.474049ms 149.550659ms 149.779872ms 149.788252ms 149.889968ms 149.903498ms 150.008793ms 150.243726ms 150.460723ms 150.520596ms 150.522141ms 150.577016ms 150.798044ms 151.066435ms 151.069488ms 151.135957ms 151.249356ms 151.294378ms 151.430617ms 151.45682ms 151.694924ms 151.823173ms 152.26944ms 152.449739ms 152.580114ms 152.586848ms 152.597079ms 152.861435ms 152.984987ms 153.011621ms 153.160869ms 153.213699ms 153.264032ms 153.302321ms 153.382971ms 153.495869ms 153.580119ms 153.621077ms 153.679765ms 153.759928ms 154.091501ms 154.118652ms 154.31206ms 154.334479ms 154.403971ms 154.602508ms 154.643935ms 154.991119ms 155.044036ms 155.063808ms 155.127642ms 155.464178ms 155.497189ms 155.508836ms 155.67171ms 156.009661ms 156.430125ms 156.688254ms 157.099146ms 157.264234ms 157.641723ms 158.297376ms 158.400062ms 158.405445ms 158.575584ms 158.630048ms 158.674448ms 158.935403ms 159.618752ms 160.314995ms 160.46591ms 160.817887ms 160.910003ms 161.0787ms 161.10121ms 161.194144ms 161.333945ms 161.501145ms 161.583985ms 161.938825ms 161.949272ms 162.005473ms 162.437938ms 162.511692ms 162.553865ms 162.567378ms 162.610342ms 162.702804ms 163.079344ms 163.199324ms 163.394986ms 163.511871ms 163.573673ms 163.674553ms 164.474747ms 164.62289ms 164.850859ms 164.924216ms 165.128616ms 165.22059ms 165.406679ms 165.559864ms 165.926489ms 166.544127ms 167.030469ms 167.094685ms 167.132267ms 167.269937ms 167.500802ms 168.140625ms 168.397023ms 168.453781ms 168.53706ms 169.061394ms 169.116337ms 169.257903ms 169.287341ms 169.304222ms 169.358157ms 169.555476ms 169.739356ms 169.786315ms 169.820981ms 170.006278ms 170.264379ms 170.266313ms 170.492702ms 170.678259ms 170.864538ms 171.467911ms 171.731114ms 171.842588ms 171.929252ms 172.009348ms 172.051027ms 172.06926ms 173.002045ms 173.089442ms 173.260609ms 173.603706ms 174.014356ms 174.291914ms 174.421203ms 175.819156ms 175.88029ms 176.156617ms 176.380722ms 176.685024ms 176.958511ms 177.251388ms 181.783897ms 182.425549ms 184.812223ms 185.42838ms 186.8944ms 188.357358ms 189.820703ms 190.564088ms 191.242829ms 197.451272ms 198.873414ms 203.346556ms 204.394019ms 210.545066ms 217.864375ms 225.311765ms 237.398431ms]
Jun 28 19:34:28.571: INFO: 50 %ile: 158.935403ms
Jun 28 19:34:28.571: INFO: 90 %ile: 176.685024ms
Jun 28 19:34:28.571: INFO: 99 %ile: 225.311765ms
Jun 28 19:34:28.572: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 19:34:28.573: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svc-latency-4318" for this suite.
•{"msg":"PASSED [sig-network] Service endpoints latency should not be very high  [Conformance]","total":305,"completed":255,"skipped":3906,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 19:34:28.615: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jun 28 19:36:28.827: INFO: Deleting pod "var-expansion-8980ec6a-1d79-401d-95e9-4defe90c0b6b" in namespace "var-expansion-5340"
Jun 28 19:36:28.857: INFO: Wait up to 5m0s for pod "var-expansion-8980ec6a-1d79-401d-95e9-4defe90c0b6b" to be fully deleted
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 19:36:38.891: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-5340" for this suite.

• [SLOW TEST:130.313 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Variable Expansion should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]","total":305,"completed":256,"skipped":3948,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Aggregator 
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 19:36:38.936: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename aggregator
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:76
Jun 28 19:36:39.088: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
[It] Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Registering the sample API server.
Jun 28 19:36:39.709: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
Jun 28 19:36:41.858: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63760505799, loc:(*time.Location)(0x7702840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63760505799, loc:(*time.Location)(0x7702840)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63760505799, loc:(*time.Location)(0x7702840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63760505799, loc:(*time.Location)(0x7702840)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67c46cd746\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 28 19:36:43.872: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63760505799, loc:(*time.Location)(0x7702840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63760505799, loc:(*time.Location)(0x7702840)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63760505799, loc:(*time.Location)(0x7702840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63760505799, loc:(*time.Location)(0x7702840)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67c46cd746\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 28 19:36:45.870: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63760505799, loc:(*time.Location)(0x7702840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63760505799, loc:(*time.Location)(0x7702840)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63760505799, loc:(*time.Location)(0x7702840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63760505799, loc:(*time.Location)(0x7702840)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67c46cd746\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 28 19:36:47.880: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63760505799, loc:(*time.Location)(0x7702840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63760505799, loc:(*time.Location)(0x7702840)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63760505799, loc:(*time.Location)(0x7702840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63760505799, loc:(*time.Location)(0x7702840)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67c46cd746\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 28 19:36:49.874: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63760505799, loc:(*time.Location)(0x7702840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63760505799, loc:(*time.Location)(0x7702840)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63760505799, loc:(*time.Location)(0x7702840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63760505799, loc:(*time.Location)(0x7702840)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67c46cd746\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 28 19:36:51.875: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63760505799, loc:(*time.Location)(0x7702840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63760505799, loc:(*time.Location)(0x7702840)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63760505799, loc:(*time.Location)(0x7702840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63760505799, loc:(*time.Location)(0x7702840)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67c46cd746\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 28 19:36:57.750: INFO: Waited 3.842440618s for the sample-apiserver to be ready to handle requests.
I0628 19:36:58.985738      22 request.go:645] Throttling request took 1.027666923s, request: GET:https://172.21.0.1:443/apis/network.operator.openshift.io/v1?timeout=32s
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:67
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 19:37:00.197: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "aggregator-5407" for this suite.

• [SLOW TEST:21.366 seconds]
[sig-api-machinery] Aggregator
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Aggregator Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]","total":305,"completed":257,"skipped":3986,"failed":0}
SSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 19:37:00.303: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod liveness-a80d4e50-c4b5-44bb-9699-91cc33b0f4ac in namespace container-probe-6182
Jun 28 19:37:02.504: INFO: Started pod liveness-a80d4e50-c4b5-44bb-9699-91cc33b0f4ac in namespace container-probe-6182
STEP: checking the pod's current state and verifying that restartCount is present
Jun 28 19:37:02.515: INFO: Initial restart count of pod liveness-a80d4e50-c4b5-44bb-9699-91cc33b0f4ac is 0
Jun 28 19:37:14.614: INFO: Restart count of pod container-probe-6182/liveness-a80d4e50-c4b5-44bb-9699-91cc33b0f4ac is now 1 (12.099081714s elapsed)
Jun 28 19:37:34.778: INFO: Restart count of pod container-probe-6182/liveness-a80d4e50-c4b5-44bb-9699-91cc33b0f4ac is now 2 (32.262538472s elapsed)
Jun 28 19:37:54.956: INFO: Restart count of pod container-probe-6182/liveness-a80d4e50-c4b5-44bb-9699-91cc33b0f4ac is now 3 (52.44085078s elapsed)
Jun 28 19:38:15.130: INFO: Restart count of pod container-probe-6182/liveness-a80d4e50-c4b5-44bb-9699-91cc33b0f4ac is now 4 (1m12.614711315s elapsed)
Jun 28 19:39:22.748: INFO: Restart count of pod container-probe-6182/liveness-a80d4e50-c4b5-44bb-9699-91cc33b0f4ac is now 5 (2m20.232552567s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 19:39:22.780: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-6182" for this suite.

• [SLOW TEST:142.512 seconds]
[k8s.io] Probing container
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Probing container should have monotonically increasing restart count [NodeConformance] [Conformance]","total":305,"completed":258,"skipped":3995,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl diff 
  should check if kubectl diff finds a difference for Deployments [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 19:39:22.817: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[It] should check if kubectl diff finds a difference for Deployments [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create deployment with httpd image
Jun 28 19:39:22.958: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-108209140 create -f -'
Jun 28 19:39:23.641: INFO: stderr: ""
Jun 28 19:39:23.642: INFO: stdout: "deployment.apps/httpd-deployment created\n"
STEP: verify diff finds difference between live and declared image
Jun 28 19:39:23.642: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-108209140 diff -f -'
Jun 28 19:39:24.837: INFO: rc: 1
Jun 28 19:39:24.837: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-108209140 delete -f -'
Jun 28 19:39:25.022: INFO: stderr: ""
Jun 28 19:39:25.022: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 19:39:25.022: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2347" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl diff should check if kubectl diff finds a difference for Deployments [Conformance]","total":305,"completed":259,"skipped":4020,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny attaching pod [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 19:39:25.059: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jun 28 19:39:25.566: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jun 28 19:39:27.606: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63760505965, loc:(*time.Location)(0x7702840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63760505965, loc:(*time.Location)(0x7702840)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63760505965, loc:(*time.Location)(0x7702840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63760505965, loc:(*time.Location)(0x7702840)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun 28 19:39:30.653: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny attaching pod [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod
STEP: 'kubectl attach' the pod, should be denied by the webhook
Jun 28 19:39:32.825: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-108209140 attach --namespace=webhook-2643 to-be-attached-pod -i -c=container1'
Jun 28 19:39:33.080: INFO: rc: 1
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 19:39:33.103: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2643" for this suite.
STEP: Destroying namespace "webhook-2643-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:8.231 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny attaching pod [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny attaching pod [Conformance]","total":305,"completed":260,"skipped":4026,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] 
  should include custom resource definition resources in discovery documents [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 19:39:33.290: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] should include custom resource definition resources in discovery documents [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: fetching the /apis discovery document
STEP: finding the apiextensions.k8s.io API group in the /apis discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/apiextensions.k8s.io discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document
STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document
STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 19:39:33.427: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-1883" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] should include custom resource definition resources in discovery documents [Conformance]","total":305,"completed":261,"skipped":4054,"failed":0}
SSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 19:39:33.474: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:134
[It] should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Jun 28 19:39:33.735: INFO: Number of nodes with available pods: 0
Jun 28 19:39:33.735: INFO: Node 10.13.107.37 is running more than one daemon pod
Jun 28 19:39:34.763: INFO: Number of nodes with available pods: 0
Jun 28 19:39:34.763: INFO: Node 10.13.107.37 is running more than one daemon pod
Jun 28 19:39:35.769: INFO: Number of nodes with available pods: 2
Jun 28 19:39:35.769: INFO: Node 10.13.107.60 is running more than one daemon pod
Jun 28 19:39:36.769: INFO: Number of nodes with available pods: 3
Jun 28 19:39:36.769: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived.
Jun 28 19:39:36.851: INFO: Number of nodes with available pods: 2
Jun 28 19:39:36.852: INFO: Node 10.13.107.37 is running more than one daemon pod
Jun 28 19:39:37.949: INFO: Number of nodes with available pods: 2
Jun 28 19:39:37.951: INFO: Node 10.13.107.37 is running more than one daemon pod
Jun 28 19:39:38.913: INFO: Number of nodes with available pods: 3
Jun 28 19:39:38.914: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Wait for the failed daemon pod to be completely deleted.
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:100
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-827, will wait for the garbage collector to delete the pods
Jun 28 19:39:39.015: INFO: Deleting DaemonSet.extensions daemon-set took: 22.219717ms
Jun 28 19:39:39.115: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.419124ms
Jun 28 19:39:47.231: INFO: Number of nodes with available pods: 0
Jun 28 19:39:47.232: INFO: Number of running nodes: 0, number of available pods: 0
Jun 28 19:39:47.244: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-827/daemonsets","resourceVersion":"108729"},"items":null}

Jun 28 19:39:47.253: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-827/pods","resourceVersion":"108729"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 19:39:47.317: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-827" for this suite.

• [SLOW TEST:13.885 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should retry creating failed daemon pods [Conformance]","total":305,"completed":262,"skipped":4060,"failed":0}
SSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 19:39:47.362: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jun 28 19:39:48.091: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun 28 19:39:51.155: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jun 28 19:39:51.169: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Registering the custom resource webhook via the AdmissionRegistration API
STEP: Creating a custom resource that should be denied by the webhook
STEP: Creating a custom resource whose deletion would be denied by the webhook
STEP: Updating the custom resource with disallowed data should be denied
STEP: Deleting the custom resource should be denied
STEP: Remove the offending key and value from the custom resource data
STEP: Deleting the updated custom resource should be successful
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 19:39:52.655: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1060" for this suite.
STEP: Destroying namespace "webhook-1060-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:5.474 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny custom resource creation, update and deletion [Conformance]","total":305,"completed":263,"skipped":4067,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 19:39:52.841: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Jun 28 19:39:53.012: INFO: Waiting up to 5m0s for pod "downwardapi-volume-51a3e7fd-09c8-48eb-ae4a-9fa44291f173" in namespace "projected-8393" to be "Succeeded or Failed"
Jun 28 19:39:53.022: INFO: Pod "downwardapi-volume-51a3e7fd-09c8-48eb-ae4a-9fa44291f173": Phase="Pending", Reason="", readiness=false. Elapsed: 9.720188ms
Jun 28 19:39:55.034: INFO: Pod "downwardapi-volume-51a3e7fd-09c8-48eb-ae4a-9fa44291f173": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.021767403s
STEP: Saw pod success
Jun 28 19:39:55.034: INFO: Pod "downwardapi-volume-51a3e7fd-09c8-48eb-ae4a-9fa44291f173" satisfied condition "Succeeded or Failed"
Jun 28 19:39:55.044: INFO: Trying to get logs from node 10.13.107.37 pod downwardapi-volume-51a3e7fd-09c8-48eb-ae4a-9fa44291f173 container client-container: <nil>
STEP: delete the pod
Jun 28 19:39:55.155: INFO: Waiting for pod downwardapi-volume-51a3e7fd-09c8-48eb-ae4a-9fa44291f173 to disappear
Jun 28 19:39:55.164: INFO: Pod downwardapi-volume-51a3e7fd-09c8-48eb-ae4a-9fa44291f173 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 19:39:55.164: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8393" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","total":305,"completed":264,"skipped":4077,"failed":0}
SSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 19:39:55.197: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-6221
[It] Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Looking for a node to schedule stateful set and pod
STEP: Creating pod with conflicting port in namespace statefulset-6221
STEP: Creating statefulset with conflicting port in namespace statefulset-6221
STEP: Waiting until pod test-pod will start running in namespace statefulset-6221
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-6221
Jun 28 19:39:59.479: INFO: Observed stateful pod in namespace: statefulset-6221, name: ss-0, uid: 9697338e-5295-41aa-8c72-a0d75e4fbc30, status phase: Pending. Waiting for statefulset controller to delete.
Jun 28 19:39:59.577: INFO: Observed stateful pod in namespace: statefulset-6221, name: ss-0, uid: 9697338e-5295-41aa-8c72-a0d75e4fbc30, status phase: Failed. Waiting for statefulset controller to delete.
Jun 28 19:39:59.594: INFO: Observed stateful pod in namespace: statefulset-6221, name: ss-0, uid: 9697338e-5295-41aa-8c72-a0d75e4fbc30, status phase: Failed. Waiting for statefulset controller to delete.
Jun 28 19:39:59.602: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-6221
STEP: Removing pod with conflicting port in namespace statefulset-6221
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-6221 and will be in running state
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
Jun 28 19:40:01.657: INFO: Deleting all statefulset in ns statefulset-6221
Jun 28 19:40:01.669: INFO: Scaling statefulset ss to 0
Jun 28 19:40:11.738: INFO: Waiting for statefulset status.replicas updated to 0
Jun 28 19:40:11.748: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 19:40:11.786: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-6221" for this suite.

• [SLOW TEST:16.620 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
    Should recreate evicted statefulset [Conformance]
    /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] Should recreate evicted statefulset [Conformance]","total":305,"completed":265,"skipped":4085,"failed":0}
SSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 19:40:11.819: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:90
Jun 28 19:40:11.943: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jun 28 19:40:12.013: INFO: Waiting for terminating namespaces to be deleted...
Jun 28 19:40:12.028: INFO: 
Logging pods the apiserver thinks is on node 10.13.107.37 before test
Jun 28 19:40:12.070: INFO: calico-node-7ntd9 from calico-system started at 2021-06-28 16:43:04 +0000 UTC (1 container statuses recorded)
Jun 28 19:40:12.071: INFO: 	Container calico-node ready: true, restart count 0
Jun 28 19:40:12.072: INFO: calico-typha-649969bb55-mj7tk from calico-system started at 2021-06-28 16:45:03 +0000 UTC (1 container statuses recorded)
Jun 28 19:40:12.072: INFO: 	Container calico-typha ready: true, restart count 0
Jun 28 19:40:12.072: INFO: ibm-keepalived-watcher-f9nwv from kube-system started at 2021-06-28 16:41:30 +0000 UTC (1 container statuses recorded)
Jun 28 19:40:12.072: INFO: 	Container keepalived-watcher ready: true, restart count 0
Jun 28 19:40:12.073: INFO: ibm-master-proxy-static-10.13.107.37 from kube-system started at 2021-06-28 16:40:48 +0000 UTC (2 container statuses recorded)
Jun 28 19:40:12.073: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Jun 28 19:40:12.073: INFO: 	Container pause ready: true, restart count 0
Jun 28 19:40:12.075: INFO: ibmcloud-block-storage-driver-kbnst from kube-system started at 2021-06-28 16:41:30 +0000 UTC (1 container statuses recorded)
Jun 28 19:40:12.075: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Jun 28 19:40:12.075: INFO: tuned-2tw2w from openshift-cluster-node-tuning-operator started at 2021-06-28 16:43:24 +0000 UTC (1 container statuses recorded)
Jun 28 19:40:12.075: INFO: 	Container tuned ready: true, restart count 0
Jun 28 19:40:12.075: INFO: dns-default-crxtq from openshift-dns started at 2021-06-28 16:44:04 +0000 UTC (3 container statuses recorded)
Jun 28 19:40:12.076: INFO: 	Container dns ready: true, restart count 0
Jun 28 19:40:12.076: INFO: 	Container dns-node-resolver ready: true, restart count 0
Jun 28 19:40:12.076: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 28 19:40:12.077: INFO: node-ca-z64gj from openshift-image-registry started at 2021-06-28 16:44:14 +0000 UTC (1 container statuses recorded)
Jun 28 19:40:12.077: INFO: 	Container node-ca ready: true, restart count 0
Jun 28 19:40:12.077: INFO: registry-pvc-permissions-lfjks from openshift-image-registry started at 2021-06-28 16:46:50 +0000 UTC (1 container statuses recorded)
Jun 28 19:40:12.077: INFO: 	Container pvc-permissions ready: false, restart count 0
Jun 28 19:40:12.077: INFO: openshift-kube-proxy-8l7d5 from openshift-kube-proxy started at 2021-06-28 16:42:08 +0000 UTC (1 container statuses recorded)
Jun 28 19:40:12.078: INFO: 	Container kube-proxy ready: true, restart count 0
Jun 28 19:40:12.078: INFO: node-exporter-qj7hs from openshift-monitoring started at 2021-06-28 16:44:29 +0000 UTC (2 container statuses recorded)
Jun 28 19:40:12.078: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 28 19:40:12.079: INFO: 	Container node-exporter ready: true, restart count 0
Jun 28 19:40:12.079: INFO: multus-admission-controller-nfv4l from openshift-multus started at 2021-06-28 19:12:27 +0000 UTC (2 container statuses recorded)
Jun 28 19:40:12.079: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 28 19:40:12.079: INFO: 	Container multus-admission-controller ready: true, restart count 0
Jun 28 19:40:12.079: INFO: multus-brfr7 from openshift-multus started at 2021-06-28 16:42:07 +0000 UTC (1 container statuses recorded)
Jun 28 19:40:12.080: INFO: 	Container kube-multus ready: true, restart count 0
Jun 28 19:40:12.080: INFO: network-metrics-daemon-5rjvj from openshift-multus started at 2021-06-28 16:42:08 +0000 UTC (2 container statuses recorded)
Jun 28 19:40:12.080: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 28 19:40:12.081: INFO: 	Container network-metrics-daemon ready: true, restart count 0
Jun 28 19:40:12.081: INFO: sonobuoy-systemd-logs-daemon-set-bc3c64b0befe49cb-gp9nl from sonobuoy started at 2021-06-28 18:11:47 +0000 UTC (2 container statuses recorded)
Jun 28 19:40:12.081: INFO: 	Container sonobuoy-worker ready: false, restart count 10
Jun 28 19:40:12.081: INFO: 	Container systemd-logs ready: true, restart count 0
Jun 28 19:40:12.082: INFO: 
Logging pods the apiserver thinks is on node 10.13.107.57 before test
Jun 28 19:40:12.134: INFO: calico-node-tr9lp from calico-system started at 2021-06-28 16:43:04 +0000 UTC (1 container statuses recorded)
Jun 28 19:40:12.134: INFO: 	Container calico-node ready: true, restart count 0
Jun 28 19:40:12.134: INFO: calico-typha-649969bb55-tpj68 from calico-system started at 2021-06-28 16:43:04 +0000 UTC (1 container statuses recorded)
Jun 28 19:40:12.134: INFO: 	Container calico-typha ready: true, restart count 0
Jun 28 19:40:12.134: INFO: ibm-cloud-provider-ip-149-81-71-226-77554565fb-clmlm from ibm-system started at 2021-06-28 16:58:09 +0000 UTC (1 container statuses recorded)
Jun 28 19:40:12.134: INFO: 	Container ibm-cloud-provider-ip-149-81-71-226 ready: true, restart count 0
Jun 28 19:40:12.134: INFO: ibm-file-plugin-7d974bf47c-bw8qh from kube-system started at 2021-06-28 16:43:14 +0000 UTC (1 container statuses recorded)
Jun 28 19:40:12.134: INFO: 	Container ibm-file-plugin-container ready: true, restart count 0
Jun 28 19:40:12.134: INFO: ibm-keepalived-watcher-rqj9s from kube-system started at 2021-06-28 16:41:35 +0000 UTC (1 container statuses recorded)
Jun 28 19:40:12.134: INFO: 	Container keepalived-watcher ready: true, restart count 0
Jun 28 19:40:12.134: INFO: ibm-master-proxy-static-10.13.107.57 from kube-system started at 2021-06-28 16:40:56 +0000 UTC (2 container statuses recorded)
Jun 28 19:40:12.134: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Jun 28 19:40:12.134: INFO: 	Container pause ready: true, restart count 0
Jun 28 19:40:12.134: INFO: ibm-storage-watcher-869677c8b-q8rh5 from kube-system started at 2021-06-28 16:43:14 +0000 UTC (1 container statuses recorded)
Jun 28 19:40:12.134: INFO: 	Container ibm-storage-watcher-container ready: true, restart count 0
Jun 28 19:40:12.134: INFO: ibmcloud-block-storage-driver-p9gg7 from kube-system started at 2021-06-28 16:41:35 +0000 UTC (1 container statuses recorded)
Jun 28 19:40:12.134: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Jun 28 19:40:12.134: INFO: ibmcloud-block-storage-plugin-665488684b-h5fhz from kube-system started at 2021-06-28 16:43:14 +0000 UTC (1 container statuses recorded)
Jun 28 19:40:12.134: INFO: 	Container ibmcloud-block-storage-plugin-container ready: true, restart count 0
Jun 28 19:40:12.134: INFO: vpn-6c58c4756c-rw748 from kube-system started at 2021-06-28 16:50:23 +0000 UTC (1 container statuses recorded)
Jun 28 19:40:12.134: INFO: 	Container vpn ready: true, restart count 0
Jun 28 19:40:12.134: INFO: cluster-node-tuning-operator-77dfdd89b8-chjmh from openshift-cluster-node-tuning-operator started at 2021-06-28 16:43:14 +0000 UTC (1 container statuses recorded)
Jun 28 19:40:12.134: INFO: 	Container cluster-node-tuning-operator ready: true, restart count 0
Jun 28 19:40:12.134: INFO: tuned-r5kts from openshift-cluster-node-tuning-operator started at 2021-06-28 16:43:24 +0000 UTC (1 container statuses recorded)
Jun 28 19:40:12.134: INFO: 	Container tuned ready: true, restart count 0
Jun 28 19:40:12.134: INFO: cluster-samples-operator-56dd9bbcb-8mjfl from openshift-cluster-samples-operator started at 2021-06-28 16:43:14 +0000 UTC (2 container statuses recorded)
Jun 28 19:40:12.134: INFO: 	Container cluster-samples-operator ready: true, restart count 0
Jun 28 19:40:12.134: INFO: 	Container cluster-samples-operator-watch ready: true, restart count 0
Jun 28 19:40:12.134: INFO: cluster-storage-operator-56664f46b8-kjjbs from openshift-cluster-storage-operator started at 2021-06-28 16:43:14 +0000 UTC (1 container statuses recorded)
Jun 28 19:40:12.134: INFO: 	Container cluster-storage-operator ready: true, restart count 0
Jun 28 19:40:12.134: INFO: console-operator-cd789fcfb-4j2b8 from openshift-console-operator started at 2021-06-28 16:43:14 +0000 UTC (1 container statuses recorded)
Jun 28 19:40:12.134: INFO: 	Container console-operator ready: true, restart count 1
Jun 28 19:40:12.134: INFO: console-74dd6cb4c6-vnhx8 from openshift-console started at 2021-06-28 18:29:06 +0000 UTC (1 container statuses recorded)
Jun 28 19:40:12.134: INFO: 	Container console ready: true, restart count 0
Jun 28 19:40:12.134: INFO: downloads-65dcc64f75-2bzn8 from openshift-console started at 2021-06-28 16:43:14 +0000 UTC (1 container statuses recorded)
Jun 28 19:40:12.134: INFO: 	Container download-server ready: true, restart count 0
Jun 28 19:40:12.134: INFO: downloads-65dcc64f75-5hm2s from openshift-console started at 2021-06-28 16:43:14 +0000 UTC (1 container statuses recorded)
Jun 28 19:40:12.134: INFO: 	Container download-server ready: true, restart count 0
Jun 28 19:40:12.134: INFO: dns-operator-7d8cb9bb6d-kkfvf from openshift-dns-operator started at 2021-06-28 16:43:14 +0000 UTC (2 container statuses recorded)
Jun 28 19:40:12.134: INFO: 	Container dns-operator ready: true, restart count 0
Jun 28 19:40:12.134: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 28 19:40:12.134: INFO: dns-default-48b6x from openshift-dns started at 2021-06-28 16:44:04 +0000 UTC (3 container statuses recorded)
Jun 28 19:40:12.134: INFO: 	Container dns ready: true, restart count 0
Jun 28 19:40:12.134: INFO: 	Container dns-node-resolver ready: true, restart count 0
Jun 28 19:40:12.134: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 28 19:40:12.134: INFO: cluster-image-registry-operator-999d7b49c-d458n from openshift-image-registry started at 2021-06-28 16:43:14 +0000 UTC (1 container statuses recorded)
Jun 28 19:40:12.134: INFO: 	Container cluster-image-registry-operator ready: true, restart count 0
Jun 28 19:40:12.134: INFO: node-ca-49d6t from openshift-image-registry started at 2021-06-28 16:44:14 +0000 UTC (1 container statuses recorded)
Jun 28 19:40:12.134: INFO: 	Container node-ca ready: true, restart count 0
Jun 28 19:40:12.134: INFO: ingress-operator-795fb4477f-sgnmp from openshift-ingress-operator started at 2021-06-28 16:43:14 +0000 UTC (2 container statuses recorded)
Jun 28 19:40:12.134: INFO: 	Container ingress-operator ready: true, restart count 0
Jun 28 19:40:12.134: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 28 19:40:12.134: INFO: router-default-8b99f5968-b84k7 from openshift-ingress started at 2021-06-28 18:29:06 +0000 UTC (1 container statuses recorded)
Jun 28 19:40:12.134: INFO: 	Container router ready: true, restart count 0
Jun 28 19:40:12.134: INFO: openshift-kube-proxy-xq2t8 from openshift-kube-proxy started at 2021-06-28 16:42:08 +0000 UTC (1 container statuses recorded)
Jun 28 19:40:12.134: INFO: 	Container kube-proxy ready: true, restart count 0
Jun 28 19:40:12.134: INFO: kube-storage-version-migrator-operator-77d5dd5f6c-z9m4j from openshift-kube-storage-version-migrator-operator started at 2021-06-28 16:43:14 +0000 UTC (1 container statuses recorded)
Jun 28 19:40:12.134: INFO: 	Container kube-storage-version-migrator-operator ready: true, restart count 1
Jun 28 19:40:12.135: INFO: marketplace-operator-7b9d5dcb99-nzgf8 from openshift-marketplace started at 2021-06-28 16:43:14 +0000 UTC (1 container statuses recorded)
Jun 28 19:40:12.135: INFO: 	Container marketplace-operator ready: true, restart count 0
Jun 28 19:40:12.135: INFO: alertmanager-main-1 from openshift-monitoring started at 2021-06-28 16:45:25 +0000 UTC (5 container statuses recorded)
Jun 28 19:40:12.135: INFO: 	Container alertmanager ready: true, restart count 0
Jun 28 19:40:12.135: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Jun 28 19:40:12.135: INFO: 	Container config-reloader ready: true, restart count 0
Jun 28 19:40:12.135: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 28 19:40:12.135: INFO: 	Container prom-label-proxy ready: true, restart count 0
Jun 28 19:40:12.135: INFO: cluster-monitoring-operator-6964464f67-jbfjk from openshift-monitoring started at 2021-06-28 16:43:14 +0000 UTC (2 container statuses recorded)
Jun 28 19:40:12.135: INFO: 	Container cluster-monitoring-operator ready: true, restart count 0
Jun 28 19:40:12.135: INFO: 	Container kube-rbac-proxy ready: true, restart count 2
Jun 28 19:40:12.135: INFO: node-exporter-ld8qc from openshift-monitoring started at 2021-06-28 16:44:29 +0000 UTC (2 container statuses recorded)
Jun 28 19:40:12.135: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 28 19:40:12.135: INFO: 	Container node-exporter ready: true, restart count 0
Jun 28 19:40:12.135: INFO: prometheus-adapter-8646bbc64d-stfnh from openshift-monitoring started at 2021-06-28 18:29:07 +0000 UTC (1 container statuses recorded)
Jun 28 19:40:12.135: INFO: 	Container prometheus-adapter ready: true, restart count 0
Jun 28 19:40:12.135: INFO: prometheus-k8s-0 from openshift-monitoring started at 2021-06-28 18:29:17 +0000 UTC (6 container statuses recorded)
Jun 28 19:40:12.135: INFO: 	Container config-reloader ready: true, restart count 0
Jun 28 19:40:12.135: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 28 19:40:12.135: INFO: 	Container prom-label-proxy ready: true, restart count 0
Jun 28 19:40:12.135: INFO: 	Container prometheus ready: true, restart count 1
Jun 28 19:40:12.135: INFO: 	Container prometheus-proxy ready: true, restart count 0
Jun 28 19:40:12.135: INFO: 	Container thanos-sidecar ready: true, restart count 0
Jun 28 19:40:12.135: INFO: thanos-querier-b46bb8497-dcbxw from openshift-monitoring started at 2021-06-28 18:29:07 +0000 UTC (5 container statuses recorded)
Jun 28 19:40:12.135: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 28 19:40:12.135: INFO: 	Container kube-rbac-proxy-rules ready: true, restart count 0
Jun 28 19:40:12.135: INFO: 	Container oauth-proxy ready: true, restart count 0
Jun 28 19:40:12.135: INFO: 	Container prom-label-proxy ready: true, restart count 0
Jun 28 19:40:12.135: INFO: 	Container thanos-query ready: true, restart count 0
Jun 28 19:40:12.136: INFO: multus-admission-controller-58k5w from openshift-multus started at 2021-06-28 16:43:13 +0000 UTC (2 container statuses recorded)
Jun 28 19:40:12.136: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 28 19:40:12.136: INFO: 	Container multus-admission-controller ready: true, restart count 0
Jun 28 19:40:12.136: INFO: multus-hvm6b from openshift-multus started at 2021-06-28 16:42:08 +0000 UTC (1 container statuses recorded)
Jun 28 19:40:12.136: INFO: 	Container kube-multus ready: true, restart count 0
Jun 28 19:40:12.136: INFO: network-metrics-daemon-78dfb from openshift-multus started at 2021-06-28 16:42:08 +0000 UTC (2 container statuses recorded)
Jun 28 19:40:12.136: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 28 19:40:12.136: INFO: 	Container network-metrics-daemon ready: true, restart count 0
Jun 28 19:40:12.136: INFO: network-operator-5b78d575b8-gvzvg from openshift-network-operator started at 2021-06-28 16:41:35 +0000 UTC (1 container statuses recorded)
Jun 28 19:40:12.136: INFO: 	Container network-operator ready: true, restart count 0
Jun 28 19:40:12.136: INFO: catalog-operator-67c774db7d-5ps99 from openshift-operator-lifecycle-manager started at 2021-06-28 16:43:14 +0000 UTC (1 container statuses recorded)
Jun 28 19:40:12.136: INFO: 	Container catalog-operator ready: true, restart count 0
Jun 28 19:40:12.136: INFO: olm-operator-68fcf6954f-nfsgq from openshift-operator-lifecycle-manager started at 2021-06-28 16:43:14 +0000 UTC (1 container statuses recorded)
Jun 28 19:40:12.136: INFO: 	Container olm-operator ready: true, restart count 0
Jun 28 19:40:12.136: INFO: packageserver-65544748bd-hhnld from openshift-operator-lifecycle-manager started at 2021-06-28 18:29:07 +0000 UTC (1 container statuses recorded)
Jun 28 19:40:12.136: INFO: 	Container packageserver ready: true, restart count 0
Jun 28 19:40:12.136: INFO: metrics-7df79584fc-f6g2g from openshift-roks-metrics started at 2021-06-28 16:43:14 +0000 UTC (1 container statuses recorded)
Jun 28 19:40:12.136: INFO: 	Container metrics ready: true, restart count 1
Jun 28 19:40:12.136: INFO: push-gateway-6dc86bc94f-dz7nt from openshift-roks-metrics started at 2021-06-28 16:43:14 +0000 UTC (1 container statuses recorded)
Jun 28 19:40:12.136: INFO: 	Container push-gateway ready: true, restart count 0
Jun 28 19:40:12.136: INFO: service-ca-operator-d4bfd498b-4xhnv from openshift-service-ca-operator started at 2021-06-28 16:43:14 +0000 UTC (1 container statuses recorded)
Jun 28 19:40:12.136: INFO: 	Container service-ca-operator ready: true, restart count 1
Jun 28 19:40:12.136: INFO: service-ca-665fb97685-mn77q from openshift-service-ca started at 2021-06-28 18:29:06 +0000 UTC (1 container statuses recorded)
Jun 28 19:40:12.136: INFO: 	Container service-ca-controller ready: true, restart count 0
Jun 28 19:40:12.136: INFO: sonobuoy from sonobuoy started at 2021-06-28 18:11:40 +0000 UTC (1 container statuses recorded)
Jun 28 19:40:12.136: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jun 28 19:40:12.136: INFO: sonobuoy-systemd-logs-daemon-set-bc3c64b0befe49cb-s48t5 from sonobuoy started at 2021-06-28 18:11:47 +0000 UTC (2 container statuses recorded)
Jun 28 19:40:12.136: INFO: 	Container sonobuoy-worker ready: false, restart count 10
Jun 28 19:40:12.136: INFO: 	Container systemd-logs ready: true, restart count 0
Jun 28 19:40:12.136: INFO: tigera-operator-64c8f4c7d7-hzwkg from tigera-operator started at 2021-06-28 18:29:06 +0000 UTC (1 container statuses recorded)
Jun 28 19:40:12.136: INFO: 	Container tigera-operator ready: true, restart count 0
Jun 28 19:40:12.136: INFO: 
Logging pods the apiserver thinks is on node 10.13.107.60 before test
Jun 28 19:40:12.188: INFO: calico-kube-controllers-57df785794-xp76v from calico-system started at 2021-06-28 18:29:06 +0000 UTC (1 container statuses recorded)
Jun 28 19:40:12.188: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Jun 28 19:40:12.188: INFO: calico-node-vr9g2 from calico-system started at 2021-06-28 16:43:04 +0000 UTC (1 container statuses recorded)
Jun 28 19:40:12.188: INFO: 	Container calico-node ready: true, restart count 0
Jun 28 19:40:12.188: INFO: calico-typha-649969bb55-mc6s8 from calico-system started at 2021-06-28 16:45:02 +0000 UTC (1 container statuses recorded)
Jun 28 19:40:12.188: INFO: 	Container calico-typha ready: true, restart count 0
Jun 28 19:40:12.188: INFO: test-k8s-e2e-pvg-master-verification from default started at 2021-06-28 16:51:21 +0000 UTC (1 container statuses recorded)
Jun 28 19:40:12.189: INFO: 	Container test-k8s-e2e-pvg-master-verification ready: true, restart count 0
Jun 28 19:40:12.189: INFO: ibm-cloud-provider-ip-149-81-71-226-77554565fb-djwh7 from ibm-system started at 2021-06-28 18:29:06 +0000 UTC (1 container statuses recorded)
Jun 28 19:40:12.189: INFO: 	Container ibm-cloud-provider-ip-149-81-71-226 ready: true, restart count 0
Jun 28 19:40:12.189: INFO: ibm-keepalived-watcher-w6fhq from kube-system started at 2021-06-28 16:42:37 +0000 UTC (1 container statuses recorded)
Jun 28 19:40:12.189: INFO: 	Container keepalived-watcher ready: true, restart count 0
Jun 28 19:40:12.189: INFO: ibm-master-proxy-static-10.13.107.60 from kube-system started at 2021-06-28 16:41:49 +0000 UTC (2 container statuses recorded)
Jun 28 19:40:12.189: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Jun 28 19:40:12.189: INFO: 	Container pause ready: true, restart count 0
Jun 28 19:40:12.189: INFO: ibmcloud-block-storage-driver-kwxt2 from kube-system started at 2021-06-28 16:42:37 +0000 UTC (1 container statuses recorded)
Jun 28 19:40:12.189: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Jun 28 19:40:12.190: INFO: tuned-djhn5 from openshift-cluster-node-tuning-operator started at 2021-06-28 16:43:24 +0000 UTC (1 container statuses recorded)
Jun 28 19:40:12.190: INFO: 	Container tuned ready: true, restart count 0
Jun 28 19:40:12.190: INFO: console-74dd6cb4c6-gbthm from openshift-console started at 2021-06-28 17:20:10 +0000 UTC (1 container statuses recorded)
Jun 28 19:40:12.190: INFO: 	Container console ready: true, restart count 0
Jun 28 19:40:12.190: INFO: dns-default-84l5j from openshift-dns started at 2021-06-28 16:44:04 +0000 UTC (3 container statuses recorded)
Jun 28 19:40:12.190: INFO: 	Container dns ready: true, restart count 0
Jun 28 19:40:12.190: INFO: 	Container dns-node-resolver ready: true, restart count 0
Jun 28 19:40:12.190: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 28 19:40:12.190: INFO: image-registry-7f9574d5c6-tmmns from openshift-image-registry started at 2021-06-28 16:46:50 +0000 UTC (1 container statuses recorded)
Jun 28 19:40:12.190: INFO: 	Container registry ready: true, restart count 0
Jun 28 19:40:12.191: INFO: node-ca-42txp from openshift-image-registry started at 2021-06-28 16:44:14 +0000 UTC (1 container statuses recorded)
Jun 28 19:40:12.191: INFO: 	Container node-ca ready: true, restart count 0
Jun 28 19:40:12.191: INFO: router-default-8b99f5968-8nzw5 from openshift-ingress started at 2021-06-28 17:18:25 +0000 UTC (1 container statuses recorded)
Jun 28 19:40:12.191: INFO: 	Container router ready: true, restart count 0
Jun 28 19:40:12.191: INFO: openshift-kube-proxy-n6pzf from openshift-kube-proxy started at 2021-06-28 16:42:37 +0000 UTC (1 container statuses recorded)
Jun 28 19:40:12.191: INFO: 	Container kube-proxy ready: true, restart count 0
Jun 28 19:40:12.191: INFO: migrator-858cfc6f4c-tvj6h from openshift-kube-storage-version-migrator started at 2021-06-28 18:29:06 +0000 UTC (1 container statuses recorded)
Jun 28 19:40:12.191: INFO: 	Container migrator ready: true, restart count 0
Jun 28 19:40:12.191: INFO: certified-operators-d5d5w from openshift-marketplace started at 2021-06-28 16:44:19 +0000 UTC (1 container statuses recorded)
Jun 28 19:40:12.191: INFO: 	Container registry-server ready: true, restart count 0
Jun 28 19:40:12.191: INFO: community-operators-dbmgh from openshift-marketplace started at 2021-06-28 17:01:52 +0000 UTC (1 container statuses recorded)
Jun 28 19:40:12.191: INFO: 	Container registry-server ready: true, restart count 0
Jun 28 19:40:12.191: INFO: redhat-marketplace-4t4vz from openshift-marketplace started at 2021-06-28 16:44:21 +0000 UTC (1 container statuses recorded)
Jun 28 19:40:12.192: INFO: 	Container registry-server ready: true, restart count 0
Jun 28 19:40:12.192: INFO: redhat-operators-dvpnz from openshift-marketplace started at 2021-06-28 16:44:19 +0000 UTC (1 container statuses recorded)
Jun 28 19:40:12.192: INFO: 	Container registry-server ready: true, restart count 0
Jun 28 19:40:12.192: INFO: alertmanager-main-0 from openshift-monitoring started at 2021-06-28 16:45:25 +0000 UTC (5 container statuses recorded)
Jun 28 19:40:12.192: INFO: 	Container alertmanager ready: true, restart count 0
Jun 28 19:40:12.192: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Jun 28 19:40:12.192: INFO: 	Container config-reloader ready: true, restart count 0
Jun 28 19:40:12.192: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 28 19:40:12.192: INFO: 	Container prom-label-proxy ready: true, restart count 0
Jun 28 19:40:12.192: INFO: alertmanager-main-2 from openshift-monitoring started at 2021-06-28 18:29:17 +0000 UTC (5 container statuses recorded)
Jun 28 19:40:12.192: INFO: 	Container alertmanager ready: true, restart count 0
Jun 28 19:40:12.192: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Jun 28 19:40:12.192: INFO: 	Container config-reloader ready: true, restart count 0
Jun 28 19:40:12.192: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 28 19:40:12.192: INFO: 	Container prom-label-proxy ready: true, restart count 0
Jun 28 19:40:12.192: INFO: grafana-85d4b8dbd-jn67b from openshift-monitoring started at 2021-06-28 16:44:32 +0000 UTC (2 container statuses recorded)
Jun 28 19:40:12.192: INFO: 	Container grafana ready: true, restart count 0
Jun 28 19:40:12.193: INFO: 	Container grafana-proxy ready: true, restart count 0
Jun 28 19:40:12.193: INFO: kube-state-metrics-659c7b865d-974cs from openshift-monitoring started at 2021-06-28 16:44:29 +0000 UTC (3 container statuses recorded)
Jun 28 19:40:12.193: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Jun 28 19:40:12.193: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Jun 28 19:40:12.193: INFO: 	Container kube-state-metrics ready: true, restart count 0
Jun 28 19:40:12.193: INFO: node-exporter-6s54d from openshift-monitoring started at 2021-06-28 16:44:29 +0000 UTC (2 container statuses recorded)
Jun 28 19:40:12.193: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 28 19:40:12.193: INFO: 	Container node-exporter ready: true, restart count 0
Jun 28 19:40:12.193: INFO: openshift-state-metrics-7cf4dc694b-d9w6x from openshift-monitoring started at 2021-06-28 16:44:29 +0000 UTC (3 container statuses recorded)
Jun 28 19:40:12.193: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Jun 28 19:40:12.193: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Jun 28 19:40:12.193: INFO: 	Container openshift-state-metrics ready: true, restart count 0
Jun 28 19:40:12.193: INFO: prometheus-adapter-8646bbc64d-nq9hj from openshift-monitoring started at 2021-06-28 16:44:29 +0000 UTC (1 container statuses recorded)
Jun 28 19:40:12.193: INFO: 	Container prometheus-adapter ready: true, restart count 0
Jun 28 19:40:12.193: INFO: prometheus-k8s-1 from openshift-monitoring started at 2021-06-28 16:45:26 +0000 UTC (6 container statuses recorded)
Jun 28 19:40:12.193: INFO: 	Container config-reloader ready: true, restart count 0
Jun 28 19:40:12.194: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 28 19:40:12.194: INFO: 	Container prom-label-proxy ready: true, restart count 0
Jun 28 19:40:12.194: INFO: 	Container prometheus ready: true, restart count 1
Jun 28 19:40:12.194: INFO: 	Container prometheus-proxy ready: true, restart count 0
Jun 28 19:40:12.194: INFO: 	Container thanos-sidecar ready: true, restart count 0
Jun 28 19:40:12.194: INFO: prometheus-operator-5d48db6d9c-z9sxl from openshift-monitoring started at 2021-06-28 16:44:29 +0000 UTC (2 container statuses recorded)
Jun 28 19:40:12.194: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 28 19:40:12.194: INFO: 	Container prometheus-operator ready: true, restart count 0
Jun 28 19:40:12.194: INFO: telemeter-client-5584d54f85-bhqql from openshift-monitoring started at 2021-06-28 16:44:29 +0000 UTC (3 container statuses recorded)
Jun 28 19:40:12.194: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 28 19:40:12.194: INFO: 	Container reload ready: true, restart count 0
Jun 28 19:40:12.194: INFO: 	Container telemeter-client ready: true, restart count 0
Jun 28 19:40:12.194: INFO: thanos-querier-b46bb8497-wxrxb from openshift-monitoring started at 2021-06-28 16:44:31 +0000 UTC (5 container statuses recorded)
Jun 28 19:40:12.194: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 28 19:40:12.194: INFO: 	Container kube-rbac-proxy-rules ready: true, restart count 0
Jun 28 19:40:12.194: INFO: 	Container oauth-proxy ready: true, restart count 0
Jun 28 19:40:12.194: INFO: 	Container prom-label-proxy ready: true, restart count 0
Jun 28 19:40:12.195: INFO: 	Container thanos-query ready: true, restart count 0
Jun 28 19:40:12.195: INFO: multus-admission-controller-5rwk7 from openshift-multus started at 2021-06-28 16:43:56 +0000 UTC (2 container statuses recorded)
Jun 28 19:40:12.195: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 28 19:40:12.195: INFO: 	Container multus-admission-controller ready: true, restart count 0
Jun 28 19:40:12.195: INFO: multus-tsj42 from openshift-multus started at 2021-06-28 16:42:37 +0000 UTC (1 container statuses recorded)
Jun 28 19:40:12.195: INFO: 	Container kube-multus ready: true, restart count 0
Jun 28 19:40:12.195: INFO: network-metrics-daemon-gzxgq from openshift-multus started at 2021-06-28 16:42:37 +0000 UTC (2 container statuses recorded)
Jun 28 19:40:12.195: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 28 19:40:12.195: INFO: 	Container network-metrics-daemon ready: true, restart count 0
Jun 28 19:40:12.195: INFO: packageserver-65544748bd-h6vfz from openshift-operator-lifecycle-manager started at 2021-06-28 16:44:29 +0000 UTC (1 container statuses recorded)
Jun 28 19:40:12.195: INFO: 	Container packageserver ready: true, restart count 0
Jun 28 19:40:12.195: INFO: sonobuoy-e2e-job-3182b6be48fa4a64 from sonobuoy started at 2021-06-28 18:11:47 +0000 UTC (2 container statuses recorded)
Jun 28 19:40:12.195: INFO: 	Container e2e ready: true, restart count 0
Jun 28 19:40:12.195: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 28 19:40:12.195: INFO: sonobuoy-systemd-logs-daemon-set-bc3c64b0befe49cb-47fm7 from sonobuoy started at 2021-06-28 18:11:47 +0000 UTC (2 container statuses recorded)
Jun 28 19:40:12.196: INFO: 	Container sonobuoy-worker ready: false, restart count 10
Jun 28 19:40:12.196: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: verifying the node has the label node 10.13.107.37
STEP: verifying the node has the label node 10.13.107.57
STEP: verifying the node has the label node 10.13.107.60
Jun 28 19:40:12.494: INFO: Pod calico-kube-controllers-57df785794-xp76v requesting resource cpu=10m on Node 10.13.107.60
Jun 28 19:40:12.494: INFO: Pod calico-node-7ntd9 requesting resource cpu=250m on Node 10.13.107.37
Jun 28 19:40:12.494: INFO: Pod calico-node-tr9lp requesting resource cpu=250m on Node 10.13.107.57
Jun 28 19:40:12.494: INFO: Pod calico-node-vr9g2 requesting resource cpu=250m on Node 10.13.107.60
Jun 28 19:40:12.494: INFO: Pod calico-typha-649969bb55-mc6s8 requesting resource cpu=250m on Node 10.13.107.60
Jun 28 19:40:12.494: INFO: Pod calico-typha-649969bb55-mj7tk requesting resource cpu=250m on Node 10.13.107.37
Jun 28 19:40:12.494: INFO: Pod calico-typha-649969bb55-tpj68 requesting resource cpu=250m on Node 10.13.107.57
Jun 28 19:40:12.494: INFO: Pod test-k8s-e2e-pvg-master-verification requesting resource cpu=0m on Node 10.13.107.60
Jun 28 19:40:12.494: INFO: Pod ibm-cloud-provider-ip-149-81-71-226-77554565fb-clmlm requesting resource cpu=5m on Node 10.13.107.57
Jun 28 19:40:12.494: INFO: Pod ibm-cloud-provider-ip-149-81-71-226-77554565fb-djwh7 requesting resource cpu=5m on Node 10.13.107.60
Jun 28 19:40:12.495: INFO: Pod ibm-file-plugin-7d974bf47c-bw8qh requesting resource cpu=50m on Node 10.13.107.57
Jun 28 19:40:12.495: INFO: Pod ibm-keepalived-watcher-f9nwv requesting resource cpu=5m on Node 10.13.107.37
Jun 28 19:40:12.495: INFO: Pod ibm-keepalived-watcher-rqj9s requesting resource cpu=5m on Node 10.13.107.57
Jun 28 19:40:12.495: INFO: Pod ibm-keepalived-watcher-w6fhq requesting resource cpu=5m on Node 10.13.107.60
Jun 28 19:40:12.495: INFO: Pod ibm-master-proxy-static-10.13.107.37 requesting resource cpu=25m on Node 10.13.107.37
Jun 28 19:40:12.495: INFO: Pod ibm-master-proxy-static-10.13.107.57 requesting resource cpu=25m on Node 10.13.107.57
Jun 28 19:40:12.495: INFO: Pod ibm-master-proxy-static-10.13.107.60 requesting resource cpu=25m on Node 10.13.107.60
Jun 28 19:40:12.495: INFO: Pod ibm-storage-watcher-869677c8b-q8rh5 requesting resource cpu=50m on Node 10.13.107.57
Jun 28 19:40:12.495: INFO: Pod ibmcloud-block-storage-driver-kbnst requesting resource cpu=50m on Node 10.13.107.37
Jun 28 19:40:12.495: INFO: Pod ibmcloud-block-storage-driver-kwxt2 requesting resource cpu=50m on Node 10.13.107.60
Jun 28 19:40:12.495: INFO: Pod ibmcloud-block-storage-driver-p9gg7 requesting resource cpu=50m on Node 10.13.107.57
Jun 28 19:40:12.495: INFO: Pod ibmcloud-block-storage-plugin-665488684b-h5fhz requesting resource cpu=50m on Node 10.13.107.57
Jun 28 19:40:12.495: INFO: Pod vpn-6c58c4756c-rw748 requesting resource cpu=5m on Node 10.13.107.57
Jun 28 19:40:12.495: INFO: Pod cluster-node-tuning-operator-77dfdd89b8-chjmh requesting resource cpu=10m on Node 10.13.107.57
Jun 28 19:40:12.495: INFO: Pod tuned-2tw2w requesting resource cpu=10m on Node 10.13.107.37
Jun 28 19:40:12.495: INFO: Pod tuned-djhn5 requesting resource cpu=10m on Node 10.13.107.60
Jun 28 19:40:12.495: INFO: Pod tuned-r5kts requesting resource cpu=10m on Node 10.13.107.57
Jun 28 19:40:12.495: INFO: Pod cluster-samples-operator-56dd9bbcb-8mjfl requesting resource cpu=20m on Node 10.13.107.57
Jun 28 19:40:12.495: INFO: Pod cluster-storage-operator-56664f46b8-kjjbs requesting resource cpu=10m on Node 10.13.107.57
Jun 28 19:40:12.495: INFO: Pod console-operator-cd789fcfb-4j2b8 requesting resource cpu=10m on Node 10.13.107.57
Jun 28 19:40:12.495: INFO: Pod console-74dd6cb4c6-gbthm requesting resource cpu=10m on Node 10.13.107.60
Jun 28 19:40:12.495: INFO: Pod console-74dd6cb4c6-vnhx8 requesting resource cpu=10m on Node 10.13.107.57
Jun 28 19:40:12.495: INFO: Pod downloads-65dcc64f75-2bzn8 requesting resource cpu=10m on Node 10.13.107.57
Jun 28 19:40:12.495: INFO: Pod downloads-65dcc64f75-5hm2s requesting resource cpu=10m on Node 10.13.107.57
Jun 28 19:40:12.495: INFO: Pod dns-operator-7d8cb9bb6d-kkfvf requesting resource cpu=20m on Node 10.13.107.57
Jun 28 19:40:12.495: INFO: Pod dns-default-48b6x requesting resource cpu=65m on Node 10.13.107.57
Jun 28 19:40:12.495: INFO: Pod dns-default-84l5j requesting resource cpu=65m on Node 10.13.107.60
Jun 28 19:40:12.495: INFO: Pod dns-default-crxtq requesting resource cpu=65m on Node 10.13.107.37
Jun 28 19:40:12.495: INFO: Pod cluster-image-registry-operator-999d7b49c-d458n requesting resource cpu=10m on Node 10.13.107.57
Jun 28 19:40:12.495: INFO: Pod image-registry-7f9574d5c6-tmmns requesting resource cpu=100m on Node 10.13.107.60
Jun 28 19:40:12.495: INFO: Pod node-ca-42txp requesting resource cpu=10m on Node 10.13.107.60
Jun 28 19:40:12.495: INFO: Pod node-ca-49d6t requesting resource cpu=10m on Node 10.13.107.57
Jun 28 19:40:12.497: INFO: Pod node-ca-z64gj requesting resource cpu=10m on Node 10.13.107.37
Jun 28 19:40:12.497: INFO: Pod ingress-operator-795fb4477f-sgnmp requesting resource cpu=20m on Node 10.13.107.57
Jun 28 19:40:12.497: INFO: Pod router-default-8b99f5968-8nzw5 requesting resource cpu=100m on Node 10.13.107.60
Jun 28 19:40:12.497: INFO: Pod router-default-8b99f5968-b84k7 requesting resource cpu=100m on Node 10.13.107.57
Jun 28 19:40:12.497: INFO: Pod openshift-kube-proxy-8l7d5 requesting resource cpu=100m on Node 10.13.107.37
Jun 28 19:40:12.497: INFO: Pod openshift-kube-proxy-n6pzf requesting resource cpu=100m on Node 10.13.107.60
Jun 28 19:40:12.497: INFO: Pod openshift-kube-proxy-xq2t8 requesting resource cpu=100m on Node 10.13.107.57
Jun 28 19:40:12.497: INFO: Pod kube-storage-version-migrator-operator-77d5dd5f6c-z9m4j requesting resource cpu=10m on Node 10.13.107.57
Jun 28 19:40:12.497: INFO: Pod migrator-858cfc6f4c-tvj6h requesting resource cpu=100m on Node 10.13.107.60
Jun 28 19:40:12.497: INFO: Pod certified-operators-d5d5w requesting resource cpu=10m on Node 10.13.107.60
Jun 28 19:40:12.497: INFO: Pod community-operators-dbmgh requesting resource cpu=10m on Node 10.13.107.60
Jun 28 19:40:12.497: INFO: Pod marketplace-operator-7b9d5dcb99-nzgf8 requesting resource cpu=10m on Node 10.13.107.57
Jun 28 19:40:12.497: INFO: Pod redhat-marketplace-4t4vz requesting resource cpu=10m on Node 10.13.107.60
Jun 28 19:40:12.497: INFO: Pod redhat-operators-dvpnz requesting resource cpu=10m on Node 10.13.107.60
Jun 28 19:40:12.497: INFO: Pod alertmanager-main-0 requesting resource cpu=8m on Node 10.13.107.60
Jun 28 19:40:12.497: INFO: Pod alertmanager-main-1 requesting resource cpu=8m on Node 10.13.107.57
Jun 28 19:40:12.497: INFO: Pod alertmanager-main-2 requesting resource cpu=8m on Node 10.13.107.60
Jun 28 19:40:12.497: INFO: Pod cluster-monitoring-operator-6964464f67-jbfjk requesting resource cpu=11m on Node 10.13.107.57
Jun 28 19:40:12.497: INFO: Pod grafana-85d4b8dbd-jn67b requesting resource cpu=5m on Node 10.13.107.60
Jun 28 19:40:12.497: INFO: Pod kube-state-metrics-659c7b865d-974cs requesting resource cpu=4m on Node 10.13.107.60
Jun 28 19:40:12.497: INFO: Pod node-exporter-6s54d requesting resource cpu=9m on Node 10.13.107.60
Jun 28 19:40:12.497: INFO: Pod node-exporter-ld8qc requesting resource cpu=9m on Node 10.13.107.57
Jun 28 19:40:12.497: INFO: Pod node-exporter-qj7hs requesting resource cpu=9m on Node 10.13.107.37
Jun 28 19:40:12.497: INFO: Pod openshift-state-metrics-7cf4dc694b-d9w6x requesting resource cpu=3m on Node 10.13.107.60
Jun 28 19:40:12.497: INFO: Pod prometheus-adapter-8646bbc64d-nq9hj requesting resource cpu=1m on Node 10.13.107.60
Jun 28 19:40:12.497: INFO: Pod prometheus-adapter-8646bbc64d-stfnh requesting resource cpu=1m on Node 10.13.107.57
Jun 28 19:40:12.497: INFO: Pod prometheus-k8s-0 requesting resource cpu=75m on Node 10.13.107.57
Jun 28 19:40:12.497: INFO: Pod prometheus-k8s-1 requesting resource cpu=75m on Node 10.13.107.60
Jun 28 19:40:12.497: INFO: Pod prometheus-operator-5d48db6d9c-z9sxl requesting resource cpu=6m on Node 10.13.107.60
Jun 28 19:40:12.497: INFO: Pod telemeter-client-5584d54f85-bhqql requesting resource cpu=3m on Node 10.13.107.60
Jun 28 19:40:12.497: INFO: Pod thanos-querier-b46bb8497-dcbxw requesting resource cpu=9m on Node 10.13.107.57
Jun 28 19:40:12.497: INFO: Pod thanos-querier-b46bb8497-wxrxb requesting resource cpu=9m on Node 10.13.107.60
Jun 28 19:40:12.497: INFO: Pod multus-admission-controller-58k5w requesting resource cpu=20m on Node 10.13.107.57
Jun 28 19:40:12.497: INFO: Pod multus-admission-controller-5rwk7 requesting resource cpu=20m on Node 10.13.107.60
Jun 28 19:40:12.497: INFO: Pod multus-admission-controller-nfv4l requesting resource cpu=20m on Node 10.13.107.37
Jun 28 19:40:12.497: INFO: Pod multus-brfr7 requesting resource cpu=10m on Node 10.13.107.37
Jun 28 19:40:12.497: INFO: Pod multus-hvm6b requesting resource cpu=10m on Node 10.13.107.57
Jun 28 19:40:12.497: INFO: Pod multus-tsj42 requesting resource cpu=10m on Node 10.13.107.60
Jun 28 19:40:12.497: INFO: Pod network-metrics-daemon-5rjvj requesting resource cpu=20m on Node 10.13.107.37
Jun 28 19:40:12.497: INFO: Pod network-metrics-daemon-78dfb requesting resource cpu=20m on Node 10.13.107.57
Jun 28 19:40:12.497: INFO: Pod network-metrics-daemon-gzxgq requesting resource cpu=20m on Node 10.13.107.60
Jun 28 19:40:12.497: INFO: Pod network-operator-5b78d575b8-gvzvg requesting resource cpu=10m on Node 10.13.107.57
Jun 28 19:40:12.497: INFO: Pod catalog-operator-67c774db7d-5ps99 requesting resource cpu=10m on Node 10.13.107.57
Jun 28 19:40:12.497: INFO: Pod olm-operator-68fcf6954f-nfsgq requesting resource cpu=10m on Node 10.13.107.57
Jun 28 19:40:12.497: INFO: Pod packageserver-65544748bd-h6vfz requesting resource cpu=10m on Node 10.13.107.60
Jun 28 19:40:12.497: INFO: Pod packageserver-65544748bd-hhnld requesting resource cpu=10m on Node 10.13.107.57
Jun 28 19:40:12.497: INFO: Pod metrics-7df79584fc-f6g2g requesting resource cpu=10m on Node 10.13.107.57
Jun 28 19:40:12.497: INFO: Pod push-gateway-6dc86bc94f-dz7nt requesting resource cpu=10m on Node 10.13.107.57
Jun 28 19:40:12.497: INFO: Pod service-ca-operator-d4bfd498b-4xhnv requesting resource cpu=10m on Node 10.13.107.57
Jun 28 19:40:12.497: INFO: Pod service-ca-665fb97685-mn77q requesting resource cpu=10m on Node 10.13.107.57
Jun 28 19:40:12.497: INFO: Pod sonobuoy requesting resource cpu=0m on Node 10.13.107.57
Jun 28 19:40:12.497: INFO: Pod sonobuoy-e2e-job-3182b6be48fa4a64 requesting resource cpu=0m on Node 10.13.107.60
Jun 28 19:40:12.497: INFO: Pod sonobuoy-systemd-logs-daemon-set-bc3c64b0befe49cb-47fm7 requesting resource cpu=0m on Node 10.13.107.60
Jun 28 19:40:12.497: INFO: Pod sonobuoy-systemd-logs-daemon-set-bc3c64b0befe49cb-gp9nl requesting resource cpu=0m on Node 10.13.107.37
Jun 28 19:40:12.497: INFO: Pod sonobuoy-systemd-logs-daemon-set-bc3c64b0befe49cb-s48t5 requesting resource cpu=0m on Node 10.13.107.57
Jun 28 19:40:12.498: INFO: Pod tigera-operator-64c8f4c7d7-hzwkg requesting resource cpu=100m on Node 10.13.107.57
STEP: Starting Pods to consume most of the cluster CPU.
Jun 28 19:40:12.498: INFO: Creating a pod which consumes cpu=2160m on Node 10.13.107.37
Jun 28 19:40:12.538: INFO: Creating a pod which consumes cpu=1674m on Node 10.13.107.57
Jun 28 19:40:12.584: INFO: Creating a pod which consumes cpu=1812m on Node 10.13.107.60
STEP: Creating another pod that requires unavailable amount of CPU.
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-0f656577-69bc-4507-9103-26f1ae78f88f.168cd651db4df890], Reason = [Scheduled], Message = [Successfully assigned sched-pred-3866/filler-pod-0f656577-69bc-4507-9103-26f1ae78f88f to 10.13.107.60]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-0f656577-69bc-4507-9103-26f1ae78f88f.168cd6522d12de29], Reason = [AddedInterface], Message = [Add eth0 [172.30.196.123/32]]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-0f656577-69bc-4507-9103-26f1ae78f88f.168cd652310fe04f], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.2" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-0f656577-69bc-4507-9103-26f1ae78f88f.168cd652422be073], Reason = [Created], Message = [Created container filler-pod-0f656577-69bc-4507-9103-26f1ae78f88f]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-0f656577-69bc-4507-9103-26f1ae78f88f.168cd65246a91b1d], Reason = [Started], Message = [Started container filler-pod-0f656577-69bc-4507-9103-26f1ae78f88f]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-244a7279-8a35-45c7-a42e-c2ac5ba4a43c.168cd651d9456aec], Reason = [Scheduled], Message = [Successfully assigned sched-pred-3866/filler-pod-244a7279-8a35-45c7-a42e-c2ac5ba4a43c to 10.13.107.57]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-244a7279-8a35-45c7-a42e-c2ac5ba4a43c.168cd6523076a068], Reason = [AddedInterface], Message = [Add eth0 [172.30.54.45/32]]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-244a7279-8a35-45c7-a42e-c2ac5ba4a43c.168cd652344f9e62], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.2" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-244a7279-8a35-45c7-a42e-c2ac5ba4a43c.168cd6523ef25e90], Reason = [Created], Message = [Created container filler-pod-244a7279-8a35-45c7-a42e-c2ac5ba4a43c]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-244a7279-8a35-45c7-a42e-c2ac5ba4a43c.168cd652418f9b85], Reason = [Started], Message = [Started container filler-pod-244a7279-8a35-45c7-a42e-c2ac5ba4a43c]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-51767382-f428-4ce7-a043-354ae8d03ef2.168cd651d6619af3], Reason = [Scheduled], Message = [Successfully assigned sched-pred-3866/filler-pod-51767382-f428-4ce7-a043-354ae8d03ef2 to 10.13.107.37]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-51767382-f428-4ce7-a043-354ae8d03ef2.168cd652122eee76], Reason = [AddedInterface], Message = [Add eth0 [172.30.221.163/32]]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-51767382-f428-4ce7-a043-354ae8d03ef2.168cd6521476ea09], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.2" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-51767382-f428-4ce7-a043-354ae8d03ef2.168cd6521ee35c14], Reason = [Created], Message = [Created container filler-pod-51767382-f428-4ce7-a043-354ae8d03ef2]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-51767382-f428-4ce7-a043-354ae8d03ef2.168cd6522113b10d], Reason = [Started], Message = [Started container filler-pod-51767382-f428-4ce7-a043-354ae8d03ef2]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.168cd652cfbe5945], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 Insufficient cpu.]
STEP: removing the label node off the node 10.13.107.37
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node 10.13.107.57
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node 10.13.107.60
STEP: verifying the node doesn't have the label node
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 19:40:17.896: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-3866" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81

• [SLOW TEST:6.110 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates resource limits of pods that are allowed to run  [Conformance]","total":305,"completed":266,"skipped":4095,"failed":0}
SSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 19:40:17.937: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the rc1
STEP: create the rc2
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well
STEP: delete the rc simpletest-rc-to-be-deleted
STEP: wait for the rc to be deleted
STEP: Gathering metrics
Jun 28 19:40:28.337: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

W0628 19:40:28.337566      22 metrics_grabber.go:98] Can't find kube-scheduler pod. Grabbing metrics from kube-scheduler is disabled.
W0628 19:40:28.337596      22 metrics_grabber.go:102] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
W0628 19:40:28.337603      22 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
Jun 28 19:40:28.337: INFO: Deleting pod "simpletest-rc-to-be-deleted-5q7ct" in namespace "gc-3037"
Jun 28 19:40:28.379: INFO: Deleting pod "simpletest-rc-to-be-deleted-6sd97" in namespace "gc-3037"
Jun 28 19:40:28.493: INFO: Deleting pod "simpletest-rc-to-be-deleted-8h7fb" in namespace "gc-3037"
Jun 28 19:40:28.525: INFO: Deleting pod "simpletest-rc-to-be-deleted-bm287" in namespace "gc-3037"
Jun 28 19:40:28.556: INFO: Deleting pod "simpletest-rc-to-be-deleted-gvrgc" in namespace "gc-3037"
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 19:40:28.587: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-3037" for this suite.

• [SLOW TEST:10.684 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]","total":305,"completed":267,"skipped":4099,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 19:40:28.625: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount an API token into pods  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: getting the auto-created API token
STEP: reading a file in the container
Jun 28 19:40:31.375: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-1443 pod-service-account-67d2305e-de28-4190-9c30-034989a23f34 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container
Jun 28 19:40:31.791: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-1443 pod-service-account-67d2305e-de28-4190-9c30-034989a23f34 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container
Jun 28 19:40:32.213: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-1443 pod-service-account-67d2305e-de28-4190-9c30-034989a23f34 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 19:40:32.705: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-1443" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should mount an API token into pods  [Conformance]","total":305,"completed":268,"skipped":4143,"failed":0}
SSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 19:40:32.736: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: modifying the configmap a second time
STEP: deleting the configmap
STEP: creating a watch on configmaps from the resource version returned by the first update
STEP: Expecting to observe notifications for all changes to the configmap after the first update
Jun 28 19:40:32.954: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-1187 /api/v1/namespaces/watch-1187/configmaps/e2e-watch-test-resource-version f56eefc5-2d42-4ae6-a43c-c9d63b05bcff 110225 0 2021-06-28 19:40:32 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  [{e2e.test Update v1 2021-06-28 19:40:32 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Jun 28 19:40:32.954: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-1187 /api/v1/namespaces/watch-1187/configmaps/e2e-watch-test-resource-version f56eefc5-2d42-4ae6-a43c-c9d63b05bcff 110226 0 2021-06-28 19:40:32 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  [{e2e.test Update v1 2021-06-28 19:40:32 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 19:40:32.955: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-1187" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should be able to start watching from a specific resource version [Conformance]","total":305,"completed":269,"skipped":4153,"failed":0}
SSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing validating webhooks should work [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 19:40:32.986: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jun 28 19:40:33.996: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jun 28 19:40:36.032: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63760506034, loc:(*time.Location)(0x7702840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63760506034, loc:(*time.Location)(0x7702840)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63760506034, loc:(*time.Location)(0x7702840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63760506033, loc:(*time.Location)(0x7702840)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun 28 19:40:39.076: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing validating webhooks should work [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 19:40:39.626: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1667" for this suite.
STEP: Destroying namespace "webhook-1667-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:6.817 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  listing validating webhooks should work [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing validating webhooks should work [Conformance]","total":305,"completed":270,"skipped":4160,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 19:40:39.803: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for services  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-2837.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-2837.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-2837.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-2837.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-2837.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-2837.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-2837.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-2837.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-2837.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-2837.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-2837.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-2837.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-2837.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 116.161.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.161.116_udp@PTR;check="$$(dig +tcp +noall +answer +search 116.161.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.161.116_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-2837.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-2837.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-2837.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-2837.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-2837.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-2837.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-2837.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-2837.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-2837.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-2837.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-2837.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-2837.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-2837.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 116.161.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.161.116_udp@PTR;check="$$(dig +tcp +noall +answer +search 116.161.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.161.116_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jun 28 19:40:42.222: INFO: Unable to read wheezy_udp@dns-test-service.dns-2837.svc.cluster.local from pod dns-2837/dns-test-aec202c6-9853-49c9-9c5c-1608f168e0b6: the server could not find the requested resource (get pods dns-test-aec202c6-9853-49c9-9c5c-1608f168e0b6)
Jun 28 19:40:42.275: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2837.svc.cluster.local from pod dns-2837/dns-test-aec202c6-9853-49c9-9c5c-1608f168e0b6: the server could not find the requested resource (get pods dns-test-aec202c6-9853-49c9-9c5c-1608f168e0b6)
Jun 28 19:40:42.300: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-2837.svc.cluster.local from pod dns-2837/dns-test-aec202c6-9853-49c9-9c5c-1608f168e0b6: the server could not find the requested resource (get pods dns-test-aec202c6-9853-49c9-9c5c-1608f168e0b6)
Jun 28 19:40:42.338: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-2837.svc.cluster.local from pod dns-2837/dns-test-aec202c6-9853-49c9-9c5c-1608f168e0b6: the server could not find the requested resource (get pods dns-test-aec202c6-9853-49c9-9c5c-1608f168e0b6)
Jun 28 19:40:42.577: INFO: Unable to read jessie_udp@dns-test-service.dns-2837.svc.cluster.local from pod dns-2837/dns-test-aec202c6-9853-49c9-9c5c-1608f168e0b6: the server could not find the requested resource (get pods dns-test-aec202c6-9853-49c9-9c5c-1608f168e0b6)
Jun 28 19:40:42.642: INFO: Unable to read jessie_tcp@dns-test-service.dns-2837.svc.cluster.local from pod dns-2837/dns-test-aec202c6-9853-49c9-9c5c-1608f168e0b6: the server could not find the requested resource (get pods dns-test-aec202c6-9853-49c9-9c5c-1608f168e0b6)
Jun 28 19:40:42.671: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-2837.svc.cluster.local from pod dns-2837/dns-test-aec202c6-9853-49c9-9c5c-1608f168e0b6: the server could not find the requested resource (get pods dns-test-aec202c6-9853-49c9-9c5c-1608f168e0b6)
Jun 28 19:40:42.712: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-2837.svc.cluster.local from pod dns-2837/dns-test-aec202c6-9853-49c9-9c5c-1608f168e0b6: the server could not find the requested resource (get pods dns-test-aec202c6-9853-49c9-9c5c-1608f168e0b6)
Jun 28 19:40:42.838: INFO: Lookups using dns-2837/dns-test-aec202c6-9853-49c9-9c5c-1608f168e0b6 failed for: [wheezy_udp@dns-test-service.dns-2837.svc.cluster.local wheezy_tcp@dns-test-service.dns-2837.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-2837.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-2837.svc.cluster.local jessie_udp@dns-test-service.dns-2837.svc.cluster.local jessie_tcp@dns-test-service.dns-2837.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-2837.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-2837.svc.cluster.local]

Jun 28 19:40:48.353: INFO: DNS probes using dns-2837/dns-test-aec202c6-9853-49c9-9c5c-1608f168e0b6 succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 19:40:48.470: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-2837" for this suite.

• [SLOW TEST:8.699 seconds]
[sig-network] DNS
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for services  [Conformance]","total":305,"completed":271,"skipped":4178,"failed":0}
SSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 19:40:48.504: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name projected-configmap-test-volume-map-8ebf8605-fa27-44d5-b662-14b1d09e8c12
STEP: Creating a pod to test consume configMaps
Jun 28 19:40:48.812: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-99db176a-12f8-4c7a-b6c5-571819d99ef9" in namespace "projected-3881" to be "Succeeded or Failed"
Jun 28 19:40:48.823: INFO: Pod "pod-projected-configmaps-99db176a-12f8-4c7a-b6c5-571819d99ef9": Phase="Pending", Reason="", readiness=false. Elapsed: 10.592179ms
Jun 28 19:40:50.835: INFO: Pod "pod-projected-configmaps-99db176a-12f8-4c7a-b6c5-571819d99ef9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022470602s
Jun 28 19:40:52.858: INFO: Pod "pod-projected-configmaps-99db176a-12f8-4c7a-b6c5-571819d99ef9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.046244476s
STEP: Saw pod success
Jun 28 19:40:52.859: INFO: Pod "pod-projected-configmaps-99db176a-12f8-4c7a-b6c5-571819d99ef9" satisfied condition "Succeeded or Failed"
Jun 28 19:40:52.872: INFO: Trying to get logs from node 10.13.107.37 pod pod-projected-configmaps-99db176a-12f8-4c7a-b6c5-571819d99ef9 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jun 28 19:40:52.943: INFO: Waiting for pod pod-projected-configmaps-99db176a-12f8-4c7a-b6c5-571819d99ef9 to disappear
Jun 28 19:40:52.952: INFO: Pod pod-projected-configmaps-99db176a-12f8-4c7a-b6c5-571819d99ef9 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 19:40:52.952: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3881" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":305,"completed":272,"skipped":4181,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 19:40:52.987: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jun 28 19:40:53.127: INFO: Creating ReplicaSet my-hostname-basic-b0f568a7-d616-4c76-9a57-63b30c733280
Jun 28 19:40:53.158: INFO: Pod name my-hostname-basic-b0f568a7-d616-4c76-9a57-63b30c733280: Found 0 pods out of 1
Jun 28 19:40:58.180: INFO: Pod name my-hostname-basic-b0f568a7-d616-4c76-9a57-63b30c733280: Found 1 pods out of 1
Jun 28 19:40:58.180: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-b0f568a7-d616-4c76-9a57-63b30c733280" is running
Jun 28 19:40:58.190: INFO: Pod "my-hostname-basic-b0f568a7-d616-4c76-9a57-63b30c733280-2nb2j" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-06-28 19:40:53 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-06-28 19:40:54 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-06-28 19:40:54 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-06-28 19:40:53 +0000 UTC Reason: Message:}])
Jun 28 19:40:58.190: INFO: Trying to dial the pod
Jun 28 19:41:03.246: INFO: Controller my-hostname-basic-b0f568a7-d616-4c76-9a57-63b30c733280: Got expected result from replica 1 [my-hostname-basic-b0f568a7-d616-4c76-9a57-63b30c733280-2nb2j]: "my-hostname-basic-b0f568a7-d616-4c76-9a57-63b30c733280-2nb2j", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 19:41:03.247: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-7534" for this suite.

• [SLOW TEST:10.306 seconds]
[sig-apps] ReplicaSet
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet should serve a basic image on each replica with a public image  [Conformance]","total":305,"completed":273,"skipped":4197,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 19:41:03.293: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0666 on node default medium
Jun 28 19:41:03.497: INFO: Waiting up to 5m0s for pod "pod-06e9054f-f7c3-480f-acb6-23f4a46cbb47" in namespace "emptydir-3794" to be "Succeeded or Failed"
Jun 28 19:41:03.520: INFO: Pod "pod-06e9054f-f7c3-480f-acb6-23f4a46cbb47": Phase="Pending", Reason="", readiness=false. Elapsed: 23.289106ms
Jun 28 19:41:05.532: INFO: Pod "pod-06e9054f-f7c3-480f-acb6-23f4a46cbb47": Phase="Pending", Reason="", readiness=false. Elapsed: 2.034787068s
Jun 28 19:41:07.547: INFO: Pod "pod-06e9054f-f7c3-480f-acb6-23f4a46cbb47": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.050216791s
STEP: Saw pod success
Jun 28 19:41:07.547: INFO: Pod "pod-06e9054f-f7c3-480f-acb6-23f4a46cbb47" satisfied condition "Succeeded or Failed"
Jun 28 19:41:07.558: INFO: Trying to get logs from node 10.13.107.37 pod pod-06e9054f-f7c3-480f-acb6-23f4a46cbb47 container test-container: <nil>
STEP: delete the pod
Jun 28 19:41:07.631: INFO: Waiting for pod pod-06e9054f-f7c3-480f-acb6-23f4a46cbb47 to disappear
Jun 28 19:41:07.643: INFO: Pod pod-06e9054f-f7c3-480f-acb6-23f4a46cbb47 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 19:41:07.643: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3794" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":274,"skipped":4222,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should include webhook resources in discovery documents [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 19:41:07.681: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jun 28 19:41:08.499: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jun 28 19:41:10.531: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63760506068, loc:(*time.Location)(0x7702840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63760506068, loc:(*time.Location)(0x7702840)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63760506068, loc:(*time.Location)(0x7702840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63760506068, loc:(*time.Location)(0x7702840)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun 28 19:41:13.582: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should include webhook resources in discovery documents [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: fetching the /apis discovery document
STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/admissionregistration.k8s.io discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document
STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document
STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 19:41:13.610: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3530" for this suite.
STEP: Destroying namespace "webhook-3530-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:6.119 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should include webhook resources in discovery documents [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should include webhook resources in discovery documents [Conformance]","total":305,"completed":275,"skipped":4248,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 19:41:13.801: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Performing setup for networking test in namespace pod-network-test-772
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Jun 28 19:41:13.940: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Jun 28 19:41:14.122: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jun 28 19:41:16.136: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun 28 19:41:18.142: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun 28 19:41:20.136: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun 28 19:41:22.140: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun 28 19:41:24.142: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun 28 19:41:26.138: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun 28 19:41:28.140: INFO: The status of Pod netserver-0 is Running (Ready = true)
Jun 28 19:41:28.160: INFO: The status of Pod netserver-1 is Running (Ready = false)
Jun 28 19:41:30.174: INFO: The status of Pod netserver-1 is Running (Ready = false)
Jun 28 19:41:32.179: INFO: The status of Pod netserver-1 is Running (Ready = false)
Jun 28 19:41:34.175: INFO: The status of Pod netserver-1 is Running (Ready = true)
Jun 28 19:41:34.196: INFO: The status of Pod netserver-2 is Running (Ready = true)
STEP: Creating test pods
Jun 28 19:41:36.349: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.30.221.174 8081 | grep -v '^\s*$'] Namespace:pod-network-test-772 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 28 19:41:36.349: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
Jun 28 19:41:37.634: INFO: Found all expected endpoints: [netserver-0]
Jun 28 19:41:37.646: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.30.54.47 8081 | grep -v '^\s*$'] Namespace:pod-network-test-772 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 28 19:41:37.646: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
Jun 28 19:41:38.831: INFO: Found all expected endpoints: [netserver-1]
Jun 28 19:41:38.849: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.30.196.89 8081 | grep -v '^\s*$'] Namespace:pod-network-test-772 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 28 19:41:38.849: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
Jun 28 19:41:40.058: INFO: Found all expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 19:41:40.058: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-772" for this suite.

• [SLOW TEST:26.291 seconds]
[sig-network] Networking
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:26
  Granular Checks: Pods
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:29
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":276,"skipped":4259,"failed":0}
S
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 19:41:40.092: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name projected-configmap-test-volume-map-b5b09048-6912-4bac-9a2a-f3f884cb1ddf
STEP: Creating a pod to test consume configMaps
Jun 28 19:41:40.334: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-9f423999-331c-4368-bf41-40b5142e0b31" in namespace "projected-7916" to be "Succeeded or Failed"
Jun 28 19:41:40.349: INFO: Pod "pod-projected-configmaps-9f423999-331c-4368-bf41-40b5142e0b31": Phase="Pending", Reason="", readiness=false. Elapsed: 14.532505ms
Jun 28 19:41:42.369: INFO: Pod "pod-projected-configmaps-9f423999-331c-4368-bf41-40b5142e0b31": Phase="Pending", Reason="", readiness=false. Elapsed: 2.034433371s
Jun 28 19:41:44.399: INFO: Pod "pod-projected-configmaps-9f423999-331c-4368-bf41-40b5142e0b31": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.064865227s
STEP: Saw pod success
Jun 28 19:41:44.399: INFO: Pod "pod-projected-configmaps-9f423999-331c-4368-bf41-40b5142e0b31" satisfied condition "Succeeded or Failed"
Jun 28 19:41:44.409: INFO: Trying to get logs from node 10.13.107.37 pod pod-projected-configmaps-9f423999-331c-4368-bf41-40b5142e0b31 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jun 28 19:41:44.471: INFO: Waiting for pod pod-projected-configmaps-9f423999-331c-4368-bf41-40b5142e0b31 to disappear
Jun 28 19:41:44.482: INFO: Pod pod-projected-configmaps-9f423999-331c-4368-bf41-40b5142e0b31 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 19:41:44.482: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7916" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":277,"skipped":4260,"failed":0}
SSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should succeed in writing subpaths in container [sig-storage][Slow] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 19:41:44.513: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should succeed in writing subpaths in container [sig-storage][Slow] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod
STEP: waiting for pod running
STEP: creating a file in subpath
Jun 28 19:41:46.737: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-6652 PodName:var-expansion-d2f916e2-e5d2-4ec0-acb0-232936f1b505 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 28 19:41:46.738: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: test for file in mounted path
Jun 28 19:41:47.240: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-6652 PodName:var-expansion-d2f916e2-e5d2-4ec0-acb0-232936f1b505 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 28 19:41:47.240: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: updating the annotation value
Jun 28 19:41:48.385: INFO: Successfully updated pod "var-expansion-d2f916e2-e5d2-4ec0-acb0-232936f1b505"
STEP: waiting for annotated pod running
STEP: deleting the pod gracefully
Jun 28 19:41:48.395: INFO: Deleting pod "var-expansion-d2f916e2-e5d2-4ec0-acb0-232936f1b505" in namespace "var-expansion-6652"
Jun 28 19:41:48.412: INFO: Wait up to 5m0s for pod "var-expansion-d2f916e2-e5d2-4ec0-acb0-232936f1b505" to be fully deleted
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 19:42:28.432: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-6652" for this suite.

• [SLOW TEST:43.953 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should succeed in writing subpaths in container [sig-storage][Slow] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Variable Expansion should succeed in writing subpaths in container [sig-storage][Slow] [Conformance]","total":305,"completed":278,"skipped":4268,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should honor timeout [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 19:42:28.470: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jun 28 19:42:29.094: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Jun 28 19:42:31.124: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63760506149, loc:(*time.Location)(0x7702840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63760506149, loc:(*time.Location)(0x7702840)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63760506149, loc:(*time.Location)(0x7702840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63760506149, loc:(*time.Location)(0x7702840)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun 28 19:42:34.176: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should honor timeout [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Setting timeout (1s) shorter than webhook latency (5s)
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s)
STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is longer than webhook latency
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is empty (defaulted to 10s in v1)
STEP: Registering slow webhook via the AdmissionRegistration API
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 19:42:46.759: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3058" for this suite.
STEP: Destroying namespace "webhook-3058-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:18.449 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should honor timeout [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should honor timeout [Conformance]","total":305,"completed":279,"skipped":4286,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should run through the lifecycle of a ServiceAccount [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 19:42:46.922: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run through the lifecycle of a ServiceAccount [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a ServiceAccount
STEP: watching for the ServiceAccount to be added
STEP: patching the ServiceAccount
STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector)
STEP: deleting the ServiceAccount
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 19:42:47.324: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-1968" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should run through the lifecycle of a ServiceAccount [Conformance]","total":305,"completed":280,"skipped":4318,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate pod and apply defaults after mutation [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 19:42:47.369: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jun 28 19:42:48.217: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jun 28 19:42:50.317: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63760506168, loc:(*time.Location)(0x7702840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63760506168, loc:(*time.Location)(0x7702840)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63760506168, loc:(*time.Location)(0x7702840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63760506168, loc:(*time.Location)(0x7702840)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun 28 19:42:53.360: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate pod and apply defaults after mutation [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Registering the mutating pod webhook via the AdmissionRegistration API
STEP: create a pod that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 19:42:53.578: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6915" for this suite.
STEP: Destroying namespace "webhook-6915-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:6.375 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate pod and apply defaults after mutation [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate pod and apply defaults after mutation [Conformance]","total":305,"completed":281,"skipped":4391,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 19:42:53.755: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating secret with name secret-test-map-102dc358-b315-47fa-ae0e-f10de1090e6e
STEP: Creating a pod to test consume secrets
Jun 28 19:42:53.959: INFO: Waiting up to 5m0s for pod "pod-secrets-1eccf4c4-fd46-4847-85b5-0ed145572373" in namespace "secrets-7337" to be "Succeeded or Failed"
Jun 28 19:42:53.969: INFO: Pod "pod-secrets-1eccf4c4-fd46-4847-85b5-0ed145572373": Phase="Pending", Reason="", readiness=false. Elapsed: 9.970836ms
Jun 28 19:42:55.982: INFO: Pod "pod-secrets-1eccf4c4-fd46-4847-85b5-0ed145572373": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023049923s
Jun 28 19:42:58.000: INFO: Pod "pod-secrets-1eccf4c4-fd46-4847-85b5-0ed145572373": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.041737988s
STEP: Saw pod success
Jun 28 19:42:58.000: INFO: Pod "pod-secrets-1eccf4c4-fd46-4847-85b5-0ed145572373" satisfied condition "Succeeded or Failed"
Jun 28 19:42:58.010: INFO: Trying to get logs from node 10.13.107.37 pod pod-secrets-1eccf4c4-fd46-4847-85b5-0ed145572373 container secret-volume-test: <nil>
STEP: delete the pod
Jun 28 19:42:58.074: INFO: Waiting for pod pod-secrets-1eccf4c4-fd46-4847-85b5-0ed145572373 to disappear
Jun 28 19:42:58.083: INFO: Pod pod-secrets-1eccf4c4-fd46-4847-85b5-0ed145572373 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 19:42:58.084: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-7337" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":282,"skipped":4409,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should provide secure master service  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 19:42:58.133: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should provide secure master service  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 19:42:58.267: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4827" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786
•{"msg":"PASSED [sig-network] Services should provide secure master service  [Conformance]","total":305,"completed":283,"skipped":4477,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 19:42:58.307: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Discovering how many secrets are in namespace by default
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Secret
STEP: Ensuring resource quota status captures secret creation
STEP: Deleting a secret
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 19:43:15.601: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-4896" for this suite.

• [SLOW TEST:17.332 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a secret. [Conformance]","total":305,"completed":284,"skipped":4563,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 19:43:15.641: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ConfigMap
STEP: Ensuring resource quota status captures configMap creation
STEP: Deleting a ConfigMap
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 19:43:31.929: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-6757" for this suite.

• [SLOW TEST:16.318 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a configMap. [Conformance]","total":305,"completed":285,"skipped":4577,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 19:43:31.963: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jun 28 19:43:32.109: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
STEP: Creating secret with name s-test-opt-del-d01958bb-660f-4e6a-bd49-7d6509c39696
STEP: Creating secret with name s-test-opt-upd-b601ce31-c910-4208-8623-d11a79b97737
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-d01958bb-660f-4e6a-bd49-7d6509c39696
STEP: Updating secret s-test-opt-upd-b601ce31-c910-4208-8623-d11a79b97737
STEP: Creating secret with name s-test-opt-create-e10e6384-1713-4776-899d-3d2c3170eff3
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 19:45:01.879: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1197" for this suite.

• [SLOW TEST:89.960 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Projected secret optional updates should be reflected in volume [NodeConformance] [Conformance]","total":305,"completed":286,"skipped":4607,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 19:45:01.931: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jun 28 19:45:02.198: INFO: The status of Pod test-webserver-8687b8dd-6f62-48dd-be40-a85c0df48935 is Pending, waiting for it to be Running (with Ready = true)
Jun 28 19:45:04.215: INFO: The status of Pod test-webserver-8687b8dd-6f62-48dd-be40-a85c0df48935 is Pending, waiting for it to be Running (with Ready = true)
Jun 28 19:45:06.214: INFO: The status of Pod test-webserver-8687b8dd-6f62-48dd-be40-a85c0df48935 is Running (Ready = false)
Jun 28 19:45:08.213: INFO: The status of Pod test-webserver-8687b8dd-6f62-48dd-be40-a85c0df48935 is Running (Ready = false)
Jun 28 19:45:10.215: INFO: The status of Pod test-webserver-8687b8dd-6f62-48dd-be40-a85c0df48935 is Running (Ready = false)
Jun 28 19:45:12.265: INFO: The status of Pod test-webserver-8687b8dd-6f62-48dd-be40-a85c0df48935 is Running (Ready = false)
Jun 28 19:45:14.212: INFO: The status of Pod test-webserver-8687b8dd-6f62-48dd-be40-a85c0df48935 is Running (Ready = false)
Jun 28 19:45:16.214: INFO: The status of Pod test-webserver-8687b8dd-6f62-48dd-be40-a85c0df48935 is Running (Ready = false)
Jun 28 19:45:18.216: INFO: The status of Pod test-webserver-8687b8dd-6f62-48dd-be40-a85c0df48935 is Running (Ready = false)
Jun 28 19:45:20.218: INFO: The status of Pod test-webserver-8687b8dd-6f62-48dd-be40-a85c0df48935 is Running (Ready = false)
Jun 28 19:45:22.222: INFO: The status of Pod test-webserver-8687b8dd-6f62-48dd-be40-a85c0df48935 is Running (Ready = true)
Jun 28 19:45:22.233: INFO: Container started at 2021-06-28 19:45:03 +0000 UTC, pod became ready at 2021-06-28 19:45:21 +0000 UTC
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 19:45:22.234: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-456" for this suite.

• [SLOW TEST:20.448 seconds]
[k8s.io] Probing container
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Probing container with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]","total":305,"completed":287,"skipped":4628,"failed":0}
SSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 19:45:22.385: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicaSet
STEP: Ensuring resource quota status captures replicaset creation
STEP: Deleting a ReplicaSet
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 19:45:33.676: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-4429" for this suite.

• [SLOW TEST:11.327 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replica set. [Conformance]","total":305,"completed":288,"skipped":4635,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 19:45:33.714: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating secret with name secret-test-168068da-2dd5-4d74-ac79-e4acec78f33d
STEP: Creating a pod to test consume secrets
Jun 28 19:45:33.910: INFO: Waiting up to 5m0s for pod "pod-secrets-51d9e23b-b4d3-4370-8979-8ee393a6f63e" in namespace "secrets-6236" to be "Succeeded or Failed"
Jun 28 19:45:33.923: INFO: Pod "pod-secrets-51d9e23b-b4d3-4370-8979-8ee393a6f63e": Phase="Pending", Reason="", readiness=false. Elapsed: 12.896956ms
Jun 28 19:45:35.941: INFO: Pod "pod-secrets-51d9e23b-b4d3-4370-8979-8ee393a6f63e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.031224585s
STEP: Saw pod success
Jun 28 19:45:35.942: INFO: Pod "pod-secrets-51d9e23b-b4d3-4370-8979-8ee393a6f63e" satisfied condition "Succeeded or Failed"
Jun 28 19:45:35.952: INFO: Trying to get logs from node 10.13.107.37 pod pod-secrets-51d9e23b-b4d3-4370-8979-8ee393a6f63e container secret-volume-test: <nil>
STEP: delete the pod
Jun 28 19:45:36.014: INFO: Waiting for pod pod-secrets-51d9e23b-b4d3-4370-8979-8ee393a6f63e to disappear
Jun 28 19:45:36.027: INFO: Pod pod-secrets-51d9e23b-b4d3-4370-8979-8ee393a6f63e no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 19:45:36.027: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6236" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":289,"skipped":4659,"failed":0}
SSSSSSSSSSSS
------------------------------
[k8s.io] Security Context When creating a pod with readOnlyRootFilesystem 
  should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 19:45:36.061: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:41
[It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jun 28 19:45:36.222: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-73a70b4e-3dd5-4a33-8176-91e9eb00aa5b" in namespace "security-context-test-4289" to be "Succeeded or Failed"
Jun 28 19:45:36.234: INFO: Pod "busybox-readonly-false-73a70b4e-3dd5-4a33-8176-91e9eb00aa5b": Phase="Pending", Reason="", readiness=false. Elapsed: 12.320219ms
Jun 28 19:45:38.251: INFO: Pod "busybox-readonly-false-73a70b4e-3dd5-4a33-8176-91e9eb00aa5b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.02952296s
Jun 28 19:45:38.251: INFO: Pod "busybox-readonly-false-73a70b4e-3dd5-4a33-8176-91e9eb00aa5b" satisfied condition "Succeeded or Failed"
[AfterEach] [k8s.io] Security Context
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 19:45:38.252: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-4289" for this suite.
•{"msg":"PASSED [k8s.io] Security Context When creating a pod with readOnlyRootFilesystem should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]","total":305,"completed":290,"skipped":4671,"failed":0}
SS
------------------------------
[sig-apps] Job 
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Job
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 19:45:38.288: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a job
STEP: Ensuring job reaches completions
[AfterEach] [sig-apps] Job
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 19:45:46.450: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-2808" for this suite.

• [SLOW TEST:8.207 seconds]
[sig-apps] Job
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Job should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]","total":305,"completed":291,"skipped":4673,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 19:45:46.494: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating projection with secret that has name projected-secret-test-map-a2f9916c-56c0-4c57-9b72-858790535da1
STEP: Creating a pod to test consume secrets
Jun 28 19:45:46.686: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-109bd956-00d2-4587-bd23-52f202a87e40" in namespace "projected-8608" to be "Succeeded or Failed"
Jun 28 19:45:46.697: INFO: Pod "pod-projected-secrets-109bd956-00d2-4587-bd23-52f202a87e40": Phase="Pending", Reason="", readiness=false. Elapsed: 10.088981ms
Jun 28 19:45:48.708: INFO: Pod "pod-projected-secrets-109bd956-00d2-4587-bd23-52f202a87e40": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.020951297s
STEP: Saw pod success
Jun 28 19:45:48.708: INFO: Pod "pod-projected-secrets-109bd956-00d2-4587-bd23-52f202a87e40" satisfied condition "Succeeded or Failed"
Jun 28 19:45:48.718: INFO: Trying to get logs from node 10.13.107.37 pod pod-projected-secrets-109bd956-00d2-4587-bd23-52f202a87e40 container projected-secret-volume-test: <nil>
STEP: delete the pod
Jun 28 19:45:48.777: INFO: Waiting for pod pod-projected-secrets-109bd956-00d2-4587-bd23-52f202a87e40 to disappear
Jun 28 19:45:48.795: INFO: Pod pod-projected-secrets-109bd956-00d2-4587-bd23-52f202a87e40 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 19:45:48.795: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8608" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":292,"skipped":4697,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 19:45:48.836: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward api env vars
Jun 28 19:45:49.005: INFO: Waiting up to 5m0s for pod "downward-api-2e45d554-6f76-48f5-8d53-6e3ccd07bbac" in namespace "downward-api-5808" to be "Succeeded or Failed"
Jun 28 19:45:49.014: INFO: Pod "downward-api-2e45d554-6f76-48f5-8d53-6e3ccd07bbac": Phase="Pending", Reason="", readiness=false. Elapsed: 9.19056ms
Jun 28 19:45:51.031: INFO: Pod "downward-api-2e45d554-6f76-48f5-8d53-6e3ccd07bbac": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.026274627s
STEP: Saw pod success
Jun 28 19:45:51.031: INFO: Pod "downward-api-2e45d554-6f76-48f5-8d53-6e3ccd07bbac" satisfied condition "Succeeded or Failed"
Jun 28 19:45:51.040: INFO: Trying to get logs from node 10.13.107.37 pod downward-api-2e45d554-6f76-48f5-8d53-6e3ccd07bbac container dapi-container: <nil>
STEP: delete the pod
Jun 28 19:45:51.095: INFO: Waiting for pod downward-api-2e45d554-6f76-48f5-8d53-6e3ccd07bbac to disappear
Jun 28 19:45:51.105: INFO: Pod downward-api-2e45d554-6f76-48f5-8d53-6e3ccd07bbac no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 19:45:51.106: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5808" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]","total":305,"completed":293,"skipped":4717,"failed":0}
SS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 19:45:51.139: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name configmap-test-volume-a13d3f2d-2a23-4cd0-bd37-aef5c0a13088
STEP: Creating a pod to test consume configMaps
Jun 28 19:45:51.359: INFO: Waiting up to 5m0s for pod "pod-configmaps-decf96c3-9269-4103-acff-db2a7eefaa46" in namespace "configmap-241" to be "Succeeded or Failed"
Jun 28 19:45:51.372: INFO: Pod "pod-configmaps-decf96c3-9269-4103-acff-db2a7eefaa46": Phase="Pending", Reason="", readiness=false. Elapsed: 11.999099ms
Jun 28 19:45:53.385: INFO: Pod "pod-configmaps-decf96c3-9269-4103-acff-db2a7eefaa46": Phase="Running", Reason="", readiness=true. Elapsed: 2.02539929s
Jun 28 19:45:55.403: INFO: Pod "pod-configmaps-decf96c3-9269-4103-acff-db2a7eefaa46": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.042819829s
STEP: Saw pod success
Jun 28 19:45:55.403: INFO: Pod "pod-configmaps-decf96c3-9269-4103-acff-db2a7eefaa46" satisfied condition "Succeeded or Failed"
Jun 28 19:45:55.414: INFO: Trying to get logs from node 10.13.107.37 pod pod-configmaps-decf96c3-9269-4103-acff-db2a7eefaa46 container configmap-volume-test: <nil>
STEP: delete the pod
Jun 28 19:45:55.480: INFO: Waiting for pod pod-configmaps-decf96c3-9269-4103-acff-db2a7eefaa46 to disappear
Jun 28 19:45:55.492: INFO: Pod pod-configmaps-decf96c3-9269-4103-acff-db2a7eefaa46 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 19:45:55.492: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-241" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":294,"skipped":4719,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 19:45:55.531: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[It] should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: starting the proxy server
Jun 28 19:45:55.663: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-108209140 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 19:45:55.850: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2065" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support proxy with --port 0  [Conformance]","total":305,"completed":295,"skipped":4732,"failed":0}
SSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 19:45:55.883: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Jun 28 19:45:56.097: INFO: Waiting up to 5m0s for pod "downwardapi-volume-7b9c647b-a581-444b-9bde-c94c184e5bf8" in namespace "projected-6706" to be "Succeeded or Failed"
Jun 28 19:45:56.108: INFO: Pod "downwardapi-volume-7b9c647b-a581-444b-9bde-c94c184e5bf8": Phase="Pending", Reason="", readiness=false. Elapsed: 11.088831ms
Jun 28 19:45:58.123: INFO: Pod "downwardapi-volume-7b9c647b-a581-444b-9bde-c94c184e5bf8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.025477107s
STEP: Saw pod success
Jun 28 19:45:58.123: INFO: Pod "downwardapi-volume-7b9c647b-a581-444b-9bde-c94c184e5bf8" satisfied condition "Succeeded or Failed"
Jun 28 19:45:58.135: INFO: Trying to get logs from node 10.13.107.37 pod downwardapi-volume-7b9c647b-a581-444b-9bde-c94c184e5bf8 container client-container: <nil>
STEP: delete the pod
Jun 28 19:45:58.207: INFO: Waiting for pod downwardapi-volume-7b9c647b-a581-444b-9bde-c94c184e5bf8 to disappear
Jun 28 19:45:58.216: INFO: Pod downwardapi-volume-7b9c647b-a581-444b-9bde-c94c184e5bf8 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 19:45:58.216: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6706" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu request [NodeConformance] [Conformance]","total":305,"completed":296,"skipped":4735,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-api-machinery] Servers with support for Table transformation 
  should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 19:45:58.247: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename tables
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/table_conversion.go:47
[It] should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 19:45:58.396: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "tables-9856" for this suite.
•{"msg":"PASSED [sig-api-machinery] Servers with support for Table transformation should return a 406 for a backend which does not implement metadata [Conformance]","total":305,"completed":297,"skipped":4746,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 19:45:58.432: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0644 on node default medium
Jun 28 19:45:58.593: INFO: Waiting up to 5m0s for pod "pod-c90fdbd4-dcff-4fd2-8a2e-d1d82db1931d" in namespace "emptydir-6905" to be "Succeeded or Failed"
Jun 28 19:45:58.604: INFO: Pod "pod-c90fdbd4-dcff-4fd2-8a2e-d1d82db1931d": Phase="Pending", Reason="", readiness=false. Elapsed: 10.536414ms
Jun 28 19:46:00.614: INFO: Pod "pod-c90fdbd4-dcff-4fd2-8a2e-d1d82db1931d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.021406569s
STEP: Saw pod success
Jun 28 19:46:00.615: INFO: Pod "pod-c90fdbd4-dcff-4fd2-8a2e-d1d82db1931d" satisfied condition "Succeeded or Failed"
Jun 28 19:46:00.628: INFO: Trying to get logs from node 10.13.107.37 pod pod-c90fdbd4-dcff-4fd2-8a2e-d1d82db1931d container test-container: <nil>
STEP: delete the pod
Jun 28 19:46:00.682: INFO: Waiting for pod pod-c90fdbd4-dcff-4fd2-8a2e-d1d82db1931d to disappear
Jun 28 19:46:00.692: INFO: Pod pod-c90fdbd4-dcff-4fd2-8a2e-d1d82db1931d no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 19:46:00.692: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6905" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":298,"skipped":4774,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 19:46:00.726: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Jun 28 19:46:00.913: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d8102c6a-463a-41fe-8a57-3f42b0ac5f1a" in namespace "projected-6015" to be "Succeeded or Failed"
Jun 28 19:46:00.925: INFO: Pod "downwardapi-volume-d8102c6a-463a-41fe-8a57-3f42b0ac5f1a": Phase="Pending", Reason="", readiness=false. Elapsed: 11.55494ms
Jun 28 19:46:02.936: INFO: Pod "downwardapi-volume-d8102c6a-463a-41fe-8a57-3f42b0ac5f1a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.023371131s
STEP: Saw pod success
Jun 28 19:46:02.937: INFO: Pod "downwardapi-volume-d8102c6a-463a-41fe-8a57-3f42b0ac5f1a" satisfied condition "Succeeded or Failed"
Jun 28 19:46:02.948: INFO: Trying to get logs from node 10.13.107.37 pod downwardapi-volume-d8102c6a-463a-41fe-8a57-3f42b0ac5f1a container client-container: <nil>
STEP: delete the pod
Jun 28 19:46:03.004: INFO: Waiting for pod downwardapi-volume-d8102c6a-463a-41fe-8a57-3f42b0ac5f1a to disappear
Jun 28 19:46:03.013: INFO: Pod downwardapi-volume-d8102c6a-463a-41fe-8a57-3f42b0ac5f1a no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 19:46:03.013: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6015" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide podname only [NodeConformance] [Conformance]","total":305,"completed":299,"skipped":4787,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 19:46:03.049: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod busybox-e19d86fd-825b-4f0b-a2c6-099d9f073534 in namespace container-probe-791
Jun 28 19:46:05.237: INFO: Started pod busybox-e19d86fd-825b-4f0b-a2c6-099d9f073534 in namespace container-probe-791
STEP: checking the pod's current state and verifying that restartCount is present
Jun 28 19:46:05.249: INFO: Initial restart count of pod busybox-e19d86fd-825b-4f0b-a2c6-099d9f073534 is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 19:50:06.355: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-791" for this suite.

• [SLOW TEST:243.337 seconds]
[k8s.io] Probing container
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Probing container should *not* be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","total":305,"completed":300,"skipped":4811,"failed":0}
SSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 19:50:06.389: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name projected-configmap-test-volume-ac7fb8dd-9a26-417d-a89e-24d2710ef404
STEP: Creating a pod to test consume configMaps
Jun 28 19:50:06.562: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-a62ebd37-e83e-46a6-8fa3-83a1d1a343ee" in namespace "projected-892" to be "Succeeded or Failed"
Jun 28 19:50:06.581: INFO: Pod "pod-projected-configmaps-a62ebd37-e83e-46a6-8fa3-83a1d1a343ee": Phase="Pending", Reason="", readiness=false. Elapsed: 9.131315ms
Jun 28 19:50:08.597: INFO: Pod "pod-projected-configmaps-a62ebd37-e83e-46a6-8fa3-83a1d1a343ee": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025278217s
Jun 28 19:50:10.616: INFO: Pod "pod-projected-configmaps-a62ebd37-e83e-46a6-8fa3-83a1d1a343ee": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.044404776s
STEP: Saw pod success
Jun 28 19:50:10.617: INFO: Pod "pod-projected-configmaps-a62ebd37-e83e-46a6-8fa3-83a1d1a343ee" satisfied condition "Succeeded or Failed"
Jun 28 19:50:10.627: INFO: Trying to get logs from node 10.13.107.37 pod pod-projected-configmaps-a62ebd37-e83e-46a6-8fa3-83a1d1a343ee container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jun 28 19:50:10.728: INFO: Waiting for pod pod-projected-configmaps-a62ebd37-e83e-46a6-8fa3-83a1d1a343ee to disappear
Jun 28 19:50:10.738: INFO: Pod pod-projected-configmaps-a62ebd37-e83e-46a6-8fa3-83a1d1a343ee no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 19:50:10.738: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-892" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":301,"skipped":4815,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 19:50:10.773: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jun 28 19:50:11.371: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jun 28 19:50:13.405: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63760506611, loc:(*time.Location)(0x7702840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63760506611, loc:(*time.Location)(0x7702840)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63760506611, loc:(*time.Location)(0x7702840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63760506611, loc:(*time.Location)(0x7702840)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun 28 19:50:16.444: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API
STEP: create a namespace for the webhook
STEP: create a configmap should be unconditionally rejected by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 19:50:16.586: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6073" for this suite.
STEP: Destroying namespace "webhook-6073-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:5.996 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should unconditionally reject operations on fail closed webhook [Conformance]","total":305,"completed":302,"skipped":4866,"failed":0}
S
------------------------------
[sig-network] Services 
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 19:50:16.769: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating service in namespace services-6020
STEP: creating service affinity-nodeport in namespace services-6020
STEP: creating replication controller affinity-nodeport in namespace services-6020
I0628 19:50:16.998853      22 runners.go:190] Created replication controller with name: affinity-nodeport, namespace: services-6020, replica count: 3
I0628 19:50:20.049784      22 runners.go:190] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jun 28 19:50:20.095: INFO: Creating new exec pod
Jun 28 19:50:23.195: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-108209140 exec --namespace=services-6020 execpod-affinityw97bx -- /bin/sh -x -c nc -zv -t -w 2 affinity-nodeport 80'
Jun 28 19:50:23.909: INFO: stderr: "+ nc -zv -t -w 2 affinity-nodeport 80\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
Jun 28 19:50:23.909: INFO: stdout: ""
Jun 28 19:50:23.911: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-108209140 exec --namespace=services-6020 execpod-affinityw97bx -- /bin/sh -x -c nc -zv -t -w 2 172.21.18.27 80'
Jun 28 19:50:24.305: INFO: stderr: "+ nc -zv -t -w 2 172.21.18.27 80\nConnection to 172.21.18.27 80 port [tcp/http] succeeded!\n"
Jun 28 19:50:24.305: INFO: stdout: ""
Jun 28 19:50:24.305: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-108209140 exec --namespace=services-6020 execpod-affinityw97bx -- /bin/sh -x -c nc -zv -t -w 2 10.13.107.57 31899'
Jun 28 19:50:24.779: INFO: stderr: "+ nc -zv -t -w 2 10.13.107.57 31899\nConnection to 10.13.107.57 31899 port [tcp/31899] succeeded!\n"
Jun 28 19:50:24.779: INFO: stdout: ""
Jun 28 19:50:24.779: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-108209140 exec --namespace=services-6020 execpod-affinityw97bx -- /bin/sh -x -c nc -zv -t -w 2 10.13.107.37 31899'
Jun 28 19:50:25.162: INFO: stderr: "+ nc -zv -t -w 2 10.13.107.37 31899\nConnection to 10.13.107.37 31899 port [tcp/31899] succeeded!\n"
Jun 28 19:50:25.162: INFO: stdout: ""
Jun 28 19:50:25.163: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-108209140 exec --namespace=services-6020 execpod-affinityw97bx -- /bin/sh -x -c nc -zv -t -w 2 149.81.178.54 31899'
Jun 28 19:50:25.526: INFO: stderr: "+ nc -zv -t -w 2 149.81.178.54 31899\nConnection to 149.81.178.54 31899 port [tcp/31899] succeeded!\n"
Jun 28 19:50:25.526: INFO: stdout: ""
Jun 28 19:50:25.526: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-108209140 exec --namespace=services-6020 execpod-affinityw97bx -- /bin/sh -x -c nc -zv -t -w 2 149.81.178.52 31899'
Jun 28 19:50:25.976: INFO: stderr: "+ nc -zv -t -w 2 149.81.178.52 31899\nConnection to 149.81.178.52 31899 port [tcp/31899] succeeded!\n"
Jun 28 19:50:25.976: INFO: stdout: ""
Jun 28 19:50:25.976: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-108209140 exec --namespace=services-6020 execpod-affinityw97bx -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.13.107.37:31899/ ; done'
Jun 28 19:50:26.513: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.13.107.37:31899/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.13.107.37:31899/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.13.107.37:31899/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.13.107.37:31899/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.13.107.37:31899/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.13.107.37:31899/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.13.107.37:31899/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.13.107.37:31899/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.13.107.37:31899/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.13.107.37:31899/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.13.107.37:31899/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.13.107.37:31899/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.13.107.37:31899/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.13.107.37:31899/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.13.107.37:31899/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.13.107.37:31899/\n"
Jun 28 19:50:26.513: INFO: stdout: "\naffinity-nodeport-zfs4x\naffinity-nodeport-zfs4x\naffinity-nodeport-zfs4x\naffinity-nodeport-zfs4x\naffinity-nodeport-zfs4x\naffinity-nodeport-zfs4x\naffinity-nodeport-zfs4x\naffinity-nodeport-zfs4x\naffinity-nodeport-zfs4x\naffinity-nodeport-zfs4x\naffinity-nodeport-zfs4x\naffinity-nodeport-zfs4x\naffinity-nodeport-zfs4x\naffinity-nodeport-zfs4x\naffinity-nodeport-zfs4x\naffinity-nodeport-zfs4x"
Jun 28 19:50:26.513: INFO: Received response from host: affinity-nodeport-zfs4x
Jun 28 19:50:26.513: INFO: Received response from host: affinity-nodeport-zfs4x
Jun 28 19:50:26.513: INFO: Received response from host: affinity-nodeport-zfs4x
Jun 28 19:50:26.513: INFO: Received response from host: affinity-nodeport-zfs4x
Jun 28 19:50:26.513: INFO: Received response from host: affinity-nodeport-zfs4x
Jun 28 19:50:26.513: INFO: Received response from host: affinity-nodeport-zfs4x
Jun 28 19:50:26.513: INFO: Received response from host: affinity-nodeport-zfs4x
Jun 28 19:50:26.513: INFO: Received response from host: affinity-nodeport-zfs4x
Jun 28 19:50:26.513: INFO: Received response from host: affinity-nodeport-zfs4x
Jun 28 19:50:26.513: INFO: Received response from host: affinity-nodeport-zfs4x
Jun 28 19:50:26.513: INFO: Received response from host: affinity-nodeport-zfs4x
Jun 28 19:50:26.513: INFO: Received response from host: affinity-nodeport-zfs4x
Jun 28 19:50:26.513: INFO: Received response from host: affinity-nodeport-zfs4x
Jun 28 19:50:26.513: INFO: Received response from host: affinity-nodeport-zfs4x
Jun 28 19:50:26.513: INFO: Received response from host: affinity-nodeport-zfs4x
Jun 28 19:50:26.513: INFO: Received response from host: affinity-nodeport-zfs4x
Jun 28 19:50:26.513: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport in namespace services-6020, will wait for the garbage collector to delete the pods
Jun 28 19:50:26.680: INFO: Deleting ReplicationController affinity-nodeport took: 18.143594ms
Jun 28 19:50:26.780: INFO: Terminating ReplicationController affinity-nodeport pods took: 100.368587ms
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 19:50:37.938: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6020" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

• [SLOW TEST:21.205 seconds]
[sig-network] Services
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity work for NodePort service [LinuxOnly] [Conformance]","total":305,"completed":303,"skipped":4867,"failed":0}
S
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 19:50:37.974: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod pod-subpath-test-downwardapi-g4lr
STEP: Creating a pod to test atomic-volume-subpath
Jun 28 19:50:38.176: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-g4lr" in namespace "subpath-7025" to be "Succeeded or Failed"
Jun 28 19:50:38.186: INFO: Pod "pod-subpath-test-downwardapi-g4lr": Phase="Pending", Reason="", readiness=false. Elapsed: 10.197614ms
Jun 28 19:50:40.207: INFO: Pod "pod-subpath-test-downwardapi-g4lr": Phase="Running", Reason="", readiness=true. Elapsed: 2.031026978s
Jun 28 19:50:42.220: INFO: Pod "pod-subpath-test-downwardapi-g4lr": Phase="Running", Reason="", readiness=true. Elapsed: 4.043994241s
Jun 28 19:50:44.232: INFO: Pod "pod-subpath-test-downwardapi-g4lr": Phase="Running", Reason="", readiness=true. Elapsed: 6.05550336s
Jun 28 19:50:46.248: INFO: Pod "pod-subpath-test-downwardapi-g4lr": Phase="Running", Reason="", readiness=true. Elapsed: 8.072061558s
Jun 28 19:50:48.264: INFO: Pod "pod-subpath-test-downwardapi-g4lr": Phase="Running", Reason="", readiness=true. Elapsed: 10.087996472s
Jun 28 19:50:50.280: INFO: Pod "pod-subpath-test-downwardapi-g4lr": Phase="Running", Reason="", readiness=true. Elapsed: 12.103820515s
Jun 28 19:50:52.302: INFO: Pod "pod-subpath-test-downwardapi-g4lr": Phase="Running", Reason="", readiness=true. Elapsed: 14.126002193s
Jun 28 19:50:54.315: INFO: Pod "pod-subpath-test-downwardapi-g4lr": Phase="Running", Reason="", readiness=true. Elapsed: 16.138936476s
Jun 28 19:50:56.337: INFO: Pod "pod-subpath-test-downwardapi-g4lr": Phase="Running", Reason="", readiness=true. Elapsed: 18.160691821s
Jun 28 19:50:58.355: INFO: Pod "pod-subpath-test-downwardapi-g4lr": Phase="Running", Reason="", readiness=true. Elapsed: 20.178937056s
Jun 28 19:51:00.372: INFO: Pod "pod-subpath-test-downwardapi-g4lr": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.195997596s
STEP: Saw pod success
Jun 28 19:51:00.373: INFO: Pod "pod-subpath-test-downwardapi-g4lr" satisfied condition "Succeeded or Failed"
Jun 28 19:51:00.384: INFO: Trying to get logs from node 10.13.107.37 pod pod-subpath-test-downwardapi-g4lr container test-container-subpath-downwardapi-g4lr: <nil>
STEP: delete the pod
Jun 28 19:51:00.452: INFO: Waiting for pod pod-subpath-test-downwardapi-g4lr to disappear
Jun 28 19:51:00.465: INFO: Pod pod-subpath-test-downwardapi-g4lr no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-g4lr
Jun 28 19:51:00.466: INFO: Deleting pod "pod-subpath-test-downwardapi-g4lr" in namespace "subpath-7025"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 19:51:00.479: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-7025" for this suite.

• [SLOW TEST:22.545 seconds]
[sig-storage] Subpath
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with downward pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with downward pod [LinuxOnly] [Conformance]","total":305,"completed":304,"skipped":4868,"failed":0}
SSSSSSSSS
------------------------------
[sig-network] Services 
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 19:51:00.522: INFO: >>> kubeConfig: /tmp/kubeconfig-108209140
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating service endpoint-test2 in namespace services-1867
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-1867 to expose endpoints map[]
Jun 28 19:51:00.717: INFO: successfully validated that service endpoint-test2 in namespace services-1867 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-1867
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-1867 to expose endpoints map[pod1:[80]]
Jun 28 19:51:02.811: INFO: successfully validated that service endpoint-test2 in namespace services-1867 exposes endpoints map[pod1:[80]]
STEP: Creating pod pod2 in namespace services-1867
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-1867 to expose endpoints map[pod1:[80] pod2:[80]]
Jun 28 19:51:04.925: INFO: successfully validated that service endpoint-test2 in namespace services-1867 exposes endpoints map[pod1:[80] pod2:[80]]
STEP: Deleting pod pod1 in namespace services-1867
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-1867 to expose endpoints map[pod2:[80]]
Jun 28 19:51:04.992: INFO: successfully validated that service endpoint-test2 in namespace services-1867 exposes endpoints map[pod2:[80]]
STEP: Deleting pod pod2 in namespace services-1867
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-1867 to expose endpoints map[]
Jun 28 19:51:05.046: INFO: successfully validated that service endpoint-test2 in namespace services-1867 exposes endpoints map[]
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 19:51:05.097: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1867" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786
•{"msg":"PASSED [sig-network] Services should serve a basic endpoint from pods  [Conformance]","total":305,"completed":305,"skipped":4877,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSJun 28 19:51:05.141: INFO: Running AfterSuite actions on all nodes
Jun 28 19:51:05.141: INFO: Running AfterSuite actions on node 1
Jun 28 19:51:05.141: INFO: Skipping dumping logs from cluster

JUnit report was created: /tmp/results/junit_01.xml
{"msg":"Test Suite completed","total":305,"completed":305,"skipped":4927,"failed":0}

Ran 305 of 5232 Specs in 5932.334 seconds
SUCCESS! -- 305 Passed | 0 Failed | 0 Pending | 4927 Skipped
PASS

Ginkgo ran 1 suite in 1h38m54.114957541s
Test Suite Passed
