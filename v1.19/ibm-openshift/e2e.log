I0927 19:22:06.829879      22 test_context.go:416] Using a temporary kubeconfig file from in-cluster config : /tmp/kubeconfig-679324930
I0927 19:22:06.830471      22 test_context.go:429] Tolerating taints "node-role.kubernetes.io/master" when considering if nodes are ready
I0927 19:22:06.830882      22 e2e.go:129] Starting e2e run "94d08252-a62c-43eb-a6d0-b17ed5f2fc00" on Ginkgo node 1
{"msg":"Test Suite starting","total":305,"completed":0,"skipped":0,"failed":0}
Running Suite: Kubernetes e2e suite
===================================
Random Seed: 1632770524 - Will randomize all specs
Will run 305 of 5232 specs

Sep 27 19:22:06.868: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
Sep 27 19:22:06.872: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
Sep 27 19:22:06.939: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Sep 27 19:22:07.011: INFO: 13 / 13 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Sep 27 19:22:07.011: INFO: expected 4 pod replicas in namespace 'kube-system', 4 are Running and Ready.
Sep 27 19:22:07.011: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Sep 27 19:22:07.031: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'ibm-keepalived-watcher' (0 seconds elapsed)
Sep 27 19:22:07.031: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'ibmcloud-block-storage-driver' (0 seconds elapsed)
Sep 27 19:22:07.032: INFO: e2e test version: v1.19.0
Sep 27 19:22:07.038: INFO: kube-apiserver version: v1.19.0+4c3480d
Sep 27 19:22:07.038: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
Sep 27 19:22:07.056: INFO: Cluster IP family: ipv4
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 19:22:07.058: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename configmap
Sep 27 19:22:07.238: INFO: No PodSecurityPolicies found; assuming PodSecurityPolicy is disabled.
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap configmap-5654/configmap-test-c7415f03-5f60-4980-a1ca-8af3de5e6171
STEP: Creating a pod to test consume configMaps
Sep 27 19:22:07.326: INFO: Waiting up to 5m0s for pod "pod-configmaps-4a3d3e76-b419-4c39-a848-83dc27bce12d" in namespace "configmap-5654" to be "Succeeded or Failed"
Sep 27 19:22:07.340: INFO: Pod "pod-configmaps-4a3d3e76-b419-4c39-a848-83dc27bce12d": Phase="Pending", Reason="", readiness=false. Elapsed: 14.21984ms
Sep 27 19:22:09.354: INFO: Pod "pod-configmaps-4a3d3e76-b419-4c39-a848-83dc27bce12d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027842772s
Sep 27 19:22:11.369: INFO: Pod "pod-configmaps-4a3d3e76-b419-4c39-a848-83dc27bce12d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.043103106s
STEP: Saw pod success
Sep 27 19:22:11.369: INFO: Pod "pod-configmaps-4a3d3e76-b419-4c39-a848-83dc27bce12d" satisfied condition "Succeeded or Failed"
Sep 27 19:22:11.383: INFO: Trying to get logs from node 10.177.248.117 pod pod-configmaps-4a3d3e76-b419-4c39-a848-83dc27bce12d container env-test: <nil>
STEP: delete the pod
Sep 27 19:22:11.507: INFO: Waiting for pod pod-configmaps-4a3d3e76-b419-4c39-a848-83dc27bce12d to disappear
Sep 27 19:22:11.519: INFO: Pod pod-configmaps-4a3d3e76-b419-4c39-a848-83dc27bce12d no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 19:22:11.519: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5654" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should be consumable via environment variable [NodeConformance] [Conformance]","total":305,"completed":1,"skipped":51,"failed":0}
S
------------------------------
[k8s.io] Lease 
  lease API should be available [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Lease
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 19:22:11.557: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename lease-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] lease API should be available [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [k8s.io] Lease
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 19:22:11.999: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "lease-test-2428" for this suite.
•{"msg":"PASSED [k8s.io] Lease lease API should be available [Conformance]","total":305,"completed":2,"skipped":52,"failed":0}
SS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 19:22:12.051: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0644 on tmpfs
Sep 27 19:22:12.267: INFO: Waiting up to 5m0s for pod "pod-736b3580-1913-43a8-acad-c717ac16de9d" in namespace "emptydir-8873" to be "Succeeded or Failed"
Sep 27 19:22:12.305: INFO: Pod "pod-736b3580-1913-43a8-acad-c717ac16de9d": Phase="Pending", Reason="", readiness=false. Elapsed: 37.659191ms
Sep 27 19:22:14.320: INFO: Pod "pod-736b3580-1913-43a8-acad-c717ac16de9d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.052169727s
Sep 27 19:22:16.335: INFO: Pod "pod-736b3580-1913-43a8-acad-c717ac16de9d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.067396224s
Sep 27 19:22:18.348: INFO: Pod "pod-736b3580-1913-43a8-acad-c717ac16de9d": Phase="Pending", Reason="", readiness=false. Elapsed: 6.080987584s
Sep 27 19:22:20.362: INFO: Pod "pod-736b3580-1913-43a8-acad-c717ac16de9d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.094737507s
STEP: Saw pod success
Sep 27 19:22:20.362: INFO: Pod "pod-736b3580-1913-43a8-acad-c717ac16de9d" satisfied condition "Succeeded or Failed"
Sep 27 19:22:20.378: INFO: Trying to get logs from node 10.177.248.117 pod pod-736b3580-1913-43a8-acad-c717ac16de9d container test-container: <nil>
STEP: delete the pod
Sep 27 19:22:20.472: INFO: Waiting for pod pod-736b3580-1913-43a8-acad-c717ac16de9d to disappear
Sep 27 19:22:20.483: INFO: Pod pod-736b3580-1913-43a8-acad-c717ac16de9d no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 19:22:20.484: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8873" for this suite.

• [SLOW TEST:8.473 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:42
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":3,"skipped":54,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 19:22:20.531: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating 50 configmaps
STEP: Creating RC which spawns configmap-volume pods
Sep 27 19:22:21.799: INFO: Pod name wrapped-volume-race-1ac2ee84-f50b-4586-84d6-88ae33e678e4: Found 0 pods out of 5
Sep 27 19:22:26.822: INFO: Pod name wrapped-volume-race-1ac2ee84-f50b-4586-84d6-88ae33e678e4: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-1ac2ee84-f50b-4586-84d6-88ae33e678e4 in namespace emptydir-wrapper-6837, will wait for the garbage collector to delete the pods
Sep 27 19:22:26.987: INFO: Deleting ReplicationController wrapped-volume-race-1ac2ee84-f50b-4586-84d6-88ae33e678e4 took: 29.257227ms
Sep 27 19:22:27.088: INFO: Terminating ReplicationController wrapped-volume-race-1ac2ee84-f50b-4586-84d6-88ae33e678e4 pods took: 100.301904ms
STEP: Creating RC which spawns configmap-volume pods
Sep 27 19:22:42.061: INFO: Pod name wrapped-volume-race-1a26a76a-10db-4861-b123-6a2f936f0ea6: Found 0 pods out of 5
Sep 27 19:22:47.086: INFO: Pod name wrapped-volume-race-1a26a76a-10db-4861-b123-6a2f936f0ea6: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-1a26a76a-10db-4861-b123-6a2f936f0ea6 in namespace emptydir-wrapper-6837, will wait for the garbage collector to delete the pods
Sep 27 19:22:47.252: INFO: Deleting ReplicationController wrapped-volume-race-1a26a76a-10db-4861-b123-6a2f936f0ea6 took: 29.888304ms
Sep 27 19:22:47.352: INFO: Terminating ReplicationController wrapped-volume-race-1a26a76a-10db-4861-b123-6a2f936f0ea6 pods took: 100.43046ms
STEP: Creating RC which spawns configmap-volume pods
Sep 27 19:22:59.740: INFO: Pod name wrapped-volume-race-c067af42-13d2-4946-acca-1d4f4c45153d: Found 0 pods out of 5
Sep 27 19:23:04.763: INFO: Pod name wrapped-volume-race-c067af42-13d2-4946-acca-1d4f4c45153d: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-c067af42-13d2-4946-acca-1d4f4c45153d in namespace emptydir-wrapper-6837, will wait for the garbage collector to delete the pods
Sep 27 19:23:04.929: INFO: Deleting ReplicationController wrapped-volume-race-c067af42-13d2-4946-acca-1d4f4c45153d took: 29.46898ms
Sep 27 19:23:05.130: INFO: Terminating ReplicationController wrapped-volume-race-c067af42-13d2-4946-acca-1d4f4c45153d pods took: 200.245167ms
STEP: Cleaning up the configMaps
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 19:23:14.258: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-6837" for this suite.

• [SLOW TEST:53.774 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not cause race condition when used for configmaps [Serial] [Conformance]","total":305,"completed":4,"skipped":85,"failed":0}
SSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 19:23:14.305: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-466
[It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating stateful set ss in namespace statefulset-466
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-466
Sep 27 19:23:14.524: INFO: Found 0 stateful pods, waiting for 1
Sep 27 19:23:24.537: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod
Sep 27 19:23:24.550: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-679324930 exec --namespace=statefulset-466 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Sep 27 19:23:25.210: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Sep 27 19:23:25.210: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Sep 27 19:23:25.210: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Sep 27 19:23:25.224: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Sep 27 19:23:35.240: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Sep 27 19:23:35.240: INFO: Waiting for statefulset status.replicas updated to 0
Sep 27 19:23:35.303: INFO: POD   NODE            PHASE    GRACE  CONDITIONS
Sep 27 19:23:35.303: INFO: ss-0  10.177.248.117  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-09-27 19:23:14 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-09-27 19:23:26 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-09-27 19:23:26 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-09-27 19:23:14 +0000 UTC  }]
Sep 27 19:23:35.303: INFO: 
Sep 27 19:23:35.303: INFO: StatefulSet ss has not reached scale 3, at 1
Sep 27 19:23:36.318: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.984833302s
Sep 27 19:23:37.332: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.970290471s
Sep 27 19:23:38.347: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.956144266s
Sep 27 19:23:39.362: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.940741566s
Sep 27 19:23:40.390: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.926332409s
Sep 27 19:23:41.403: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.898769214s
Sep 27 19:23:42.421: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.884641862s
Sep 27 19:23:43.442: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.867238341s
Sep 27 19:23:44.458: INFO: Verifying statefulset ss doesn't scale past 3 for another 846.217828ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-466
Sep 27 19:23:45.474: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-679324930 exec --namespace=statefulset-466 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 27 19:23:45.903: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Sep 27 19:23:45.903: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Sep 27 19:23:45.903: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Sep 27 19:23:45.903: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-679324930 exec --namespace=statefulset-466 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 27 19:23:46.407: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Sep 27 19:23:46.408: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Sep 27 19:23:46.408: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Sep 27 19:23:46.408: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-679324930 exec --namespace=statefulset-466 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 27 19:23:46.884: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Sep 27 19:23:46.884: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Sep 27 19:23:46.884: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Sep 27 19:23:46.901: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Sep 27 19:23:46.901: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Sep 27 19:23:46.901: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod
Sep 27 19:23:46.914: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-679324930 exec --namespace=statefulset-466 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Sep 27 19:23:47.335: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Sep 27 19:23:47.335: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Sep 27 19:23:47.335: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Sep 27 19:23:47.335: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-679324930 exec --namespace=statefulset-466 ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Sep 27 19:23:47.941: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Sep 27 19:23:47.941: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Sep 27 19:23:47.941: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Sep 27 19:23:47.941: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-679324930 exec --namespace=statefulset-466 ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Sep 27 19:23:48.440: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Sep 27 19:23:48.440: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Sep 27 19:23:48.440: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Sep 27 19:23:48.440: INFO: Waiting for statefulset status.replicas updated to 0
Sep 27 19:23:48.455: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 1
Sep 27 19:23:58.482: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Sep 27 19:23:58.482: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Sep 27 19:23:58.482: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Sep 27 19:23:58.527: INFO: POD   NODE            PHASE    GRACE  CONDITIONS
Sep 27 19:23:58.527: INFO: ss-0  10.177.248.117  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-09-27 19:23:14 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-09-27 19:23:48 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-09-27 19:23:48 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-09-27 19:23:14 +0000 UTC  }]
Sep 27 19:23:58.527: INFO: ss-1  10.177.248.126  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-09-27 19:23:35 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-09-27 19:23:48 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-09-27 19:23:48 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-09-27 19:23:35 +0000 UTC  }]
Sep 27 19:23:58.527: INFO: ss-2  10.177.248.114  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-09-27 19:23:35 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-09-27 19:23:49 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-09-27 19:23:49 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-09-27 19:23:35 +0000 UTC  }]
Sep 27 19:23:58.527: INFO: 
Sep 27 19:23:58.527: INFO: StatefulSet ss has not reached scale 0, at 3
Sep 27 19:23:59.548: INFO: POD   NODE            PHASE    GRACE  CONDITIONS
Sep 27 19:23:59.548: INFO: ss-0  10.177.248.117  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-09-27 19:23:14 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-09-27 19:23:48 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-09-27 19:23:48 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-09-27 19:23:14 +0000 UTC  }]
Sep 27 19:23:59.548: INFO: ss-1  10.177.248.126  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-09-27 19:23:35 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-09-27 19:23:48 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-09-27 19:23:48 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-09-27 19:23:35 +0000 UTC  }]
Sep 27 19:23:59.548: INFO: ss-2  10.177.248.114  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-09-27 19:23:35 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-09-27 19:23:49 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-09-27 19:23:49 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-09-27 19:23:35 +0000 UTC  }]
Sep 27 19:23:59.548: INFO: 
Sep 27 19:23:59.548: INFO: StatefulSet ss has not reached scale 0, at 3
Sep 27 19:24:00.573: INFO: POD   NODE            PHASE    GRACE  CONDITIONS
Sep 27 19:24:00.573: INFO: ss-0  10.177.248.117  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-09-27 19:23:14 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-09-27 19:23:48 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-09-27 19:23:48 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-09-27 19:23:14 +0000 UTC  }]
Sep 27 19:24:00.573: INFO: ss-1  10.177.248.126  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-09-27 19:23:35 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-09-27 19:23:48 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-09-27 19:23:48 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-09-27 19:23:35 +0000 UTC  }]
Sep 27 19:24:00.573: INFO: ss-2  10.177.248.114  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-09-27 19:23:35 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-09-27 19:23:49 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-09-27 19:23:49 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-09-27 19:23:35 +0000 UTC  }]
Sep 27 19:24:00.573: INFO: 
Sep 27 19:24:00.573: INFO: StatefulSet ss has not reached scale 0, at 3
Sep 27 19:24:01.588: INFO: POD   NODE            PHASE    GRACE  CONDITIONS
Sep 27 19:24:01.588: INFO: ss-0  10.177.248.117  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-09-27 19:23:14 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-09-27 19:23:48 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-09-27 19:23:48 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-09-27 19:23:14 +0000 UTC  }]
Sep 27 19:24:01.589: INFO: ss-1  10.177.248.126  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-09-27 19:23:35 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-09-27 19:23:48 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-09-27 19:23:48 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-09-27 19:23:35 +0000 UTC  }]
Sep 27 19:24:01.589: INFO: ss-2  10.177.248.114  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-09-27 19:23:35 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-09-27 19:23:49 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-09-27 19:23:49 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-09-27 19:23:35 +0000 UTC  }]
Sep 27 19:24:01.589: INFO: 
Sep 27 19:24:01.589: INFO: StatefulSet ss has not reached scale 0, at 3
Sep 27 19:24:02.604: INFO: POD   NODE            PHASE    GRACE  CONDITIONS
Sep 27 19:24:02.604: INFO: ss-0  10.177.248.117  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-09-27 19:23:14 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-09-27 19:23:48 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-09-27 19:23:48 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-09-27 19:23:14 +0000 UTC  }]
Sep 27 19:24:02.604: INFO: ss-1  10.177.248.126  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-09-27 19:23:35 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-09-27 19:23:48 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-09-27 19:23:48 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-09-27 19:23:35 +0000 UTC  }]
Sep 27 19:24:02.604: INFO: ss-2  10.177.248.114  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-09-27 19:23:35 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-09-27 19:23:49 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-09-27 19:23:49 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-09-27 19:23:35 +0000 UTC  }]
Sep 27 19:24:02.604: INFO: 
Sep 27 19:24:02.604: INFO: StatefulSet ss has not reached scale 0, at 3
Sep 27 19:24:03.624: INFO: POD   NODE            PHASE    GRACE  CONDITIONS
Sep 27 19:24:03.624: INFO: ss-0  10.177.248.117  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-09-27 19:23:14 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-09-27 19:23:48 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-09-27 19:23:48 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-09-27 19:23:14 +0000 UTC  }]
Sep 27 19:24:03.624: INFO: ss-1  10.177.248.126  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-09-27 19:23:35 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-09-27 19:23:48 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-09-27 19:23:48 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-09-27 19:23:35 +0000 UTC  }]
Sep 27 19:24:03.624: INFO: ss-2  10.177.248.114  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-09-27 19:23:35 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-09-27 19:23:49 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-09-27 19:23:49 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-09-27 19:23:35 +0000 UTC  }]
Sep 27 19:24:03.624: INFO: 
Sep 27 19:24:03.624: INFO: StatefulSet ss has not reached scale 0, at 3
Sep 27 19:24:04.640: INFO: POD   NODE            PHASE    GRACE  CONDITIONS
Sep 27 19:24:04.640: INFO: ss-0  10.177.248.117  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-09-27 19:23:14 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-09-27 19:23:48 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-09-27 19:23:48 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-09-27 19:23:14 +0000 UTC  }]
Sep 27 19:24:04.640: INFO: ss-1  10.177.248.126  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-09-27 19:23:35 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-09-27 19:23:48 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-09-27 19:23:48 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-09-27 19:23:35 +0000 UTC  }]
Sep 27 19:24:04.640: INFO: ss-2  10.177.248.114  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-09-27 19:23:35 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-09-27 19:23:49 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-09-27 19:23:49 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-09-27 19:23:35 +0000 UTC  }]
Sep 27 19:24:04.640: INFO: 
Sep 27 19:24:04.640: INFO: StatefulSet ss has not reached scale 0, at 3
Sep 27 19:24:05.654: INFO: POD   NODE            PHASE    GRACE  CONDITIONS
Sep 27 19:24:05.654: INFO: ss-0  10.177.248.117  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-09-27 19:23:14 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-09-27 19:23:48 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-09-27 19:23:48 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-09-27 19:23:14 +0000 UTC  }]
Sep 27 19:24:05.654: INFO: ss-1  10.177.248.126  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-09-27 19:23:35 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-09-27 19:23:48 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-09-27 19:23:48 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-09-27 19:23:35 +0000 UTC  }]
Sep 27 19:24:05.654: INFO: ss-2  10.177.248.114  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-09-27 19:23:35 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-09-27 19:23:49 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-09-27 19:23:49 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-09-27 19:23:35 +0000 UTC  }]
Sep 27 19:24:05.654: INFO: 
Sep 27 19:24:05.654: INFO: StatefulSet ss has not reached scale 0, at 3
Sep 27 19:24:06.676: INFO: POD   NODE            PHASE    GRACE  CONDITIONS
Sep 27 19:24:06.676: INFO: ss-0  10.177.248.117  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-09-27 19:23:14 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-09-27 19:23:48 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-09-27 19:23:48 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-09-27 19:23:14 +0000 UTC  }]
Sep 27 19:24:06.676: INFO: ss-1  10.177.248.126  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-09-27 19:23:35 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-09-27 19:23:48 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-09-27 19:23:48 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-09-27 19:23:35 +0000 UTC  }]
Sep 27 19:24:06.676: INFO: ss-2  10.177.248.114  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-09-27 19:23:35 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-09-27 19:23:49 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-09-27 19:23:49 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-09-27 19:23:35 +0000 UTC  }]
Sep 27 19:24:06.676: INFO: 
Sep 27 19:24:06.676: INFO: StatefulSet ss has not reached scale 0, at 3
Sep 27 19:24:07.693: INFO: POD   NODE            PHASE    GRACE  CONDITIONS
Sep 27 19:24:07.693: INFO: ss-0  10.177.248.117  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-09-27 19:23:14 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-09-27 19:23:48 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-09-27 19:23:48 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-09-27 19:23:14 +0000 UTC  }]
Sep 27 19:24:07.693: INFO: ss-1  10.177.248.126  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-09-27 19:23:35 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-09-27 19:23:48 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-09-27 19:23:48 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-09-27 19:23:35 +0000 UTC  }]
Sep 27 19:24:07.693: INFO: ss-2  10.177.248.114  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-09-27 19:23:35 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-09-27 19:23:49 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-09-27 19:23:49 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-09-27 19:23:35 +0000 UTC  }]
Sep 27 19:24:07.693: INFO: 
Sep 27 19:24:07.693: INFO: StatefulSet ss has not reached scale 0, at 3
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-466
Sep 27 19:24:08.707: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-679324930 exec --namespace=statefulset-466 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 27 19:24:09.007: INFO: rc: 1
Sep 27 19:24:09.008: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-679324930 exec --namespace=statefulset-466 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
error: unable to upgrade connection: container not found ("webserver")

error:
exit status 1
Sep 27 19:24:19.008: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-679324930 exec --namespace=statefulset-466 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 27 19:24:19.197: INFO: rc: 1
Sep 27 19:24:19.197: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-679324930 exec --namespace=statefulset-466 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Sep 27 19:24:29.197: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-679324930 exec --namespace=statefulset-466 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 27 19:24:29.374: INFO: rc: 1
Sep 27 19:24:29.374: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-679324930 exec --namespace=statefulset-466 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Sep 27 19:24:39.375: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-679324930 exec --namespace=statefulset-466 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 27 19:24:39.564: INFO: rc: 1
Sep 27 19:24:39.564: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-679324930 exec --namespace=statefulset-466 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Sep 27 19:24:49.565: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-679324930 exec --namespace=statefulset-466 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 27 19:24:49.731: INFO: rc: 1
Sep 27 19:24:49.731: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-679324930 exec --namespace=statefulset-466 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Sep 27 19:24:59.732: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-679324930 exec --namespace=statefulset-466 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 27 19:24:59.914: INFO: rc: 1
Sep 27 19:24:59.914: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-679324930 exec --namespace=statefulset-466 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Sep 27 19:25:09.917: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-679324930 exec --namespace=statefulset-466 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 27 19:25:10.728: INFO: rc: 1
Sep 27 19:25:10.729: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-679324930 exec --namespace=statefulset-466 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Sep 27 19:25:20.729: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-679324930 exec --namespace=statefulset-466 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 27 19:25:20.913: INFO: rc: 1
Sep 27 19:25:20.913: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-679324930 exec --namespace=statefulset-466 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Sep 27 19:25:30.914: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-679324930 exec --namespace=statefulset-466 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 27 19:25:31.106: INFO: rc: 1
Sep 27 19:25:31.106: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-679324930 exec --namespace=statefulset-466 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Sep 27 19:25:41.106: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-679324930 exec --namespace=statefulset-466 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 27 19:25:41.291: INFO: rc: 1
Sep 27 19:25:41.291: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-679324930 exec --namespace=statefulset-466 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Sep 27 19:25:51.291: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-679324930 exec --namespace=statefulset-466 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 27 19:25:51.538: INFO: rc: 1
Sep 27 19:25:51.538: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-679324930 exec --namespace=statefulset-466 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Sep 27 19:26:01.538: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-679324930 exec --namespace=statefulset-466 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 27 19:26:01.742: INFO: rc: 1
Sep 27 19:26:01.742: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-679324930 exec --namespace=statefulset-466 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Sep 27 19:26:11.742: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-679324930 exec --namespace=statefulset-466 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 27 19:26:11.927: INFO: rc: 1
Sep 27 19:26:11.927: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-679324930 exec --namespace=statefulset-466 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Sep 27 19:26:21.928: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-679324930 exec --namespace=statefulset-466 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 27 19:26:22.108: INFO: rc: 1
Sep 27 19:26:22.108: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-679324930 exec --namespace=statefulset-466 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Sep 27 19:26:32.108: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-679324930 exec --namespace=statefulset-466 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 27 19:26:32.346: INFO: rc: 1
Sep 27 19:26:32.346: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-679324930 exec --namespace=statefulset-466 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Sep 27 19:26:42.347: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-679324930 exec --namespace=statefulset-466 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 27 19:26:42.518: INFO: rc: 1
Sep 27 19:26:42.518: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-679324930 exec --namespace=statefulset-466 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Sep 27 19:26:52.518: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-679324930 exec --namespace=statefulset-466 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 27 19:26:52.711: INFO: rc: 1
Sep 27 19:26:52.711: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-679324930 exec --namespace=statefulset-466 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Sep 27 19:27:02.711: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-679324930 exec --namespace=statefulset-466 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 27 19:27:02.908: INFO: rc: 1
Sep 27 19:27:02.908: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-679324930 exec --namespace=statefulset-466 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Sep 27 19:27:12.908: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-679324930 exec --namespace=statefulset-466 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 27 19:27:13.098: INFO: rc: 1
Sep 27 19:27:13.099: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-679324930 exec --namespace=statefulset-466 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Sep 27 19:27:23.099: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-679324930 exec --namespace=statefulset-466 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 27 19:27:23.292: INFO: rc: 1
Sep 27 19:27:23.292: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-679324930 exec --namespace=statefulset-466 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Sep 27 19:27:33.292: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-679324930 exec --namespace=statefulset-466 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 27 19:27:33.468: INFO: rc: 1
Sep 27 19:27:33.468: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-679324930 exec --namespace=statefulset-466 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Sep 27 19:27:43.468: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-679324930 exec --namespace=statefulset-466 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 27 19:27:43.641: INFO: rc: 1
Sep 27 19:27:43.641: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-679324930 exec --namespace=statefulset-466 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Sep 27 19:27:53.642: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-679324930 exec --namespace=statefulset-466 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 27 19:27:53.831: INFO: rc: 1
Sep 27 19:27:53.831: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-679324930 exec --namespace=statefulset-466 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Sep 27 19:28:03.831: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-679324930 exec --namespace=statefulset-466 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 27 19:28:04.025: INFO: rc: 1
Sep 27 19:28:04.027: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-679324930 exec --namespace=statefulset-466 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Sep 27 19:28:14.027: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-679324930 exec --namespace=statefulset-466 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 27 19:28:14.222: INFO: rc: 1
Sep 27 19:28:14.222: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-679324930 exec --namespace=statefulset-466 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Sep 27 19:28:24.223: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-679324930 exec --namespace=statefulset-466 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 27 19:28:24.383: INFO: rc: 1
Sep 27 19:28:24.383: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-679324930 exec --namespace=statefulset-466 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Sep 27 19:28:34.383: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-679324930 exec --namespace=statefulset-466 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 27 19:28:34.540: INFO: rc: 1
Sep 27 19:28:34.540: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-679324930 exec --namespace=statefulset-466 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Sep 27 19:28:44.540: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-679324930 exec --namespace=statefulset-466 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 27 19:28:44.749: INFO: rc: 1
Sep 27 19:28:44.749: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-679324930 exec --namespace=statefulset-466 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Sep 27 19:28:54.750: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-679324930 exec --namespace=statefulset-466 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 27 19:28:54.922: INFO: rc: 1
Sep 27 19:28:54.922: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-679324930 exec --namespace=statefulset-466 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Sep 27 19:29:04.922: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-679324930 exec --namespace=statefulset-466 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 27 19:29:05.131: INFO: rc: 1
Sep 27 19:29:05.131: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-679324930 exec --namespace=statefulset-466 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Sep 27 19:29:15.131: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-679324930 exec --namespace=statefulset-466 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 27 19:29:15.385: INFO: rc: 1
Sep 27 19:29:15.385: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: 
Sep 27 19:29:15.385: INFO: Scaling statefulset ss to 0
Sep 27 19:29:15.468: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
Sep 27 19:29:15.482: INFO: Deleting all statefulset in ns statefulset-466
Sep 27 19:29:15.497: INFO: Scaling statefulset ss to 0
Sep 27 19:29:15.558: INFO: Waiting for statefulset status.replicas updated to 0
Sep 27 19:29:15.571: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 19:29:15.624: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-466" for this suite.

• [SLOW TEST:361.382 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
    Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
    /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]","total":305,"completed":5,"skipped":91,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 19:29:15.687: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:181
[It] should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating pod
Sep 27 19:29:18.005: INFO: Pod pod-hostip-dedf07ee-8846-4cda-872a-5d687a4820f4 has hostIP: 10.177.248.117
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 19:29:18.005: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-835" for this suite.
•{"msg":"PASSED [k8s.io] Pods should get a host IP [NodeConformance] [Conformance]","total":305,"completed":6,"skipped":105,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 19:29:18.050: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Sep 27 19:29:19.123: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Sep 27 19:29:21.175: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63768367759, loc:(*time.Location)(0x7702840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63768367759, loc:(*time.Location)(0x7702840)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63768367759, loc:(*time.Location)(0x7702840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63768367759, loc:(*time.Location)(0x7702840)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 27 19:29:23.193: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63768367759, loc:(*time.Location)(0x7702840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63768367759, loc:(*time.Location)(0x7702840)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63768367759, loc:(*time.Location)(0x7702840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63768367759, loc:(*time.Location)(0x7702840)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 27 19:29:25.200: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63768367759, loc:(*time.Location)(0x7702840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63768367759, loc:(*time.Location)(0x7702840)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63768367759, loc:(*time.Location)(0x7702840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63768367759, loc:(*time.Location)(0x7702840)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 27 19:29:27.195: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63768367759, loc:(*time.Location)(0x7702840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63768367759, loc:(*time.Location)(0x7702840)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63768367759, loc:(*time.Location)(0x7702840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63768367759, loc:(*time.Location)(0x7702840)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep 27 19:29:30.250: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Creating a dummy validating-webhook-configuration object
STEP: Deleting the validating-webhook-configuration, which should be possible to remove
STEP: Creating a dummy mutating-webhook-configuration object
STEP: Deleting the mutating-webhook-configuration, which should be possible to remove
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 19:29:30.720: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5149" for this suite.
STEP: Destroying namespace "webhook-5149-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:12.959 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]","total":305,"completed":7,"skipped":135,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 19:29:31.008: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename crd-webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:126
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
Sep 27 19:29:32.185: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
Sep 27 19:29:34.238: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63768367772, loc:(*time.Location)(0x7702840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63768367772, loc:(*time.Location)(0x7702840)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63768367772, loc:(*time.Location)(0x7702840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63768367772, loc:(*time.Location)(0x7702840)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-85d57b96d6\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep 27 19:29:37.304: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Sep 27 19:29:37.323: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Creating a v1 custom resource
STEP: v2 custom resource should be converted
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 19:29:38.859: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-1982" for this suite.
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:137

• [SLOW TEST:8.078 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert from CR v1 to CR v2 [Conformance]","total":305,"completed":8,"skipped":140,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 19:29:39.087: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod busybox-3a6fc56d-6fbe-4945-84e7-10f66b750bb8 in namespace container-probe-6466
Sep 27 19:29:43.391: INFO: Started pod busybox-3a6fc56d-6fbe-4945-84e7-10f66b750bb8 in namespace container-probe-6466
STEP: checking the pod's current state and verifying that restartCount is present
Sep 27 19:29:43.409: INFO: Initial restart count of pod busybox-3a6fc56d-6fbe-4945-84e7-10f66b750bb8 is 0
Sep 27 19:30:27.987: INFO: Restart count of pod container-probe-6466/busybox-3a6fc56d-6fbe-4945-84e7-10f66b750bb8 is now 1 (44.578139636s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 19:30:28.081: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-6466" for this suite.

• [SLOW TEST:49.048 seconds]
[k8s.io] Probing container
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Probing container should be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","total":305,"completed":9,"skipped":164,"failed":0}
SSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 19:30:28.150: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating Pod
STEP: Waiting for the pod running
STEP: Geting the pod
STEP: Reading file content from the nginx-container
Sep 27 19:30:32.615: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-539 PodName:pod-sharedvolume-efc1a5f7-aae8-4a69-834f-1aac11f5b1d6 ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep 27 19:30:32.615: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
Sep 27 19:30:33.068: INFO: Exec stderr: ""
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 19:30:33.068: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-539" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes pod should support shared volumes between containers [Conformance]","total":305,"completed":10,"skipped":171,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [sig-storage][Slow] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 19:30:33.124: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify that a failing subpath expansion can be modified during the lifecycle of a container [sig-storage][Slow] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod with failed condition
STEP: updating the pod
Sep 27 19:32:34.161: INFO: Successfully updated pod "var-expansion-3ed0996f-50e6-4c88-96ed-0a2e03a08f38"
STEP: waiting for pod running
STEP: deleting the pod gracefully
Sep 27 19:32:36.199: INFO: Deleting pod "var-expansion-3ed0996f-50e6-4c88-96ed-0a2e03a08f38" in namespace "var-expansion-7348"
Sep 27 19:32:36.229: INFO: Wait up to 5m0s for pod "var-expansion-3ed0996f-50e6-4c88-96ed-0a2e03a08f38" to be fully deleted
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 19:33:10.271: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-7348" for this suite.

• [SLOW TEST:157.192 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [sig-storage][Slow] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Variable Expansion should verify that a failing subpath expansion can be modified during the lifecycle of a container [sig-storage][Slow] [Conformance]","total":305,"completed":11,"skipped":194,"failed":0}
SSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 19:33:10.316: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating the pod
Sep 27 19:33:16.405: INFO: Successfully updated pod "annotationupdated978cee6-9806-4278-8614-f6adf21aa185"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 19:33:18.484: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6291" for this suite.

• [SLOW TEST:8.221 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:36
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should update annotations on modification [NodeConformance] [Conformance]","total":305,"completed":12,"skipped":197,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 19:33:18.539: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating service in namespace services-4592
STEP: creating service affinity-clusterip in namespace services-4592
STEP: creating replication controller affinity-clusterip in namespace services-4592
I0927 19:33:18.794395      22 runners.go:190] Created replication controller with name: affinity-clusterip, namespace: services-4592, replica count: 3
I0927 19:33:21.845044      22 runners.go:190] affinity-clusterip Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0927 19:33:24.845363      22 runners.go:190] affinity-clusterip Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0927 19:33:27.845803      22 runners.go:190] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Sep 27 19:33:27.881: INFO: Creating new exec pod
Sep 27 19:33:32.978: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-679324930 exec --namespace=services-4592 execpod-affinitylqkjd -- /bin/sh -x -c nc -zv -t -w 2 affinity-clusterip 80'
Sep 27 19:33:33.921: INFO: stderr: "+ nc -zv -t -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
Sep 27 19:33:33.921: INFO: stdout: ""
Sep 27 19:33:33.921: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-679324930 exec --namespace=services-4592 execpod-affinitylqkjd -- /bin/sh -x -c nc -zv -t -w 2 172.21.131.203 80'
Sep 27 19:33:34.359: INFO: stderr: "+ nc -zv -t -w 2 172.21.131.203 80\nConnection to 172.21.131.203 80 port [tcp/http] succeeded!\n"
Sep 27 19:33:34.359: INFO: stdout: ""
Sep 27 19:33:34.360: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-679324930 exec --namespace=services-4592 execpod-affinitylqkjd -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.21.131.203:80/ ; done'
Sep 27 19:33:34.906: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.131.203:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.131.203:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.131.203:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.131.203:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.131.203:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.131.203:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.131.203:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.131.203:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.131.203:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.131.203:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.131.203:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.131.203:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.131.203:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.131.203:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.131.203:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.131.203:80/\n"
Sep 27 19:33:34.906: INFO: stdout: "\naffinity-clusterip-8fdn7\naffinity-clusterip-8fdn7\naffinity-clusterip-8fdn7\naffinity-clusterip-8fdn7\naffinity-clusterip-8fdn7\naffinity-clusterip-8fdn7\naffinity-clusterip-8fdn7\naffinity-clusterip-8fdn7\naffinity-clusterip-8fdn7\naffinity-clusterip-8fdn7\naffinity-clusterip-8fdn7\naffinity-clusterip-8fdn7\naffinity-clusterip-8fdn7\naffinity-clusterip-8fdn7\naffinity-clusterip-8fdn7\naffinity-clusterip-8fdn7"
Sep 27 19:33:34.906: INFO: Received response from host: affinity-clusterip-8fdn7
Sep 27 19:33:34.906: INFO: Received response from host: affinity-clusterip-8fdn7
Sep 27 19:33:34.906: INFO: Received response from host: affinity-clusterip-8fdn7
Sep 27 19:33:34.906: INFO: Received response from host: affinity-clusterip-8fdn7
Sep 27 19:33:34.906: INFO: Received response from host: affinity-clusterip-8fdn7
Sep 27 19:33:34.906: INFO: Received response from host: affinity-clusterip-8fdn7
Sep 27 19:33:34.906: INFO: Received response from host: affinity-clusterip-8fdn7
Sep 27 19:33:34.906: INFO: Received response from host: affinity-clusterip-8fdn7
Sep 27 19:33:34.906: INFO: Received response from host: affinity-clusterip-8fdn7
Sep 27 19:33:34.906: INFO: Received response from host: affinity-clusterip-8fdn7
Sep 27 19:33:34.906: INFO: Received response from host: affinity-clusterip-8fdn7
Sep 27 19:33:34.906: INFO: Received response from host: affinity-clusterip-8fdn7
Sep 27 19:33:34.906: INFO: Received response from host: affinity-clusterip-8fdn7
Sep 27 19:33:34.906: INFO: Received response from host: affinity-clusterip-8fdn7
Sep 27 19:33:34.906: INFO: Received response from host: affinity-clusterip-8fdn7
Sep 27 19:33:34.906: INFO: Received response from host: affinity-clusterip-8fdn7
Sep 27 19:33:34.906: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip in namespace services-4592, will wait for the garbage collector to delete the pods
Sep 27 19:33:35.057: INFO: Deleting ReplicationController affinity-clusterip took: 31.768314ms
Sep 27 19:33:35.157: INFO: Terminating ReplicationController affinity-clusterip pods took: 100.256757ms
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 19:33:41.937: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4592" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

• [SLOW TEST:23.455 seconds]
[sig-network] Services
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]","total":305,"completed":13,"skipped":217,"failed":0}
SSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl server-side dry-run 
  should check if kubectl can dry-run update Pods [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 19:33:41.997: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[It] should check if kubectl can dry-run update Pods [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Sep 27 19:33:42.182: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-679324930 run e2e-test-httpd-pod --image=docker.io/library/httpd:2.4.38-alpine --labels=run=e2e-test-httpd-pod --namespace=kubectl-7449'
Sep 27 19:33:42.422: INFO: stderr: ""
Sep 27 19:33:42.422: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: replace the image in the pod with server-side dry-run
Sep 27 19:33:42.422: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-679324930 get pod e2e-test-httpd-pod -o json --namespace=kubectl-7449'
Sep 27 19:33:42.579: INFO: stderr: ""
Sep 27 19:33:42.579: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"openshift.io/scc\": \"anyuid\"\n        },\n        \"creationTimestamp\": \"2021-09-27T19:33:42Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"managedFields\": [\n            {\n                \"apiVersion\": \"v1\",\n                \"fieldsType\": \"FieldsV1\",\n                \"fieldsV1\": {\n                    \"f:metadata\": {\n                        \"f:labels\": {\n                            \".\": {},\n                            \"f:run\": {}\n                        }\n                    },\n                    \"f:spec\": {\n                        \"f:containers\": {\n                            \"k:{\\\"name\\\":\\\"e2e-test-httpd-pod\\\"}\": {\n                                \".\": {},\n                                \"f:image\": {},\n                                \"f:imagePullPolicy\": {},\n                                \"f:name\": {},\n                                \"f:resources\": {},\n                                \"f:terminationMessagePath\": {},\n                                \"f:terminationMessagePolicy\": {}\n                            }\n                        },\n                        \"f:dnsPolicy\": {},\n                        \"f:enableServiceLinks\": {},\n                        \"f:restartPolicy\": {},\n                        \"f:schedulerName\": {},\n                        \"f:securityContext\": {},\n                        \"f:terminationGracePeriodSeconds\": {}\n                    }\n                },\n                \"manager\": \"kubectl-run\",\n                \"operation\": \"Update\",\n                \"time\": \"2021-09-27T19:33:42Z\"\n            },\n            {\n                \"apiVersion\": \"v1\",\n                \"fieldsType\": \"FieldsV1\",\n                \"fieldsV1\": {\n                    \"f:status\": {\n                        \"f:conditions\": {\n                            \"k:{\\\"type\\\":\\\"ContainersReady\\\"}\": {\n                                \".\": {},\n                                \"f:lastProbeTime\": {},\n                                \"f:lastTransitionTime\": {},\n                                \"f:message\": {},\n                                \"f:reason\": {},\n                                \"f:status\": {},\n                                \"f:type\": {}\n                            },\n                            \"k:{\\\"type\\\":\\\"Initialized\\\"}\": {\n                                \".\": {},\n                                \"f:lastProbeTime\": {},\n                                \"f:lastTransitionTime\": {},\n                                \"f:status\": {},\n                                \"f:type\": {}\n                            },\n                            \"k:{\\\"type\\\":\\\"Ready\\\"}\": {\n                                \".\": {},\n                                \"f:lastProbeTime\": {},\n                                \"f:lastTransitionTime\": {},\n                                \"f:message\": {},\n                                \"f:reason\": {},\n                                \"f:status\": {},\n                                \"f:type\": {}\n                            }\n                        },\n                        \"f:containerStatuses\": {},\n                        \"f:hostIP\": {},\n                        \"f:startTime\": {}\n                    }\n                },\n                \"manager\": \"kubelet\",\n                \"operation\": \"Update\",\n                \"time\": \"2021-09-27T19:33:42Z\"\n            }\n        ],\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-7449\",\n        \"resourceVersion\": \"56682\",\n        \"selfLink\": \"/api/v1/namespaces/kubectl-7449/pods/e2e-test-httpd-pod\",\n        \"uid\": \"65e3dcdb-ff96-48c2-81ce-f74a7fc2f7aa\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"docker.io/library/httpd:2.4.38-alpine\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"securityContext\": {\n                    \"capabilities\": {\n                        \"drop\": [\n                            \"MKNOD\"\n                        ]\n                    }\n                },\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"default-token-pd94s\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"imagePullSecrets\": [\n            {\n                \"name\": \"default-dockercfg-n4zw4\"\n            }\n        ],\n        \"nodeName\": \"10.177.248.117\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {\n            \"seLinuxOptions\": {\n                \"level\": \"s0:c31,c10\"\n            }\n        },\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"default-token-pd94s\",\n                \"secret\": {\n                    \"defaultMode\": 420,\n                    \"secretName\": \"default-token-pd94s\"\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2021-09-27T19:33:42Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2021-09-27T19:33:42Z\",\n                \"message\": \"containers with unready status: [e2e-test-httpd-pod]\",\n                \"reason\": \"ContainersNotReady\",\n                \"status\": \"False\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2021-09-27T19:33:42Z\",\n                \"message\": \"containers with unready status: [e2e-test-httpd-pod]\",\n                \"reason\": \"ContainersNotReady\",\n                \"status\": \"False\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2021-09-27T19:33:42Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"image\": \"docker.io/library/httpd:2.4.38-alpine\",\n                \"imageID\": \"\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": false,\n                \"restartCount\": 0,\n                \"started\": false,\n                \"state\": {\n                    \"waiting\": {\n                        \"reason\": \"ContainerCreating\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"10.177.248.117\",\n        \"phase\": \"Pending\",\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2021-09-27T19:33:42Z\"\n    }\n}\n"
Sep 27 19:33:42.579: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-679324930 replace -f - --dry-run server --namespace=kubectl-7449'
Sep 27 19:33:43.247: INFO: stderr: "W0927 19:33:42.675211     333 helpers.go:553] --dry-run is deprecated and can be replaced with --dry-run=client.\n"
Sep 27 19:33:43.247: INFO: stdout: "pod/e2e-test-httpd-pod replaced (dry run)\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image docker.io/library/httpd:2.4.38-alpine
Sep 27 19:33:43.272: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-679324930 delete pods e2e-test-httpd-pod --namespace=kubectl-7449'
Sep 27 19:33:44.997: INFO: stderr: ""
Sep 27 19:33:44.997: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 19:33:44.998: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7449" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl server-side dry-run should check if kubectl can dry-run update Pods [Conformance]","total":305,"completed":14,"skipped":225,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 19:33:45.050: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 19:33:49.572: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-3645" for this suite.
•{"msg":"PASSED [k8s.io] Docker Containers should use the image defaults if command and args are blank [NodeConformance] [Conformance]","total":305,"completed":15,"skipped":259,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run pod 
  should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 19:33:49.625: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[BeforeEach] Kubectl run pod
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1545
[It] should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Sep 27 19:33:49.809: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-679324930 run e2e-test-httpd-pod --restart=Never --image=docker.io/library/httpd:2.4.38-alpine --namespace=kubectl-5937'
Sep 27 19:33:50.047: INFO: stderr: ""
Sep 27 19:33:50.047: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod was created
[AfterEach] Kubectl run pod
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1550
Sep 27 19:33:50.068: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-679324930 delete pods e2e-test-httpd-pod --namespace=kubectl-5937'
Sep 27 19:33:59.295: INFO: stderr: ""
Sep 27 19:33:59.295: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 19:33:59.295: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5937" for this suite.

• [SLOW TEST:9.723 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl run pod
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1541
    should create a pod from an image when restart is Never  [Conformance]
    /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl run pod should create a pod from an image when restart is Never  [Conformance]","total":305,"completed":16,"skipped":272,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 19:33:59.349: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:90
Sep 27 19:33:59.562: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Sep 27 19:33:59.622: INFO: Waiting for terminating namespaces to be deleted...
Sep 27 19:33:59.649: INFO: 
Logging pods the apiserver thinks is on node 10.177.248.114 before test
Sep 27 19:33:59.706: INFO: calico-node-kxvx5 from calico-system started at 2021-09-27 17:50:17 +0000 UTC (1 container statuses recorded)
Sep 27 19:33:59.706: INFO: 	Container calico-node ready: true, restart count 0
Sep 27 19:33:59.706: INFO: calico-typha-7d789bfc7c-8ftcw from calico-system started at 2021-09-27 17:52:15 +0000 UTC (1 container statuses recorded)
Sep 27 19:33:59.706: INFO: 	Container calico-typha ready: true, restart count 0
Sep 27 19:33:59.706: INFO: test-k8s-e2e-pvg-master-verification from default started at 2021-09-27 17:52:52 +0000 UTC (1 container statuses recorded)
Sep 27 19:33:59.706: INFO: 	Container test-k8s-e2e-pvg-master-verification ready: true, restart count 0
Sep 27 19:33:59.706: INFO: ibm-cloud-provider-ip-169-61-239-242-59b4964694-ngqgl from ibm-system started at 2021-09-27 18:03:32 +0000 UTC (1 container statuses recorded)
Sep 27 19:33:59.707: INFO: 	Container ibm-cloud-provider-ip-169-61-239-242 ready: true, restart count 0
Sep 27 19:33:59.707: INFO: ibm-keepalived-watcher-8lj4c from kube-system started at 2021-09-27 17:50:13 +0000 UTC (1 container statuses recorded)
Sep 27 19:33:59.707: INFO: 	Container keepalived-watcher ready: true, restart count 0
Sep 27 19:33:59.707: INFO: ibm-master-proxy-static-10.177.248.114 from kube-system started at 2021-09-27 17:49:33 +0000 UTC (2 container statuses recorded)
Sep 27 19:33:59.707: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Sep 27 19:33:59.707: INFO: 	Container pause ready: true, restart count 0
Sep 27 19:33:59.707: INFO: ibmcloud-block-storage-driver-mxb6q from kube-system started at 2021-09-27 17:50:13 +0000 UTC (1 container statuses recorded)
Sep 27 19:33:59.707: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Sep 27 19:33:59.707: INFO: vpn-bc979587-kglsg from kube-system started at 2021-09-27 17:53:53 +0000 UTC (1 container statuses recorded)
Sep 27 19:33:59.708: INFO: 	Container vpn ready: true, restart count 0
Sep 27 19:33:59.708: INFO: tuned-dsf6r from openshift-cluster-node-tuning-operator started at 2021-09-27 17:51:40 +0000 UTC (1 container statuses recorded)
Sep 27 19:33:59.708: INFO: 	Container tuned ready: true, restart count 0
Sep 27 19:33:59.708: INFO: console-5b57dff4ff-2hr57 from openshift-console started at 2021-09-27 17:54:58 +0000 UTC (1 container statuses recorded)
Sep 27 19:33:59.708: INFO: 	Container console ready: true, restart count 0
Sep 27 19:33:59.708: INFO: dns-default-ktd8w from openshift-dns started at 2021-09-27 17:52:55 +0000 UTC (3 container statuses recorded)
Sep 27 19:33:59.708: INFO: 	Container dns ready: true, restart count 0
Sep 27 19:33:59.708: INFO: 	Container dns-node-resolver ready: true, restart count 0
Sep 27 19:33:59.708: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep 27 19:33:59.708: INFO: node-ca-bv4wj from openshift-image-registry started at 2021-09-27 17:53:17 +0000 UTC (1 container statuses recorded)
Sep 27 19:33:59.708: INFO: 	Container node-ca ready: true, restart count 0
Sep 27 19:33:59.708: INFO: router-default-768f4875db-n8g9c from openshift-ingress started at 2021-09-27 17:53:15 +0000 UTC (1 container statuses recorded)
Sep 27 19:33:59.709: INFO: 	Container router ready: true, restart count 0
Sep 27 19:33:59.709: INFO: openshift-kube-proxy-lxdz4 from openshift-kube-proxy started at 2021-09-27 17:50:13 +0000 UTC (1 container statuses recorded)
Sep 27 19:33:59.709: INFO: 	Container kube-proxy ready: true, restart count 0
Sep 27 19:33:59.709: INFO: certified-operators-qvnmv from openshift-marketplace started at 2021-09-27 17:54:49 +0000 UTC (1 container statuses recorded)
Sep 27 19:33:59.709: INFO: 	Container registry-server ready: true, restart count 0
Sep 27 19:33:59.709: INFO: community-operators-mc4rj from openshift-marketplace started at 2021-09-27 17:54:50 +0000 UTC (1 container statuses recorded)
Sep 27 19:33:59.709: INFO: 	Container registry-server ready: true, restart count 0
Sep 27 19:33:59.709: INFO: redhat-marketplace-tf75q from openshift-marketplace started at 2021-09-27 17:54:48 +0000 UTC (1 container statuses recorded)
Sep 27 19:33:59.709: INFO: 	Container registry-server ready: true, restart count 0
Sep 27 19:33:59.709: INFO: redhat-operators-2sd6b from openshift-marketplace started at 2021-09-27 17:54:49 +0000 UTC (1 container statuses recorded)
Sep 27 19:33:59.709: INFO: 	Container registry-server ready: true, restart count 0
Sep 27 19:33:59.709: INFO: alertmanager-main-0 from openshift-monitoring started at 2021-09-27 17:53:55 +0000 UTC (5 container statuses recorded)
Sep 27 19:33:59.709: INFO: 	Container alertmanager ready: true, restart count 0
Sep 27 19:33:59.710: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Sep 27 19:33:59.710: INFO: 	Container config-reloader ready: true, restart count 0
Sep 27 19:33:59.710: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep 27 19:33:59.710: INFO: 	Container prom-label-proxy ready: true, restart count 0
Sep 27 19:33:59.710: INFO: grafana-6f7d589b46-r88hj from openshift-monitoring started at 2021-09-27 17:53:53 +0000 UTC (2 container statuses recorded)
Sep 27 19:33:59.710: INFO: 	Container grafana ready: true, restart count 0
Sep 27 19:33:59.710: INFO: 	Container grafana-proxy ready: true, restart count 0
Sep 27 19:33:59.710: INFO: kube-state-metrics-5d4985f6b7-jx87q from openshift-monitoring started at 2021-09-27 17:51:49 +0000 UTC (3 container statuses recorded)
Sep 27 19:33:59.710: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Sep 27 19:33:59.710: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Sep 27 19:33:59.710: INFO: 	Container kube-state-metrics ready: true, restart count 0
Sep 27 19:33:59.711: INFO: node-exporter-r84r5 from openshift-monitoring started at 2021-09-27 17:51:50 +0000 UTC (2 container statuses recorded)
Sep 27 19:33:59.711: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep 27 19:33:59.711: INFO: 	Container node-exporter ready: true, restart count 0
Sep 27 19:33:59.711: INFO: openshift-state-metrics-58bfb7bff-qxd59 from openshift-monitoring started at 2021-09-27 17:51:50 +0000 UTC (3 container statuses recorded)
Sep 27 19:33:59.711: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Sep 27 19:33:59.711: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Sep 27 19:33:59.711: INFO: 	Container openshift-state-metrics ready: true, restart count 0
Sep 27 19:33:59.711: INFO: prometheus-adapter-66469d976-gl6wl from openshift-monitoring started at 2021-09-27 17:54:45 +0000 UTC (1 container statuses recorded)
Sep 27 19:33:59.711: INFO: 	Container prometheus-adapter ready: true, restart count 0
Sep 27 19:33:59.711: INFO: prometheus-k8s-0 from openshift-monitoring started at 2021-09-27 17:56:49 +0000 UTC (6 container statuses recorded)
Sep 27 19:33:59.711: INFO: 	Container config-reloader ready: true, restart count 0
Sep 27 19:33:59.712: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep 27 19:33:59.712: INFO: 	Container prom-label-proxy ready: true, restart count 0
Sep 27 19:33:59.712: INFO: 	Container prometheus ready: true, restart count 1
Sep 27 19:33:59.712: INFO: 	Container prometheus-proxy ready: true, restart count 0
Sep 27 19:33:59.712: INFO: 	Container thanos-sidecar ready: true, restart count 0
Sep 27 19:33:59.712: INFO: prometheus-operator-856d6cddd4-blncm from openshift-monitoring started at 2021-09-27 17:54:31 +0000 UTC (2 container statuses recorded)
Sep 27 19:33:59.712: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep 27 19:33:59.712: INFO: 	Container prometheus-operator ready: true, restart count 0
Sep 27 19:33:59.712: INFO: telemeter-client-68cfc9967c-sr2hj from openshift-monitoring started at 2021-09-27 17:51:59 +0000 UTC (3 container statuses recorded)
Sep 27 19:33:59.712: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep 27 19:33:59.712: INFO: 	Container reload ready: true, restart count 0
Sep 27 19:33:59.712: INFO: 	Container telemeter-client ready: true, restart count 0
Sep 27 19:33:59.712: INFO: thanos-querier-6b5789bbbd-qwc9q from openshift-monitoring started at 2021-09-27 17:53:50 +0000 UTC (5 container statuses recorded)
Sep 27 19:33:59.713: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep 27 19:33:59.713: INFO: 	Container kube-rbac-proxy-rules ready: true, restart count 0
Sep 27 19:33:59.713: INFO: 	Container oauth-proxy ready: true, restart count 0
Sep 27 19:33:59.713: INFO: 	Container prom-label-proxy ready: true, restart count 0
Sep 27 19:33:59.713: INFO: 	Container thanos-query ready: true, restart count 0
Sep 27 19:33:59.713: INFO: multus-admission-controller-pp4c5 from openshift-multus started at 2021-09-27 17:51:20 +0000 UTC (2 container statuses recorded)
Sep 27 19:33:59.713: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep 27 19:33:59.713: INFO: 	Container multus-admission-controller ready: true, restart count 0
Sep 27 19:33:59.713: INFO: multus-qwrqb from openshift-multus started at 2021-09-27 17:50:13 +0000 UTC (1 container statuses recorded)
Sep 27 19:33:59.713: INFO: 	Container kube-multus ready: true, restart count 0
Sep 27 19:33:59.713: INFO: network-metrics-daemon-zdwnk from openshift-multus started at 2021-09-27 17:50:13 +0000 UTC (2 container statuses recorded)
Sep 27 19:33:59.714: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep 27 19:33:59.714: INFO: 	Container network-metrics-daemon ready: true, restart count 0
Sep 27 19:33:59.714: INFO: packageserver-67b4ccc984-2lht2 from openshift-operator-lifecycle-manager started at 2021-09-27 17:53:13 +0000 UTC (1 container statuses recorded)
Sep 27 19:33:59.714: INFO: 	Container packageserver ready: true, restart count 0
Sep 27 19:33:59.714: INFO: service-ca-5655fcb96b-ntfqm from openshift-service-ca started at 2021-09-27 17:51:48 +0000 UTC (1 container statuses recorded)
Sep 27 19:33:59.714: INFO: 	Container service-ca-controller ready: true, restart count 0
Sep 27 19:33:59.714: INFO: sonobuoy from sonobuoy started at 2021-09-27 19:21:37 +0000 UTC (1 container statuses recorded)
Sep 27 19:33:59.714: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Sep 27 19:33:59.714: INFO: sonobuoy-e2e-job-f7e56101f16c459b from sonobuoy started at 2021-09-27 19:21:44 +0000 UTC (2 container statuses recorded)
Sep 27 19:33:59.714: INFO: 	Container e2e ready: true, restart count 0
Sep 27 19:33:59.714: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep 27 19:33:59.715: INFO: sonobuoy-systemd-logs-daemon-set-a625d2c9d0384434-sl5dj from sonobuoy started at 2021-09-27 19:21:44 +0000 UTC (2 container statuses recorded)
Sep 27 19:33:59.715: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep 27 19:33:59.715: INFO: 	Container systemd-logs ready: true, restart count 0
Sep 27 19:33:59.715: INFO: 
Logging pods the apiserver thinks is on node 10.177.248.117 before test
Sep 27 19:33:59.771: INFO: calico-node-d4rns from calico-system started at 2021-09-27 17:50:17 +0000 UTC (1 container statuses recorded)
Sep 27 19:33:59.772: INFO: 	Container calico-node ready: true, restart count 0
Sep 27 19:33:59.772: INFO: calico-typha-7d789bfc7c-79bn2 from calico-system started at 2021-09-27 17:52:15 +0000 UTC (1 container statuses recorded)
Sep 27 19:33:59.772: INFO: 	Container calico-typha ready: true, restart count 0
Sep 27 19:33:59.772: INFO: ibm-cloud-provider-ip-169-61-239-242-59b4964694-nq8r6 from ibm-system started at 2021-09-27 18:03:26 +0000 UTC (1 container statuses recorded)
Sep 27 19:33:59.772: INFO: 	Container ibm-cloud-provider-ip-169-61-239-242 ready: true, restart count 0
Sep 27 19:33:59.772: INFO: ibm-keepalived-watcher-9fxhn from kube-system started at 2021-09-27 17:48:31 +0000 UTC (1 container statuses recorded)
Sep 27 19:33:59.772: INFO: 	Container keepalived-watcher ready: true, restart count 0
Sep 27 19:33:59.772: INFO: ibm-master-proxy-static-10.177.248.117 from kube-system started at 2021-09-27 17:47:53 +0000 UTC (2 container statuses recorded)
Sep 27 19:33:59.772: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Sep 27 19:33:59.772: INFO: 	Container pause ready: true, restart count 0
Sep 27 19:33:59.772: INFO: ibm-storage-watcher-7cd75f8d4f-gk2bv from kube-system started at 2021-09-27 17:50:40 +0000 UTC (1 container statuses recorded)
Sep 27 19:33:59.772: INFO: 	Container ibm-storage-watcher-container ready: true, restart count 0
Sep 27 19:33:59.772: INFO: ibmcloud-block-storage-driver-4ldjm from kube-system started at 2021-09-27 17:48:31 +0000 UTC (1 container statuses recorded)
Sep 27 19:33:59.772: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Sep 27 19:33:59.772: INFO: tuned-k9qlw from openshift-cluster-node-tuning-operator started at 2021-09-27 17:51:40 +0000 UTC (1 container statuses recorded)
Sep 27 19:33:59.772: INFO: 	Container tuned ready: true, restart count 0
Sep 27 19:33:59.773: INFO: console-operator-d6cf9dd7d-hnfht from openshift-console-operator started at 2021-09-27 17:50:40 +0000 UTC (1 container statuses recorded)
Sep 27 19:33:59.773: INFO: 	Container console-operator ready: true, restart count 1
Sep 27 19:33:59.773: INFO: console-5b57dff4ff-mqhrj from openshift-console started at 2021-09-27 17:55:15 +0000 UTC (1 container statuses recorded)
Sep 27 19:33:59.773: INFO: 	Container console ready: true, restart count 0
Sep 27 19:33:59.773: INFO: dns-default-cb2zk from openshift-dns started at 2021-09-27 17:52:55 +0000 UTC (3 container statuses recorded)
Sep 27 19:33:59.773: INFO: 	Container dns ready: true, restart count 0
Sep 27 19:33:59.773: INFO: 	Container dns-node-resolver ready: true, restart count 0
Sep 27 19:33:59.773: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep 27 19:33:59.773: INFO: node-ca-82tbs from openshift-image-registry started at 2021-09-27 17:53:17 +0000 UTC (1 container statuses recorded)
Sep 27 19:33:59.773: INFO: 	Container node-ca ready: true, restart count 0
Sep 27 19:33:59.773: INFO: registry-pvc-permissions-vncj6 from openshift-image-registry started at 2021-09-27 17:55:54 +0000 UTC (1 container statuses recorded)
Sep 27 19:33:59.773: INFO: 	Container pvc-permissions ready: false, restart count 0
Sep 27 19:33:59.773: INFO: openshift-kube-proxy-2dc5b from openshift-kube-proxy started at 2021-09-27 17:49:34 +0000 UTC (1 container statuses recorded)
Sep 27 19:33:59.773: INFO: 	Container kube-proxy ready: true, restart count 0
Sep 27 19:33:59.774: INFO: migrator-6656c87b46-zr5mp from openshift-kube-storage-version-migrator started at 2021-09-27 17:51:09 +0000 UTC (1 container statuses recorded)
Sep 27 19:33:59.774: INFO: 	Container migrator ready: true, restart count 0
Sep 27 19:33:59.774: INFO: alertmanager-main-2 from openshift-monitoring started at 2021-09-27 17:53:55 +0000 UTC (5 container statuses recorded)
Sep 27 19:33:59.774: INFO: 	Container alertmanager ready: true, restart count 0
Sep 27 19:33:59.774: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Sep 27 19:33:59.774: INFO: 	Container config-reloader ready: true, restart count 0
Sep 27 19:33:59.774: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep 27 19:33:59.774: INFO: 	Container prom-label-proxy ready: true, restart count 0
Sep 27 19:33:59.774: INFO: node-exporter-r6mdq from openshift-monitoring started at 2021-09-27 17:51:50 +0000 UTC (2 container statuses recorded)
Sep 27 19:33:59.774: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep 27 19:33:59.775: INFO: 	Container node-exporter ready: true, restart count 0
Sep 27 19:33:59.775: INFO: prometheus-k8s-1 from openshift-monitoring started at 2021-09-27 17:56:49 +0000 UTC (6 container statuses recorded)
Sep 27 19:33:59.775: INFO: 	Container config-reloader ready: true, restart count 0
Sep 27 19:33:59.775: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep 27 19:33:59.775: INFO: 	Container prom-label-proxy ready: true, restart count 0
Sep 27 19:33:59.776: INFO: 	Container prometheus ready: true, restart count 1
Sep 27 19:33:59.776: INFO: 	Container prometheus-proxy ready: true, restart count 0
Sep 27 19:33:59.776: INFO: 	Container thanos-sidecar ready: true, restart count 0
Sep 27 19:33:59.776: INFO: multus-admission-controller-k7549 from openshift-multus started at 2021-09-27 17:50:30 +0000 UTC (2 container statuses recorded)
Sep 27 19:33:59.776: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep 27 19:33:59.776: INFO: 	Container multus-admission-controller ready: true, restart count 0
Sep 27 19:33:59.776: INFO: multus-vjt2q from openshift-multus started at 2021-09-27 17:49:24 +0000 UTC (1 container statuses recorded)
Sep 27 19:33:59.776: INFO: 	Container kube-multus ready: true, restart count 0
Sep 27 19:33:59.776: INFO: network-metrics-daemon-4t2kd from openshift-multus started at 2021-09-27 17:49:26 +0000 UTC (2 container statuses recorded)
Sep 27 19:33:59.776: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep 27 19:33:59.776: INFO: 	Container network-metrics-daemon ready: true, restart count 0
Sep 27 19:33:59.776: INFO: network-operator-dbdd595f7-vfdhn from openshift-network-operator started at 2021-09-27 17:48:31 +0000 UTC (1 container statuses recorded)
Sep 27 19:33:59.777: INFO: 	Container network-operator ready: true, restart count 0
Sep 27 19:33:59.777: INFO: push-gateway-6f6b5cb7f7-n726l from openshift-roks-metrics started at 2021-09-27 17:50:38 +0000 UTC (1 container statuses recorded)
Sep 27 19:33:59.777: INFO: 	Container push-gateway ready: true, restart count 0
Sep 27 19:33:59.777: INFO: sonobuoy-systemd-logs-daemon-set-a625d2c9d0384434-z8xz7 from sonobuoy started at 2021-09-27 19:21:44 +0000 UTC (2 container statuses recorded)
Sep 27 19:33:59.777: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep 27 19:33:59.777: INFO: 	Container systemd-logs ready: true, restart count 0
Sep 27 19:33:59.777: INFO: tigera-operator-db8ddcc79-w2x4s from tigera-operator started at 2021-09-27 17:48:31 +0000 UTC (1 container statuses recorded)
Sep 27 19:33:59.777: INFO: 	Container tigera-operator ready: true, restart count 3
Sep 27 19:33:59.777: INFO: 
Logging pods the apiserver thinks is on node 10.177.248.126 before test
Sep 27 19:33:59.838: INFO: calico-kube-controllers-5465c95dd-7hlj7 from calico-system started at 2021-09-27 17:50:29 +0000 UTC (1 container statuses recorded)
Sep 27 19:33:59.838: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Sep 27 19:33:59.838: INFO: calico-node-dhz9l from calico-system started at 2021-09-27 17:50:17 +0000 UTC (1 container statuses recorded)
Sep 27 19:33:59.838: INFO: 	Container calico-node ready: true, restart count 0
Sep 27 19:33:59.838: INFO: calico-typha-7d789bfc7c-7669x from calico-system started at 2021-09-27 17:50:17 +0000 UTC (1 container statuses recorded)
Sep 27 19:33:59.838: INFO: 	Container calico-typha ready: true, restart count 0
Sep 27 19:33:59.838: INFO: ibm-file-plugin-fcdf5f569-x8hfq from kube-system started at 2021-09-27 17:50:29 +0000 UTC (1 container statuses recorded)
Sep 27 19:33:59.838: INFO: 	Container ibm-file-plugin-container ready: true, restart count 0
Sep 27 19:33:59.838: INFO: ibm-keepalived-watcher-rgwfm from kube-system started at 2021-09-27 17:49:04 +0000 UTC (1 container statuses recorded)
Sep 27 19:33:59.838: INFO: 	Container keepalived-watcher ready: true, restart count 0
Sep 27 19:33:59.838: INFO: ibm-master-proxy-static-10.177.248.126 from kube-system started at 2021-09-27 17:48:20 +0000 UTC (2 container statuses recorded)
Sep 27 19:33:59.838: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Sep 27 19:33:59.838: INFO: 	Container pause ready: true, restart count 0
Sep 27 19:33:59.838: INFO: ibmcloud-block-storage-driver-kbd9g from kube-system started at 2021-09-27 17:49:04 +0000 UTC (1 container statuses recorded)
Sep 27 19:33:59.838: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Sep 27 19:33:59.838: INFO: ibmcloud-block-storage-plugin-74d6877898-zqtnb from kube-system started at 2021-09-27 17:50:29 +0000 UTC (1 container statuses recorded)
Sep 27 19:33:59.838: INFO: 	Container ibmcloud-block-storage-plugin-container ready: true, restart count 0
Sep 27 19:33:59.838: INFO: cluster-node-tuning-operator-67b4b4fbf5-zslfs from openshift-cluster-node-tuning-operator started at 2021-09-27 17:50:29 +0000 UTC (1 container statuses recorded)
Sep 27 19:33:59.838: INFO: 	Container cluster-node-tuning-operator ready: true, restart count 0
Sep 27 19:33:59.838: INFO: tuned-qc5dt from openshift-cluster-node-tuning-operator started at 2021-09-27 17:51:41 +0000 UTC (1 container statuses recorded)
Sep 27 19:33:59.838: INFO: 	Container tuned ready: true, restart count 0
Sep 27 19:33:59.838: INFO: cluster-samples-operator-97cc95ff8-nmc7q from openshift-cluster-samples-operator started at 2021-09-27 17:50:29 +0000 UTC (2 container statuses recorded)
Sep 27 19:33:59.838: INFO: 	Container cluster-samples-operator ready: true, restart count 0
Sep 27 19:33:59.838: INFO: 	Container cluster-samples-operator-watch ready: true, restart count 0
Sep 27 19:33:59.838: INFO: cluster-storage-operator-59698ddbdf-w86gw from openshift-cluster-storage-operator started at 2021-09-27 17:50:29 +0000 UTC (1 container statuses recorded)
Sep 27 19:33:59.838: INFO: 	Container cluster-storage-operator ready: true, restart count 0
Sep 27 19:33:59.838: INFO: downloads-c785794b6-q92tc from openshift-console started at 2021-09-27 17:50:29 +0000 UTC (1 container statuses recorded)
Sep 27 19:33:59.838: INFO: 	Container download-server ready: true, restart count 0
Sep 27 19:33:59.838: INFO: downloads-c785794b6-w28kb from openshift-console started at 2021-09-27 17:50:29 +0000 UTC (1 container statuses recorded)
Sep 27 19:33:59.838: INFO: 	Container download-server ready: true, restart count 0
Sep 27 19:33:59.838: INFO: dns-operator-7489bbc67f-9ztr4 from openshift-dns-operator started at 2021-09-27 17:50:29 +0000 UTC (2 container statuses recorded)
Sep 27 19:33:59.838: INFO: 	Container dns-operator ready: true, restart count 0
Sep 27 19:33:59.838: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep 27 19:33:59.838: INFO: dns-default-msjgz from openshift-dns started at 2021-09-27 17:52:55 +0000 UTC (3 container statuses recorded)
Sep 27 19:33:59.838: INFO: 	Container dns ready: true, restart count 0
Sep 27 19:33:59.838: INFO: 	Container dns-node-resolver ready: true, restart count 0
Sep 27 19:33:59.838: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep 27 19:33:59.838: INFO: cluster-image-registry-operator-675674456f-k2fc9 from openshift-image-registry started at 2021-09-27 17:50:29 +0000 UTC (1 container statuses recorded)
Sep 27 19:33:59.838: INFO: 	Container cluster-image-registry-operator ready: true, restart count 0
Sep 27 19:33:59.838: INFO: image-registry-ffbbf6ddb-xl9b9 from openshift-image-registry started at 2021-09-27 17:55:54 +0000 UTC (1 container statuses recorded)
Sep 27 19:33:59.838: INFO: 	Container registry ready: true, restart count 0
Sep 27 19:33:59.838: INFO: node-ca-2rrgl from openshift-image-registry started at 2021-09-27 17:53:17 +0000 UTC (1 container statuses recorded)
Sep 27 19:33:59.838: INFO: 	Container node-ca ready: true, restart count 0
Sep 27 19:33:59.838: INFO: ingress-operator-8469759c95-4mm9m from openshift-ingress-operator started at 2021-09-27 17:50:29 +0000 UTC (2 container statuses recorded)
Sep 27 19:33:59.838: INFO: 	Container ingress-operator ready: true, restart count 0
Sep 27 19:33:59.838: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep 27 19:33:59.838: INFO: router-default-768f4875db-8qx4w from openshift-ingress started at 2021-09-27 17:53:15 +0000 UTC (1 container statuses recorded)
Sep 27 19:33:59.838: INFO: 	Container router ready: true, restart count 0
Sep 27 19:33:59.838: INFO: openshift-kube-proxy-7gx28 from openshift-kube-proxy started at 2021-09-27 17:49:34 +0000 UTC (1 container statuses recorded)
Sep 27 19:33:59.838: INFO: 	Container kube-proxy ready: true, restart count 0
Sep 27 19:33:59.838: INFO: kube-storage-version-migrator-operator-55c7c8f996-xr7ds from openshift-kube-storage-version-migrator-operator started at 2021-09-27 17:50:29 +0000 UTC (1 container statuses recorded)
Sep 27 19:33:59.838: INFO: 	Container kube-storage-version-migrator-operator ready: true, restart count 1
Sep 27 19:33:59.838: INFO: marketplace-operator-7cd49bbf56-nggpg from openshift-marketplace started at 2021-09-27 17:50:29 +0000 UTC (1 container statuses recorded)
Sep 27 19:33:59.838: INFO: 	Container marketplace-operator ready: true, restart count 0
Sep 27 19:33:59.838: INFO: alertmanager-main-1 from openshift-monitoring started at 2021-09-27 17:53:55 +0000 UTC (5 container statuses recorded)
Sep 27 19:33:59.838: INFO: 	Container alertmanager ready: true, restart count 0
Sep 27 19:33:59.838: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Sep 27 19:33:59.838: INFO: 	Container config-reloader ready: true, restart count 0
Sep 27 19:33:59.838: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep 27 19:33:59.838: INFO: 	Container prom-label-proxy ready: true, restart count 0
Sep 27 19:33:59.838: INFO: cluster-monitoring-operator-7656b489c8-z2wmx from openshift-monitoring started at 2021-09-27 17:50:29 +0000 UTC (2 container statuses recorded)
Sep 27 19:33:59.838: INFO: 	Container cluster-monitoring-operator ready: true, restart count 0
Sep 27 19:33:59.838: INFO: 	Container kube-rbac-proxy ready: true, restart count 3
Sep 27 19:33:59.838: INFO: node-exporter-42nr4 from openshift-monitoring started at 2021-09-27 17:51:50 +0000 UTC (2 container statuses recorded)
Sep 27 19:33:59.838: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep 27 19:33:59.838: INFO: 	Container node-exporter ready: true, restart count 0
Sep 27 19:33:59.838: INFO: prometheus-adapter-66469d976-t6rf2 from openshift-monitoring started at 2021-09-27 17:54:46 +0000 UTC (1 container statuses recorded)
Sep 27 19:33:59.838: INFO: 	Container prometheus-adapter ready: true, restart count 0
Sep 27 19:33:59.838: INFO: thanos-querier-6b5789bbbd-l8dmh from openshift-monitoring started at 2021-09-27 17:53:51 +0000 UTC (5 container statuses recorded)
Sep 27 19:33:59.838: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep 27 19:33:59.838: INFO: 	Container kube-rbac-proxy-rules ready: true, restart count 0
Sep 27 19:33:59.838: INFO: 	Container oauth-proxy ready: true, restart count 0
Sep 27 19:33:59.838: INFO: 	Container prom-label-proxy ready: true, restart count 0
Sep 27 19:33:59.838: INFO: 	Container thanos-query ready: true, restart count 0
Sep 27 19:33:59.838: INFO: multus-admission-controller-fgtsq from openshift-multus started at 2021-09-27 17:50:29 +0000 UTC (2 container statuses recorded)
Sep 27 19:33:59.838: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep 27 19:33:59.838: INFO: 	Container multus-admission-controller ready: true, restart count 0
Sep 27 19:33:59.838: INFO: multus-fzpdl from openshift-multus started at 2021-09-27 17:49:24 +0000 UTC (1 container statuses recorded)
Sep 27 19:33:59.838: INFO: 	Container kube-multus ready: true, restart count 0
Sep 27 19:33:59.838: INFO: network-metrics-daemon-5wdz5 from openshift-multus started at 2021-09-27 17:49:26 +0000 UTC (2 container statuses recorded)
Sep 27 19:33:59.838: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep 27 19:33:59.838: INFO: 	Container network-metrics-daemon ready: true, restart count 0
Sep 27 19:33:59.838: INFO: catalog-operator-69c4599997-btc8l from openshift-operator-lifecycle-manager started at 2021-09-27 17:50:29 +0000 UTC (1 container statuses recorded)
Sep 27 19:33:59.838: INFO: 	Container catalog-operator ready: true, restart count 0
Sep 27 19:33:59.838: INFO: olm-operator-db7cd6974-2gxjx from openshift-operator-lifecycle-manager started at 2021-09-27 17:50:29 +0000 UTC (1 container statuses recorded)
Sep 27 19:33:59.838: INFO: 	Container olm-operator ready: true, restart count 0
Sep 27 19:33:59.838: INFO: packageserver-67b4ccc984-w6sdn from openshift-operator-lifecycle-manager started at 2021-09-27 17:53:13 +0000 UTC (1 container statuses recorded)
Sep 27 19:33:59.838: INFO: 	Container packageserver ready: true, restart count 0
Sep 27 19:33:59.838: INFO: metrics-666cbf9545-t95w7 from openshift-roks-metrics started at 2021-09-27 17:50:29 +0000 UTC (1 container statuses recorded)
Sep 27 19:33:59.838: INFO: 	Container metrics ready: true, restart count 3
Sep 27 19:33:59.838: INFO: service-ca-operator-6ccc7cff9-l9wfj from openshift-service-ca-operator started at 2021-09-27 17:50:29 +0000 UTC (1 container statuses recorded)
Sep 27 19:33:59.838: INFO: 	Container service-ca-operator ready: true, restart count 1
Sep 27 19:33:59.838: INFO: sonobuoy-systemd-logs-daemon-set-a625d2c9d0384434-6w8vw from sonobuoy started at 2021-09-27 19:21:44 +0000 UTC (2 container statuses recorded)
Sep 27 19:33:59.838: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep 27 19:33:59.838: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-886a64ff-55fa-4441-9510-651d66c7cb11 95
STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled
STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 127.0.0.1 on the node which pod4 resides and expect not scheduled
STEP: removing the label kubernetes.io/e2e-886a64ff-55fa-4441-9510-651d66c7cb11 off the node 10.177.248.117
STEP: verifying the node doesn't have the label kubernetes.io/e2e-886a64ff-55fa-4441-9510-651d66c7cb11
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 19:39:08.413: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-4423" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81

• [SLOW TEST:309.150 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]","total":305,"completed":17,"skipped":287,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 19:39:08.501: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for all rs to be garbage collected
STEP: expected 0 rs, got 1 rs
STEP: expected 0 pods, got 2 pods
STEP: Gathering metrics
Sep 27 19:39:09.470: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

W0927 19:39:09.470866      22 metrics_grabber.go:98] Can't find kube-scheduler pod. Grabbing metrics from kube-scheduler is disabled.
W0927 19:39:09.470901      22 metrics_grabber.go:102] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
W0927 19:39:09.470910      22 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 19:39:09.471: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-2547" for this suite.
•{"msg":"PASSED [sig-api-machinery] Garbage collector should delete RS created by deployment when not orphaning [Conformance]","total":305,"completed":18,"skipped":335,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 19:39:09.543: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Sep 27 19:39:09.799: INFO: The status of Pod test-webserver-a3eefbff-6113-42b6-afd8-8071c8df2922 is Pending, waiting for it to be Running (with Ready = true)
Sep 27 19:39:11.819: INFO: The status of Pod test-webserver-a3eefbff-6113-42b6-afd8-8071c8df2922 is Pending, waiting for it to be Running (with Ready = true)
Sep 27 19:39:13.839: INFO: The status of Pod test-webserver-a3eefbff-6113-42b6-afd8-8071c8df2922 is Running (Ready = false)
Sep 27 19:39:15.827: INFO: The status of Pod test-webserver-a3eefbff-6113-42b6-afd8-8071c8df2922 is Running (Ready = false)
Sep 27 19:39:17.822: INFO: The status of Pod test-webserver-a3eefbff-6113-42b6-afd8-8071c8df2922 is Running (Ready = false)
Sep 27 19:39:19.818: INFO: The status of Pod test-webserver-a3eefbff-6113-42b6-afd8-8071c8df2922 is Running (Ready = false)
Sep 27 19:39:21.822: INFO: The status of Pod test-webserver-a3eefbff-6113-42b6-afd8-8071c8df2922 is Running (Ready = false)
Sep 27 19:39:23.825: INFO: The status of Pod test-webserver-a3eefbff-6113-42b6-afd8-8071c8df2922 is Running (Ready = false)
Sep 27 19:39:25.912: INFO: The status of Pod test-webserver-a3eefbff-6113-42b6-afd8-8071c8df2922 is Running (Ready = false)
Sep 27 19:39:27.822: INFO: The status of Pod test-webserver-a3eefbff-6113-42b6-afd8-8071c8df2922 is Running (Ready = false)
Sep 27 19:39:29.825: INFO: The status of Pod test-webserver-a3eefbff-6113-42b6-afd8-8071c8df2922 is Running (Ready = false)
Sep 27 19:39:31.826: INFO: The status of Pod test-webserver-a3eefbff-6113-42b6-afd8-8071c8df2922 is Running (Ready = false)
Sep 27 19:39:33.837: INFO: The status of Pod test-webserver-a3eefbff-6113-42b6-afd8-8071c8df2922 is Running (Ready = false)
Sep 27 19:39:35.824: INFO: The status of Pod test-webserver-a3eefbff-6113-42b6-afd8-8071c8df2922 is Running (Ready = true)
Sep 27 19:39:35.847: INFO: Container started at 2021-09-27 19:39:11 +0000 UTC, pod became ready at 2021-09-27 19:39:33 +0000 UTC
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 19:39:35.847: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-9395" for this suite.

• [SLOW TEST:26.359 seconds]
[k8s.io] Probing container
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Probing container with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]","total":305,"completed":19,"skipped":367,"failed":0}
SSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 19:39:35.902: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Sep 27 19:41:36.286: INFO: Deleting pod "var-expansion-a856a132-2442-43ce-b3f3-c27007a0dd2e" in namespace "var-expansion-4111"
Sep 27 19:41:36.318: INFO: Wait up to 5m0s for pod "var-expansion-a856a132-2442-43ce-b3f3-c27007a0dd2e" to be fully deleted
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 19:41:47.266: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-4111" for this suite.

• [SLOW TEST:131.587 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]","total":305,"completed":20,"skipped":378,"failed":0}
SSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 19:41:47.490: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Service
STEP: Ensuring resource quota status captures service creation
STEP: Deleting a Service
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 19:41:58.895: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-4467" for this suite.

• [SLOW TEST:11.486 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a service. [Conformance]","total":305,"completed":21,"skipped":385,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a volume subpath [sig-storage] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 19:41:58.981: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a volume subpath [sig-storage] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test substitution in volume subpath
Sep 27 19:41:59.224: INFO: Waiting up to 5m0s for pod "var-expansion-d39c828c-019e-4876-9010-e7195ec80426" in namespace "var-expansion-5683" to be "Succeeded or Failed"
Sep 27 19:41:59.246: INFO: Pod "var-expansion-d39c828c-019e-4876-9010-e7195ec80426": Phase="Pending", Reason="", readiness=false. Elapsed: 22.030854ms
Sep 27 19:42:01.266: INFO: Pod "var-expansion-d39c828c-019e-4876-9010-e7195ec80426": Phase="Pending", Reason="", readiness=false. Elapsed: 2.042300206s
Sep 27 19:42:03.291: INFO: Pod "var-expansion-d39c828c-019e-4876-9010-e7195ec80426": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.066903139s
STEP: Saw pod success
Sep 27 19:42:03.291: INFO: Pod "var-expansion-d39c828c-019e-4876-9010-e7195ec80426" satisfied condition "Succeeded or Failed"
Sep 27 19:42:03.307: INFO: Trying to get logs from node 10.177.248.117 pod var-expansion-d39c828c-019e-4876-9010-e7195ec80426 container dapi-container: <nil>
STEP: delete the pod
Sep 27 19:42:03.851: INFO: Waiting for pod var-expansion-d39c828c-019e-4876-9010-e7195ec80426 to disappear
Sep 27 19:42:03.868: INFO: Pod var-expansion-d39c828c-019e-4876-9010-e7195ec80426 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 19:42:03.868: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-5683" for this suite.
•{"msg":"PASSED [k8s.io] Variable Expansion should allow substituting values in a volume subpath [sig-storage] [Conformance]","total":305,"completed":22,"skipped":440,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny pod and configmap creation [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 19:42:03.942: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Sep 27 19:42:04.871: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Sep 27 19:42:06.924: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63768368524, loc:(*time.Location)(0x7702840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63768368524, loc:(*time.Location)(0x7702840)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63768368524, loc:(*time.Location)(0x7702840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63768368524, loc:(*time.Location)(0x7702840)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep 27 19:42:09.984: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny pod and configmap creation [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod that should be denied by the webhook
STEP: create a pod that causes the webhook to hang
STEP: create a configmap that should be denied by the webhook
STEP: create a configmap that should be admitted by the webhook
STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: create a namespace that bypass the webhook
STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 19:42:20.710: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4553" for this suite.
STEP: Destroying namespace "webhook-4553-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:17.050 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny pod and configmap creation [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny pod and configmap creation [Conformance]","total":305,"completed":23,"skipped":465,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Subdomain [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 19:42:20.993: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for pods for Subdomain [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-7171.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-7171.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-7171.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7171.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-7171.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-7171.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-7171.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-7171.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-7171.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-7171.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-7171.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-7171.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-7171.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-7171.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-7171.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-7171.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-7171.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-7171.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Sep 27 19:42:31.341: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-7171.svc.cluster.local from pod dns-7171/dns-test-6451a105-906e-4085-98cb-be5fbf895f39: the server could not find the requested resource (get pods dns-test-6451a105-906e-4085-98cb-be5fbf895f39)
Sep 27 19:42:31.376: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7171.svc.cluster.local from pod dns-7171/dns-test-6451a105-906e-4085-98cb-be5fbf895f39: the server could not find the requested resource (get pods dns-test-6451a105-906e-4085-98cb-be5fbf895f39)
Sep 27 19:42:31.419: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-7171.svc.cluster.local from pod dns-7171/dns-test-6451a105-906e-4085-98cb-be5fbf895f39: the server could not find the requested resource (get pods dns-test-6451a105-906e-4085-98cb-be5fbf895f39)
Sep 27 19:42:31.527: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-7171.svc.cluster.local from pod dns-7171/dns-test-6451a105-906e-4085-98cb-be5fbf895f39: the server could not find the requested resource (get pods dns-test-6451a105-906e-4085-98cb-be5fbf895f39)
Sep 27 19:42:31.553: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-7171.svc.cluster.local from pod dns-7171/dns-test-6451a105-906e-4085-98cb-be5fbf895f39: the server could not find the requested resource (get pods dns-test-6451a105-906e-4085-98cb-be5fbf895f39)
Sep 27 19:42:31.583: INFO: Unable to read jessie_udp@dns-test-service-2.dns-7171.svc.cluster.local from pod dns-7171/dns-test-6451a105-906e-4085-98cb-be5fbf895f39: the server could not find the requested resource (get pods dns-test-6451a105-906e-4085-98cb-be5fbf895f39)
Sep 27 19:42:31.675: INFO: Lookups using dns-7171/dns-test-6451a105-906e-4085-98cb-be5fbf895f39 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-7171.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7171.svc.cluster.local wheezy_udp@dns-test-service-2.dns-7171.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-7171.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-7171.svc.cluster.local jessie_udp@dns-test-service-2.dns-7171.svc.cluster.local]

Sep 27 19:42:37.023: INFO: DNS probes using dns-7171/dns-test-6451a105-906e-4085-98cb-be5fbf895f39 succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 19:42:37.130: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-7171" for this suite.

• [SLOW TEST:16.201 seconds]
[sig-network] DNS
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for pods for Subdomain [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Subdomain [Conformance]","total":305,"completed":24,"skipped":485,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should run through a ConfigMap lifecycle [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 19:42:37.196: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run through a ConfigMap lifecycle [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a ConfigMap
STEP: fetching the ConfigMap
STEP: patching the ConfigMap
STEP: listing all ConfigMaps in all namespaces with a label selector
STEP: deleting the ConfigMap by collection with a label selector
STEP: listing all ConfigMaps in test namespace
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 19:42:37.559: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2843" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should run through a ConfigMap lifecycle [Conformance]","total":305,"completed":25,"skipped":508,"failed":0}
S
------------------------------
[sig-cli] Kubectl client Kubectl logs 
  should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 19:42:37.636: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[BeforeEach] Kubectl logs
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1415
STEP: creating an pod
Sep 27 19:42:37.774: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-679324930 run logs-generator --image=k8s.gcr.io/e2e-test-images/agnhost:2.20 --namespace=kubectl-9914 --restart=Never -- logs-generator --log-lines-total 100 --run-duration 20s'
Sep 27 19:42:37.985: INFO: stderr: ""
Sep 27 19:42:37.985: INFO: stdout: "pod/logs-generator created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Waiting for log generator to start.
Sep 27 19:42:37.985: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
Sep 27 19:42:37.985: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-9914" to be "running and ready, or succeeded"
Sep 27 19:42:38.001: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 16.073651ms
Sep 27 19:42:40.020: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.03506386s
Sep 27 19:42:42.043: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 4.058295764s
Sep 27 19:42:42.043: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
Sep 27 19:42:42.043: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
STEP: checking for a matching strings
Sep 27 19:42:42.043: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-679324930 logs logs-generator logs-generator --namespace=kubectl-9914'
Sep 27 19:42:42.335: INFO: stderr: ""
Sep 27 19:42:42.335: INFO: stdout: "I0927 19:42:39.644285       1 logs_generator.go:76] 0 GET /api/v1/namespaces/ns/pods/t5g 352\nI0927 19:42:39.843736       1 logs_generator.go:76] 1 PUT /api/v1/namespaces/ns/pods/c6d 448\nI0927 19:42:40.043714       1 logs_generator.go:76] 2 POST /api/v1/namespaces/ns/pods/tn9 330\nI0927 19:42:40.243763       1 logs_generator.go:76] 3 PUT /api/v1/namespaces/kube-system/pods/fsz9 509\nI0927 19:42:40.443772       1 logs_generator.go:76] 4 PUT /api/v1/namespaces/ns/pods/dfhl 234\nI0927 19:42:40.643790       1 logs_generator.go:76] 5 POST /api/v1/namespaces/ns/pods/ljmw 419\nI0927 19:42:40.843755       1 logs_generator.go:76] 6 PUT /api/v1/namespaces/ns/pods/4jzf 365\nI0927 19:42:41.043681       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/ns/pods/m87 357\nI0927 19:42:41.243719       1 logs_generator.go:76] 8 GET /api/v1/namespaces/default/pods/2b8 404\nI0927 19:42:41.443730       1 logs_generator.go:76] 9 POST /api/v1/namespaces/ns/pods/r9gk 395\nI0927 19:42:41.643734       1 logs_generator.go:76] 10 GET /api/v1/namespaces/default/pods/55dd 482\nI0927 19:42:41.843728       1 logs_generator.go:76] 11 POST /api/v1/namespaces/ns/pods/mfn 277\nI0927 19:42:42.043722       1 logs_generator.go:76] 12 POST /api/v1/namespaces/default/pods/kt4q 396\nI0927 19:42:42.243750       1 logs_generator.go:76] 13 POST /api/v1/namespaces/ns/pods/fnpb 336\n"
STEP: limiting log lines
Sep 27 19:42:42.335: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-679324930 logs logs-generator logs-generator --namespace=kubectl-9914 --tail=1'
Sep 27 19:42:42.552: INFO: stderr: ""
Sep 27 19:42:42.552: INFO: stdout: "I0927 19:42:42.449002       1 logs_generator.go:76] 14 PUT /api/v1/namespaces/kube-system/pods/dxj 369\n"
Sep 27 19:42:42.552: INFO: got output "I0927 19:42:42.449002       1 logs_generator.go:76] 14 PUT /api/v1/namespaces/kube-system/pods/dxj 369\n"
STEP: limiting log bytes
Sep 27 19:42:42.552: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-679324930 logs logs-generator logs-generator --namespace=kubectl-9914 --limit-bytes=1'
Sep 27 19:42:42.753: INFO: stderr: ""
Sep 27 19:42:42.753: INFO: stdout: "I"
Sep 27 19:42:42.753: INFO: got output "I"
STEP: exposing timestamps
Sep 27 19:42:42.753: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-679324930 logs logs-generator logs-generator --namespace=kubectl-9914 --tail=1 --timestamps'
Sep 27 19:42:42.953: INFO: stderr: ""
Sep 27 19:42:42.953: INFO: stdout: "2021-09-27T14:42:42.843856430-05:00 I0927 19:42:42.843741       1 logs_generator.go:76] 16 PUT /api/v1/namespaces/kube-system/pods/z7wq 223\n"
Sep 27 19:42:42.953: INFO: got output "2021-09-27T14:42:42.843856430-05:00 I0927 19:42:42.843741       1 logs_generator.go:76] 16 PUT /api/v1/namespaces/kube-system/pods/z7wq 223\n"
STEP: restricting to a time range
Sep 27 19:42:45.454: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-679324930 logs logs-generator logs-generator --namespace=kubectl-9914 --since=1s'
Sep 27 19:42:45.635: INFO: stderr: ""
Sep 27 19:42:45.635: INFO: stdout: "I0927 19:42:44.643657       1 logs_generator.go:76] 25 GET /api/v1/namespaces/kube-system/pods/md88 443\nI0927 19:42:44.843741       1 logs_generator.go:76] 26 PUT /api/v1/namespaces/ns/pods/6vbq 343\nI0927 19:42:45.043732       1 logs_generator.go:76] 27 PUT /api/v1/namespaces/default/pods/wtcm 245\nI0927 19:42:45.243737       1 logs_generator.go:76] 28 POST /api/v1/namespaces/kube-system/pods/8qj 233\nI0927 19:42:45.443768       1 logs_generator.go:76] 29 POST /api/v1/namespaces/ns/pods/t698 387\n"
Sep 27 19:42:45.635: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-679324930 logs logs-generator logs-generator --namespace=kubectl-9914 --since=24h'
Sep 27 19:42:45.878: INFO: stderr: ""
Sep 27 19:42:45.878: INFO: stdout: "I0927 19:42:39.644285       1 logs_generator.go:76] 0 GET /api/v1/namespaces/ns/pods/t5g 352\nI0927 19:42:39.843736       1 logs_generator.go:76] 1 PUT /api/v1/namespaces/ns/pods/c6d 448\nI0927 19:42:40.043714       1 logs_generator.go:76] 2 POST /api/v1/namespaces/ns/pods/tn9 330\nI0927 19:42:40.243763       1 logs_generator.go:76] 3 PUT /api/v1/namespaces/kube-system/pods/fsz9 509\nI0927 19:42:40.443772       1 logs_generator.go:76] 4 PUT /api/v1/namespaces/ns/pods/dfhl 234\nI0927 19:42:40.643790       1 logs_generator.go:76] 5 POST /api/v1/namespaces/ns/pods/ljmw 419\nI0927 19:42:40.843755       1 logs_generator.go:76] 6 PUT /api/v1/namespaces/ns/pods/4jzf 365\nI0927 19:42:41.043681       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/ns/pods/m87 357\nI0927 19:42:41.243719       1 logs_generator.go:76] 8 GET /api/v1/namespaces/default/pods/2b8 404\nI0927 19:42:41.443730       1 logs_generator.go:76] 9 POST /api/v1/namespaces/ns/pods/r9gk 395\nI0927 19:42:41.643734       1 logs_generator.go:76] 10 GET /api/v1/namespaces/default/pods/55dd 482\nI0927 19:42:41.843728       1 logs_generator.go:76] 11 POST /api/v1/namespaces/ns/pods/mfn 277\nI0927 19:42:42.043722       1 logs_generator.go:76] 12 POST /api/v1/namespaces/default/pods/kt4q 396\nI0927 19:42:42.243750       1 logs_generator.go:76] 13 POST /api/v1/namespaces/ns/pods/fnpb 336\nI0927 19:42:42.449002       1 logs_generator.go:76] 14 PUT /api/v1/namespaces/kube-system/pods/dxj 369\nI0927 19:42:42.643710       1 logs_generator.go:76] 15 PUT /api/v1/namespaces/default/pods/7sg 509\nI0927 19:42:42.843741       1 logs_generator.go:76] 16 PUT /api/v1/namespaces/kube-system/pods/z7wq 223\nI0927 19:42:43.043735       1 logs_generator.go:76] 17 GET /api/v1/namespaces/default/pods/dvr2 464\nI0927 19:42:43.243739       1 logs_generator.go:76] 18 POST /api/v1/namespaces/default/pods/6n2 523\nI0927 19:42:43.443722       1 logs_generator.go:76] 19 GET /api/v1/namespaces/default/pods/5nlq 452\nI0927 19:42:43.643708       1 logs_generator.go:76] 20 GET /api/v1/namespaces/kube-system/pods/rbl 373\nI0927 19:42:43.843692       1 logs_generator.go:76] 21 GET /api/v1/namespaces/ns/pods/rhp 304\nI0927 19:42:44.043721       1 logs_generator.go:76] 22 POST /api/v1/namespaces/kube-system/pods/q2t 316\nI0927 19:42:44.243736       1 logs_generator.go:76] 23 POST /api/v1/namespaces/default/pods/z5sw 237\nI0927 19:42:44.443713       1 logs_generator.go:76] 24 GET /api/v1/namespaces/ns/pods/zc28 402\nI0927 19:42:44.643657       1 logs_generator.go:76] 25 GET /api/v1/namespaces/kube-system/pods/md88 443\nI0927 19:42:44.843741       1 logs_generator.go:76] 26 PUT /api/v1/namespaces/ns/pods/6vbq 343\nI0927 19:42:45.043732       1 logs_generator.go:76] 27 PUT /api/v1/namespaces/default/pods/wtcm 245\nI0927 19:42:45.243737       1 logs_generator.go:76] 28 POST /api/v1/namespaces/kube-system/pods/8qj 233\nI0927 19:42:45.443768       1 logs_generator.go:76] 29 POST /api/v1/namespaces/ns/pods/t698 387\nI0927 19:42:45.643705       1 logs_generator.go:76] 30 GET /api/v1/namespaces/ns/pods/77gc 397\nI0927 19:42:45.843760       1 logs_generator.go:76] 31 PUT /api/v1/namespaces/default/pods/qflg 537\n"
[AfterEach] Kubectl logs
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1421
Sep 27 19:42:45.879: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-679324930 delete pod logs-generator --namespace=kubectl-9914'
Sep 27 19:42:51.809: INFO: stderr: ""
Sep 27 19:42:51.809: INFO: stdout: "pod \"logs-generator\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 19:42:51.809: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9914" for this suite.

• [SLOW TEST:14.251 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl logs
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1411
    should be able to retrieve and filter logs  [Conformance]
    /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl logs should be able to retrieve and filter logs  [Conformance]","total":305,"completed":26,"skipped":509,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 19:42:51.887: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:181
[It] should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Sep 27 19:42:58.764: INFO: Successfully updated pod "pod-update-94d63895-26f7-4e75-a0bd-d94e213fb106"
STEP: verifying the updated pod is in kubernetes
Sep 27 19:42:58.798: INFO: Pod update OK
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 19:42:58.798: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-2861" for this suite.

• [SLOW TEST:6.978 seconds]
[k8s.io] Pods
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Pods should be updated [NodeConformance] [Conformance]","total":305,"completed":27,"skipped":523,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  updates the published spec when one version gets renamed [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 19:42:58.866: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates the published spec when one version gets renamed [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: set up a multi version CRD
Sep 27 19:42:59.035: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: rename a version
STEP: check the new version name is served
STEP: check the old version name is removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 19:43:49.061: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-256" for this suite.

• [SLOW TEST:50.242 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  updates the published spec when one version gets renamed [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] updates the published spec when one version gets renamed [Conformance]","total":305,"completed":28,"skipped":531,"failed":0}
SSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test when starting a container that exits 
  should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 19:43:49.109: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpa': should get the expected 'State'
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpof': should get the expected 'State'
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpn': should get the expected 'State'
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance]
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 19:44:15.497: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-5672" for this suite.

• [SLOW TEST:26.427 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  blackbox test
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:41
    when starting a container that exits
    /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:42
      should run with the expected status [NodeConformance] [Conformance]
      /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Container Runtime blackbox test when starting a container that exits should run with the expected status [NodeConformance] [Conformance]","total":305,"completed":29,"skipped":539,"failed":0}
SSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl patch 
  should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 19:44:15.536: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[It] should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating Agnhost RC
Sep 27 19:44:15.728: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-679324930 create -f - --namespace=kubectl-861'
Sep 27 19:44:16.652: INFO: stderr: ""
Sep 27 19:44:16.652: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
Sep 27 19:44:17.673: INFO: Selector matched 1 pods for map[app:agnhost]
Sep 27 19:44:17.673: INFO: Found 0 / 1
Sep 27 19:44:18.672: INFO: Selector matched 1 pods for map[app:agnhost]
Sep 27 19:44:18.672: INFO: Found 1 / 1
Sep 27 19:44:18.672: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods
Sep 27 19:44:18.693: INFO: Selector matched 1 pods for map[app:agnhost]
Sep 27 19:44:18.693: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Sep 27 19:44:18.693: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-679324930 patch pod agnhost-primary-msrvt --namespace=kubectl-861 -p {"metadata":{"annotations":{"x":"y"}}}'
Sep 27 19:44:18.914: INFO: stderr: ""
Sep 27 19:44:18.914: INFO: stdout: "pod/agnhost-primary-msrvt patched\n"
STEP: checking annotations
Sep 27 19:44:18.935: INFO: Selector matched 1 pods for map[app:agnhost]
Sep 27 19:44:18.936: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 19:44:18.936: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-861" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl patch should add annotations for pods in rc  [Conformance]","total":305,"completed":30,"skipped":548,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 19:44:18.980: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Sep 27 19:44:27.502: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Sep 27 19:44:27.521: INFO: Pod pod-with-poststart-http-hook still exists
Sep 27 19:44:29.521: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Sep 27 19:44:29.543: INFO: Pod pod-with-poststart-http-hook still exists
Sep 27 19:44:31.521: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Sep 27 19:44:31.543: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 19:44:31.543: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-7654" for this suite.

• [SLOW TEST:12.604 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  when create a pod with lifecycle hook
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute poststart http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart http hook properly [NodeConformance] [Conformance]","total":305,"completed":31,"skipped":574,"failed":0}
SSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl expose 
  should create services for rc  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 19:44:31.585: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[It] should create services for rc  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating Agnhost RC
Sep 27 19:44:31.757: INFO: namespace kubectl-8997
Sep 27 19:44:31.758: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-679324930 create -f - --namespace=kubectl-8997'
Sep 27 19:44:32.309: INFO: stderr: ""
Sep 27 19:44:32.309: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
Sep 27 19:44:33.328: INFO: Selector matched 1 pods for map[app:agnhost]
Sep 27 19:44:33.328: INFO: Found 0 / 1
Sep 27 19:44:34.329: INFO: Selector matched 1 pods for map[app:agnhost]
Sep 27 19:44:34.329: INFO: Found 0 / 1
Sep 27 19:44:35.335: INFO: Selector matched 1 pods for map[app:agnhost]
Sep 27 19:44:35.335: INFO: Found 1 / 1
Sep 27 19:44:35.335: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Sep 27 19:44:35.359: INFO: Selector matched 1 pods for map[app:agnhost]
Sep 27 19:44:35.359: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Sep 27 19:44:35.359: INFO: wait on agnhost-primary startup in kubectl-8997 
Sep 27 19:44:35.359: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-679324930 logs agnhost-primary-c7csn agnhost-primary --namespace=kubectl-8997'
Sep 27 19:44:35.673: INFO: stderr: ""
Sep 27 19:44:35.673: INFO: stdout: "Paused\n"
STEP: exposing RC
Sep 27 19:44:35.673: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-679324930 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379 --namespace=kubectl-8997'
Sep 27 19:44:35.896: INFO: stderr: ""
Sep 27 19:44:35.896: INFO: stdout: "service/rm2 exposed\n"
Sep 27 19:44:35.920: INFO: Service rm2 in namespace kubectl-8997 found.
STEP: exposing service
Sep 27 19:44:37.950: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-679324930 expose service rm2 --name=rm3 --port=2345 --target-port=6379 --namespace=kubectl-8997'
Sep 27 19:44:38.173: INFO: stderr: ""
Sep 27 19:44:38.173: INFO: stdout: "service/rm3 exposed\n"
Sep 27 19:44:38.186: INFO: Service rm3 in namespace kubectl-8997 found.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 19:44:40.221: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8997" for this suite.

• [SLOW TEST:8.678 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl expose
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1246
    should create services for rc  [Conformance]
    /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl expose should create services for rc  [Conformance]","total":305,"completed":32,"skipped":580,"failed":0}
[sig-network] DNS 
  should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 19:44:40.263: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a test externalName service
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-5514.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-5514.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-5514.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-5514.svc.cluster.local; sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Sep 27 19:44:44.779: INFO: DNS probes using dns-test-af16ab4b-510d-4a4f-b50d-493734170e77 succeeded

STEP: deleting the pod
STEP: changing the externalName to bar.example.com
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-5514.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-5514.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-5514.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-5514.svc.cluster.local; sleep 1; done

STEP: creating a second pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Sep 27 19:44:49.042: INFO: DNS probes using dns-test-b0cce4f2-2a0d-480f-888b-4ef11204e56b succeeded

STEP: deleting the pod
STEP: changing the service to type=ClusterIP
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-5514.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-5514.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-5514.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-5514.svc.cluster.local; sleep 1; done

STEP: creating a third pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Sep 27 19:44:53.654: INFO: DNS probes using dns-test-aebb7bd0-524c-49ca-a4f0-ababd16fa8a4 succeeded

STEP: deleting the pod
STEP: deleting the test externalName service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 19:44:53.785: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-5514" for this suite.

• [SLOW TEST:13.567 seconds]
[sig-network] DNS
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for ExternalName services [Conformance]","total":305,"completed":33,"skipped":580,"failed":0}
S
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 19:44:53.830: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: set up a multi version CRD
Sep 27 19:44:54.024: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: mark a version not serverd
STEP: check the unserved version gets removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 19:45:44.454: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-1499" for this suite.

• [SLOW TEST:50.698 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] removes definition from spec when one version gets changed to not be served [Conformance]","total":305,"completed":34,"skipped":581,"failed":0}
SSSSS
------------------------------
[sig-node] Downward API 
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 19:45:44.533: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward api env vars
Sep 27 19:45:44.778: INFO: Waiting up to 5m0s for pod "downward-api-12ed7a6d-5e41-4979-bbc8-a577f0796b9c" in namespace "downward-api-6615" to be "Succeeded or Failed"
Sep 27 19:45:44.829: INFO: Pod "downward-api-12ed7a6d-5e41-4979-bbc8-a577f0796b9c": Phase="Pending", Reason="", readiness=false. Elapsed: 51.819197ms
Sep 27 19:45:46.847: INFO: Pod "downward-api-12ed7a6d-5e41-4979-bbc8-a577f0796b9c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.069498755s
Sep 27 19:45:48.861: INFO: Pod "downward-api-12ed7a6d-5e41-4979-bbc8-a577f0796b9c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.083104151s
STEP: Saw pod success
Sep 27 19:45:48.861: INFO: Pod "downward-api-12ed7a6d-5e41-4979-bbc8-a577f0796b9c" satisfied condition "Succeeded or Failed"
Sep 27 19:45:48.875: INFO: Trying to get logs from node 10.177.248.117 pod downward-api-12ed7a6d-5e41-4979-bbc8-a577f0796b9c container dapi-container: <nil>
STEP: delete the pod
Sep 27 19:45:48.953: INFO: Waiting for pod downward-api-12ed7a6d-5e41-4979-bbc8-a577f0796b9c to disappear
Sep 27 19:45:48.967: INFO: Pod downward-api-12ed7a6d-5e41-4979-bbc8-a577f0796b9c no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 19:45:48.967: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6615" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide pod UID as env vars [NodeConformance] [Conformance]","total":305,"completed":35,"skipped":586,"failed":0}
S
------------------------------
[k8s.io] Security Context When creating a container with runAsUser 
  should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 19:45:49.005: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:41
[It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Sep 27 19:45:49.180: INFO: Waiting up to 5m0s for pod "busybox-user-65534-bc594d40-b759-43ab-a5bd-9187d1e03d4c" in namespace "security-context-test-559" to be "Succeeded or Failed"
Sep 27 19:45:49.195: INFO: Pod "busybox-user-65534-bc594d40-b759-43ab-a5bd-9187d1e03d4c": Phase="Pending", Reason="", readiness=false. Elapsed: 14.574611ms
Sep 27 19:45:51.246: INFO: Pod "busybox-user-65534-bc594d40-b759-43ab-a5bd-9187d1e03d4c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.065677031s
Sep 27 19:45:51.246: INFO: Pod "busybox-user-65534-bc594d40-b759-43ab-a5bd-9187d1e03d4c" satisfied condition "Succeeded or Failed"
[AfterEach] [k8s.io] Security Context
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 19:45:51.246: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-559" for this suite.
•{"msg":"PASSED [k8s.io] Security Context When creating a container with runAsUser should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":36,"skipped":587,"failed":0}
SSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 19:45:51.418: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0644 on node default medium
Sep 27 19:45:51.713: INFO: Waiting up to 5m0s for pod "pod-ffeb007f-c10d-4bac-ba5e-5247da51a0d8" in namespace "emptydir-4245" to be "Succeeded or Failed"
Sep 27 19:45:51.731: INFO: Pod "pod-ffeb007f-c10d-4bac-ba5e-5247da51a0d8": Phase="Pending", Reason="", readiness=false. Elapsed: 18.365289ms
Sep 27 19:45:53.812: INFO: Pod "pod-ffeb007f-c10d-4bac-ba5e-5247da51a0d8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.098965999s
Sep 27 19:45:55.836: INFO: Pod "pod-ffeb007f-c10d-4bac-ba5e-5247da51a0d8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.122847199s
STEP: Saw pod success
Sep 27 19:45:55.836: INFO: Pod "pod-ffeb007f-c10d-4bac-ba5e-5247da51a0d8" satisfied condition "Succeeded or Failed"
Sep 27 19:45:55.900: INFO: Trying to get logs from node 10.177.248.117 pod pod-ffeb007f-c10d-4bac-ba5e-5247da51a0d8 container test-container: <nil>
STEP: delete the pod
Sep 27 19:45:56.079: INFO: Waiting for pod pod-ffeb007f-c10d-4bac-ba5e-5247da51a0d8 to disappear
Sep 27 19:45:56.110: INFO: Pod pod-ffeb007f-c10d-4bac-ba5e-5247da51a0d8 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 19:45:56.110: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4245" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":37,"skipped":594,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 19:45:56.217: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Sep 27 19:45:56.459: INFO: Waiting up to 5m0s for pod "downwardapi-volume-75fd8ef9-a5fd-4960-b77f-8e19c57cee4b" in namespace "downward-api-6599" to be "Succeeded or Failed"
Sep 27 19:45:56.475: INFO: Pod "downwardapi-volume-75fd8ef9-a5fd-4960-b77f-8e19c57cee4b": Phase="Pending", Reason="", readiness=false. Elapsed: 15.460305ms
Sep 27 19:45:58.490: INFO: Pod "downwardapi-volume-75fd8ef9-a5fd-4960-b77f-8e19c57cee4b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030143828s
Sep 27 19:46:00.506: INFO: Pod "downwardapi-volume-75fd8ef9-a5fd-4960-b77f-8e19c57cee4b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.046030359s
STEP: Saw pod success
Sep 27 19:46:00.506: INFO: Pod "downwardapi-volume-75fd8ef9-a5fd-4960-b77f-8e19c57cee4b" satisfied condition "Succeeded or Failed"
Sep 27 19:46:00.522: INFO: Trying to get logs from node 10.177.248.117 pod downwardapi-volume-75fd8ef9-a5fd-4960-b77f-8e19c57cee4b container client-container: <nil>
STEP: delete the pod
Sep 27 19:46:00.621: INFO: Waiting for pod downwardapi-volume-75fd8ef9-a5fd-4960-b77f-8e19c57cee4b to disappear
Sep 27 19:46:00.635: INFO: Pod downwardapi-volume-75fd8ef9-a5fd-4960-b77f-8e19c57cee4b no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 19:46:00.635: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6599" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu request [NodeConformance] [Conformance]","total":305,"completed":38,"skipped":639,"failed":0}
SSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: http [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 19:46:00.674: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: http [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Performing setup for networking test in namespace pod-network-test-5016
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Sep 27 19:46:00.814: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Sep 27 19:46:00.995: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Sep 27 19:46:03.017: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Sep 27 19:46:05.009: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep 27 19:46:07.016: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep 27 19:46:09.009: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep 27 19:46:11.023: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep 27 19:46:13.033: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep 27 19:46:15.010: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep 27 19:46:17.011: INFO: The status of Pod netserver-0 is Running (Ready = true)
Sep 27 19:46:17.041: INFO: The status of Pod netserver-1 is Running (Ready = false)
Sep 27 19:46:19.058: INFO: The status of Pod netserver-1 is Running (Ready = false)
Sep 27 19:46:21.055: INFO: The status of Pod netserver-1 is Running (Ready = false)
Sep 27 19:46:23.055: INFO: The status of Pod netserver-1 is Running (Ready = true)
Sep 27 19:46:23.102: INFO: The status of Pod netserver-2 is Running (Ready = true)
STEP: Creating test pods
Sep 27 19:46:25.226: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.75.23:8080/dial?request=hostname&protocol=http&host=172.30.137.113&port=8080&tries=1'] Namespace:pod-network-test-5016 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep 27 19:46:25.226: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
Sep 27 19:46:25.513: INFO: Waiting for responses: map[]
Sep 27 19:46:25.527: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.75.23:8080/dial?request=hostname&protocol=http&host=172.30.75.22&port=8080&tries=1'] Namespace:pod-network-test-5016 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep 27 19:46:25.527: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
Sep 27 19:46:25.864: INFO: Waiting for responses: map[]
Sep 27 19:46:25.939: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.75.23:8080/dial?request=hostname&protocol=http&host=172.30.85.159&port=8080&tries=1'] Namespace:pod-network-test-5016 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep 27 19:46:25.985: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
Sep 27 19:46:26.330: INFO: Waiting for responses: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 19:46:26.330: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-5016" for this suite.

• [SLOW TEST:25.698 seconds]
[sig-network] Networking
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:26
  Granular Checks: Pods
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:29
    should function for intra-pod communication: http [NodeConformance] [Conformance]
    /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: http [NodeConformance] [Conformance]","total":305,"completed":39,"skipped":646,"failed":0}
SSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] version v1
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 19:46:26.374: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: starting an echo server on multiple ports
STEP: creating replication controller proxy-service-lwsvq in namespace proxy-8758
I0927 19:46:26.598426      22 runners.go:190] Created replication controller with name: proxy-service-lwsvq, namespace: proxy-8758, replica count: 1
I0927 19:46:27.648960      22 runners.go:190] proxy-service-lwsvq Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0927 19:46:28.649344      22 runners.go:190] proxy-service-lwsvq Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0927 19:46:29.649607      22 runners.go:190] proxy-service-lwsvq Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0927 19:46:30.651660      22 runners.go:190] proxy-service-lwsvq Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0927 19:46:31.652002      22 runners.go:190] proxy-service-lwsvq Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Sep 27 19:46:31.667: INFO: setup took 5.123192101s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts
Sep 27 19:46:31.710: INFO: (0) /api/v1/namespaces/proxy-8758/pods/http:proxy-service-lwsvq-zr9jg:160/proxy/: foo (200; 39.301096ms)
Sep 27 19:46:31.718: INFO: (0) /api/v1/namespaces/proxy-8758/pods/proxy-service-lwsvq-zr9jg/proxy/: <a href="/api/v1/namespaces/proxy-8758/pods/proxy-service-lwsvq-zr9jg/proxy/rewriteme">test</a> (200; 50.640603ms)
Sep 27 19:46:31.718: INFO: (0) /api/v1/namespaces/proxy-8758/pods/proxy-service-lwsvq-zr9jg:1080/proxy/: <a href="/api/v1/namespaces/proxy-8758/pods/proxy-service-lwsvq-zr9jg:1080/proxy/rewriteme">test<... (200; 43.783315ms)
Sep 27 19:46:31.719: INFO: (0) /api/v1/namespaces/proxy-8758/pods/proxy-service-lwsvq-zr9jg:162/proxy/: bar (200; 41.893003ms)
Sep 27 19:46:31.719: INFO: (0) /api/v1/namespaces/proxy-8758/pods/http:proxy-service-lwsvq-zr9jg:162/proxy/: bar (200; 48.711568ms)
Sep 27 19:46:31.723: INFO: (0) /api/v1/namespaces/proxy-8758/pods/proxy-service-lwsvq-zr9jg:160/proxy/: foo (200; 47.495575ms)
Sep 27 19:46:31.723: INFO: (0) /api/v1/namespaces/proxy-8758/pods/http:proxy-service-lwsvq-zr9jg:1080/proxy/: <a href="/api/v1/namespaces/proxy-8758/pods/http:proxy-service-lwsvq-zr9jg:1080/proxy/rewriteme">... (200; 48.255158ms)
Sep 27 19:46:31.724: INFO: (0) /api/v1/namespaces/proxy-8758/services/http:proxy-service-lwsvq:portname1/proxy/: foo (200; 50.422505ms)
Sep 27 19:46:31.735: INFO: (0) /api/v1/namespaces/proxy-8758/services/proxy-service-lwsvq:portname1/proxy/: foo (200; 58.487085ms)
Sep 27 19:46:31.738: INFO: (0) /api/v1/namespaces/proxy-8758/services/http:proxy-service-lwsvq:portname2/proxy/: bar (200; 68.296705ms)
Sep 27 19:46:31.738: INFO: (0) /api/v1/namespaces/proxy-8758/services/proxy-service-lwsvq:portname2/proxy/: bar (200; 64.419269ms)
Sep 27 19:46:31.738: INFO: (0) /api/v1/namespaces/proxy-8758/services/https:proxy-service-lwsvq:tlsportname2/proxy/: tls qux (200; 68.187656ms)
Sep 27 19:46:31.745: INFO: (0) /api/v1/namespaces/proxy-8758/pods/https:proxy-service-lwsvq-zr9jg:462/proxy/: tls qux (200; 74.01611ms)
Sep 27 19:46:31.745: INFO: (0) /api/v1/namespaces/proxy-8758/pods/https:proxy-service-lwsvq-zr9jg:460/proxy/: tls baz (200; 75.954223ms)
Sep 27 19:46:31.745: INFO: (0) /api/v1/namespaces/proxy-8758/services/https:proxy-service-lwsvq:tlsportname1/proxy/: tls baz (200; 75.616875ms)
Sep 27 19:46:31.745: INFO: (0) /api/v1/namespaces/proxy-8758/pods/https:proxy-service-lwsvq-zr9jg:443/proxy/: <a href="/api/v1/namespaces/proxy-8758/pods/https:proxy-service-lwsvq-zr9jg:443/proxy/tlsrewritem... (200; 78.243528ms)
Sep 27 19:46:31.771: INFO: (1) /api/v1/namespaces/proxy-8758/pods/http:proxy-service-lwsvq-zr9jg:160/proxy/: foo (200; 24.700181ms)
Sep 27 19:46:31.781: INFO: (1) /api/v1/namespaces/proxy-8758/pods/https:proxy-service-lwsvq-zr9jg:443/proxy/: <a href="/api/v1/namespaces/proxy-8758/pods/https:proxy-service-lwsvq-zr9jg:443/proxy/tlsrewritem... (200; 34.35904ms)
Sep 27 19:46:31.783: INFO: (1) /api/v1/namespaces/proxy-8758/pods/http:proxy-service-lwsvq-zr9jg:162/proxy/: bar (200; 37.22998ms)
Sep 27 19:46:31.784: INFO: (1) /api/v1/namespaces/proxy-8758/pods/http:proxy-service-lwsvq-zr9jg:1080/proxy/: <a href="/api/v1/namespaces/proxy-8758/pods/http:proxy-service-lwsvq-zr9jg:1080/proxy/rewriteme">... (200; 37.363246ms)
Sep 27 19:46:31.784: INFO: (1) /api/v1/namespaces/proxy-8758/pods/https:proxy-service-lwsvq-zr9jg:460/proxy/: tls baz (200; 37.02796ms)
Sep 27 19:46:31.784: INFO: (1) /api/v1/namespaces/proxy-8758/pods/proxy-service-lwsvq-zr9jg:160/proxy/: foo (200; 37.781406ms)
Sep 27 19:46:31.785: INFO: (1) /api/v1/namespaces/proxy-8758/pods/proxy-service-lwsvq-zr9jg:1080/proxy/: <a href="/api/v1/namespaces/proxy-8758/pods/proxy-service-lwsvq-zr9jg:1080/proxy/rewriteme">test<... (200; 38.338707ms)
Sep 27 19:46:31.785: INFO: (1) /api/v1/namespaces/proxy-8758/pods/proxy-service-lwsvq-zr9jg:162/proxy/: bar (200; 38.184382ms)
Sep 27 19:46:31.785: INFO: (1) /api/v1/namespaces/proxy-8758/pods/proxy-service-lwsvq-zr9jg/proxy/: <a href="/api/v1/namespaces/proxy-8758/pods/proxy-service-lwsvq-zr9jg/proxy/rewriteme">test</a> (200; 38.267838ms)
Sep 27 19:46:31.785: INFO: (1) /api/v1/namespaces/proxy-8758/pods/https:proxy-service-lwsvq-zr9jg:462/proxy/: tls qux (200; 38.814534ms)
Sep 27 19:46:31.802: INFO: (1) /api/v1/namespaces/proxy-8758/services/https:proxy-service-lwsvq:tlsportname1/proxy/: tls baz (200; 56.139422ms)
Sep 27 19:46:31.802: INFO: (1) /api/v1/namespaces/proxy-8758/services/https:proxy-service-lwsvq:tlsportname2/proxy/: tls qux (200; 55.789177ms)
Sep 27 19:46:31.802: INFO: (1) /api/v1/namespaces/proxy-8758/services/http:proxy-service-lwsvq:portname1/proxy/: foo (200; 55.341329ms)
Sep 27 19:46:31.805: INFO: (1) /api/v1/namespaces/proxy-8758/services/http:proxy-service-lwsvq:portname2/proxy/: bar (200; 58.916354ms)
Sep 27 19:46:31.806: INFO: (1) /api/v1/namespaces/proxy-8758/services/proxy-service-lwsvq:portname2/proxy/: bar (200; 59.556611ms)
Sep 27 19:46:31.807: INFO: (1) /api/v1/namespaces/proxy-8758/services/proxy-service-lwsvq:portname1/proxy/: foo (200; 59.652537ms)
Sep 27 19:46:31.832: INFO: (2) /api/v1/namespaces/proxy-8758/pods/https:proxy-service-lwsvq-zr9jg:460/proxy/: tls baz (200; 25.292328ms)
Sep 27 19:46:31.843: INFO: (2) /api/v1/namespaces/proxy-8758/pods/https:proxy-service-lwsvq-zr9jg:443/proxy/: <a href="/api/v1/namespaces/proxy-8758/pods/https:proxy-service-lwsvq-zr9jg:443/proxy/tlsrewritem... (200; 35.043819ms)
Sep 27 19:46:31.843: INFO: (2) /api/v1/namespaces/proxy-8758/pods/proxy-service-lwsvq-zr9jg:1080/proxy/: <a href="/api/v1/namespaces/proxy-8758/pods/proxy-service-lwsvq-zr9jg:1080/proxy/rewriteme">test<... (200; 35.36885ms)
Sep 27 19:46:31.843: INFO: (2) /api/v1/namespaces/proxy-8758/pods/http:proxy-service-lwsvq-zr9jg:1080/proxy/: <a href="/api/v1/namespaces/proxy-8758/pods/http:proxy-service-lwsvq-zr9jg:1080/proxy/rewriteme">... (200; 35.330677ms)
Sep 27 19:46:31.843: INFO: (2) /api/v1/namespaces/proxy-8758/pods/proxy-service-lwsvq-zr9jg/proxy/: <a href="/api/v1/namespaces/proxy-8758/pods/proxy-service-lwsvq-zr9jg/proxy/rewriteme">test</a> (200; 35.479463ms)
Sep 27 19:46:31.850: INFO: (2) /api/v1/namespaces/proxy-8758/pods/http:proxy-service-lwsvq-zr9jg:162/proxy/: bar (200; 42.547322ms)
Sep 27 19:46:31.856: INFO: (2) /api/v1/namespaces/proxy-8758/pods/http:proxy-service-lwsvq-zr9jg:160/proxy/: foo (200; 48.673136ms)
Sep 27 19:46:31.858: INFO: (2) /api/v1/namespaces/proxy-8758/pods/https:proxy-service-lwsvq-zr9jg:462/proxy/: tls qux (200; 50.220555ms)
Sep 27 19:46:31.858: INFO: (2) /api/v1/namespaces/proxy-8758/services/https:proxy-service-lwsvq:tlsportname2/proxy/: tls qux (200; 50.490459ms)
Sep 27 19:46:31.862: INFO: (2) /api/v1/namespaces/proxy-8758/pods/proxy-service-lwsvq-zr9jg:162/proxy/: bar (200; 54.397992ms)
Sep 27 19:46:31.863: INFO: (2) /api/v1/namespaces/proxy-8758/services/proxy-service-lwsvq:portname2/proxy/: bar (200; 55.677073ms)
Sep 27 19:46:31.863: INFO: (2) /api/v1/namespaces/proxy-8758/pods/proxy-service-lwsvq-zr9jg:160/proxy/: foo (200; 55.002803ms)
Sep 27 19:46:31.863: INFO: (2) /api/v1/namespaces/proxy-8758/services/https:proxy-service-lwsvq:tlsportname1/proxy/: tls baz (200; 55.938101ms)
Sep 27 19:46:31.864: INFO: (2) /api/v1/namespaces/proxy-8758/services/http:proxy-service-lwsvq:portname1/proxy/: foo (200; 56.841235ms)
Sep 27 19:46:31.868: INFO: (2) /api/v1/namespaces/proxy-8758/services/proxy-service-lwsvq:portname1/proxy/: foo (200; 60.505674ms)
Sep 27 19:46:31.868: INFO: (2) /api/v1/namespaces/proxy-8758/services/http:proxy-service-lwsvq:portname2/proxy/: bar (200; 61.155189ms)
Sep 27 19:46:31.896: INFO: (3) /api/v1/namespaces/proxy-8758/pods/proxy-service-lwsvq-zr9jg:162/proxy/: bar (200; 27.444733ms)
Sep 27 19:46:31.907: INFO: (3) /api/v1/namespaces/proxy-8758/pods/https:proxy-service-lwsvq-zr9jg:462/proxy/: tls qux (200; 37.429277ms)
Sep 27 19:46:31.907: INFO: (3) /api/v1/namespaces/proxy-8758/pods/proxy-service-lwsvq-zr9jg:1080/proxy/: <a href="/api/v1/namespaces/proxy-8758/pods/proxy-service-lwsvq-zr9jg:1080/proxy/rewriteme">test<... (200; 37.995655ms)
Sep 27 19:46:31.908: INFO: (3) /api/v1/namespaces/proxy-8758/pods/https:proxy-service-lwsvq-zr9jg:460/proxy/: tls baz (200; 39.328762ms)
Sep 27 19:46:31.909: INFO: (3) /api/v1/namespaces/proxy-8758/pods/http:proxy-service-lwsvq-zr9jg:1080/proxy/: <a href="/api/v1/namespaces/proxy-8758/pods/http:proxy-service-lwsvq-zr9jg:1080/proxy/rewriteme">... (200; 39.241267ms)
Sep 27 19:46:31.909: INFO: (3) /api/v1/namespaces/proxy-8758/pods/proxy-service-lwsvq-zr9jg:160/proxy/: foo (200; 39.20425ms)
Sep 27 19:46:31.909: INFO: (3) /api/v1/namespaces/proxy-8758/pods/proxy-service-lwsvq-zr9jg/proxy/: <a href="/api/v1/namespaces/proxy-8758/pods/proxy-service-lwsvq-zr9jg/proxy/rewriteme">test</a> (200; 39.436169ms)
Sep 27 19:46:31.909: INFO: (3) /api/v1/namespaces/proxy-8758/pods/http:proxy-service-lwsvq-zr9jg:160/proxy/: foo (200; 39.815005ms)
Sep 27 19:46:31.912: INFO: (3) /api/v1/namespaces/proxy-8758/pods/http:proxy-service-lwsvq-zr9jg:162/proxy/: bar (200; 43.329133ms)
Sep 27 19:46:31.913: INFO: (3) /api/v1/namespaces/proxy-8758/pods/https:proxy-service-lwsvq-zr9jg:443/proxy/: <a href="/api/v1/namespaces/proxy-8758/pods/https:proxy-service-lwsvq-zr9jg:443/proxy/tlsrewritem... (200; 43.994803ms)
Sep 27 19:46:31.917: INFO: (3) /api/v1/namespaces/proxy-8758/services/https:proxy-service-lwsvq:tlsportname2/proxy/: tls qux (200; 47.923327ms)
Sep 27 19:46:31.917: INFO: (3) /api/v1/namespaces/proxy-8758/services/proxy-service-lwsvq:portname1/proxy/: foo (200; 47.6392ms)
Sep 27 19:46:31.923: INFO: (3) /api/v1/namespaces/proxy-8758/services/https:proxy-service-lwsvq:tlsportname1/proxy/: tls baz (200; 53.986039ms)
Sep 27 19:46:31.923: INFO: (3) /api/v1/namespaces/proxy-8758/services/proxy-service-lwsvq:portname2/proxy/: bar (200; 53.899317ms)
Sep 27 19:46:31.926: INFO: (3) /api/v1/namespaces/proxy-8758/services/http:proxy-service-lwsvq:portname1/proxy/: foo (200; 56.800964ms)
Sep 27 19:46:31.926: INFO: (3) /api/v1/namespaces/proxy-8758/services/http:proxy-service-lwsvq:portname2/proxy/: bar (200; 56.714178ms)
Sep 27 19:46:31.962: INFO: (4) /api/v1/namespaces/proxy-8758/pods/proxy-service-lwsvq-zr9jg:160/proxy/: foo (200; 35.26013ms)
Sep 27 19:46:31.962: INFO: (4) /api/v1/namespaces/proxy-8758/pods/proxy-service-lwsvq-zr9jg/proxy/: <a href="/api/v1/namespaces/proxy-8758/pods/proxy-service-lwsvq-zr9jg/proxy/rewriteme">test</a> (200; 35.291449ms)
Sep 27 19:46:31.962: INFO: (4) /api/v1/namespaces/proxy-8758/pods/http:proxy-service-lwsvq-zr9jg:160/proxy/: foo (200; 35.41775ms)
Sep 27 19:46:31.962: INFO: (4) /api/v1/namespaces/proxy-8758/pods/http:proxy-service-lwsvq-zr9jg:162/proxy/: bar (200; 35.406273ms)
Sep 27 19:46:31.962: INFO: (4) /api/v1/namespaces/proxy-8758/pods/https:proxy-service-lwsvq-zr9jg:443/proxy/: <a href="/api/v1/namespaces/proxy-8758/pods/https:proxy-service-lwsvq-zr9jg:443/proxy/tlsrewritem... (200; 35.240313ms)
Sep 27 19:46:31.962: INFO: (4) /api/v1/namespaces/proxy-8758/pods/proxy-service-lwsvq-zr9jg:1080/proxy/: <a href="/api/v1/namespaces/proxy-8758/pods/proxy-service-lwsvq-zr9jg:1080/proxy/rewriteme">test<... (200; 35.820593ms)
Sep 27 19:46:31.962: INFO: (4) /api/v1/namespaces/proxy-8758/pods/http:proxy-service-lwsvq-zr9jg:1080/proxy/: <a href="/api/v1/namespaces/proxy-8758/pods/http:proxy-service-lwsvq-zr9jg:1080/proxy/rewriteme">... (200; 35.810955ms)
Sep 27 19:46:31.969: INFO: (4) /api/v1/namespaces/proxy-8758/pods/https:proxy-service-lwsvq-zr9jg:462/proxy/: tls qux (200; 43.437267ms)
Sep 27 19:46:31.970: INFO: (4) /api/v1/namespaces/proxy-8758/pods/proxy-service-lwsvq-zr9jg:162/proxy/: bar (200; 42.772694ms)
Sep 27 19:46:31.970: INFO: (4) /api/v1/namespaces/proxy-8758/pods/https:proxy-service-lwsvq-zr9jg:460/proxy/: tls baz (200; 42.761671ms)
Sep 27 19:46:31.970: INFO: (4) /api/v1/namespaces/proxy-8758/services/https:proxy-service-lwsvq:tlsportname1/proxy/: tls baz (200; 43.907155ms)
Sep 27 19:46:31.973: INFO: (4) /api/v1/namespaces/proxy-8758/services/proxy-service-lwsvq:portname1/proxy/: foo (200; 46.361989ms)
Sep 27 19:46:31.973: INFO: (4) /api/v1/namespaces/proxy-8758/services/https:proxy-service-lwsvq:tlsportname2/proxy/: tls qux (200; 46.231314ms)
Sep 27 19:46:31.973: INFO: (4) /api/v1/namespaces/proxy-8758/services/http:proxy-service-lwsvq:portname1/proxy/: foo (200; 46.367813ms)
Sep 27 19:46:31.976: INFO: (4) /api/v1/namespaces/proxy-8758/services/proxy-service-lwsvq:portname2/proxy/: bar (200; 49.113898ms)
Sep 27 19:46:31.976: INFO: (4) /api/v1/namespaces/proxy-8758/services/http:proxy-service-lwsvq:portname2/proxy/: bar (200; 49.12347ms)
Sep 27 19:46:32.000: INFO: (5) /api/v1/namespaces/proxy-8758/pods/proxy-service-lwsvq-zr9jg/proxy/: <a href="/api/v1/namespaces/proxy-8758/pods/proxy-service-lwsvq-zr9jg/proxy/rewriteme">test</a> (200; 23.647173ms)
Sep 27 19:46:32.010: INFO: (5) /api/v1/namespaces/proxy-8758/pods/http:proxy-service-lwsvq-zr9jg:160/proxy/: foo (200; 32.500336ms)
Sep 27 19:46:32.013: INFO: (5) /api/v1/namespaces/proxy-8758/pods/https:proxy-service-lwsvq-zr9jg:443/proxy/: <a href="/api/v1/namespaces/proxy-8758/pods/https:proxy-service-lwsvq-zr9jg:443/proxy/tlsrewritem... (200; 34.930003ms)
Sep 27 19:46:32.013: INFO: (5) /api/v1/namespaces/proxy-8758/pods/https:proxy-service-lwsvq-zr9jg:460/proxy/: tls baz (200; 35.066187ms)
Sep 27 19:46:32.013: INFO: (5) /api/v1/namespaces/proxy-8758/pods/https:proxy-service-lwsvq-zr9jg:462/proxy/: tls qux (200; 35.148807ms)
Sep 27 19:46:32.013: INFO: (5) /api/v1/namespaces/proxy-8758/pods/proxy-service-lwsvq-zr9jg:160/proxy/: foo (200; 35.651646ms)
Sep 27 19:46:32.013: INFO: (5) /api/v1/namespaces/proxy-8758/pods/http:proxy-service-lwsvq-zr9jg:1080/proxy/: <a href="/api/v1/namespaces/proxy-8758/pods/http:proxy-service-lwsvq-zr9jg:1080/proxy/rewriteme">... (200; 35.88757ms)
Sep 27 19:46:32.014: INFO: (5) /api/v1/namespaces/proxy-8758/pods/proxy-service-lwsvq-zr9jg:1080/proxy/: <a href="/api/v1/namespaces/proxy-8758/pods/proxy-service-lwsvq-zr9jg:1080/proxy/rewriteme">test<... (200; 36.961067ms)
Sep 27 19:46:32.014: INFO: (5) /api/v1/namespaces/proxy-8758/pods/proxy-service-lwsvq-zr9jg:162/proxy/: bar (200; 36.651944ms)
Sep 27 19:46:32.014: INFO: (5) /api/v1/namespaces/proxy-8758/pods/http:proxy-service-lwsvq-zr9jg:162/proxy/: bar (200; 36.45353ms)
Sep 27 19:46:32.029: INFO: (5) /api/v1/namespaces/proxy-8758/services/https:proxy-service-lwsvq:tlsportname2/proxy/: tls qux (200; 51.983086ms)
Sep 27 19:46:32.029: INFO: (5) /api/v1/namespaces/proxy-8758/services/http:proxy-service-lwsvq:portname1/proxy/: foo (200; 53.745223ms)
Sep 27 19:46:32.029: INFO: (5) /api/v1/namespaces/proxy-8758/services/http:proxy-service-lwsvq:portname2/proxy/: bar (200; 52.017188ms)
Sep 27 19:46:32.029: INFO: (5) /api/v1/namespaces/proxy-8758/services/proxy-service-lwsvq:portname2/proxy/: bar (200; 52.078926ms)
Sep 27 19:46:32.029: INFO: (5) /api/v1/namespaces/proxy-8758/services/https:proxy-service-lwsvq:tlsportname1/proxy/: tls baz (200; 52.362057ms)
Sep 27 19:46:32.029: INFO: (5) /api/v1/namespaces/proxy-8758/services/proxy-service-lwsvq:portname1/proxy/: foo (200; 52.304518ms)
Sep 27 19:46:32.065: INFO: (6) /api/v1/namespaces/proxy-8758/pods/proxy-service-lwsvq-zr9jg:1080/proxy/: <a href="/api/v1/namespaces/proxy-8758/pods/proxy-service-lwsvq-zr9jg:1080/proxy/rewriteme">test<... (200; 35.015172ms)
Sep 27 19:46:32.079: INFO: (6) /api/v1/namespaces/proxy-8758/pods/proxy-service-lwsvq-zr9jg:162/proxy/: bar (200; 48.805182ms)
Sep 27 19:46:32.081: INFO: (6) /api/v1/namespaces/proxy-8758/pods/http:proxy-service-lwsvq-zr9jg:162/proxy/: bar (200; 50.832436ms)
Sep 27 19:46:32.081: INFO: (6) /api/v1/namespaces/proxy-8758/pods/https:proxy-service-lwsvq-zr9jg:443/proxy/: <a href="/api/v1/namespaces/proxy-8758/pods/https:proxy-service-lwsvq-zr9jg:443/proxy/tlsrewritem... (200; 51.180298ms)
Sep 27 19:46:32.083: INFO: (6) /api/v1/namespaces/proxy-8758/pods/proxy-service-lwsvq-zr9jg:160/proxy/: foo (200; 52.884997ms)
Sep 27 19:46:32.083: INFO: (6) /api/v1/namespaces/proxy-8758/pods/https:proxy-service-lwsvq-zr9jg:462/proxy/: tls qux (200; 52.894111ms)
Sep 27 19:46:32.083: INFO: (6) /api/v1/namespaces/proxy-8758/pods/http:proxy-service-lwsvq-zr9jg:160/proxy/: foo (200; 53.214178ms)
Sep 27 19:46:32.091: INFO: (6) /api/v1/namespaces/proxy-8758/pods/http:proxy-service-lwsvq-zr9jg:1080/proxy/: <a href="/api/v1/namespaces/proxy-8758/pods/http:proxy-service-lwsvq-zr9jg:1080/proxy/rewriteme">... (200; 61.134206ms)
Sep 27 19:46:32.092: INFO: (6) /api/v1/namespaces/proxy-8758/pods/https:proxy-service-lwsvq-zr9jg:460/proxy/: tls baz (200; 62.080085ms)
Sep 27 19:46:32.092: INFO: (6) /api/v1/namespaces/proxy-8758/services/https:proxy-service-lwsvq:tlsportname1/proxy/: tls baz (200; 62.330149ms)
Sep 27 19:46:32.092: INFO: (6) /api/v1/namespaces/proxy-8758/services/https:proxy-service-lwsvq:tlsportname2/proxy/: tls qux (200; 62.118594ms)
Sep 27 19:46:32.092: INFO: (6) /api/v1/namespaces/proxy-8758/pods/proxy-service-lwsvq-zr9jg/proxy/: <a href="/api/v1/namespaces/proxy-8758/pods/proxy-service-lwsvq-zr9jg/proxy/rewriteme">test</a> (200; 61.822978ms)
Sep 27 19:46:32.097: INFO: (6) /api/v1/namespaces/proxy-8758/services/proxy-service-lwsvq:portname2/proxy/: bar (200; 66.833574ms)
Sep 27 19:46:32.097: INFO: (6) /api/v1/namespaces/proxy-8758/services/http:proxy-service-lwsvq:portname2/proxy/: bar (200; 66.526132ms)
Sep 27 19:46:32.097: INFO: (6) /api/v1/namespaces/proxy-8758/services/http:proxy-service-lwsvq:portname1/proxy/: foo (200; 66.781116ms)
Sep 27 19:46:32.106: INFO: (6) /api/v1/namespaces/proxy-8758/services/proxy-service-lwsvq:portname1/proxy/: foo (200; 75.478396ms)
Sep 27 19:46:32.145: INFO: (7) /api/v1/namespaces/proxy-8758/pods/http:proxy-service-lwsvq-zr9jg:1080/proxy/: <a href="/api/v1/namespaces/proxy-8758/pods/http:proxy-service-lwsvq-zr9jg:1080/proxy/rewriteme">... (200; 38.474894ms)
Sep 27 19:46:32.145: INFO: (7) /api/v1/namespaces/proxy-8758/pods/proxy-service-lwsvq-zr9jg:1080/proxy/: <a href="/api/v1/namespaces/proxy-8758/pods/proxy-service-lwsvq-zr9jg:1080/proxy/rewriteme">test<... (200; 38.391056ms)
Sep 27 19:46:32.155: INFO: (7) /api/v1/namespaces/proxy-8758/pods/https:proxy-service-lwsvq-zr9jg:460/proxy/: tls baz (200; 49.381967ms)
Sep 27 19:46:32.155: INFO: (7) /api/v1/namespaces/proxy-8758/pods/proxy-service-lwsvq-zr9jg/proxy/: <a href="/api/v1/namespaces/proxy-8758/pods/proxy-service-lwsvq-zr9jg/proxy/rewriteme">test</a> (200; 49.064806ms)
Sep 27 19:46:32.157: INFO: (7) /api/v1/namespaces/proxy-8758/pods/https:proxy-service-lwsvq-zr9jg:443/proxy/: <a href="/api/v1/namespaces/proxy-8758/pods/https:proxy-service-lwsvq-zr9jg:443/proxy/tlsrewritem... (200; 49.310357ms)
Sep 27 19:46:32.157: INFO: (7) /api/v1/namespaces/proxy-8758/pods/https:proxy-service-lwsvq-zr9jg:462/proxy/: tls qux (200; 50.286722ms)
Sep 27 19:46:32.157: INFO: (7) /api/v1/namespaces/proxy-8758/pods/proxy-service-lwsvq-zr9jg:160/proxy/: foo (200; 49.71946ms)
Sep 27 19:46:32.157: INFO: (7) /api/v1/namespaces/proxy-8758/services/proxy-service-lwsvq:portname1/proxy/: foo (200; 49.900797ms)
Sep 27 19:46:32.163: INFO: (7) /api/v1/namespaces/proxy-8758/services/https:proxy-service-lwsvq:tlsportname2/proxy/: tls qux (200; 55.455566ms)
Sep 27 19:46:32.170: INFO: (7) /api/v1/namespaces/proxy-8758/services/http:proxy-service-lwsvq:portname2/proxy/: bar (200; 62.404897ms)
Sep 27 19:46:32.171: INFO: (7) /api/v1/namespaces/proxy-8758/services/http:proxy-service-lwsvq:portname1/proxy/: foo (200; 63.150599ms)
Sep 27 19:46:32.171: INFO: (7) /api/v1/namespaces/proxy-8758/pods/proxy-service-lwsvq-zr9jg:162/proxy/: bar (200; 64.100745ms)
Sep 27 19:46:32.171: INFO: (7) /api/v1/namespaces/proxy-8758/pods/http:proxy-service-lwsvq-zr9jg:160/proxy/: foo (200; 64.466033ms)
Sep 27 19:46:32.175: INFO: (7) /api/v1/namespaces/proxy-8758/services/https:proxy-service-lwsvq:tlsportname1/proxy/: tls baz (200; 67.947458ms)
Sep 27 19:46:32.179: INFO: (7) /api/v1/namespaces/proxy-8758/pods/http:proxy-service-lwsvq-zr9jg:162/proxy/: bar (200; 71.740555ms)
Sep 27 19:46:32.180: INFO: (7) /api/v1/namespaces/proxy-8758/services/proxy-service-lwsvq:portname2/proxy/: bar (200; 72.347422ms)
Sep 27 19:46:32.207: INFO: (8) /api/v1/namespaces/proxy-8758/pods/proxy-service-lwsvq-zr9jg:162/proxy/: bar (200; 25.83564ms)
Sep 27 19:46:32.214: INFO: (8) /api/v1/namespaces/proxy-8758/pods/proxy-service-lwsvq-zr9jg/proxy/: <a href="/api/v1/namespaces/proxy-8758/pods/proxy-service-lwsvq-zr9jg/proxy/rewriteme">test</a> (200; 31.665609ms)
Sep 27 19:46:32.214: INFO: (8) /api/v1/namespaces/proxy-8758/pods/http:proxy-service-lwsvq-zr9jg:162/proxy/: bar (200; 32.244602ms)
Sep 27 19:46:32.215: INFO: (8) /api/v1/namespaces/proxy-8758/pods/proxy-service-lwsvq-zr9jg:1080/proxy/: <a href="/api/v1/namespaces/proxy-8758/pods/proxy-service-lwsvq-zr9jg:1080/proxy/rewriteme">test<... (200; 32.69664ms)
Sep 27 19:46:32.215: INFO: (8) /api/v1/namespaces/proxy-8758/pods/proxy-service-lwsvq-zr9jg:160/proxy/: foo (200; 32.975673ms)
Sep 27 19:46:32.215: INFO: (8) /api/v1/namespaces/proxy-8758/pods/http:proxy-service-lwsvq-zr9jg:1080/proxy/: <a href="/api/v1/namespaces/proxy-8758/pods/http:proxy-service-lwsvq-zr9jg:1080/proxy/rewriteme">... (200; 33.137741ms)
Sep 27 19:46:32.217: INFO: (8) /api/v1/namespaces/proxy-8758/pods/https:proxy-service-lwsvq-zr9jg:443/proxy/: <a href="/api/v1/namespaces/proxy-8758/pods/https:proxy-service-lwsvq-zr9jg:443/proxy/tlsrewritem... (200; 35.535353ms)
Sep 27 19:46:32.222: INFO: (8) /api/v1/namespaces/proxy-8758/services/https:proxy-service-lwsvq:tlsportname1/proxy/: tls baz (200; 42.073799ms)
Sep 27 19:46:32.222: INFO: (8) /api/v1/namespaces/proxy-8758/pods/https:proxy-service-lwsvq-zr9jg:462/proxy/: tls qux (200; 39.990354ms)
Sep 27 19:46:32.223: INFO: (8) /api/v1/namespaces/proxy-8758/pods/http:proxy-service-lwsvq-zr9jg:160/proxy/: foo (200; 41.343618ms)
Sep 27 19:46:32.223: INFO: (8) /api/v1/namespaces/proxy-8758/pods/https:proxy-service-lwsvq-zr9jg:460/proxy/: tls baz (200; 41.043506ms)
Sep 27 19:46:32.230: INFO: (8) /api/v1/namespaces/proxy-8758/services/https:proxy-service-lwsvq:tlsportname2/proxy/: tls qux (200; 48.350794ms)
Sep 27 19:46:32.230: INFO: (8) /api/v1/namespaces/proxy-8758/services/proxy-service-lwsvq:portname1/proxy/: foo (200; 48.201921ms)
Sep 27 19:46:32.232: INFO: (8) /api/v1/namespaces/proxy-8758/services/proxy-service-lwsvq:portname2/proxy/: bar (200; 51.742722ms)
Sep 27 19:46:32.234: INFO: (8) /api/v1/namespaces/proxy-8758/services/http:proxy-service-lwsvq:portname2/proxy/: bar (200; 52.968865ms)
Sep 27 19:46:32.239: INFO: (8) /api/v1/namespaces/proxy-8758/services/http:proxy-service-lwsvq:portname1/proxy/: foo (200; 58.839883ms)
Sep 27 19:46:32.269: INFO: (9) /api/v1/namespaces/proxy-8758/pods/http:proxy-service-lwsvq-zr9jg:162/proxy/: bar (200; 29.894204ms)
Sep 27 19:46:32.279: INFO: (9) /api/v1/namespaces/proxy-8758/pods/https:proxy-service-lwsvq-zr9jg:443/proxy/: <a href="/api/v1/namespaces/proxy-8758/pods/https:proxy-service-lwsvq-zr9jg:443/proxy/tlsrewritem... (200; 39.608307ms)
Sep 27 19:46:32.280: INFO: (9) /api/v1/namespaces/proxy-8758/pods/proxy-service-lwsvq-zr9jg:162/proxy/: bar (200; 40.015064ms)
Sep 27 19:46:32.280: INFO: (9) /api/v1/namespaces/proxy-8758/pods/http:proxy-service-lwsvq-zr9jg:160/proxy/: foo (200; 40.18479ms)
Sep 27 19:46:32.280: INFO: (9) /api/v1/namespaces/proxy-8758/pods/proxy-service-lwsvq-zr9jg/proxy/: <a href="/api/v1/namespaces/proxy-8758/pods/proxy-service-lwsvq-zr9jg/proxy/rewriteme">test</a> (200; 40.198753ms)
Sep 27 19:46:32.280: INFO: (9) /api/v1/namespaces/proxy-8758/pods/http:proxy-service-lwsvq-zr9jg:1080/proxy/: <a href="/api/v1/namespaces/proxy-8758/pods/http:proxy-service-lwsvq-zr9jg:1080/proxy/rewriteme">... (200; 40.427693ms)
Sep 27 19:46:32.282: INFO: (9) /api/v1/namespaces/proxy-8758/pods/proxy-service-lwsvq-zr9jg:160/proxy/: foo (200; 41.923764ms)
Sep 27 19:46:32.282: INFO: (9) /api/v1/namespaces/proxy-8758/pods/https:proxy-service-lwsvq-zr9jg:462/proxy/: tls qux (200; 42.237145ms)
Sep 27 19:46:32.289: INFO: (9) /api/v1/namespaces/proxy-8758/pods/https:proxy-service-lwsvq-zr9jg:460/proxy/: tls baz (200; 49.489572ms)
Sep 27 19:46:32.295: INFO: (9) /api/v1/namespaces/proxy-8758/services/https:proxy-service-lwsvq:tlsportname2/proxy/: tls qux (200; 55.167285ms)
Sep 27 19:46:32.296: INFO: (9) /api/v1/namespaces/proxy-8758/pods/proxy-service-lwsvq-zr9jg:1080/proxy/: <a href="/api/v1/namespaces/proxy-8758/pods/proxy-service-lwsvq-zr9jg:1080/proxy/rewriteme">test<... (200; 56.568952ms)
Sep 27 19:46:32.296: INFO: (9) /api/v1/namespaces/proxy-8758/services/https:proxy-service-lwsvq:tlsportname1/proxy/: tls baz (200; 57.11917ms)
Sep 27 19:46:32.303: INFO: (9) /api/v1/namespaces/proxy-8758/services/http:proxy-service-lwsvq:portname2/proxy/: bar (200; 62.955101ms)
Sep 27 19:46:32.303: INFO: (9) /api/v1/namespaces/proxy-8758/services/proxy-service-lwsvq:portname1/proxy/: foo (200; 63.023327ms)
Sep 27 19:46:32.303: INFO: (9) /api/v1/namespaces/proxy-8758/services/http:proxy-service-lwsvq:portname1/proxy/: foo (200; 63.169038ms)
Sep 27 19:46:32.303: INFO: (9) /api/v1/namespaces/proxy-8758/services/proxy-service-lwsvq:portname2/proxy/: bar (200; 63.327781ms)
Sep 27 19:46:32.344: INFO: (10) /api/v1/namespaces/proxy-8758/pods/proxy-service-lwsvq-zr9jg:160/proxy/: foo (200; 40.0628ms)
Sep 27 19:46:32.344: INFO: (10) /api/v1/namespaces/proxy-8758/pods/http:proxy-service-lwsvq-zr9jg:160/proxy/: foo (200; 40.307867ms)
Sep 27 19:46:32.345: INFO: (10) /api/v1/namespaces/proxy-8758/pods/proxy-service-lwsvq-zr9jg/proxy/: <a href="/api/v1/namespaces/proxy-8758/pods/proxy-service-lwsvq-zr9jg/proxy/rewriteme">test</a> (200; 40.889649ms)
Sep 27 19:46:32.345: INFO: (10) /api/v1/namespaces/proxy-8758/pods/proxy-service-lwsvq-zr9jg:162/proxy/: bar (200; 41.900286ms)
Sep 27 19:46:32.345: INFO: (10) /api/v1/namespaces/proxy-8758/pods/https:proxy-service-lwsvq-zr9jg:443/proxy/: <a href="/api/v1/namespaces/proxy-8758/pods/https:proxy-service-lwsvq-zr9jg:443/proxy/tlsrewritem... (200; 41.830319ms)
Sep 27 19:46:32.345: INFO: (10) /api/v1/namespaces/proxy-8758/pods/proxy-service-lwsvq-zr9jg:1080/proxy/: <a href="/api/v1/namespaces/proxy-8758/pods/proxy-service-lwsvq-zr9jg:1080/proxy/rewriteme">test<... (200; 41.120435ms)
Sep 27 19:46:32.345: INFO: (10) /api/v1/namespaces/proxy-8758/pods/https:proxy-service-lwsvq-zr9jg:462/proxy/: tls qux (200; 41.368457ms)
Sep 27 19:46:32.345: INFO: (10) /api/v1/namespaces/proxy-8758/pods/http:proxy-service-lwsvq-zr9jg:162/proxy/: bar (200; 41.832188ms)
Sep 27 19:46:32.356: INFO: (10) /api/v1/namespaces/proxy-8758/pods/http:proxy-service-lwsvq-zr9jg:1080/proxy/: <a href="/api/v1/namespaces/proxy-8758/pods/http:proxy-service-lwsvq-zr9jg:1080/proxy/rewriteme">... (200; 52.521813ms)
Sep 27 19:46:32.357: INFO: (10) /api/v1/namespaces/proxy-8758/services/https:proxy-service-lwsvq:tlsportname2/proxy/: tls qux (200; 53.554677ms)
Sep 27 19:46:32.357: INFO: (10) /api/v1/namespaces/proxy-8758/services/http:proxy-service-lwsvq:portname1/proxy/: foo (200; 53.666037ms)
Sep 27 19:46:32.357: INFO: (10) /api/v1/namespaces/proxy-8758/services/proxy-service-lwsvq:portname2/proxy/: bar (200; 53.663413ms)
Sep 27 19:46:32.358: INFO: (10) /api/v1/namespaces/proxy-8758/services/proxy-service-lwsvq:portname1/proxy/: foo (200; 54.622515ms)
Sep 27 19:46:32.358: INFO: (10) /api/v1/namespaces/proxy-8758/pods/https:proxy-service-lwsvq-zr9jg:460/proxy/: tls baz (200; 55.289697ms)
Sep 27 19:46:32.358: INFO: (10) /api/v1/namespaces/proxy-8758/services/http:proxy-service-lwsvq:portname2/proxy/: bar (200; 54.877991ms)
Sep 27 19:46:32.358: INFO: (10) /api/v1/namespaces/proxy-8758/services/https:proxy-service-lwsvq:tlsportname1/proxy/: tls baz (200; 55.023337ms)
Sep 27 19:46:32.383: INFO: (11) /api/v1/namespaces/proxy-8758/pods/https:proxy-service-lwsvq-zr9jg:460/proxy/: tls baz (200; 24.923685ms)
Sep 27 19:46:32.392: INFO: (11) /api/v1/namespaces/proxy-8758/pods/proxy-service-lwsvq-zr9jg:162/proxy/: bar (200; 32.189269ms)
Sep 27 19:46:32.392: INFO: (11) /api/v1/namespaces/proxy-8758/pods/http:proxy-service-lwsvq-zr9jg:160/proxy/: foo (200; 32.788624ms)
Sep 27 19:46:32.392: INFO: (11) /api/v1/namespaces/proxy-8758/pods/proxy-service-lwsvq-zr9jg/proxy/: <a href="/api/v1/namespaces/proxy-8758/pods/proxy-service-lwsvq-zr9jg/proxy/rewriteme">test</a> (200; 32.458518ms)
Sep 27 19:46:32.392: INFO: (11) /api/v1/namespaces/proxy-8758/pods/proxy-service-lwsvq-zr9jg:160/proxy/: foo (200; 32.523449ms)
Sep 27 19:46:32.393: INFO: (11) /api/v1/namespaces/proxy-8758/pods/https:proxy-service-lwsvq-zr9jg:462/proxy/: tls qux (200; 33.592124ms)
Sep 27 19:46:32.393: INFO: (11) /api/v1/namespaces/proxy-8758/pods/http:proxy-service-lwsvq-zr9jg:1080/proxy/: <a href="/api/v1/namespaces/proxy-8758/pods/http:proxy-service-lwsvq-zr9jg:1080/proxy/rewriteme">... (200; 33.073753ms)
Sep 27 19:46:32.393: INFO: (11) /api/v1/namespaces/proxy-8758/pods/proxy-service-lwsvq-zr9jg:1080/proxy/: <a href="/api/v1/namespaces/proxy-8758/pods/proxy-service-lwsvq-zr9jg:1080/proxy/rewriteme">test<... (200; 33.489028ms)
Sep 27 19:46:32.393: INFO: (11) /api/v1/namespaces/proxy-8758/pods/http:proxy-service-lwsvq-zr9jg:162/proxy/: bar (200; 34.155556ms)
Sep 27 19:46:32.393: INFO: (11) /api/v1/namespaces/proxy-8758/pods/https:proxy-service-lwsvq-zr9jg:443/proxy/: <a href="/api/v1/namespaces/proxy-8758/pods/https:proxy-service-lwsvq-zr9jg:443/proxy/tlsrewritem... (200; 33.739326ms)
Sep 27 19:46:32.397: INFO: (11) /api/v1/namespaces/proxy-8758/services/https:proxy-service-lwsvq:tlsportname1/proxy/: tls baz (200; 38.54998ms)
Sep 27 19:46:32.404: INFO: (11) /api/v1/namespaces/proxy-8758/services/http:proxy-service-lwsvq:portname1/proxy/: foo (200; 44.93654ms)
Sep 27 19:46:32.405: INFO: (11) /api/v1/namespaces/proxy-8758/services/proxy-service-lwsvq:portname1/proxy/: foo (200; 45.061262ms)
Sep 27 19:46:32.405: INFO: (11) /api/v1/namespaces/proxy-8758/services/proxy-service-lwsvq:portname2/proxy/: bar (200; 46.057765ms)
Sep 27 19:46:32.406: INFO: (11) /api/v1/namespaces/proxy-8758/services/http:proxy-service-lwsvq:portname2/proxy/: bar (200; 46.35696ms)
Sep 27 19:46:32.406: INFO: (11) /api/v1/namespaces/proxy-8758/services/https:proxy-service-lwsvq:tlsportname2/proxy/: tls qux (200; 46.505438ms)
Sep 27 19:46:32.434: INFO: (12) /api/v1/namespaces/proxy-8758/pods/http:proxy-service-lwsvq-zr9jg:1080/proxy/: <a href="/api/v1/namespaces/proxy-8758/pods/http:proxy-service-lwsvq-zr9jg:1080/proxy/rewriteme">... (200; 27.299609ms)
Sep 27 19:46:32.443: INFO: (12) /api/v1/namespaces/proxy-8758/pods/https:proxy-service-lwsvq-zr9jg:460/proxy/: tls baz (200; 33.591564ms)
Sep 27 19:46:32.443: INFO: (12) /api/v1/namespaces/proxy-8758/pods/proxy-service-lwsvq-zr9jg:162/proxy/: bar (200; 33.87864ms)
Sep 27 19:46:32.444: INFO: (12) /api/v1/namespaces/proxy-8758/pods/https:proxy-service-lwsvq-zr9jg:462/proxy/: tls qux (200; 36.640559ms)
Sep 27 19:46:32.445: INFO: (12) /api/v1/namespaces/proxy-8758/pods/proxy-service-lwsvq-zr9jg:1080/proxy/: <a href="/api/v1/namespaces/proxy-8758/pods/proxy-service-lwsvq-zr9jg:1080/proxy/rewriteme">test<... (200; 35.732845ms)
Sep 27 19:46:32.446: INFO: (12) /api/v1/namespaces/proxy-8758/pods/http:proxy-service-lwsvq-zr9jg:160/proxy/: foo (200; 38.868705ms)
Sep 27 19:46:32.447: INFO: (12) /api/v1/namespaces/proxy-8758/pods/proxy-service-lwsvq-zr9jg/proxy/: <a href="/api/v1/namespaces/proxy-8758/pods/proxy-service-lwsvq-zr9jg/proxy/rewriteme">test</a> (200; 37.657711ms)
Sep 27 19:46:32.447: INFO: (12) /api/v1/namespaces/proxy-8758/pods/proxy-service-lwsvq-zr9jg:160/proxy/: foo (200; 37.988388ms)
Sep 27 19:46:32.451: INFO: (12) /api/v1/namespaces/proxy-8758/services/https:proxy-service-lwsvq:tlsportname2/proxy/: tls qux (200; 44.453146ms)
Sep 27 19:46:32.451: INFO: (12) /api/v1/namespaces/proxy-8758/services/https:proxy-service-lwsvq:tlsportname1/proxy/: tls baz (200; 44.980441ms)
Sep 27 19:46:32.454: INFO: (12) /api/v1/namespaces/proxy-8758/services/http:proxy-service-lwsvq:portname2/proxy/: bar (200; 47.503036ms)
Sep 27 19:46:32.454: INFO: (12) /api/v1/namespaces/proxy-8758/services/http:proxy-service-lwsvq:portname1/proxy/: foo (200; 46.090304ms)
Sep 27 19:46:32.455: INFO: (12) /api/v1/namespaces/proxy-8758/services/proxy-service-lwsvq:portname1/proxy/: foo (200; 45.852969ms)
Sep 27 19:46:32.460: INFO: (12) /api/v1/namespaces/proxy-8758/pods/http:proxy-service-lwsvq-zr9jg:162/proxy/: bar (200; 52.082306ms)
Sep 27 19:46:32.460: INFO: (12) /api/v1/namespaces/proxy-8758/pods/https:proxy-service-lwsvq-zr9jg:443/proxy/: <a href="/api/v1/namespaces/proxy-8758/pods/https:proxy-service-lwsvq-zr9jg:443/proxy/tlsrewritem... (200; 50.689792ms)
Sep 27 19:46:32.460: INFO: (12) /api/v1/namespaces/proxy-8758/services/proxy-service-lwsvq:portname2/proxy/: bar (200; 50.637336ms)
Sep 27 19:46:32.482: INFO: (13) /api/v1/namespaces/proxy-8758/pods/proxy-service-lwsvq-zr9jg/proxy/: <a href="/api/v1/namespaces/proxy-8758/pods/proxy-service-lwsvq-zr9jg/proxy/rewriteme">test</a> (200; 22.403384ms)
Sep 27 19:46:32.499: INFO: (13) /api/v1/namespaces/proxy-8758/pods/http:proxy-service-lwsvq-zr9jg:160/proxy/: foo (200; 37.765048ms)
Sep 27 19:46:32.499: INFO: (13) /api/v1/namespaces/proxy-8758/pods/https:proxy-service-lwsvq-zr9jg:443/proxy/: <a href="/api/v1/namespaces/proxy-8758/pods/https:proxy-service-lwsvq-zr9jg:443/proxy/tlsrewritem... (200; 38.806119ms)
Sep 27 19:46:32.499: INFO: (13) /api/v1/namespaces/proxy-8758/pods/proxy-service-lwsvq-zr9jg:162/proxy/: bar (200; 36.697162ms)
Sep 27 19:46:32.504: INFO: (13) /api/v1/namespaces/proxy-8758/pods/https:proxy-service-lwsvq-zr9jg:460/proxy/: tls baz (200; 43.758682ms)
Sep 27 19:46:32.505: INFO: (13) /api/v1/namespaces/proxy-8758/pods/http:proxy-service-lwsvq-zr9jg:1080/proxy/: <a href="/api/v1/namespaces/proxy-8758/pods/http:proxy-service-lwsvq-zr9jg:1080/proxy/rewriteme">... (200; 42.771133ms)
Sep 27 19:46:32.505: INFO: (13) /api/v1/namespaces/proxy-8758/services/https:proxy-service-lwsvq:tlsportname2/proxy/: tls qux (200; 44.105834ms)
Sep 27 19:46:32.512: INFO: (13) /api/v1/namespaces/proxy-8758/services/http:proxy-service-lwsvq:portname2/proxy/: bar (200; 51.512951ms)
Sep 27 19:46:32.512: INFO: (13) /api/v1/namespaces/proxy-8758/pods/proxy-service-lwsvq-zr9jg:1080/proxy/: <a href="/api/v1/namespaces/proxy-8758/pods/proxy-service-lwsvq-zr9jg:1080/proxy/rewriteme">test<... (200; 50.506536ms)
Sep 27 19:46:32.513: INFO: (13) /api/v1/namespaces/proxy-8758/pods/https:proxy-service-lwsvq-zr9jg:462/proxy/: tls qux (200; 51.010782ms)
Sep 27 19:46:32.513: INFO: (13) /api/v1/namespaces/proxy-8758/pods/proxy-service-lwsvq-zr9jg:160/proxy/: foo (200; 50.342557ms)
Sep 27 19:46:32.513: INFO: (13) /api/v1/namespaces/proxy-8758/pods/http:proxy-service-lwsvq-zr9jg:162/proxy/: bar (200; 51.212065ms)
Sep 27 19:46:32.516: INFO: (13) /api/v1/namespaces/proxy-8758/services/https:proxy-service-lwsvq:tlsportname1/proxy/: tls baz (200; 55.385942ms)
Sep 27 19:46:32.516: INFO: (13) /api/v1/namespaces/proxy-8758/services/http:proxy-service-lwsvq:portname1/proxy/: foo (200; 54.723565ms)
Sep 27 19:46:32.521: INFO: (13) /api/v1/namespaces/proxy-8758/services/proxy-service-lwsvq:portname1/proxy/: foo (200; 58.380852ms)
Sep 27 19:46:32.525: INFO: (13) /api/v1/namespaces/proxy-8758/services/proxy-service-lwsvq:portname2/proxy/: bar (200; 63.569891ms)
Sep 27 19:46:32.551: INFO: (14) /api/v1/namespaces/proxy-8758/pods/proxy-service-lwsvq-zr9jg:162/proxy/: bar (200; 25.611582ms)
Sep 27 19:46:32.559: INFO: (14) /api/v1/namespaces/proxy-8758/pods/proxy-service-lwsvq-zr9jg/proxy/: <a href="/api/v1/namespaces/proxy-8758/pods/proxy-service-lwsvq-zr9jg/proxy/rewriteme">test</a> (200; 32.536538ms)
Sep 27 19:46:32.559: INFO: (14) /api/v1/namespaces/proxy-8758/pods/http:proxy-service-lwsvq-zr9jg:162/proxy/: bar (200; 32.13217ms)
Sep 27 19:46:32.559: INFO: (14) /api/v1/namespaces/proxy-8758/pods/http:proxy-service-lwsvq-zr9jg:160/proxy/: foo (200; 32.147278ms)
Sep 27 19:46:32.559: INFO: (14) /api/v1/namespaces/proxy-8758/pods/proxy-service-lwsvq-zr9jg:1080/proxy/: <a href="/api/v1/namespaces/proxy-8758/pods/proxy-service-lwsvq-zr9jg:1080/proxy/rewriteme">test<... (200; 33.049102ms)
Sep 27 19:46:32.559: INFO: (14) /api/v1/namespaces/proxy-8758/pods/https:proxy-service-lwsvq-zr9jg:443/proxy/: <a href="/api/v1/namespaces/proxy-8758/pods/https:proxy-service-lwsvq-zr9jg:443/proxy/tlsrewritem... (200; 33.618026ms)
Sep 27 19:46:32.560: INFO: (14) /api/v1/namespaces/proxy-8758/pods/https:proxy-service-lwsvq-zr9jg:460/proxy/: tls baz (200; 33.660551ms)
Sep 27 19:46:32.560: INFO: (14) /api/v1/namespaces/proxy-8758/pods/proxy-service-lwsvq-zr9jg:160/proxy/: foo (200; 33.032114ms)
Sep 27 19:46:32.560: INFO: (14) /api/v1/namespaces/proxy-8758/pods/https:proxy-service-lwsvq-zr9jg:462/proxy/: tls qux (200; 33.872012ms)
Sep 27 19:46:32.567: INFO: (14) /api/v1/namespaces/proxy-8758/pods/http:proxy-service-lwsvq-zr9jg:1080/proxy/: <a href="/api/v1/namespaces/proxy-8758/pods/http:proxy-service-lwsvq-zr9jg:1080/proxy/rewriteme">... (200; 40.727476ms)
Sep 27 19:46:32.569: INFO: (14) /api/v1/namespaces/proxy-8758/services/https:proxy-service-lwsvq:tlsportname2/proxy/: tls qux (200; 43.530857ms)
Sep 27 19:46:32.575: INFO: (14) /api/v1/namespaces/proxy-8758/services/http:proxy-service-lwsvq:portname1/proxy/: foo (200; 49.097443ms)
Sep 27 19:46:32.575: INFO: (14) /api/v1/namespaces/proxy-8758/services/proxy-service-lwsvq:portname2/proxy/: bar (200; 49.096355ms)
Sep 27 19:46:32.576: INFO: (14) /api/v1/namespaces/proxy-8758/services/proxy-service-lwsvq:portname1/proxy/: foo (200; 49.2179ms)
Sep 27 19:46:32.577: INFO: (14) /api/v1/namespaces/proxy-8758/services/http:proxy-service-lwsvq:portname2/proxy/: bar (200; 50.632636ms)
Sep 27 19:46:32.577: INFO: (14) /api/v1/namespaces/proxy-8758/services/https:proxy-service-lwsvq:tlsportname1/proxy/: tls baz (200; 50.824928ms)
Sep 27 19:46:32.600: INFO: (15) /api/v1/namespaces/proxy-8758/pods/https:proxy-service-lwsvq-zr9jg:460/proxy/: tls baz (200; 23.269597ms)
Sep 27 19:46:32.607: INFO: (15) /api/v1/namespaces/proxy-8758/pods/proxy-service-lwsvq-zr9jg:1080/proxy/: <a href="/api/v1/namespaces/proxy-8758/pods/proxy-service-lwsvq-zr9jg:1080/proxy/rewriteme">test<... (200; 29.821564ms)
Sep 27 19:46:32.607: INFO: (15) /api/v1/namespaces/proxy-8758/pods/http:proxy-service-lwsvq-zr9jg:162/proxy/: bar (200; 29.98102ms)
Sep 27 19:46:32.608: INFO: (15) /api/v1/namespaces/proxy-8758/pods/proxy-service-lwsvq-zr9jg:160/proxy/: foo (200; 30.567273ms)
Sep 27 19:46:32.608: INFO: (15) /api/v1/namespaces/proxy-8758/pods/https:proxy-service-lwsvq-zr9jg:443/proxy/: <a href="/api/v1/namespaces/proxy-8758/pods/https:proxy-service-lwsvq-zr9jg:443/proxy/tlsrewritem... (200; 30.958512ms)
Sep 27 19:46:32.609: INFO: (15) /api/v1/namespaces/proxy-8758/pods/https:proxy-service-lwsvq-zr9jg:462/proxy/: tls qux (200; 31.790594ms)
Sep 27 19:46:32.609: INFO: (15) /api/v1/namespaces/proxy-8758/pods/proxy-service-lwsvq-zr9jg/proxy/: <a href="/api/v1/namespaces/proxy-8758/pods/proxy-service-lwsvq-zr9jg/proxy/rewriteme">test</a> (200; 31.813287ms)
Sep 27 19:46:32.609: INFO: (15) /api/v1/namespaces/proxy-8758/pods/proxy-service-lwsvq-zr9jg:162/proxy/: bar (200; 31.983121ms)
Sep 27 19:46:32.610: INFO: (15) /api/v1/namespaces/proxy-8758/pods/http:proxy-service-lwsvq-zr9jg:1080/proxy/: <a href="/api/v1/namespaces/proxy-8758/pods/http:proxy-service-lwsvq-zr9jg:1080/proxy/rewriteme">... (200; 32.692093ms)
Sep 27 19:46:32.610: INFO: (15) /api/v1/namespaces/proxy-8758/pods/http:proxy-service-lwsvq-zr9jg:160/proxy/: foo (200; 32.851541ms)
Sep 27 19:46:32.616: INFO: (15) /api/v1/namespaces/proxy-8758/services/proxy-service-lwsvq:portname2/proxy/: bar (200; 39.375673ms)
Sep 27 19:46:32.623: INFO: (15) /api/v1/namespaces/proxy-8758/services/http:proxy-service-lwsvq:portname1/proxy/: foo (200; 45.717703ms)
Sep 27 19:46:32.624: INFO: (15) /api/v1/namespaces/proxy-8758/services/http:proxy-service-lwsvq:portname2/proxy/: bar (200; 47.285295ms)
Sep 27 19:46:32.625: INFO: (15) /api/v1/namespaces/proxy-8758/services/https:proxy-service-lwsvq:tlsportname1/proxy/: tls baz (200; 47.37821ms)
Sep 27 19:46:32.625: INFO: (15) /api/v1/namespaces/proxy-8758/services/proxy-service-lwsvq:portname1/proxy/: foo (200; 47.203972ms)
Sep 27 19:46:32.626: INFO: (15) /api/v1/namespaces/proxy-8758/services/https:proxy-service-lwsvq:tlsportname2/proxy/: tls qux (200; 48.950294ms)
Sep 27 19:46:32.655: INFO: (16) /api/v1/namespaces/proxy-8758/pods/http:proxy-service-lwsvq-zr9jg:162/proxy/: bar (200; 28.319097ms)
Sep 27 19:46:32.656: INFO: (16) /api/v1/namespaces/proxy-8758/pods/proxy-service-lwsvq-zr9jg:160/proxy/: foo (200; 28.831617ms)
Sep 27 19:46:32.657: INFO: (16) /api/v1/namespaces/proxy-8758/pods/https:proxy-service-lwsvq-zr9jg:443/proxy/: <a href="/api/v1/namespaces/proxy-8758/pods/https:proxy-service-lwsvq-zr9jg:443/proxy/tlsrewritem... (200; 30.068396ms)
Sep 27 19:46:32.658: INFO: (16) /api/v1/namespaces/proxy-8758/pods/proxy-service-lwsvq-zr9jg:1080/proxy/: <a href="/api/v1/namespaces/proxy-8758/pods/proxy-service-lwsvq-zr9jg:1080/proxy/rewriteme">test<... (200; 30.841675ms)
Sep 27 19:46:32.658: INFO: (16) /api/v1/namespaces/proxy-8758/pods/proxy-service-lwsvq-zr9jg:162/proxy/: bar (200; 31.820317ms)
Sep 27 19:46:32.659: INFO: (16) /api/v1/namespaces/proxy-8758/pods/proxy-service-lwsvq-zr9jg/proxy/: <a href="/api/v1/namespaces/proxy-8758/pods/proxy-service-lwsvq-zr9jg/proxy/rewriteme">test</a> (200; 31.927774ms)
Sep 27 19:46:32.664: INFO: (16) /api/v1/namespaces/proxy-8758/pods/http:proxy-service-lwsvq-zr9jg:160/proxy/: foo (200; 37.462417ms)
Sep 27 19:46:32.664: INFO: (16) /api/v1/namespaces/proxy-8758/pods/http:proxy-service-lwsvq-zr9jg:1080/proxy/: <a href="/api/v1/namespaces/proxy-8758/pods/http:proxy-service-lwsvq-zr9jg:1080/proxy/rewriteme">... (200; 37.265769ms)
Sep 27 19:46:32.665: INFO: (16) /api/v1/namespaces/proxy-8758/pods/https:proxy-service-lwsvq-zr9jg:460/proxy/: tls baz (200; 38.515525ms)
Sep 27 19:46:32.665: INFO: (16) /api/v1/namespaces/proxy-8758/pods/https:proxy-service-lwsvq-zr9jg:462/proxy/: tls qux (200; 38.378827ms)
Sep 27 19:46:32.666: INFO: (16) /api/v1/namespaces/proxy-8758/services/http:proxy-service-lwsvq:portname1/proxy/: foo (200; 39.44035ms)
Sep 27 19:46:32.668: INFO: (16) /api/v1/namespaces/proxy-8758/services/proxy-service-lwsvq:portname2/proxy/: bar (200; 40.915545ms)
Sep 27 19:46:32.668: INFO: (16) /api/v1/namespaces/proxy-8758/services/https:proxy-service-lwsvq:tlsportname2/proxy/: tls qux (200; 41.083299ms)
Sep 27 19:46:32.670: INFO: (16) /api/v1/namespaces/proxy-8758/services/https:proxy-service-lwsvq:tlsportname1/proxy/: tls baz (200; 43.655168ms)
Sep 27 19:46:32.675: INFO: (16) /api/v1/namespaces/proxy-8758/services/http:proxy-service-lwsvq:portname2/proxy/: bar (200; 47.609092ms)
Sep 27 19:46:32.675: INFO: (16) /api/v1/namespaces/proxy-8758/services/proxy-service-lwsvq:portname1/proxy/: foo (200; 47.898369ms)
Sep 27 19:46:32.698: INFO: (17) /api/v1/namespaces/proxy-8758/pods/https:proxy-service-lwsvq-zr9jg:462/proxy/: tls qux (200; 22.818314ms)
Sep 27 19:46:32.707: INFO: (17) /api/v1/namespaces/proxy-8758/pods/proxy-service-lwsvq-zr9jg:160/proxy/: foo (200; 31.275087ms)
Sep 27 19:46:32.708: INFO: (17) /api/v1/namespaces/proxy-8758/pods/https:proxy-service-lwsvq-zr9jg:443/proxy/: <a href="/api/v1/namespaces/proxy-8758/pods/https:proxy-service-lwsvq-zr9jg:443/proxy/tlsrewritem... (200; 32.166216ms)
Sep 27 19:46:32.708: INFO: (17) /api/v1/namespaces/proxy-8758/pods/proxy-service-lwsvq-zr9jg:162/proxy/: bar (200; 32.224805ms)
Sep 27 19:46:32.708: INFO: (17) /api/v1/namespaces/proxy-8758/pods/proxy-service-lwsvq-zr9jg:1080/proxy/: <a href="/api/v1/namespaces/proxy-8758/pods/proxy-service-lwsvq-zr9jg:1080/proxy/rewriteme">test<... (200; 32.18945ms)
Sep 27 19:46:32.708: INFO: (17) /api/v1/namespaces/proxy-8758/pods/http:proxy-service-lwsvq-zr9jg:160/proxy/: foo (200; 32.182435ms)
Sep 27 19:46:32.708: INFO: (17) /api/v1/namespaces/proxy-8758/pods/proxy-service-lwsvq-zr9jg/proxy/: <a href="/api/v1/namespaces/proxy-8758/pods/proxy-service-lwsvq-zr9jg/proxy/rewriteme">test</a> (200; 32.343489ms)
Sep 27 19:46:32.708: INFO: (17) /api/v1/namespaces/proxy-8758/pods/http:proxy-service-lwsvq-zr9jg:1080/proxy/: <a href="/api/v1/namespaces/proxy-8758/pods/http:proxy-service-lwsvq-zr9jg:1080/proxy/rewriteme">... (200; 32.221617ms)
Sep 27 19:46:32.708: INFO: (17) /api/v1/namespaces/proxy-8758/pods/http:proxy-service-lwsvq-zr9jg:162/proxy/: bar (200; 32.190992ms)
Sep 27 19:46:32.708: INFO: (17) /api/v1/namespaces/proxy-8758/pods/https:proxy-service-lwsvq-zr9jg:460/proxy/: tls baz (200; 32.589541ms)
Sep 27 19:46:32.718: INFO: (17) /api/v1/namespaces/proxy-8758/services/proxy-service-lwsvq:portname2/proxy/: bar (200; 42.092444ms)
Sep 27 19:46:32.718: INFO: (17) /api/v1/namespaces/proxy-8758/services/http:proxy-service-lwsvq:portname1/proxy/: foo (200; 42.862399ms)
Sep 27 19:46:32.718: INFO: (17) /api/v1/namespaces/proxy-8758/services/https:proxy-service-lwsvq:tlsportname1/proxy/: tls baz (200; 42.936013ms)
Sep 27 19:46:32.718: INFO: (17) /api/v1/namespaces/proxy-8758/services/https:proxy-service-lwsvq:tlsportname2/proxy/: tls qux (200; 42.909196ms)
Sep 27 19:46:32.719: INFO: (17) /api/v1/namespaces/proxy-8758/services/proxy-service-lwsvq:portname1/proxy/: foo (200; 43.824155ms)
Sep 27 19:46:32.719: INFO: (17) /api/v1/namespaces/proxy-8758/services/http:proxy-service-lwsvq:portname2/proxy/: bar (200; 43.63935ms)
Sep 27 19:46:32.745: INFO: (18) /api/v1/namespaces/proxy-8758/pods/proxy-service-lwsvq-zr9jg:1080/proxy/: <a href="/api/v1/namespaces/proxy-8758/pods/proxy-service-lwsvq-zr9jg:1080/proxy/rewriteme">test<... (200; 25.967191ms)
Sep 27 19:46:32.747: INFO: (18) /api/v1/namespaces/proxy-8758/pods/proxy-service-lwsvq-zr9jg/proxy/: <a href="/api/v1/namespaces/proxy-8758/pods/proxy-service-lwsvq-zr9jg/proxy/rewriteme">test</a> (200; 27.316764ms)
Sep 27 19:46:32.747: INFO: (18) /api/v1/namespaces/proxy-8758/pods/https:proxy-service-lwsvq-zr9jg:443/proxy/: <a href="/api/v1/namespaces/proxy-8758/pods/https:proxy-service-lwsvq-zr9jg:443/proxy/tlsrewritem... (200; 27.641466ms)
Sep 27 19:46:32.747: INFO: (18) /api/v1/namespaces/proxy-8758/pods/https:proxy-service-lwsvq-zr9jg:460/proxy/: tls baz (200; 27.5413ms)
Sep 27 19:46:32.747: INFO: (18) /api/v1/namespaces/proxy-8758/pods/http:proxy-service-lwsvq-zr9jg:1080/proxy/: <a href="/api/v1/namespaces/proxy-8758/pods/http:proxy-service-lwsvq-zr9jg:1080/proxy/rewriteme">... (200; 28.027937ms)
Sep 27 19:46:32.748: INFO: (18) /api/v1/namespaces/proxy-8758/pods/https:proxy-service-lwsvq-zr9jg:462/proxy/: tls qux (200; 27.305743ms)
Sep 27 19:46:32.748: INFO: (18) /api/v1/namespaces/proxy-8758/pods/proxy-service-lwsvq-zr9jg:162/proxy/: bar (200; 28.461554ms)
Sep 27 19:46:32.748: INFO: (18) /api/v1/namespaces/proxy-8758/pods/http:proxy-service-lwsvq-zr9jg:160/proxy/: foo (200; 28.308714ms)
Sep 27 19:46:32.749: INFO: (18) /api/v1/namespaces/proxy-8758/pods/proxy-service-lwsvq-zr9jg:160/proxy/: foo (200; 29.939971ms)
Sep 27 19:46:32.750: INFO: (18) /api/v1/namespaces/proxy-8758/pods/http:proxy-service-lwsvq-zr9jg:162/proxy/: bar (200; 29.334885ms)
Sep 27 19:46:32.758: INFO: (18) /api/v1/namespaces/proxy-8758/services/https:proxy-service-lwsvq:tlsportname2/proxy/: tls qux (200; 37.52394ms)
Sep 27 19:46:32.758: INFO: (18) /api/v1/namespaces/proxy-8758/services/https:proxy-service-lwsvq:tlsportname1/proxy/: tls baz (200; 37.847343ms)
Sep 27 19:46:32.763: INFO: (18) /api/v1/namespaces/proxy-8758/services/proxy-service-lwsvq:portname2/proxy/: bar (200; 43.424559ms)
Sep 27 19:46:32.763: INFO: (18) /api/v1/namespaces/proxy-8758/services/http:proxy-service-lwsvq:portname1/proxy/: foo (200; 42.206471ms)
Sep 27 19:46:32.767: INFO: (18) /api/v1/namespaces/proxy-8758/services/http:proxy-service-lwsvq:portname2/proxy/: bar (200; 46.510095ms)
Sep 27 19:46:32.767: INFO: (18) /api/v1/namespaces/proxy-8758/services/proxy-service-lwsvq:portname1/proxy/: foo (200; 47.24187ms)
Sep 27 19:46:32.794: INFO: (19) /api/v1/namespaces/proxy-8758/pods/http:proxy-service-lwsvq-zr9jg:160/proxy/: foo (200; 25.226228ms)
Sep 27 19:46:32.800: INFO: (19) /api/v1/namespaces/proxy-8758/pods/http:proxy-service-lwsvq-zr9jg:1080/proxy/: <a href="/api/v1/namespaces/proxy-8758/pods/http:proxy-service-lwsvq-zr9jg:1080/proxy/rewriteme">... (200; 31.345791ms)
Sep 27 19:46:32.804: INFO: (19) /api/v1/namespaces/proxy-8758/pods/proxy-service-lwsvq-zr9jg:160/proxy/: foo (200; 34.820844ms)
Sep 27 19:46:32.804: INFO: (19) /api/v1/namespaces/proxy-8758/pods/proxy-service-lwsvq-zr9jg:162/proxy/: bar (200; 34.949937ms)
Sep 27 19:46:32.804: INFO: (19) /api/v1/namespaces/proxy-8758/pods/http:proxy-service-lwsvq-zr9jg:162/proxy/: bar (200; 35.144655ms)
Sep 27 19:46:32.806: INFO: (19) /api/v1/namespaces/proxy-8758/pods/https:proxy-service-lwsvq-zr9jg:462/proxy/: tls qux (200; 37.215305ms)
Sep 27 19:46:32.807: INFO: (19) /api/v1/namespaces/proxy-8758/pods/https:proxy-service-lwsvq-zr9jg:443/proxy/: <a href="/api/v1/namespaces/proxy-8758/pods/https:proxy-service-lwsvq-zr9jg:443/proxy/tlsrewritem... (200; 38.123591ms)
Sep 27 19:46:32.807: INFO: (19) /api/v1/namespaces/proxy-8758/pods/https:proxy-service-lwsvq-zr9jg:460/proxy/: tls baz (200; 38.13834ms)
Sep 27 19:46:32.807: INFO: (19) /api/v1/namespaces/proxy-8758/pods/proxy-service-lwsvq-zr9jg:1080/proxy/: <a href="/api/v1/namespaces/proxy-8758/pods/proxy-service-lwsvq-zr9jg:1080/proxy/rewriteme">test<... (200; 38.375965ms)
Sep 27 19:46:32.807: INFO: (19) /api/v1/namespaces/proxy-8758/pods/proxy-service-lwsvq-zr9jg/proxy/: <a href="/api/v1/namespaces/proxy-8758/pods/proxy-service-lwsvq-zr9jg/proxy/rewriteme">test</a> (200; 38.46868ms)
Sep 27 19:46:32.810: INFO: (19) /api/v1/namespaces/proxy-8758/services/http:proxy-service-lwsvq:portname1/proxy/: foo (200; 41.364442ms)
Sep 27 19:46:32.811: INFO: (19) /api/v1/namespaces/proxy-8758/services/https:proxy-service-lwsvq:tlsportname1/proxy/: tls baz (200; 43.614466ms)
Sep 27 19:46:32.812: INFO: (19) /api/v1/namespaces/proxy-8758/services/http:proxy-service-lwsvq:portname2/proxy/: bar (200; 43.746188ms)
Sep 27 19:46:32.813: INFO: (19) /api/v1/namespaces/proxy-8758/services/proxy-service-lwsvq:portname1/proxy/: foo (200; 44.173129ms)
Sep 27 19:46:32.816: INFO: (19) /api/v1/namespaces/proxy-8758/services/proxy-service-lwsvq:portname2/proxy/: bar (200; 47.677984ms)
Sep 27 19:46:32.816: INFO: (19) /api/v1/namespaces/proxy-8758/services/https:proxy-service-lwsvq:tlsportname2/proxy/: tls qux (200; 47.906578ms)
STEP: deleting ReplicationController proxy-service-lwsvq in namespace proxy-8758, will wait for the garbage collector to delete the pods
Sep 27 19:46:32.908: INFO: Deleting ReplicationController proxy-service-lwsvq took: 28.302759ms
Sep 27 19:46:33.009: INFO: Terminating ReplicationController proxy-service-lwsvq pods took: 100.24674ms
[AfterEach] version v1
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 19:46:36.009: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-8758" for this suite.

• [SLOW TEST:9.676 seconds]
[sig-network] Proxy
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  version v1
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:59
    should proxy through a service and a pod  [Conformance]
    /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Proxy version v1 should proxy through a service and a pod  [Conformance]","total":305,"completed":40,"skipped":651,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: udp [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 19:46:36.050: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Performing setup for networking test in namespace pod-network-test-9215
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Sep 27 19:46:36.212: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Sep 27 19:46:36.415: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Sep 27 19:46:38.441: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep 27 19:46:40.438: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep 27 19:46:42.429: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep 27 19:46:44.429: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep 27 19:46:46.431: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep 27 19:46:48.433: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep 27 19:46:50.437: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep 27 19:46:52.429: INFO: The status of Pod netserver-0 is Running (Ready = true)
Sep 27 19:46:52.459: INFO: The status of Pod netserver-1 is Running (Ready = false)
Sep 27 19:46:54.476: INFO: The status of Pod netserver-1 is Running (Ready = false)
Sep 27 19:46:56.474: INFO: The status of Pod netserver-1 is Running (Ready = true)
Sep 27 19:46:56.506: INFO: The status of Pod netserver-2 is Running (Ready = true)
STEP: Creating test pods
Sep 27 19:46:58.616: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.75.25:8080/dial?request=hostname&protocol=udp&host=172.30.137.116&port=8081&tries=1'] Namespace:pod-network-test-9215 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep 27 19:46:58.616: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
Sep 27 19:46:58.862: INFO: Waiting for responses: map[]
Sep 27 19:46:58.877: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.75.25:8080/dial?request=hostname&protocol=udp&host=172.30.75.24&port=8081&tries=1'] Namespace:pod-network-test-9215 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep 27 19:46:58.877: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
Sep 27 19:46:59.260: INFO: Waiting for responses: map[]
Sep 27 19:46:59.276: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.75.25:8080/dial?request=hostname&protocol=udp&host=172.30.85.158&port=8081&tries=1'] Namespace:pod-network-test-9215 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep 27 19:46:59.276: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
Sep 27 19:46:59.719: INFO: Waiting for responses: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 19:46:59.720: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-9215" for this suite.

• [SLOW TEST:23.708 seconds]
[sig-network] Networking
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:26
  Granular Checks: Pods
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:29
    should function for intra-pod communication: udp [NodeConformance] [Conformance]
    /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: udp [NodeConformance] [Conformance]","total":305,"completed":41,"skipped":666,"failed":0}
SSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 19:46:59.758: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod busybox-91794a54-b353-49d0-8243-142d89cd7105 in namespace container-probe-6221
Sep 27 19:47:03.975: INFO: Started pod busybox-91794a54-b353-49d0-8243-142d89cd7105 in namespace container-probe-6221
STEP: checking the pod's current state and verifying that restartCount is present
Sep 27 19:47:03.990: INFO: Initial restart count of pod busybox-91794a54-b353-49d0-8243-142d89cd7105 is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 19:51:06.038: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-6221" for this suite.

• [SLOW TEST:246.334 seconds]
[k8s.io] Probing container
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Probing container should *not* be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","total":305,"completed":42,"skipped":672,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 19:51:06.095: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Sep 27 19:51:06.324: INFO: Waiting up to 5m0s for pod "downwardapi-volume-3a67f6fe-6dd5-4e5c-b842-c3005b9dddb9" in namespace "downward-api-2897" to be "Succeeded or Failed"
Sep 27 19:51:06.339: INFO: Pod "downwardapi-volume-3a67f6fe-6dd5-4e5c-b842-c3005b9dddb9": Phase="Pending", Reason="", readiness=false. Elapsed: 14.879809ms
Sep 27 19:51:08.354: INFO: Pod "downwardapi-volume-3a67f6fe-6dd5-4e5c-b842-c3005b9dddb9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.030194042s
STEP: Saw pod success
Sep 27 19:51:08.354: INFO: Pod "downwardapi-volume-3a67f6fe-6dd5-4e5c-b842-c3005b9dddb9" satisfied condition "Succeeded or Failed"
Sep 27 19:51:08.376: INFO: Trying to get logs from node 10.177.248.117 pod downwardapi-volume-3a67f6fe-6dd5-4e5c-b842-c3005b9dddb9 container client-container: <nil>
STEP: delete the pod
Sep 27 19:51:08.498: INFO: Waiting for pod downwardapi-volume-3a67f6fe-6dd5-4e5c-b842-c3005b9dddb9 to disappear
Sep 27 19:51:08.512: INFO: Pod downwardapi-volume-3a67f6fe-6dd5-4e5c-b842-c3005b9dddb9 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 19:51:08.512: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2897" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide podname only [NodeConformance] [Conformance]","total":305,"completed":43,"skipped":684,"failed":0}
SS
------------------------------
[k8s.io] Kubelet when scheduling a read only busybox container 
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 19:51:08.559: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 19:51:12.823: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-2302" for this suite.
•{"msg":"PASSED [k8s.io] Kubelet when scheduling a read only busybox container should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":44,"skipped":686,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Discovery 
  should validate PreferredVersion for each APIGroup [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Discovery
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 19:51:12.868: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename discovery
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Discovery
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/discovery.go:39
STEP: Setting up server cert
[It] should validate PreferredVersion for each APIGroup [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Sep 27 19:51:13.679: INFO: Checking APIGroup: apiregistration.k8s.io
Sep 27 19:51:13.684: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
Sep 27 19:51:13.684: INFO: Versions found [{apiregistration.k8s.io/v1 v1} {apiregistration.k8s.io/v1beta1 v1beta1}]
Sep 27 19:51:13.684: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
Sep 27 19:51:13.684: INFO: Checking APIGroup: extensions
Sep 27 19:51:13.690: INFO: PreferredVersion.GroupVersion: extensions/v1beta1
Sep 27 19:51:13.690: INFO: Versions found [{extensions/v1beta1 v1beta1}]
Sep 27 19:51:13.690: INFO: extensions/v1beta1 matches extensions/v1beta1
Sep 27 19:51:13.690: INFO: Checking APIGroup: apps
Sep 27 19:51:13.694: INFO: PreferredVersion.GroupVersion: apps/v1
Sep 27 19:51:13.694: INFO: Versions found [{apps/v1 v1}]
Sep 27 19:51:13.694: INFO: apps/v1 matches apps/v1
Sep 27 19:51:13.694: INFO: Checking APIGroup: events.k8s.io
Sep 27 19:51:13.699: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
Sep 27 19:51:13.699: INFO: Versions found [{events.k8s.io/v1 v1} {events.k8s.io/v1beta1 v1beta1}]
Sep 27 19:51:13.699: INFO: events.k8s.io/v1 matches events.k8s.io/v1
Sep 27 19:51:13.699: INFO: Checking APIGroup: authentication.k8s.io
Sep 27 19:51:13.704: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
Sep 27 19:51:13.704: INFO: Versions found [{authentication.k8s.io/v1 v1} {authentication.k8s.io/v1beta1 v1beta1}]
Sep 27 19:51:13.704: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
Sep 27 19:51:13.704: INFO: Checking APIGroup: authorization.k8s.io
Sep 27 19:51:13.709: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
Sep 27 19:51:13.709: INFO: Versions found [{authorization.k8s.io/v1 v1} {authorization.k8s.io/v1beta1 v1beta1}]
Sep 27 19:51:13.709: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
Sep 27 19:51:13.709: INFO: Checking APIGroup: autoscaling
Sep 27 19:51:13.714: INFO: PreferredVersion.GroupVersion: autoscaling/v1
Sep 27 19:51:13.714: INFO: Versions found [{autoscaling/v1 v1} {autoscaling/v2beta1 v2beta1} {autoscaling/v2beta2 v2beta2}]
Sep 27 19:51:13.714: INFO: autoscaling/v1 matches autoscaling/v1
Sep 27 19:51:13.714: INFO: Checking APIGroup: batch
Sep 27 19:51:13.720: INFO: PreferredVersion.GroupVersion: batch/v1
Sep 27 19:51:13.720: INFO: Versions found [{batch/v1 v1} {batch/v1beta1 v1beta1}]
Sep 27 19:51:13.720: INFO: batch/v1 matches batch/v1
Sep 27 19:51:13.720: INFO: Checking APIGroup: certificates.k8s.io
Sep 27 19:51:13.724: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
Sep 27 19:51:13.724: INFO: Versions found [{certificates.k8s.io/v1 v1} {certificates.k8s.io/v1beta1 v1beta1}]
Sep 27 19:51:13.724: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
Sep 27 19:51:13.724: INFO: Checking APIGroup: networking.k8s.io
Sep 27 19:51:13.729: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
Sep 27 19:51:13.729: INFO: Versions found [{networking.k8s.io/v1 v1} {networking.k8s.io/v1beta1 v1beta1}]
Sep 27 19:51:13.729: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
Sep 27 19:51:13.729: INFO: Checking APIGroup: policy
Sep 27 19:51:13.734: INFO: PreferredVersion.GroupVersion: policy/v1beta1
Sep 27 19:51:13.734: INFO: Versions found [{policy/v1beta1 v1beta1}]
Sep 27 19:51:13.734: INFO: policy/v1beta1 matches policy/v1beta1
Sep 27 19:51:13.734: INFO: Checking APIGroup: rbac.authorization.k8s.io
Sep 27 19:51:13.738: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
Sep 27 19:51:13.738: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1} {rbac.authorization.k8s.io/v1beta1 v1beta1}]
Sep 27 19:51:13.738: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
Sep 27 19:51:13.738: INFO: Checking APIGroup: storage.k8s.io
Sep 27 19:51:13.747: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
Sep 27 19:51:13.747: INFO: Versions found [{storage.k8s.io/v1 v1} {storage.k8s.io/v1beta1 v1beta1}]
Sep 27 19:51:13.747: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
Sep 27 19:51:13.747: INFO: Checking APIGroup: admissionregistration.k8s.io
Sep 27 19:51:13.752: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
Sep 27 19:51:13.752: INFO: Versions found [{admissionregistration.k8s.io/v1 v1} {admissionregistration.k8s.io/v1beta1 v1beta1}]
Sep 27 19:51:13.752: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
Sep 27 19:51:13.753: INFO: Checking APIGroup: apiextensions.k8s.io
Sep 27 19:51:13.758: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
Sep 27 19:51:13.758: INFO: Versions found [{apiextensions.k8s.io/v1 v1} {apiextensions.k8s.io/v1beta1 v1beta1}]
Sep 27 19:51:13.759: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
Sep 27 19:51:13.759: INFO: Checking APIGroup: scheduling.k8s.io
Sep 27 19:51:13.779: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
Sep 27 19:51:13.779: INFO: Versions found [{scheduling.k8s.io/v1 v1} {scheduling.k8s.io/v1beta1 v1beta1}]
Sep 27 19:51:13.779: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
Sep 27 19:51:13.779: INFO: Checking APIGroup: coordination.k8s.io
Sep 27 19:51:13.784: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
Sep 27 19:51:13.784: INFO: Versions found [{coordination.k8s.io/v1 v1} {coordination.k8s.io/v1beta1 v1beta1}]
Sep 27 19:51:13.784: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
Sep 27 19:51:13.784: INFO: Checking APIGroup: node.k8s.io
Sep 27 19:51:13.789: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1beta1
Sep 27 19:51:13.789: INFO: Versions found [{node.k8s.io/v1beta1 v1beta1}]
Sep 27 19:51:13.790: INFO: node.k8s.io/v1beta1 matches node.k8s.io/v1beta1
Sep 27 19:51:13.790: INFO: Checking APIGroup: discovery.k8s.io
Sep 27 19:51:13.797: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1beta1
Sep 27 19:51:13.797: INFO: Versions found [{discovery.k8s.io/v1beta1 v1beta1}]
Sep 27 19:51:13.797: INFO: discovery.k8s.io/v1beta1 matches discovery.k8s.io/v1beta1
Sep 27 19:51:13.797: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
Sep 27 19:51:13.803: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1alpha1
Sep 27 19:51:13.803: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1alpha1 v1alpha1}]
Sep 27 19:51:13.803: INFO: flowcontrol.apiserver.k8s.io/v1alpha1 matches flowcontrol.apiserver.k8s.io/v1alpha1
Sep 27 19:51:13.803: INFO: Checking APIGroup: apps.openshift.io
Sep 27 19:51:13.808: INFO: PreferredVersion.GroupVersion: apps.openshift.io/v1
Sep 27 19:51:13.808: INFO: Versions found [{apps.openshift.io/v1 v1}]
Sep 27 19:51:13.808: INFO: apps.openshift.io/v1 matches apps.openshift.io/v1
Sep 27 19:51:13.808: INFO: Checking APIGroup: authorization.openshift.io
Sep 27 19:51:13.814: INFO: PreferredVersion.GroupVersion: authorization.openshift.io/v1
Sep 27 19:51:13.814: INFO: Versions found [{authorization.openshift.io/v1 v1}]
Sep 27 19:51:13.814: INFO: authorization.openshift.io/v1 matches authorization.openshift.io/v1
Sep 27 19:51:13.814: INFO: Checking APIGroup: build.openshift.io
Sep 27 19:51:13.820: INFO: PreferredVersion.GroupVersion: build.openshift.io/v1
Sep 27 19:51:13.820: INFO: Versions found [{build.openshift.io/v1 v1}]
Sep 27 19:51:13.820: INFO: build.openshift.io/v1 matches build.openshift.io/v1
Sep 27 19:51:13.820: INFO: Checking APIGroup: image.openshift.io
Sep 27 19:51:13.825: INFO: PreferredVersion.GroupVersion: image.openshift.io/v1
Sep 27 19:51:13.825: INFO: Versions found [{image.openshift.io/v1 v1}]
Sep 27 19:51:13.825: INFO: image.openshift.io/v1 matches image.openshift.io/v1
Sep 27 19:51:13.825: INFO: Checking APIGroup: oauth.openshift.io
Sep 27 19:51:13.829: INFO: PreferredVersion.GroupVersion: oauth.openshift.io/v1
Sep 27 19:51:13.829: INFO: Versions found [{oauth.openshift.io/v1 v1}]
Sep 27 19:51:13.829: INFO: oauth.openshift.io/v1 matches oauth.openshift.io/v1
Sep 27 19:51:13.829: INFO: Checking APIGroup: project.openshift.io
Sep 27 19:51:13.834: INFO: PreferredVersion.GroupVersion: project.openshift.io/v1
Sep 27 19:51:13.834: INFO: Versions found [{project.openshift.io/v1 v1}]
Sep 27 19:51:13.834: INFO: project.openshift.io/v1 matches project.openshift.io/v1
Sep 27 19:51:13.834: INFO: Checking APIGroup: quota.openshift.io
Sep 27 19:51:13.841: INFO: PreferredVersion.GroupVersion: quota.openshift.io/v1
Sep 27 19:51:13.842: INFO: Versions found [{quota.openshift.io/v1 v1}]
Sep 27 19:51:13.842: INFO: quota.openshift.io/v1 matches quota.openshift.io/v1
Sep 27 19:51:13.842: INFO: Checking APIGroup: route.openshift.io
Sep 27 19:51:13.847: INFO: PreferredVersion.GroupVersion: route.openshift.io/v1
Sep 27 19:51:13.847: INFO: Versions found [{route.openshift.io/v1 v1}]
Sep 27 19:51:13.847: INFO: route.openshift.io/v1 matches route.openshift.io/v1
Sep 27 19:51:13.847: INFO: Checking APIGroup: security.openshift.io
Sep 27 19:51:13.852: INFO: PreferredVersion.GroupVersion: security.openshift.io/v1
Sep 27 19:51:13.853: INFO: Versions found [{security.openshift.io/v1 v1}]
Sep 27 19:51:13.853: INFO: security.openshift.io/v1 matches security.openshift.io/v1
Sep 27 19:51:13.853: INFO: Checking APIGroup: template.openshift.io
Sep 27 19:51:13.858: INFO: PreferredVersion.GroupVersion: template.openshift.io/v1
Sep 27 19:51:13.858: INFO: Versions found [{template.openshift.io/v1 v1}]
Sep 27 19:51:13.858: INFO: template.openshift.io/v1 matches template.openshift.io/v1
Sep 27 19:51:13.858: INFO: Checking APIGroup: user.openshift.io
Sep 27 19:51:13.863: INFO: PreferredVersion.GroupVersion: user.openshift.io/v1
Sep 27 19:51:13.863: INFO: Versions found [{user.openshift.io/v1 v1}]
Sep 27 19:51:13.863: INFO: user.openshift.io/v1 matches user.openshift.io/v1
Sep 27 19:51:13.863: INFO: Checking APIGroup: packages.operators.coreos.com
Sep 27 19:51:13.868: INFO: PreferredVersion.GroupVersion: packages.operators.coreos.com/v1
Sep 27 19:51:13.868: INFO: Versions found [{packages.operators.coreos.com/v1 v1}]
Sep 27 19:51:13.868: INFO: packages.operators.coreos.com/v1 matches packages.operators.coreos.com/v1
Sep 27 19:51:13.868: INFO: Checking APIGroup: config.openshift.io
Sep 27 19:51:13.873: INFO: PreferredVersion.GroupVersion: config.openshift.io/v1
Sep 27 19:51:13.873: INFO: Versions found [{config.openshift.io/v1 v1}]
Sep 27 19:51:13.873: INFO: config.openshift.io/v1 matches config.openshift.io/v1
Sep 27 19:51:13.873: INFO: Checking APIGroup: operator.openshift.io
Sep 27 19:51:13.879: INFO: PreferredVersion.GroupVersion: operator.openshift.io/v1
Sep 27 19:51:13.879: INFO: Versions found [{operator.openshift.io/v1 v1} {operator.openshift.io/v1alpha1 v1alpha1}]
Sep 27 19:51:13.879: INFO: operator.openshift.io/v1 matches operator.openshift.io/v1
Sep 27 19:51:13.879: INFO: Checking APIGroup: cloudcredential.openshift.io
Sep 27 19:51:13.884: INFO: PreferredVersion.GroupVersion: cloudcredential.openshift.io/v1
Sep 27 19:51:13.884: INFO: Versions found [{cloudcredential.openshift.io/v1 v1}]
Sep 27 19:51:13.884: INFO: cloudcredential.openshift.io/v1 matches cloudcredential.openshift.io/v1
Sep 27 19:51:13.884: INFO: Checking APIGroup: console.openshift.io
Sep 27 19:51:13.890: INFO: PreferredVersion.GroupVersion: console.openshift.io/v1
Sep 27 19:51:13.890: INFO: Versions found [{console.openshift.io/v1 v1}]
Sep 27 19:51:13.890: INFO: console.openshift.io/v1 matches console.openshift.io/v1
Sep 27 19:51:13.890: INFO: Checking APIGroup: crd.projectcalico.org
Sep 27 19:51:13.896: INFO: PreferredVersion.GroupVersion: crd.projectcalico.org/v1
Sep 27 19:51:13.896: INFO: Versions found [{crd.projectcalico.org/v1 v1}]
Sep 27 19:51:13.896: INFO: crd.projectcalico.org/v1 matches crd.projectcalico.org/v1
Sep 27 19:51:13.896: INFO: Checking APIGroup: imageregistry.operator.openshift.io
Sep 27 19:51:13.902: INFO: PreferredVersion.GroupVersion: imageregistry.operator.openshift.io/v1
Sep 27 19:51:13.902: INFO: Versions found [{imageregistry.operator.openshift.io/v1 v1}]
Sep 27 19:51:13.902: INFO: imageregistry.operator.openshift.io/v1 matches imageregistry.operator.openshift.io/v1
Sep 27 19:51:13.902: INFO: Checking APIGroup: ingress.operator.openshift.io
Sep 27 19:51:13.907: INFO: PreferredVersion.GroupVersion: ingress.operator.openshift.io/v1
Sep 27 19:51:13.907: INFO: Versions found [{ingress.operator.openshift.io/v1 v1}]
Sep 27 19:51:13.907: INFO: ingress.operator.openshift.io/v1 matches ingress.operator.openshift.io/v1
Sep 27 19:51:13.907: INFO: Checking APIGroup: k8s.cni.cncf.io
Sep 27 19:51:13.914: INFO: PreferredVersion.GroupVersion: k8s.cni.cncf.io/v1
Sep 27 19:51:13.914: INFO: Versions found [{k8s.cni.cncf.io/v1 v1}]
Sep 27 19:51:13.914: INFO: k8s.cni.cncf.io/v1 matches k8s.cni.cncf.io/v1
Sep 27 19:51:13.914: INFO: Checking APIGroup: machineconfiguration.openshift.io
Sep 27 19:51:13.921: INFO: PreferredVersion.GroupVersion: machineconfiguration.openshift.io/v1
Sep 27 19:51:13.921: INFO: Versions found [{machineconfiguration.openshift.io/v1 v1}]
Sep 27 19:51:13.921: INFO: machineconfiguration.openshift.io/v1 matches machineconfiguration.openshift.io/v1
Sep 27 19:51:13.921: INFO: Checking APIGroup: monitoring.coreos.com
Sep 27 19:51:13.926: INFO: PreferredVersion.GroupVersion: monitoring.coreos.com/v1
Sep 27 19:51:13.926: INFO: Versions found [{monitoring.coreos.com/v1 v1}]
Sep 27 19:51:13.926: INFO: monitoring.coreos.com/v1 matches monitoring.coreos.com/v1
Sep 27 19:51:13.926: INFO: Checking APIGroup: network.operator.openshift.io
Sep 27 19:51:13.940: INFO: PreferredVersion.GroupVersion: network.operator.openshift.io/v1
Sep 27 19:51:13.940: INFO: Versions found [{network.operator.openshift.io/v1 v1}]
Sep 27 19:51:13.940: INFO: network.operator.openshift.io/v1 matches network.operator.openshift.io/v1
Sep 27 19:51:13.940: INFO: Checking APIGroup: operator.tigera.io
Sep 27 19:51:13.946: INFO: PreferredVersion.GroupVersion: operator.tigera.io/v1
Sep 27 19:51:13.946: INFO: Versions found [{operator.tigera.io/v1 v1}]
Sep 27 19:51:13.946: INFO: operator.tigera.io/v1 matches operator.tigera.io/v1
Sep 27 19:51:13.946: INFO: Checking APIGroup: operators.coreos.com
Sep 27 19:51:13.951: INFO: PreferredVersion.GroupVersion: operators.coreos.com/v1
Sep 27 19:51:13.951: INFO: Versions found [{operators.coreos.com/v1 v1} {operators.coreos.com/v1alpha2 v1alpha2} {operators.coreos.com/v1alpha1 v1alpha1}]
Sep 27 19:51:13.951: INFO: operators.coreos.com/v1 matches operators.coreos.com/v1
Sep 27 19:51:13.951: INFO: Checking APIGroup: samples.operator.openshift.io
Sep 27 19:51:13.968: INFO: PreferredVersion.GroupVersion: samples.operator.openshift.io/v1
Sep 27 19:51:13.968: INFO: Versions found [{samples.operator.openshift.io/v1 v1}]
Sep 27 19:51:13.968: INFO: samples.operator.openshift.io/v1 matches samples.operator.openshift.io/v1
Sep 27 19:51:13.968: INFO: Checking APIGroup: security.internal.openshift.io
Sep 27 19:51:13.982: INFO: PreferredVersion.GroupVersion: security.internal.openshift.io/v1
Sep 27 19:51:13.982: INFO: Versions found [{security.internal.openshift.io/v1 v1}]
Sep 27 19:51:13.982: INFO: security.internal.openshift.io/v1 matches security.internal.openshift.io/v1
Sep 27 19:51:13.982: INFO: Checking APIGroup: tuned.openshift.io
Sep 27 19:51:13.998: INFO: PreferredVersion.GroupVersion: tuned.openshift.io/v1
Sep 27 19:51:13.998: INFO: Versions found [{tuned.openshift.io/v1 v1}]
Sep 27 19:51:13.998: INFO: tuned.openshift.io/v1 matches tuned.openshift.io/v1
Sep 27 19:51:13.998: INFO: Checking APIGroup: ibm.com
Sep 27 19:51:14.004: INFO: PreferredVersion.GroupVersion: ibm.com/v1alpha1
Sep 27 19:51:14.004: INFO: Versions found [{ibm.com/v1alpha1 v1alpha1}]
Sep 27 19:51:14.004: INFO: ibm.com/v1alpha1 matches ibm.com/v1alpha1
Sep 27 19:51:14.004: INFO: Checking APIGroup: metal3.io
Sep 27 19:51:14.011: INFO: PreferredVersion.GroupVersion: metal3.io/v1alpha1
Sep 27 19:51:14.011: INFO: Versions found [{metal3.io/v1alpha1 v1alpha1}]
Sep 27 19:51:14.011: INFO: metal3.io/v1alpha1 matches metal3.io/v1alpha1
Sep 27 19:51:14.011: INFO: Checking APIGroup: migration.k8s.io
Sep 27 19:51:14.017: INFO: PreferredVersion.GroupVersion: migration.k8s.io/v1alpha1
Sep 27 19:51:14.017: INFO: Versions found [{migration.k8s.io/v1alpha1 v1alpha1}]
Sep 27 19:51:14.017: INFO: migration.k8s.io/v1alpha1 matches migration.k8s.io/v1alpha1
Sep 27 19:51:14.017: INFO: Checking APIGroup: whereabouts.cni.cncf.io
Sep 27 19:51:14.023: INFO: PreferredVersion.GroupVersion: whereabouts.cni.cncf.io/v1alpha1
Sep 27 19:51:14.023: INFO: Versions found [{whereabouts.cni.cncf.io/v1alpha1 v1alpha1}]
Sep 27 19:51:14.023: INFO: whereabouts.cni.cncf.io/v1alpha1 matches whereabouts.cni.cncf.io/v1alpha1
Sep 27 19:51:14.023: INFO: Checking APIGroup: helm.openshift.io
Sep 27 19:51:14.030: INFO: PreferredVersion.GroupVersion: helm.openshift.io/v1beta1
Sep 27 19:51:14.030: INFO: Versions found [{helm.openshift.io/v1beta1 v1beta1}]
Sep 27 19:51:14.030: INFO: helm.openshift.io/v1beta1 matches helm.openshift.io/v1beta1
Sep 27 19:51:14.030: INFO: Checking APIGroup: snapshot.storage.k8s.io
Sep 27 19:51:14.038: INFO: PreferredVersion.GroupVersion: snapshot.storage.k8s.io/v1beta1
Sep 27 19:51:14.038: INFO: Versions found [{snapshot.storage.k8s.io/v1beta1 v1beta1}]
Sep 27 19:51:14.038: INFO: snapshot.storage.k8s.io/v1beta1 matches snapshot.storage.k8s.io/v1beta1
Sep 27 19:51:14.038: INFO: Checking APIGroup: metrics.k8s.io
Sep 27 19:51:14.047: INFO: PreferredVersion.GroupVersion: metrics.k8s.io/v1beta1
Sep 27 19:51:14.048: INFO: Versions found [{metrics.k8s.io/v1beta1 v1beta1}]
Sep 27 19:51:14.048: INFO: metrics.k8s.io/v1beta1 matches metrics.k8s.io/v1beta1
[AfterEach] [sig-api-machinery] Discovery
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 19:51:14.048: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "discovery-5401" for this suite.
•{"msg":"PASSED [sig-api-machinery] Discovery should validate PreferredVersion for each APIGroup [Conformance]","total":305,"completed":45,"skipped":717,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 19:51:14.099: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name projected-configmap-test-volume-map-eb47244a-1bff-4e82-a53a-af362cdc9340
STEP: Creating a pod to test consume configMaps
Sep 27 19:51:14.346: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-377a9f7f-a16c-4c18-9f9b-5fa2aeba2e26" in namespace "projected-8626" to be "Succeeded or Failed"
Sep 27 19:51:14.360: INFO: Pod "pod-projected-configmaps-377a9f7f-a16c-4c18-9f9b-5fa2aeba2e26": Phase="Pending", Reason="", readiness=false. Elapsed: 13.633593ms
Sep 27 19:51:16.374: INFO: Pod "pod-projected-configmaps-377a9f7f-a16c-4c18-9f9b-5fa2aeba2e26": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028093867s
Sep 27 19:51:18.388: INFO: Pod "pod-projected-configmaps-377a9f7f-a16c-4c18-9f9b-5fa2aeba2e26": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.042372428s
STEP: Saw pod success
Sep 27 19:51:18.388: INFO: Pod "pod-projected-configmaps-377a9f7f-a16c-4c18-9f9b-5fa2aeba2e26" satisfied condition "Succeeded or Failed"
Sep 27 19:51:18.402: INFO: Trying to get logs from node 10.177.248.126 pod pod-projected-configmaps-377a9f7f-a16c-4c18-9f9b-5fa2aeba2e26 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Sep 27 19:51:18.519: INFO: Waiting for pod pod-projected-configmaps-377a9f7f-a16c-4c18-9f9b-5fa2aeba2e26 to disappear
Sep 27 19:51:18.532: INFO: Pod pod-projected-configmaps-377a9f7f-a16c-4c18-9f9b-5fa2aeba2e26 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 19:51:18.532: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8626" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":305,"completed":46,"skipped":728,"failed":0}
SSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 19:51:18.577: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Sep 27 19:51:21.824: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 19:51:21.873: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-7902" for this suite.
•{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":305,"completed":47,"skipped":741,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 19:51:21.916: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: modifying the configmap a second time
STEP: deleting the configmap
STEP: creating a watch on configmaps from the resource version returned by the first update
STEP: Expecting to observe notifications for all changes to the configmap after the first update
Sep 27 19:51:22.209: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-7020 /api/v1/namespaces/watch-7020/configmaps/e2e-watch-test-resource-version a705221c-c4dc-4111-b5c3-ba62deafe909 65294 0 2021-09-27 19:51:22 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  [{e2e.test Update v1 2021-09-27 19:51:22 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Sep 27 19:51:22.210: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-7020 /api/v1/namespaces/watch-7020/configmaps/e2e-watch-test-resource-version a705221c-c4dc-4111-b5c3-ba62deafe909 65295 0 2021-09-27 19:51:22 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  [{e2e.test Update v1 2021-09-27 19:51:22 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 19:51:22.210: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-7020" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should be able to start watching from a specific resource version [Conformance]","total":305,"completed":48,"skipped":755,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 19:51:22.257: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Sep 27 19:51:22.422: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
STEP: Creating configMap with name configmap-test-upd-f57dd102-0ed1-4d18-976c-25610da11fbb
STEP: Creating the pod
STEP: Waiting for pod with text data
STEP: Waiting for pod with binary data
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 19:51:26.602: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7154" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap binary data should be reflected in volume [NodeConformance] [Conformance]","total":305,"completed":49,"skipped":780,"failed":0}
S
------------------------------
[k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial] 
  removing taint cancels eviction [Disruptive] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 19:51:26.663: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename taint-single-pod
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/taints.go:164
Sep 27 19:51:26.786: INFO: Waiting up to 1m0s for all nodes to be ready
Sep 27 19:52:26.935: INFO: Waiting for terminating namespaces to be deleted...
[It] removing taint cancels eviction [Disruptive] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Sep 27 19:52:26.957: INFO: Starting informer...
STEP: Starting pod...
Sep 27 19:52:27.239: INFO: Pod is running on 10.177.248.117. Tainting Node
STEP: Trying to apply a taint on the Node
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting short time to make sure Pod is queued for deletion
Sep 27 19:52:27.300: INFO: Pod wasn't evicted. Proceeding
Sep 27 19:52:27.300: INFO: Removing taint from Node
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting some time to make sure that toleration time passed.
Sep 27 19:53:42.370: INFO: Pod wasn't evicted. Test successful
[AfterEach] [k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 19:53:42.370: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-single-pod-4365" for this suite.

• [SLOW TEST:135.755 seconds]
[k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial]
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  removing taint cancels eviction [Disruptive] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial] removing taint cancels eviction [Disruptive] [Conformance]","total":305,"completed":50,"skipped":781,"failed":0}
SSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 19:53:42.418: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a watch on configmaps with a certain label
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: changing the label value of the configmap
STEP: Expecting to observe a delete notification for the watched object
Sep 27 19:53:42.670: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-1884 /api/v1/namespaces/watch-1884/configmaps/e2e-watch-test-label-changed 6ddc8ff2-8723-4a8c-a47b-fb9d767caf69 66636 0 2021-09-27 19:53:42 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-09-27 19:53:42 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Sep 27 19:53:42.671: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-1884 /api/v1/namespaces/watch-1884/configmaps/e2e-watch-test-label-changed 6ddc8ff2-8723-4a8c-a47b-fb9d767caf69 66640 0 2021-09-27 19:53:42 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-09-27 19:53:42 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Sep 27 19:53:42.671: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-1884 /api/v1/namespaces/watch-1884/configmaps/e2e-watch-test-label-changed 6ddc8ff2-8723-4a8c-a47b-fb9d767caf69 66642 0 2021-09-27 19:53:42 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-09-27 19:53:42 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements
STEP: changing the label value of the configmap back
STEP: modifying the configmap a third time
STEP: deleting the configmap
STEP: Expecting to observe an add notification for the watched object when the label value was restored
Sep 27 19:53:52.814: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-1884 /api/v1/namespaces/watch-1884/configmaps/e2e-watch-test-label-changed 6ddc8ff2-8723-4a8c-a47b-fb9d767caf69 66716 0 2021-09-27 19:53:42 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-09-27 19:53:52 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Sep 27 19:53:52.814: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-1884 /api/v1/namespaces/watch-1884/configmaps/e2e-watch-test-label-changed 6ddc8ff2-8723-4a8c-a47b-fb9d767caf69 66717 0 2021-09-27 19:53:42 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-09-27 19:53:52 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
Sep 27 19:53:52.814: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-1884 /api/v1/namespaces/watch-1884/configmaps/e2e-watch-test-label-changed 6ddc8ff2-8723-4a8c-a47b-fb9d767caf69 66719 0 2021-09-27 19:53:42 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-09-27 19:53:52 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 19:53:52.814: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-1884" for this suite.

• [SLOW TEST:10.438 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should observe an object deletion if it stops meeting the requirements of the selector [Conformance]","total":305,"completed":51,"skipped":788,"failed":0}
S
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 19:53:52.857: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[It] should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Starting the proxy
Sep 27 19:53:53.026: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-679324930 proxy --unix-socket=/tmp/kubectl-proxy-unix272198921/test'
STEP: retrieving proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 19:53:53.130: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6356" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support --unix-socket=/path  [Conformance]","total":305,"completed":52,"skipped":789,"failed":0}
SSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] PreStop 
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 19:53:53.175: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename prestop
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pre_stop.go:171
[It] should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating server pod server in namespace prestop-3808
STEP: Waiting for pods to come up.
STEP: Creating tester pod tester in namespace prestop-3808
STEP: Deleting pre-stop pod
Sep 27 19:54:06.549: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod
[AfterEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 19:54:06.577: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "prestop-3808" for this suite.

• [SLOW TEST:13.443 seconds]
[k8s.io] [sig-node] PreStop
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] [sig-node] PreStop should call prestop when killing a pod  [Conformance]","total":305,"completed":53,"skipped":801,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 19:54:06.619: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name projected-configmap-test-volume-0488b390-d748-4e18-a961-f750dbc2b798
STEP: Creating a pod to test consume configMaps
Sep 27 19:54:06.833: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-487b4308-4584-4ef8-94ff-b66bf372b7bd" in namespace "projected-4791" to be "Succeeded or Failed"
Sep 27 19:54:06.848: INFO: Pod "pod-projected-configmaps-487b4308-4584-4ef8-94ff-b66bf372b7bd": Phase="Pending", Reason="", readiness=false. Elapsed: 13.979155ms
Sep 27 19:54:08.863: INFO: Pod "pod-projected-configmaps-487b4308-4584-4ef8-94ff-b66bf372b7bd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028896471s
Sep 27 19:54:10.878: INFO: Pod "pod-projected-configmaps-487b4308-4584-4ef8-94ff-b66bf372b7bd": Phase="Pending", Reason="", readiness=false. Elapsed: 4.043441539s
Sep 27 19:54:12.891: INFO: Pod "pod-projected-configmaps-487b4308-4584-4ef8-94ff-b66bf372b7bd": Phase="Pending", Reason="", readiness=false. Elapsed: 6.056640505s
Sep 27 19:54:14.905: INFO: Pod "pod-projected-configmaps-487b4308-4584-4ef8-94ff-b66bf372b7bd": Phase="Pending", Reason="", readiness=false. Elapsed: 8.07096184s
Sep 27 19:54:16.919: INFO: Pod "pod-projected-configmaps-487b4308-4584-4ef8-94ff-b66bf372b7bd": Phase="Pending", Reason="", readiness=false. Elapsed: 10.085102626s
Sep 27 19:54:18.934: INFO: Pod "pod-projected-configmaps-487b4308-4584-4ef8-94ff-b66bf372b7bd": Phase="Pending", Reason="", readiness=false. Elapsed: 12.099550433s
Sep 27 19:54:20.947: INFO: Pod "pod-projected-configmaps-487b4308-4584-4ef8-94ff-b66bf372b7bd": Phase="Pending", Reason="", readiness=false. Elapsed: 14.112286789s
Sep 27 19:54:22.961: INFO: Pod "pod-projected-configmaps-487b4308-4584-4ef8-94ff-b66bf372b7bd": Phase="Pending", Reason="", readiness=false. Elapsed: 16.126218776s
Sep 27 19:54:24.975: INFO: Pod "pod-projected-configmaps-487b4308-4584-4ef8-94ff-b66bf372b7bd": Phase="Pending", Reason="", readiness=false. Elapsed: 18.140472582s
Sep 27 19:54:26.990: INFO: Pod "pod-projected-configmaps-487b4308-4584-4ef8-94ff-b66bf372b7bd": Phase="Pending", Reason="", readiness=false. Elapsed: 20.155578095s
Sep 27 19:54:29.003: INFO: Pod "pod-projected-configmaps-487b4308-4584-4ef8-94ff-b66bf372b7bd": Phase="Pending", Reason="", readiness=false. Elapsed: 22.169007844s
Sep 27 19:54:31.017: INFO: Pod "pod-projected-configmaps-487b4308-4584-4ef8-94ff-b66bf372b7bd": Phase="Pending", Reason="", readiness=false. Elapsed: 24.182733268s
Sep 27 19:54:33.029: INFO: Pod "pod-projected-configmaps-487b4308-4584-4ef8-94ff-b66bf372b7bd": Phase="Pending", Reason="", readiness=false. Elapsed: 26.194861354s
Sep 27 19:54:35.044: INFO: Pod "pod-projected-configmaps-487b4308-4584-4ef8-94ff-b66bf372b7bd": Phase="Pending", Reason="", readiness=false. Elapsed: 28.209305369s
Sep 27 19:54:37.061: INFO: Pod "pod-projected-configmaps-487b4308-4584-4ef8-94ff-b66bf372b7bd": Phase="Pending", Reason="", readiness=false. Elapsed: 30.226631192s
Sep 27 19:54:39.078: INFO: Pod "pod-projected-configmaps-487b4308-4584-4ef8-94ff-b66bf372b7bd": Phase="Pending", Reason="", readiness=false. Elapsed: 32.244138538s
Sep 27 19:54:41.097: INFO: Pod "pod-projected-configmaps-487b4308-4584-4ef8-94ff-b66bf372b7bd": Phase="Pending", Reason="", readiness=false. Elapsed: 34.262329478s
Sep 27 19:54:43.111: INFO: Pod "pod-projected-configmaps-487b4308-4584-4ef8-94ff-b66bf372b7bd": Phase="Pending", Reason="", readiness=false. Elapsed: 36.277049073s
Sep 27 19:54:45.127: INFO: Pod "pod-projected-configmaps-487b4308-4584-4ef8-94ff-b66bf372b7bd": Phase="Pending", Reason="", readiness=false. Elapsed: 38.292460538s
Sep 27 19:54:47.142: INFO: Pod "pod-projected-configmaps-487b4308-4584-4ef8-94ff-b66bf372b7bd": Phase="Pending", Reason="", readiness=false. Elapsed: 40.307498966s
Sep 27 19:54:49.156: INFO: Pod "pod-projected-configmaps-487b4308-4584-4ef8-94ff-b66bf372b7bd": Phase="Pending", Reason="", readiness=false. Elapsed: 42.321678971s
Sep 27 19:54:51.170: INFO: Pod "pod-projected-configmaps-487b4308-4584-4ef8-94ff-b66bf372b7bd": Phase="Pending", Reason="", readiness=false. Elapsed: 44.336000151s
Sep 27 19:54:53.187: INFO: Pod "pod-projected-configmaps-487b4308-4584-4ef8-94ff-b66bf372b7bd": Phase="Pending", Reason="", readiness=false. Elapsed: 46.352457029s
Sep 27 19:54:55.203: INFO: Pod "pod-projected-configmaps-487b4308-4584-4ef8-94ff-b66bf372b7bd": Phase="Pending", Reason="", readiness=false. Elapsed: 48.368673621s
Sep 27 19:54:57.219: INFO: Pod "pod-projected-configmaps-487b4308-4584-4ef8-94ff-b66bf372b7bd": Phase="Pending", Reason="", readiness=false. Elapsed: 50.384309237s
Sep 27 19:54:59.232: INFO: Pod "pod-projected-configmaps-487b4308-4584-4ef8-94ff-b66bf372b7bd": Phase="Pending", Reason="", readiness=false. Elapsed: 52.398020347s
Sep 27 19:55:01.247: INFO: Pod "pod-projected-configmaps-487b4308-4584-4ef8-94ff-b66bf372b7bd": Phase="Pending", Reason="", readiness=false. Elapsed: 54.412602461s
Sep 27 19:55:03.262: INFO: Pod "pod-projected-configmaps-487b4308-4584-4ef8-94ff-b66bf372b7bd": Phase="Pending", Reason="", readiness=false. Elapsed: 56.427544231s
Sep 27 19:55:05.278: INFO: Pod "pod-projected-configmaps-487b4308-4584-4ef8-94ff-b66bf372b7bd": Phase="Pending", Reason="", readiness=false. Elapsed: 58.443616096s
Sep 27 19:55:07.295: INFO: Pod "pod-projected-configmaps-487b4308-4584-4ef8-94ff-b66bf372b7bd": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.460276205s
Sep 27 19:55:09.309: INFO: Pod "pod-projected-configmaps-487b4308-4584-4ef8-94ff-b66bf372b7bd": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.474790661s
Sep 27 19:55:11.326: INFO: Pod "pod-projected-configmaps-487b4308-4584-4ef8-94ff-b66bf372b7bd": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.492121986s
Sep 27 19:55:13.343: INFO: Pod "pod-projected-configmaps-487b4308-4584-4ef8-94ff-b66bf372b7bd": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.5083963s
Sep 27 19:55:15.362: INFO: Pod "pod-projected-configmaps-487b4308-4584-4ef8-94ff-b66bf372b7bd": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.527660568s
Sep 27 19:55:17.375: INFO: Pod "pod-projected-configmaps-487b4308-4584-4ef8-94ff-b66bf372b7bd": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.540343652s
Sep 27 19:55:19.393: INFO: Pod "pod-projected-configmaps-487b4308-4584-4ef8-94ff-b66bf372b7bd": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.559011679s
Sep 27 19:55:21.437: INFO: Pod "pod-projected-configmaps-487b4308-4584-4ef8-94ff-b66bf372b7bd": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.603103698s
Sep 27 19:55:23.454: INFO: Pod "pod-projected-configmaps-487b4308-4584-4ef8-94ff-b66bf372b7bd": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.619889419s
Sep 27 19:55:25.468: INFO: Pod "pod-projected-configmaps-487b4308-4584-4ef8-94ff-b66bf372b7bd": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.63417184s
Sep 27 19:55:27.483: INFO: Pod "pod-projected-configmaps-487b4308-4584-4ef8-94ff-b66bf372b7bd": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.648972349s
Sep 27 19:55:29.497: INFO: Pod "pod-projected-configmaps-487b4308-4584-4ef8-94ff-b66bf372b7bd": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.662378605s
Sep 27 19:55:31.511: INFO: Pod "pod-projected-configmaps-487b4308-4584-4ef8-94ff-b66bf372b7bd": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.677012222s
Sep 27 19:55:33.525: INFO: Pod "pod-projected-configmaps-487b4308-4584-4ef8-94ff-b66bf372b7bd": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.691075514s
Sep 27 19:55:35.540: INFO: Pod "pod-projected-configmaps-487b4308-4584-4ef8-94ff-b66bf372b7bd": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.70559594s
Sep 27 19:55:37.554: INFO: Pod "pod-projected-configmaps-487b4308-4584-4ef8-94ff-b66bf372b7bd": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.719351328s
Sep 27 19:55:39.584: INFO: Pod "pod-projected-configmaps-487b4308-4584-4ef8-94ff-b66bf372b7bd": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.749392075s
Sep 27 19:55:41.599: INFO: Pod "pod-projected-configmaps-487b4308-4584-4ef8-94ff-b66bf372b7bd": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.764370229s
Sep 27 19:55:43.667: INFO: Pod "pod-projected-configmaps-487b4308-4584-4ef8-94ff-b66bf372b7bd": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.832236639s
Sep 27 19:55:45.725: INFO: Pod "pod-projected-configmaps-487b4308-4584-4ef8-94ff-b66bf372b7bd": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.890246134s
Sep 27 19:55:47.749: INFO: Pod "pod-projected-configmaps-487b4308-4584-4ef8-94ff-b66bf372b7bd": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.914273803s
Sep 27 19:55:49.763: INFO: Pod "pod-projected-configmaps-487b4308-4584-4ef8-94ff-b66bf372b7bd": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.928375269s
Sep 27 19:55:51.778: INFO: Pod "pod-projected-configmaps-487b4308-4584-4ef8-94ff-b66bf372b7bd": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.943582101s
Sep 27 19:55:53.794: INFO: Pod "pod-projected-configmaps-487b4308-4584-4ef8-94ff-b66bf372b7bd": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.959242565s
Sep 27 19:55:55.807: INFO: Pod "pod-projected-configmaps-487b4308-4584-4ef8-94ff-b66bf372b7bd": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.973180589s
Sep 27 19:55:57.822: INFO: Pod "pod-projected-configmaps-487b4308-4584-4ef8-94ff-b66bf372b7bd": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.988061955s
Sep 27 19:55:59.837: INFO: Pod "pod-projected-configmaps-487b4308-4584-4ef8-94ff-b66bf372b7bd": Phase="Pending", Reason="", readiness=false. Elapsed: 1m53.002580499s
Sep 27 19:56:01.873: INFO: Pod "pod-projected-configmaps-487b4308-4584-4ef8-94ff-b66bf372b7bd": Phase="Pending", Reason="", readiness=false. Elapsed: 1m55.038344864s
Sep 27 19:56:03.911: INFO: Pod "pod-projected-configmaps-487b4308-4584-4ef8-94ff-b66bf372b7bd": Phase="Pending", Reason="", readiness=false. Elapsed: 1m57.076340407s
Sep 27 19:56:05.926: INFO: Pod "pod-projected-configmaps-487b4308-4584-4ef8-94ff-b66bf372b7bd": Phase="Pending", Reason="", readiness=false. Elapsed: 1m59.091359974s
Sep 27 19:56:07.968: INFO: Pod "pod-projected-configmaps-487b4308-4584-4ef8-94ff-b66bf372b7bd": Phase="Pending", Reason="", readiness=false. Elapsed: 2m1.133327704s
Sep 27 19:56:09.986: INFO: Pod "pod-projected-configmaps-487b4308-4584-4ef8-94ff-b66bf372b7bd": Phase="Pending", Reason="", readiness=false. Elapsed: 2m3.151807015s
Sep 27 19:56:12.001: INFO: Pod "pod-projected-configmaps-487b4308-4584-4ef8-94ff-b66bf372b7bd": Phase="Pending", Reason="", readiness=false. Elapsed: 2m5.166761106s
Sep 27 19:56:14.016: INFO: Pod "pod-projected-configmaps-487b4308-4584-4ef8-94ff-b66bf372b7bd": Phase="Pending", Reason="", readiness=false. Elapsed: 2m7.181260637s
Sep 27 19:56:16.029: INFO: Pod "pod-projected-configmaps-487b4308-4584-4ef8-94ff-b66bf372b7bd": Phase="Pending", Reason="", readiness=false. Elapsed: 2m9.194423626s
Sep 27 19:56:18.044: INFO: Pod "pod-projected-configmaps-487b4308-4584-4ef8-94ff-b66bf372b7bd": Phase="Pending", Reason="", readiness=false. Elapsed: 2m11.209415845s
Sep 27 19:56:20.074: INFO: Pod "pod-projected-configmaps-487b4308-4584-4ef8-94ff-b66bf372b7bd": Phase="Pending", Reason="", readiness=false. Elapsed: 2m13.239264362s
Sep 27 19:56:22.117: INFO: Pod "pod-projected-configmaps-487b4308-4584-4ef8-94ff-b66bf372b7bd": Phase="Pending", Reason="", readiness=false. Elapsed: 2m15.283064784s
Sep 27 19:56:24.132: INFO: Pod "pod-projected-configmaps-487b4308-4584-4ef8-94ff-b66bf372b7bd": Phase="Pending", Reason="", readiness=false. Elapsed: 2m17.297234604s
Sep 27 19:56:26.152: INFO: Pod "pod-projected-configmaps-487b4308-4584-4ef8-94ff-b66bf372b7bd": Phase="Pending", Reason="", readiness=false. Elapsed: 2m19.317494765s
Sep 27 19:56:28.168: INFO: Pod "pod-projected-configmaps-487b4308-4584-4ef8-94ff-b66bf372b7bd": Phase="Pending", Reason="", readiness=false. Elapsed: 2m21.333670437s
Sep 27 19:56:30.182: INFO: Pod "pod-projected-configmaps-487b4308-4584-4ef8-94ff-b66bf372b7bd": Phase="Pending", Reason="", readiness=false. Elapsed: 2m23.347797427s
Sep 27 19:56:32.195: INFO: Pod "pod-projected-configmaps-487b4308-4584-4ef8-94ff-b66bf372b7bd": Phase="Pending", Reason="", readiness=false. Elapsed: 2m25.361031323s
Sep 27 19:56:34.213: INFO: Pod "pod-projected-configmaps-487b4308-4584-4ef8-94ff-b66bf372b7bd": Phase="Pending", Reason="", readiness=false. Elapsed: 2m27.378674715s
Sep 27 19:56:36.227: INFO: Pod "pod-projected-configmaps-487b4308-4584-4ef8-94ff-b66bf372b7bd": Phase="Pending", Reason="", readiness=false. Elapsed: 2m29.392842413s
Sep 27 19:56:38.241: INFO: Pod "pod-projected-configmaps-487b4308-4584-4ef8-94ff-b66bf372b7bd": Phase="Pending", Reason="", readiness=false. Elapsed: 2m31.406322171s
Sep 27 19:56:40.254: INFO: Pod "pod-projected-configmaps-487b4308-4584-4ef8-94ff-b66bf372b7bd": Phase="Pending", Reason="", readiness=false. Elapsed: 2m33.419503235s
Sep 27 19:56:42.301: INFO: Pod "pod-projected-configmaps-487b4308-4584-4ef8-94ff-b66bf372b7bd": Phase="Pending", Reason="", readiness=false. Elapsed: 2m35.466743299s
Sep 27 19:56:44.324: INFO: Pod "pod-projected-configmaps-487b4308-4584-4ef8-94ff-b66bf372b7bd": Phase="Pending", Reason="", readiness=false. Elapsed: 2m37.48929901s
Sep 27 19:56:46.355: INFO: Pod "pod-projected-configmaps-487b4308-4584-4ef8-94ff-b66bf372b7bd": Phase="Pending", Reason="", readiness=false. Elapsed: 2m39.520385812s
Sep 27 19:56:48.370: INFO: Pod "pod-projected-configmaps-487b4308-4584-4ef8-94ff-b66bf372b7bd": Phase="Pending", Reason="", readiness=false. Elapsed: 2m41.535421233s
Sep 27 19:56:50.392: INFO: Pod "pod-projected-configmaps-487b4308-4584-4ef8-94ff-b66bf372b7bd": Phase="Pending", Reason="", readiness=false. Elapsed: 2m43.557891907s
Sep 27 19:56:52.408: INFO: Pod "pod-projected-configmaps-487b4308-4584-4ef8-94ff-b66bf372b7bd": Phase="Pending", Reason="", readiness=false. Elapsed: 2m45.573418325s
Sep 27 19:56:54.422: INFO: Pod "pod-projected-configmaps-487b4308-4584-4ef8-94ff-b66bf372b7bd": Phase="Pending", Reason="", readiness=false. Elapsed: 2m47.587782353s
Sep 27 19:56:56.436: INFO: Pod "pod-projected-configmaps-487b4308-4584-4ef8-94ff-b66bf372b7bd": Phase="Pending", Reason="", readiness=false. Elapsed: 2m49.60122212s
Sep 27 19:56:58.450: INFO: Pod "pod-projected-configmaps-487b4308-4584-4ef8-94ff-b66bf372b7bd": Phase="Pending", Reason="", readiness=false. Elapsed: 2m51.615316636s
Sep 27 19:57:00.463: INFO: Pod "pod-projected-configmaps-487b4308-4584-4ef8-94ff-b66bf372b7bd": Phase="Pending", Reason="", readiness=false. Elapsed: 2m53.628667693s
Sep 27 19:57:02.476: INFO: Pod "pod-projected-configmaps-487b4308-4584-4ef8-94ff-b66bf372b7bd": Phase="Pending", Reason="", readiness=false. Elapsed: 2m55.641733501s
Sep 27 19:57:04.492: INFO: Pod "pod-projected-configmaps-487b4308-4584-4ef8-94ff-b66bf372b7bd": Phase="Pending", Reason="", readiness=false. Elapsed: 2m57.657204846s
Sep 27 19:57:06.506: INFO: Pod "pod-projected-configmaps-487b4308-4584-4ef8-94ff-b66bf372b7bd": Phase="Pending", Reason="", readiness=false. Elapsed: 2m59.671275461s
Sep 27 19:57:08.520: INFO: Pod "pod-projected-configmaps-487b4308-4584-4ef8-94ff-b66bf372b7bd": Phase="Pending", Reason="", readiness=false. Elapsed: 3m1.685354158s
Sep 27 19:57:10.533: INFO: Pod "pod-projected-configmaps-487b4308-4584-4ef8-94ff-b66bf372b7bd": Phase="Pending", Reason="", readiness=false. Elapsed: 3m3.698946921s
Sep 27 19:57:12.547: INFO: Pod "pod-projected-configmaps-487b4308-4584-4ef8-94ff-b66bf372b7bd": Phase="Pending", Reason="", readiness=false. Elapsed: 3m5.712834321s
Sep 27 19:57:14.575: INFO: Pod "pod-projected-configmaps-487b4308-4584-4ef8-94ff-b66bf372b7bd": Phase="Pending", Reason="", readiness=false. Elapsed: 3m7.740747714s
Sep 27 19:57:16.594: INFO: Pod "pod-projected-configmaps-487b4308-4584-4ef8-94ff-b66bf372b7bd": Phase="Pending", Reason="", readiness=false. Elapsed: 3m9.759540139s
Sep 27 19:57:18.608: INFO: Pod "pod-projected-configmaps-487b4308-4584-4ef8-94ff-b66bf372b7bd": Phase="Pending", Reason="", readiness=false. Elapsed: 3m11.773267936s
Sep 27 19:57:20.622: INFO: Pod "pod-projected-configmaps-487b4308-4584-4ef8-94ff-b66bf372b7bd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 3m13.787604042s
STEP: Saw pod success
Sep 27 19:57:20.622: INFO: Pod "pod-projected-configmaps-487b4308-4584-4ef8-94ff-b66bf372b7bd" satisfied condition "Succeeded or Failed"
Sep 27 19:57:20.636: INFO: Trying to get logs from node 10.177.248.117 pod pod-projected-configmaps-487b4308-4584-4ef8-94ff-b66bf372b7bd container projected-configmap-volume-test: <nil>
STEP: delete the pod
Sep 27 19:57:20.830: INFO: Waiting for pod pod-projected-configmaps-487b4308-4584-4ef8-94ff-b66bf372b7bd to disappear
Sep 27 19:57:20.847: INFO: Pod pod-projected-configmaps-487b4308-4584-4ef8-94ff-b66bf372b7bd no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 19:57:20.847: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4791" for this suite.

• [SLOW TEST:194.276 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:36
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume [NodeConformance] [Conformance]","total":305,"completed":54,"skipped":810,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl replace 
  should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 19:57:20.895: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[BeforeEach] Kubectl replace
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1581
[It] should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Sep 27 19:57:21.022: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-679324930 run e2e-test-httpd-pod --image=docker.io/library/httpd:2.4.38-alpine --labels=run=e2e-test-httpd-pod --namespace=kubectl-5577'
Sep 27 19:57:21.436: INFO: stderr: ""
Sep 27 19:57:21.436: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod is running
STEP: verifying the pod e2e-test-httpd-pod was created
Sep 27 19:57:26.487: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-679324930 get pod e2e-test-httpd-pod --namespace=kubectl-5577 -o json'
Sep 27 19:57:26.629: INFO: stderr: ""
Sep 27 19:57:26.629: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"cni.projectcalico.org/podIP\": \"172.30.75.32/32\",\n            \"cni.projectcalico.org/podIPs\": \"172.30.75.32/32\",\n            \"k8s.v1.cni.cncf.io/network-status\": \"[{\\n    \\\"name\\\": \\\"\\\",\\n    \\\"ips\\\": [\\n        \\\"172.30.75.32\\\"\\n    ],\\n    \\\"default\\\": true,\\n    \\\"dns\\\": {}\\n}]\",\n            \"k8s.v1.cni.cncf.io/networks-status\": \"[{\\n    \\\"name\\\": \\\"\\\",\\n    \\\"ips\\\": [\\n        \\\"172.30.75.32\\\"\\n    ],\\n    \\\"default\\\": true,\\n    \\\"dns\\\": {}\\n}]\",\n            \"openshift.io/scc\": \"anyuid\"\n        },\n        \"creationTimestamp\": \"2021-09-27T19:57:21Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"managedFields\": [\n            {\n                \"apiVersion\": \"v1\",\n                \"fieldsType\": \"FieldsV1\",\n                \"fieldsV1\": {\n                    \"f:metadata\": {\n                        \"f:labels\": {\n                            \".\": {},\n                            \"f:run\": {}\n                        }\n                    },\n                    \"f:spec\": {\n                        \"f:containers\": {\n                            \"k:{\\\"name\\\":\\\"e2e-test-httpd-pod\\\"}\": {\n                                \".\": {},\n                                \"f:image\": {},\n                                \"f:imagePullPolicy\": {},\n                                \"f:name\": {},\n                                \"f:resources\": {},\n                                \"f:terminationMessagePath\": {},\n                                \"f:terminationMessagePolicy\": {}\n                            }\n                        },\n                        \"f:dnsPolicy\": {},\n                        \"f:enableServiceLinks\": {},\n                        \"f:restartPolicy\": {},\n                        \"f:schedulerName\": {},\n                        \"f:securityContext\": {},\n                        \"f:terminationGracePeriodSeconds\": {}\n                    }\n                },\n                \"manager\": \"kubectl-run\",\n                \"operation\": \"Update\",\n                \"time\": \"2021-09-27T19:57:21Z\"\n            },\n            {\n                \"apiVersion\": \"v1\",\n                \"fieldsType\": \"FieldsV1\",\n                \"fieldsV1\": {\n                    \"f:metadata\": {\n                        \"f:annotations\": {\n                            \"f:cni.projectcalico.org/podIP\": {},\n                            \"f:cni.projectcalico.org/podIPs\": {}\n                        }\n                    }\n                },\n                \"manager\": \"calico\",\n                \"operation\": \"Update\",\n                \"time\": \"2021-09-27T19:57:22Z\"\n            },\n            {\n                \"apiVersion\": \"v1\",\n                \"fieldsType\": \"FieldsV1\",\n                \"fieldsV1\": {\n                    \"f:metadata\": {\n                        \"f:annotations\": {\n                            \"f:k8s.v1.cni.cncf.io/network-status\": {},\n                            \"f:k8s.v1.cni.cncf.io/networks-status\": {}\n                        }\n                    }\n                },\n                \"manager\": \"multus\",\n                \"operation\": \"Update\",\n                \"time\": \"2021-09-27T19:57:22Z\"\n            },\n            {\n                \"apiVersion\": \"v1\",\n                \"fieldsType\": \"FieldsV1\",\n                \"fieldsV1\": {\n                    \"f:status\": {\n                        \"f:conditions\": {\n                            \"k:{\\\"type\\\":\\\"ContainersReady\\\"}\": {\n                                \".\": {},\n                                \"f:lastProbeTime\": {},\n                                \"f:lastTransitionTime\": {},\n                                \"f:status\": {},\n                                \"f:type\": {}\n                            },\n                            \"k:{\\\"type\\\":\\\"Initialized\\\"}\": {\n                                \".\": {},\n                                \"f:lastProbeTime\": {},\n                                \"f:lastTransitionTime\": {},\n                                \"f:status\": {},\n                                \"f:type\": {}\n                            },\n                            \"k:{\\\"type\\\":\\\"Ready\\\"}\": {\n                                \".\": {},\n                                \"f:lastProbeTime\": {},\n                                \"f:lastTransitionTime\": {},\n                                \"f:status\": {},\n                                \"f:type\": {}\n                            }\n                        },\n                        \"f:containerStatuses\": {},\n                        \"f:hostIP\": {},\n                        \"f:phase\": {},\n                        \"f:podIP\": {},\n                        \"f:podIPs\": {\n                            \".\": {},\n                            \"k:{\\\"ip\\\":\\\"172.30.75.32\\\"}\": {\n                                \".\": {},\n                                \"f:ip\": {}\n                            }\n                        },\n                        \"f:startTime\": {}\n                    }\n                },\n                \"manager\": \"kubelet\",\n                \"operation\": \"Update\",\n                \"time\": \"2021-09-27T19:57:23Z\"\n            }\n        ],\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-5577\",\n        \"resourceVersion\": \"68213\",\n        \"selfLink\": \"/api/v1/namespaces/kubectl-5577/pods/e2e-test-httpd-pod\",\n        \"uid\": \"c5d0ae1f-882d-4bc9-8644-2a992f56e76d\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"docker.io/library/httpd:2.4.38-alpine\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"securityContext\": {\n                    \"capabilities\": {\n                        \"drop\": [\n                            \"MKNOD\"\n                        ]\n                    }\n                },\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"default-token-pvjjc\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"imagePullSecrets\": [\n            {\n                \"name\": \"default-dockercfg-pm9hm\"\n            }\n        ],\n        \"nodeName\": \"10.177.248.117\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {\n            \"seLinuxOptions\": {\n                \"level\": \"s0:c37,c24\"\n            }\n        },\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"default-token-pvjjc\",\n                \"secret\": {\n                    \"defaultMode\": 420,\n                    \"secretName\": \"default-token-pvjjc\"\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2021-09-27T19:57:21Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2021-09-27T19:57:23Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2021-09-27T19:57:23Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2021-09-27T19:57:21Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"cri-o://df16a7f82d7d290a741e4dfd962a5ffa60b9911bada8fb2d86869c0b9ab08200\",\n                \"image\": \"docker.io/library/httpd:2.4.38-alpine\",\n                \"imageID\": \"docker.io/library/httpd@sha256:6feb0ea7b0967367da66e8d58ba813fde32bdb92f63bfc21a9e170d211539db4\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2021-09-27T19:57:23Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"10.177.248.117\",\n        \"phase\": \"Running\",\n        \"podIP\": \"172.30.75.32\",\n        \"podIPs\": [\n            {\n                \"ip\": \"172.30.75.32\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2021-09-27T19:57:21Z\"\n    }\n}\n"
STEP: replace the image in the pod
Sep 27 19:57:26.629: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-679324930 replace -f - --namespace=kubectl-5577'
Sep 27 19:57:27.429: INFO: stderr: ""
Sep 27 19:57:27.429: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image docker.io/library/busybox:1.29
[AfterEach] Kubectl replace
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1586
Sep 27 19:57:27.451: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-679324930 delete pods e2e-test-httpd-pod --namespace=kubectl-5577'
Sep 27 19:57:31.809: INFO: stderr: ""
Sep 27 19:57:31.809: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 19:57:31.810: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5577" for this suite.

• [SLOW TEST:10.976 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl replace
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1577
    should update a single-container pod's image  [Conformance]
    /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl replace should update a single-container pod's image  [Conformance]","total":305,"completed":55,"skipped":833,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should support configurable pod DNS nameservers [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 19:57:31.871: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support configurable pod DNS nameservers [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod with dnsPolicy=None and customized dnsConfig...
Sep 27 19:57:32.116: INFO: Created pod &Pod{ObjectMeta:{dns-1342  dns-1342 /api/v1/namespaces/dns-1342/pods/dns-1342 92803d02-2cb4-4ad3-9f8d-8dd9564f0b66 68382 0 2021-09-27 19:57:32 +0000 UTC <nil> <nil> map[] map[openshift.io/scc:anyuid] [] []  [{e2e.test Update v1 2021-09-27 19:57:32 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-94vlg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-94vlg,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:k8s.gcr.io/e2e-test-images/agnhost:2.20,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-94vlg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c37,c29,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 27 19:57:32.139: INFO: The status of Pod dns-1342 is Pending, waiting for it to be Running (with Ready = true)
Sep 27 19:57:34.154: INFO: The status of Pod dns-1342 is Pending, waiting for it to be Running (with Ready = true)
Sep 27 19:57:36.161: INFO: The status of Pod dns-1342 is Running (Ready = true)
STEP: Verifying customized DNS suffix list is configured on pod...
Sep 27 19:57:36.161: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-1342 PodName:dns-1342 ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep 27 19:57:36.161: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Verifying customized DNS server is configured on pod...
Sep 27 19:57:36.458: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-1342 PodName:dns-1342 ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep 27 19:57:36.458: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
Sep 27 19:57:36.753: INFO: Deleting pod dns-1342...
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 19:57:36.810: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-1342" for this suite.
•{"msg":"PASSED [sig-network] DNS should support configurable pod DNS nameservers [Conformance]","total":305,"completed":56,"skipped":870,"failed":0}
SSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 19:57:36.857: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for services  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-6253.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-6253.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-6253.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-6253.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-6253.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-6253.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-6253.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-6253.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-6253.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-6253.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-6253.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-6253.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-6253.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 131.11.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.11.131_udp@PTR;check="$$(dig +tcp +noall +answer +search 131.11.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.11.131_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-6253.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-6253.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-6253.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-6253.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-6253.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-6253.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-6253.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-6253.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-6253.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-6253.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-6253.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-6253.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-6253.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 131.11.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.11.131_udp@PTR;check="$$(dig +tcp +noall +answer +search 131.11.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.11.131_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Sep 27 19:57:41.254: INFO: Unable to read wheezy_udp@dns-test-service.dns-6253.svc.cluster.local from pod dns-6253/dns-test-b3f5bc9d-cf02-428b-a7e7-4fdc535e0ea5: the server could not find the requested resource (get pods dns-test-b3f5bc9d-cf02-428b-a7e7-4fdc535e0ea5)
Sep 27 19:57:41.281: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6253.svc.cluster.local from pod dns-6253/dns-test-b3f5bc9d-cf02-428b-a7e7-4fdc535e0ea5: the server could not find the requested resource (get pods dns-test-b3f5bc9d-cf02-428b-a7e7-4fdc535e0ea5)
Sep 27 19:57:41.308: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6253.svc.cluster.local from pod dns-6253/dns-test-b3f5bc9d-cf02-428b-a7e7-4fdc535e0ea5: the server could not find the requested resource (get pods dns-test-b3f5bc9d-cf02-428b-a7e7-4fdc535e0ea5)
Sep 27 19:57:41.334: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6253.svc.cluster.local from pod dns-6253/dns-test-b3f5bc9d-cf02-428b-a7e7-4fdc535e0ea5: the server could not find the requested resource (get pods dns-test-b3f5bc9d-cf02-428b-a7e7-4fdc535e0ea5)
Sep 27 19:57:41.517: INFO: Unable to read jessie_udp@dns-test-service.dns-6253.svc.cluster.local from pod dns-6253/dns-test-b3f5bc9d-cf02-428b-a7e7-4fdc535e0ea5: the server could not find the requested resource (get pods dns-test-b3f5bc9d-cf02-428b-a7e7-4fdc535e0ea5)
Sep 27 19:57:41.576: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6253.svc.cluster.local from pod dns-6253/dns-test-b3f5bc9d-cf02-428b-a7e7-4fdc535e0ea5: the server could not find the requested resource (get pods dns-test-b3f5bc9d-cf02-428b-a7e7-4fdc535e0ea5)
Sep 27 19:57:41.603: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6253.svc.cluster.local from pod dns-6253/dns-test-b3f5bc9d-cf02-428b-a7e7-4fdc535e0ea5: the server could not find the requested resource (get pods dns-test-b3f5bc9d-cf02-428b-a7e7-4fdc535e0ea5)
Sep 27 19:57:41.793: INFO: Lookups using dns-6253/dns-test-b3f5bc9d-cf02-428b-a7e7-4fdc535e0ea5 failed for: [wheezy_udp@dns-test-service.dns-6253.svc.cluster.local wheezy_tcp@dns-test-service.dns-6253.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-6253.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-6253.svc.cluster.local jessie_udp@dns-test-service.dns-6253.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-6253.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-6253.svc.cluster.local]

Sep 27 19:57:47.286: INFO: DNS probes using dns-6253/dns-test-b3f5bc9d-cf02-428b-a7e7-4fdc535e0ea5 succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 19:57:47.464: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-6253" for this suite.

• [SLOW TEST:10.660 seconds]
[sig-network] DNS
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for services  [Conformance]","total":305,"completed":57,"skipped":880,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job 
  should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Job
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 19:57:47.518: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: Orphaning one of the Job's Pods
Sep 27 19:57:50.226: INFO: Successfully updated pod "adopt-release-5d97v"
STEP: Checking that the Job readopts the Pod
Sep 27 19:57:50.226: INFO: Waiting up to 15m0s for pod "adopt-release-5d97v" in namespace "job-1932" to be "adopted"
Sep 27 19:57:50.247: INFO: Pod "adopt-release-5d97v": Phase="Running", Reason="", readiness=true. Elapsed: 20.97063ms
Sep 27 19:57:52.265: INFO: Pod "adopt-release-5d97v": Phase="Running", Reason="", readiness=true. Elapsed: 2.039483729s
Sep 27 19:57:52.265: INFO: Pod "adopt-release-5d97v" satisfied condition "adopted"
STEP: Removing the labels from the Job's Pod
Sep 27 19:57:52.838: INFO: Successfully updated pod "adopt-release-5d97v"
STEP: Checking that the Job releases the Pod
Sep 27 19:57:52.838: INFO: Waiting up to 15m0s for pod "adopt-release-5d97v" in namespace "job-1932" to be "released"
Sep 27 19:57:52.869: INFO: Pod "adopt-release-5d97v": Phase="Running", Reason="", readiness=true. Elapsed: 30.963829ms
Sep 27 19:57:52.869: INFO: Pod "adopt-release-5d97v" satisfied condition "released"
[AfterEach] [sig-apps] Job
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 19:57:52.869: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-1932" for this suite.

• [SLOW TEST:5.406 seconds]
[sig-apps] Job
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Job should adopt matching orphans and release non-matching pods [Conformance]","total":305,"completed":58,"skipped":915,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 19:57:52.925: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Sep 27 19:57:53.108: INFO: Waiting up to 5m0s for pod "downwardapi-volume-51ac6d09-3174-482d-b62c-b998a3b26aac" in namespace "projected-7577" to be "Succeeded or Failed"
Sep 27 19:57:53.120: INFO: Pod "downwardapi-volume-51ac6d09-3174-482d-b62c-b998a3b26aac": Phase="Pending", Reason="", readiness=false. Elapsed: 12.035053ms
Sep 27 19:57:55.136: INFO: Pod "downwardapi-volume-51ac6d09-3174-482d-b62c-b998a3b26aac": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.028076331s
STEP: Saw pod success
Sep 27 19:57:55.136: INFO: Pod "downwardapi-volume-51ac6d09-3174-482d-b62c-b998a3b26aac" satisfied condition "Succeeded or Failed"
Sep 27 19:57:55.153: INFO: Trying to get logs from node 10.177.248.114 pod downwardapi-volume-51ac6d09-3174-482d-b62c-b998a3b26aac container client-container: <nil>
STEP: delete the pod
Sep 27 19:57:55.273: INFO: Waiting for pod downwardapi-volume-51ac6d09-3174-482d-b62c-b998a3b26aac to disappear
Sep 27 19:57:55.293: INFO: Pod downwardapi-volume-51ac6d09-3174-482d-b62c-b998a3b26aac no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 19:57:55.294: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7577" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":59,"skipped":925,"failed":0}
SSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 19:57:55.346: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Sep 27 19:57:56.186: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep 27 19:57:59.264: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Sep 27 19:57:59.280: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-1240-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 19:58:00.567: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4590" for this suite.
STEP: Destroying namespace "webhook-4590-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:5.490 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource [Conformance]","total":305,"completed":60,"skipped":929,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 19:58:00.836: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Sep 27 19:58:01.096: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Sep 27 19:58:12.596: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-679324930 --namespace=crd-publish-openapi-7036 create -f -'
Sep 27 19:58:13.480: INFO: stderr: ""
Sep 27 19:58:13.480: INFO: stdout: "e2e-test-crd-publish-openapi-7032-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Sep 27 19:58:13.480: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-679324930 --namespace=crd-publish-openapi-7036 delete e2e-test-crd-publish-openapi-7032-crds test-cr'
Sep 27 19:58:13.746: INFO: stderr: ""
Sep 27 19:58:13.746: INFO: stdout: "e2e-test-crd-publish-openapi-7032-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
Sep 27 19:58:13.746: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-679324930 --namespace=crd-publish-openapi-7036 apply -f -'
Sep 27 19:58:14.641: INFO: stderr: ""
Sep 27 19:58:14.641: INFO: stdout: "e2e-test-crd-publish-openapi-7032-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Sep 27 19:58:14.641: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-679324930 --namespace=crd-publish-openapi-7036 delete e2e-test-crd-publish-openapi-7032-crds test-cr'
Sep 27 19:58:14.831: INFO: stderr: ""
Sep 27 19:58:14.831: INFO: stdout: "e2e-test-crd-publish-openapi-7032-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
Sep 27 19:58:14.831: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-679324930 explain e2e-test-crd-publish-openapi-7032-crds'
Sep 27 19:58:15.408: INFO: stderr: ""
Sep 27 19:58:15.408: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-7032-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<map[string]>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 19:58:23.826: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-7036" for this suite.

• [SLOW TEST:23.042 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields in an embedded object [Conformance]","total":305,"completed":61,"skipped":934,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a mutating webhook should work [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 19:58:23.878: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Sep 27 19:58:25.035: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Sep 27 19:58:27.082: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63768369505, loc:(*time.Location)(0x7702840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63768369505, loc:(*time.Location)(0x7702840)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63768369505, loc:(*time.Location)(0x7702840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63768369505, loc:(*time.Location)(0x7702840)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep 27 19:58:30.128: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a mutating webhook should work [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a mutating webhook configuration
STEP: Updating a mutating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that should not be mutated
STEP: Patching a mutating webhook configuration's rules to include the create operation
STEP: Creating a configMap that should be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 19:58:30.482: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1785" for this suite.
STEP: Destroying namespace "webhook-1785-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:6.843 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  patching/updating a mutating webhook should work [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a mutating webhook should work [Conformance]","total":305,"completed":62,"skipped":962,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 19:58:30.722: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating secret with name secret-test-ac3c205e-8549-4b6c-85eb-5fd59ab5889f
STEP: Creating a pod to test consume secrets
Sep 27 19:58:30.949: INFO: Waiting up to 5m0s for pod "pod-secrets-874f8700-830d-4b96-8d76-86be81f9fb5f" in namespace "secrets-9881" to be "Succeeded or Failed"
Sep 27 19:58:30.961: INFO: Pod "pod-secrets-874f8700-830d-4b96-8d76-86be81f9fb5f": Phase="Pending", Reason="", readiness=false. Elapsed: 12.255294ms
Sep 27 19:58:32.985: INFO: Pod "pod-secrets-874f8700-830d-4b96-8d76-86be81f9fb5f": Phase="Running", Reason="", readiness=true. Elapsed: 2.036697797s
Sep 27 19:58:35.001: INFO: Pod "pod-secrets-874f8700-830d-4b96-8d76-86be81f9fb5f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.052074952s
STEP: Saw pod success
Sep 27 19:58:35.001: INFO: Pod "pod-secrets-874f8700-830d-4b96-8d76-86be81f9fb5f" satisfied condition "Succeeded or Failed"
Sep 27 19:58:35.016: INFO: Trying to get logs from node 10.177.248.117 pod pod-secrets-874f8700-830d-4b96-8d76-86be81f9fb5f container secret-volume-test: <nil>
STEP: delete the pod
Sep 27 19:58:35.103: INFO: Waiting for pod pod-secrets-874f8700-830d-4b96-8d76-86be81f9fb5f to disappear
Sep 27 19:58:35.117: INFO: Pod pod-secrets-874f8700-830d-4b96-8d76-86be81f9fb5f no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 19:58:35.117: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9881" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","total":305,"completed":63,"skipped":984,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 19:58:35.160: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Sep 27 19:58:35.330: INFO: Waiting up to 5m0s for pod "downwardapi-volume-2594a610-76dd-43e4-aa4a-3b0ac0a5d43d" in namespace "downward-api-394" to be "Succeeded or Failed"
Sep 27 19:58:35.344: INFO: Pod "downwardapi-volume-2594a610-76dd-43e4-aa4a-3b0ac0a5d43d": Phase="Pending", Reason="", readiness=false. Elapsed: 13.635583ms
Sep 27 19:58:37.359: INFO: Pod "downwardapi-volume-2594a610-76dd-43e4-aa4a-3b0ac0a5d43d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029428901s
Sep 27 19:58:39.378: INFO: Pod "downwardapi-volume-2594a610-76dd-43e4-aa4a-3b0ac0a5d43d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.047652385s
STEP: Saw pod success
Sep 27 19:58:39.378: INFO: Pod "downwardapi-volume-2594a610-76dd-43e4-aa4a-3b0ac0a5d43d" satisfied condition "Succeeded or Failed"
Sep 27 19:58:39.392: INFO: Trying to get logs from node 10.177.248.117 pod downwardapi-volume-2594a610-76dd-43e4-aa4a-3b0ac0a5d43d container client-container: <nil>
STEP: delete the pod
Sep 27 19:58:39.471: INFO: Waiting for pod downwardapi-volume-2594a610-76dd-43e4-aa4a-3b0ac0a5d43d to disappear
Sep 27 19:58:39.487: INFO: Pod downwardapi-volume-2594a610-76dd-43e4-aa4a-3b0ac0a5d43d no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 19:58:39.487: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-394" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","total":305,"completed":64,"skipped":1008,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 19:58:39.534: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating projection with secret that has name projected-secret-test-ceb1f34f-827a-42c7-9675-8f0123ce6e48
STEP: Creating a pod to test consume secrets
Sep 27 19:58:39.762: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-82894eca-ec9f-4b15-92e9-00690dd0cee3" in namespace "projected-7558" to be "Succeeded or Failed"
Sep 27 19:58:39.774: INFO: Pod "pod-projected-secrets-82894eca-ec9f-4b15-92e9-00690dd0cee3": Phase="Pending", Reason="", readiness=false. Elapsed: 12.020934ms
Sep 27 19:58:41.797: INFO: Pod "pod-projected-secrets-82894eca-ec9f-4b15-92e9-00690dd0cee3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.035090792s
Sep 27 19:58:43.810: INFO: Pod "pod-projected-secrets-82894eca-ec9f-4b15-92e9-00690dd0cee3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.048200611s
STEP: Saw pod success
Sep 27 19:58:43.810: INFO: Pod "pod-projected-secrets-82894eca-ec9f-4b15-92e9-00690dd0cee3" satisfied condition "Succeeded or Failed"
Sep 27 19:58:43.824: INFO: Trying to get logs from node 10.177.248.117 pod pod-projected-secrets-82894eca-ec9f-4b15-92e9-00690dd0cee3 container projected-secret-volume-test: <nil>
STEP: delete the pod
Sep 27 19:58:43.897: INFO: Waiting for pod pod-projected-secrets-82894eca-ec9f-4b15-92e9-00690dd0cee3 to disappear
Sep 27 19:58:43.910: INFO: Pod pod-projected-secrets-82894eca-ec9f-4b15-92e9-00690dd0cee3 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 19:58:43.910: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7558" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":65,"skipped":1035,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 19:58:43.958: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Sep 27 19:58:44.150: INFO: Waiting up to 5m0s for pod "downwardapi-volume-92f67187-e0e8-4377-ad0f-71b5ee9040f0" in namespace "downward-api-1262" to be "Succeeded or Failed"
Sep 27 19:58:44.163: INFO: Pod "downwardapi-volume-92f67187-e0e8-4377-ad0f-71b5ee9040f0": Phase="Pending", Reason="", readiness=false. Elapsed: 12.957733ms
Sep 27 19:58:46.180: INFO: Pod "downwardapi-volume-92f67187-e0e8-4377-ad0f-71b5ee9040f0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029655331s
Sep 27 19:58:48.196: INFO: Pod "downwardapi-volume-92f67187-e0e8-4377-ad0f-71b5ee9040f0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.046095839s
STEP: Saw pod success
Sep 27 19:58:48.197: INFO: Pod "downwardapi-volume-92f67187-e0e8-4377-ad0f-71b5ee9040f0" satisfied condition "Succeeded or Failed"
Sep 27 19:58:48.213: INFO: Trying to get logs from node 10.177.248.117 pod downwardapi-volume-92f67187-e0e8-4377-ad0f-71b5ee9040f0 container client-container: <nil>
STEP: delete the pod
Sep 27 19:58:48.308: INFO: Waiting for pod downwardapi-volume-92f67187-e0e8-4377-ad0f-71b5ee9040f0 to disappear
Sep 27 19:58:48.323: INFO: Pod downwardapi-volume-92f67187-e0e8-4377-ad0f-71b5ee9040f0 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 19:58:48.323: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1262" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":66,"skipped":1044,"failed":0}

------------------------------
[sig-apps] Deployment 
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 19:58:48.373: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:78
[It] deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Sep 27 19:58:48.537: INFO: Creating deployment "webserver-deployment"
Sep 27 19:58:48.557: INFO: Waiting for observed generation 1
Sep 27 19:58:50.588: INFO: Waiting for all required pods to come up
Sep 27 19:58:50.616: INFO: Pod name httpd: Found 10 pods out of 10
STEP: ensuring each pod is running
Sep 27 19:58:52.666: INFO: Waiting for deployment "webserver-deployment" to complete
Sep 27 19:58:52.692: INFO: Updating deployment "webserver-deployment" with a non-existent image
Sep 27 19:58:52.777: INFO: Updating deployment webserver-deployment
Sep 27 19:58:52.777: INFO: Waiting for observed generation 2
Sep 27 19:58:54.803: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Sep 27 19:58:54.815: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Sep 27 19:58:54.829: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Sep 27 19:58:54.875: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Sep 27 19:58:54.875: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Sep 27 19:58:54.888: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Sep 27 19:58:54.919: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
Sep 27 19:58:54.919: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
Sep 27 19:58:54.951: INFO: Updating deployment webserver-deployment
Sep 27 19:58:54.951: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
Sep 27 19:58:54.984: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Sep 27 19:58:55.002: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:72
Sep 27 19:58:55.031: INFO: Deployment "webserver-deployment":
&Deployment{ObjectMeta:{webserver-deployment  deployment-9182 /apis/apps/v1/namespaces/deployment-9182/deployments/webserver-deployment 2434ef91-a6c6-4bb2-a09d-0f4ddb60c9c2 70293 3 2021-09-27 19:58:48 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2021-09-27 19:58:54 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{}}},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-09-27 19:58:55 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}}}]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0088c7d58 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:13,UpdatedReplicas:5,AvailableReplicas:8,UnavailableReplicas:25,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-795d758f88" is progressing.,LastUpdateTime:2021-09-27 19:58:53 +0000 UTC,LastTransitionTime:2021-09-27 19:58:48 +0000 UTC,},DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2021-09-27 19:58:54 +0000 UTC,LastTransitionTime:2021-09-27 19:58:54 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

Sep 27 19:58:55.056: INFO: New ReplicaSet "webserver-deployment-795d758f88" of Deployment "webserver-deployment":
&ReplicaSet{ObjectMeta:{webserver-deployment-795d758f88  deployment-9182 /apis/apps/v1/namespaces/deployment-9182/replicasets/webserver-deployment-795d758f88 910a3bac-99c1-4f6e-bf69-d393a74324ea 70288 3 2021-09-27 19:58:52 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment 2434ef91-a6c6-4bb2-a09d-0f4ddb60c9c2 0xc0089ae3c7 0xc0089ae3c8}] []  [{kube-controller-manager Update apps/v1 2021-09-27 19:58:54 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2434ef91-a6c6-4bb2-a09d-0f4ddb60c9c2\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 795d758f88,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0089ae4c8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:5,FullyLabeledReplicas:5,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Sep 27 19:58:55.056: INFO: All old ReplicaSets of Deployment "webserver-deployment":
Sep 27 19:58:55.056: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-dd94f59b7  deployment-9182 /apis/apps/v1/namespaces/deployment-9182/replicasets/webserver-deployment-dd94f59b7 165c7bc8-656c-4430-a247-9ff43a8263a9 70287 3 2021-09-27 19:58:48 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment 2434ef91-a6c6-4bb2-a09d-0f4ddb60c9c2 0xc0089ae587 0xc0089ae588}] []  [{kube-controller-manager Update apps/v1 2021-09-27 19:58:54 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2434ef91-a6c6-4bb2-a09d-0f4ddb60c9c2\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: dd94f59b7,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0089ae6b8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:8,FullyLabeledReplicas:8,ObservedGeneration:2,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
Sep 27 19:58:55.083: INFO: Pod "webserver-deployment-795d758f88-4dwz7" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-4dwz7 webserver-deployment-795d758f88- deployment-9182 /api/v1/namespaces/deployment-9182/pods/webserver-deployment-795d758f88-4dwz7 7b801255-ccf6-416e-8a98-21e937c1d7eb 70258 0 2021-09-27 19:58:52 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[cni.projectcalico.org/podIP:172.30.75.51/32 cni.projectcalico.org/podIPs:172.30.75.51/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "",
    "ips": [
        "172.30.75.51"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "",
    "ips": [
        "172.30.75.51"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 910a3bac-99c1-4f6e-bf69-d393a74324ea 0xc008988227 0xc008988228}] []  [{kube-controller-manager Update v1 2021-09-27 19:58:52 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"910a3bac-99c1-4f6e-bf69-d393a74324ea\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-09-27 19:58:52 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:hostIP":{},"f:startTime":{}}}} {calico Update v1 2021-09-27 19:58:54 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {multus Update v1 2021-09-27 19:58:54 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}},"f:status":{"f:containerStatuses":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-vlt2w,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-vlt2w,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-vlt2w,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.177.248.117,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c39,c19,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-w9gxb,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-27 19:58:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-27 19:58:52 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-27 19:58:52 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-27 19:58:52 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.177.248.117,PodIP:,StartTime:2021-09-27 19:58:52 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 27 19:58:55.083: INFO: Pod "webserver-deployment-795d758f88-g5sdg" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-g5sdg webserver-deployment-795d758f88- deployment-9182 /api/v1/namespaces/deployment-9182/pods/webserver-deployment-795d758f88-g5sdg c0d5bfca-5d5c-45ff-92d0-60399dc8fc84 70300 0 2021-09-27 19:58:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 910a3bac-99c1-4f6e-bf69-d393a74324ea 0xc008988427 0xc008988428}] []  [{kube-controller-manager Update v1 2021-09-27 19:58:54 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"910a3bac-99c1-4f6e-bf69-d393a74324ea\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-vlt2w,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-vlt2w,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-vlt2w,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c39,c19,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-w9gxb,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 27 19:58:55.084: INFO: Pod "webserver-deployment-795d758f88-j5b2l" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-j5b2l webserver-deployment-795d758f88- deployment-9182 /api/v1/namespaces/deployment-9182/pods/webserver-deployment-795d758f88-j5b2l 19aaa43c-dca7-4ac7-8448-b71bfaa530c9 70261 0 2021-09-27 19:58:52 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[cni.projectcalico.org/podIP:172.30.137.94/32 cni.projectcalico.org/podIPs:172.30.137.94/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "",
    "ips": [
        "172.30.137.94"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "",
    "ips": [
        "172.30.137.94"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 910a3bac-99c1-4f6e-bf69-d393a74324ea 0xc0089885e7 0xc0089885e8}] []  [{kube-controller-manager Update v1 2021-09-27 19:58:52 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"910a3bac-99c1-4f6e-bf69-d393a74324ea\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-09-27 19:58:53 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:hostIP":{},"f:startTime":{}}}} {calico Update v1 2021-09-27 19:58:54 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {multus Update v1 2021-09-27 19:58:54 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}},"f:status":{"f:containerStatuses":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-vlt2w,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-vlt2w,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-vlt2w,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.177.248.114,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c39,c19,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-w9gxb,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-27 19:58:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-27 19:58:53 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-27 19:58:53 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-27 19:58:53 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.177.248.114,PodIP:,StartTime:2021-09-27 19:58:53 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 27 19:58:55.084: INFO: Pod "webserver-deployment-795d758f88-pndwj" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-pndwj webserver-deployment-795d758f88- deployment-9182 /api/v1/namespaces/deployment-9182/pods/webserver-deployment-795d758f88-pndwj 8918b60d-8242-48e4-94cb-abfb599bb940 70270 0 2021-09-27 19:58:53 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[cni.projectcalico.org/podIP:172.30.75.52/32 cni.projectcalico.org/podIPs:172.30.75.52/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "",
    "ips": [
        "172.30.75.52"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "",
    "ips": [
        "172.30.75.52"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 910a3bac-99c1-4f6e-bf69-d393a74324ea 0xc0089888f7 0xc0089888f8}] []  [{kube-controller-manager Update v1 2021-09-27 19:58:53 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"910a3bac-99c1-4f6e-bf69-d393a74324ea\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-09-27 19:58:53 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:hostIP":{},"f:startTime":{}}}} {calico Update v1 2021-09-27 19:58:54 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {multus Update v1 2021-09-27 19:58:54 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}},"f:status":{"f:containerStatuses":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-vlt2w,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-vlt2w,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-vlt2w,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.177.248.117,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c39,c19,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-w9gxb,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-27 19:58:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-27 19:58:53 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-27 19:58:53 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-27 19:58:53 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.177.248.117,PodIP:,StartTime:2021-09-27 19:58:53 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 27 19:58:55.084: INFO: Pod "webserver-deployment-795d758f88-s94fj" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-s94fj webserver-deployment-795d758f88- deployment-9182 /api/v1/namespaces/deployment-9182/pods/webserver-deployment-795d758f88-s94fj 9412c454-019d-43f8-8f9f-19d7770d8d30 70273 0 2021-09-27 19:58:52 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[cni.projectcalico.org/podIP:172.30.85.170/32 cni.projectcalico.org/podIPs:172.30.85.170/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "",
    "ips": [
        "172.30.85.170"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "",
    "ips": [
        "172.30.85.170"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 910a3bac-99c1-4f6e-bf69-d393a74324ea 0xc008988b17 0xc008988b18}] []  [{kube-controller-manager Update v1 2021-09-27 19:58:52 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"910a3bac-99c1-4f6e-bf69-d393a74324ea\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-09-27 19:58:53 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:hostIP":{},"f:startTime":{}}}} {calico Update v1 2021-09-27 19:58:54 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {multus Update v1 2021-09-27 19:58:54 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}},"f:status":{"f:containerStatuses":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-vlt2w,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-vlt2w,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-vlt2w,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.177.248.126,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c39,c19,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-w9gxb,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-27 19:58:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-27 19:58:53 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-27 19:58:53 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-27 19:58:53 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.177.248.126,PodIP:,StartTime:2021-09-27 19:58:53 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 27 19:58:55.085: INFO: Pod "webserver-deployment-795d758f88-z8zzh" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-z8zzh webserver-deployment-795d758f88- deployment-9182 /api/v1/namespaces/deployment-9182/pods/webserver-deployment-795d758f88-z8zzh 2fed6c03-2a9c-45a8-9bd2-4bbf25cfa132 70283 0 2021-09-27 19:58:53 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[cni.projectcalico.org/podIP:172.30.137.127/32 cni.projectcalico.org/podIPs:172.30.137.127/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "",
    "ips": [
        "172.30.137.127"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "",
    "ips": [
        "172.30.137.127"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 910a3bac-99c1-4f6e-bf69-d393a74324ea 0xc008988d17 0xc008988d18}] []  [{kube-controller-manager Update v1 2021-09-27 19:58:53 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"910a3bac-99c1-4f6e-bf69-d393a74324ea\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-09-27 19:58:53 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:hostIP":{},"f:startTime":{}}}} {calico Update v1 2021-09-27 19:58:54 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {multus Update v1 2021-09-27 19:58:54 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}},"f:status":{"f:containerStatuses":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-vlt2w,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-vlt2w,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-vlt2w,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.177.248.114,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c39,c19,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-w9gxb,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-27 19:58:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-27 19:58:53 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-27 19:58:53 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-27 19:58:53 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.177.248.114,PodIP:,StartTime:2021-09-27 19:58:53 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 27 19:58:55.085: INFO: Pod "webserver-deployment-dd94f59b7-26x7n" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-26x7n webserver-deployment-dd94f59b7- deployment-9182 /api/v1/namespaces/deployment-9182/pods/webserver-deployment-dd94f59b7-26x7n 5f9fe9ae-2136-4e53-9da2-2eed93acb3e3 70295 0 2021-09-27 19:58:54 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 165c7bc8-656c-4430-a247-9ff43a8263a9 0xc008988f97 0xc008988f98}] []  [{kube-controller-manager Update v1 2021-09-27 19:58:54 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"165c7bc8-656c-4430-a247-9ff43a8263a9\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-vlt2w,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-vlt2w,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-vlt2w,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.177.248.117,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c39,c19,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-w9gxb,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-27 19:58:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 27 19:58:55.085: INFO: Pod "webserver-deployment-dd94f59b7-4w6fp" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-4w6fp webserver-deployment-dd94f59b7- deployment-9182 /api/v1/namespaces/deployment-9182/pods/webserver-deployment-dd94f59b7-4w6fp 99cdd1a9-46e7-4cec-9bf3-1c294ad8f737 70127 0 2021-09-27 19:58:48 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[cni.projectcalico.org/podIP:172.30.85.167/32 cni.projectcalico.org/podIPs:172.30.85.167/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "",
    "ips": [
        "172.30.85.167"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "",
    "ips": [
        "172.30.85.167"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 165c7bc8-656c-4430-a247-9ff43a8263a9 0xc008989167 0xc008989168}] []  [{kube-controller-manager Update v1 2021-09-27 19:58:48 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"165c7bc8-656c-4430-a247-9ff43a8263a9\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-09-27 19:58:50 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {multus Update v1 2021-09-27 19:58:50 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}}} {kubelet Update v1 2021-09-27 19:58:51 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.85.167\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-vlt2w,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-vlt2w,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-vlt2w,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.177.248.126,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c39,c19,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-w9gxb,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-27 19:58:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-27 19:58:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-27 19:58:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-27 19:58:48 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.177.248.126,PodIP:172.30.85.167,StartTime:2021-09-27 19:58:48 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-09-27 19:58:50 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:6feb0ea7b0967367da66e8d58ba813fde32bdb92f63bfc21a9e170d211539db4,ContainerID:cri-o://6636fb6342371fa0d694f7cc2ab5dd4e1354a1d1255ad1fd9c849a610621af92,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.85.167,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 27 19:58:55.086: INFO: Pod "webserver-deployment-dd94f59b7-8jk8z" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-8jk8z webserver-deployment-dd94f59b7- deployment-9182 /api/v1/namespaces/deployment-9182/pods/webserver-deployment-dd94f59b7-8jk8z e576f846-0d9d-4047-bd03-3a2a9a47cb07 70096 0 2021-09-27 19:58:48 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[cni.projectcalico.org/podIP:172.30.75.47/32 cni.projectcalico.org/podIPs:172.30.75.47/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "",
    "ips": [
        "172.30.75.47"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "",
    "ips": [
        "172.30.75.47"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 165c7bc8-656c-4430-a247-9ff43a8263a9 0xc008989417 0xc008989418}] []  [{kube-controller-manager Update v1 2021-09-27 19:58:48 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"165c7bc8-656c-4430-a247-9ff43a8263a9\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-09-27 19:58:49 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2021-09-27 19:58:50 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.75.47\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}} {multus Update v1 2021-09-27 19:58:50 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-vlt2w,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-vlt2w,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-vlt2w,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.177.248.117,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c39,c19,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-w9gxb,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-27 19:58:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-27 19:58:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-27 19:58:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-27 19:58:48 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.177.248.117,PodIP:172.30.75.47,StartTime:2021-09-27 19:58:48 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-09-27 19:58:50 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:6feb0ea7b0967367da66e8d58ba813fde32bdb92f63bfc21a9e170d211539db4,ContainerID:cri-o://13fc5586ec021e3be4020c2b75f6722b286b8c0e1e66286dc65719c527ce6fdb,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.75.47,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 27 19:58:55.086: INFO: Pod "webserver-deployment-dd94f59b7-946dd" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-946dd webserver-deployment-dd94f59b7- deployment-9182 /api/v1/namespaces/deployment-9182/pods/webserver-deployment-dd94f59b7-946dd 62b1e4d1-2136-40e5-934c-68d765b1a6d2 70297 0 2021-09-27 19:58:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 165c7bc8-656c-4430-a247-9ff43a8263a9 0xc008989677 0xc008989678}] []  [{kube-controller-manager Update v1 2021-09-27 19:58:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"165c7bc8-656c-4430-a247-9ff43a8263a9\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-vlt2w,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-vlt2w,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-vlt2w,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c39,c19,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-w9gxb,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 27 19:58:55.086: INFO: Pod "webserver-deployment-dd94f59b7-cdhbz" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-cdhbz webserver-deployment-dd94f59b7- deployment-9182 /api/v1/namespaces/deployment-9182/pods/webserver-deployment-dd94f59b7-cdhbz 8f0720ad-5388-4fee-9b28-f0095e2d3cbb 70130 0 2021-09-27 19:58:48 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[cni.projectcalico.org/podIP:172.30.85.168/32 cni.projectcalico.org/podIPs:172.30.85.168/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "",
    "ips": [
        "172.30.85.168"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "",
    "ips": [
        "172.30.85.168"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 165c7bc8-656c-4430-a247-9ff43a8263a9 0xc0089897e7 0xc0089897e8}] []  [{kube-controller-manager Update v1 2021-09-27 19:58:48 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"165c7bc8-656c-4430-a247-9ff43a8263a9\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-09-27 19:58:50 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {multus Update v1 2021-09-27 19:58:50 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}}} {kubelet Update v1 2021-09-27 19:58:51 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.85.168\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-vlt2w,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-vlt2w,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-vlt2w,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.177.248.126,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c39,c19,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-w9gxb,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-27 19:58:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-27 19:58:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-27 19:58:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-27 19:58:48 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.177.248.126,PodIP:172.30.85.168,StartTime:2021-09-27 19:58:48 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-09-27 19:58:50 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:6feb0ea7b0967367da66e8d58ba813fde32bdb92f63bfc21a9e170d211539db4,ContainerID:cri-o://a5efadee6f1207773a4815313b38af4f9ba0f51970478de4eb834bf27120d03a,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.85.168,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 27 19:58:55.087: INFO: Pod "webserver-deployment-dd94f59b7-fngsh" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-fngsh webserver-deployment-dd94f59b7- deployment-9182 /api/v1/namespaces/deployment-9182/pods/webserver-deployment-dd94f59b7-fngsh fb4c3a1b-455c-42c9-94f8-03082f0eefe4 70296 0 2021-09-27 19:58:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 165c7bc8-656c-4430-a247-9ff43a8263a9 0xc0089899e7 0xc0089899e8}] []  [{kube-controller-manager Update v1 2021-09-27 19:58:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"165c7bc8-656c-4430-a247-9ff43a8263a9\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-vlt2w,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-vlt2w,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-vlt2w,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c39,c19,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-w9gxb,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 27 19:58:55.087: INFO: Pod "webserver-deployment-dd94f59b7-jxs2j" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-jxs2j webserver-deployment-dd94f59b7- deployment-9182 /api/v1/namespaces/deployment-9182/pods/webserver-deployment-dd94f59b7-jxs2j 6bb52a59-b307-4ee3-b82b-9c9387c91488 70141 0 2021-09-27 19:58:48 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[cni.projectcalico.org/podIP:172.30.137.88/32 cni.projectcalico.org/podIPs:172.30.137.88/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "",
    "ips": [
        "172.30.137.88"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "",
    "ips": [
        "172.30.137.88"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 165c7bc8-656c-4430-a247-9ff43a8263a9 0xc008989b37 0xc008989b38}] []  [{kube-controller-manager Update v1 2021-09-27 19:58:48 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"165c7bc8-656c-4430-a247-9ff43a8263a9\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-09-27 19:58:50 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {multus Update v1 2021-09-27 19:58:50 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}}} {kubelet Update v1 2021-09-27 19:58:51 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.137.88\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-vlt2w,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-vlt2w,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-vlt2w,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.177.248.114,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c39,c19,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-w9gxb,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-27 19:58:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-27 19:58:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-27 19:58:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-27 19:58:48 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.177.248.114,PodIP:172.30.137.88,StartTime:2021-09-27 19:58:48 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-09-27 19:58:50 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:6feb0ea7b0967367da66e8d58ba813fde32bdb92f63bfc21a9e170d211539db4,ContainerID:cri-o://8be178e6a18daeb566b8bfccb0673a93ac5768b031a1b8dfcfe3d50aa8ddf07f,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.137.88,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 27 19:58:55.087: INFO: Pod "webserver-deployment-dd94f59b7-ncm8r" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-ncm8r webserver-deployment-dd94f59b7- deployment-9182 /api/v1/namespaces/deployment-9182/pods/webserver-deployment-dd94f59b7-ncm8r 8decd21b-e516-47d7-a062-6be11e9f5f9e 70158 0 2021-09-27 19:58:48 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[cni.projectcalico.org/podIP:172.30.85.169/32 cni.projectcalico.org/podIPs:172.30.85.169/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "",
    "ips": [
        "172.30.85.169"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "",
    "ips": [
        "172.30.85.169"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 165c7bc8-656c-4430-a247-9ff43a8263a9 0xc008989e07 0xc008989e08}] []  [{kube-controller-manager Update v1 2021-09-27 19:58:48 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"165c7bc8-656c-4430-a247-9ff43a8263a9\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-09-27 19:58:50 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {multus Update v1 2021-09-27 19:58:50 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}}} {kubelet Update v1 2021-09-27 19:58:52 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.85.169\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-vlt2w,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-vlt2w,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-vlt2w,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.177.248.126,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c39,c19,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-w9gxb,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-27 19:58:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-27 19:58:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-27 19:58:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-27 19:58:48 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.177.248.126,PodIP:172.30.85.169,StartTime:2021-09-27 19:58:48 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-09-27 19:58:51 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:6feb0ea7b0967367da66e8d58ba813fde32bdb92f63bfc21a9e170d211539db4,ContainerID:cri-o://a71628365894e5db4bececc4f8b235ac7fb1c4545a80f5c59b8d92b0d665f794,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.85.169,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 27 19:58:55.090: INFO: Pod "webserver-deployment-dd94f59b7-sskx7" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-sskx7 webserver-deployment-dd94f59b7- deployment-9182 /api/v1/namespaces/deployment-9182/pods/webserver-deployment-dd94f59b7-sskx7 da120bea-35ef-4e7e-a8f4-a8d45756b32e 70133 0 2021-09-27 19:58:48 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[cni.projectcalico.org/podIP:172.30.137.76/32 cni.projectcalico.org/podIPs:172.30.137.76/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "",
    "ips": [
        "172.30.137.76"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "",
    "ips": [
        "172.30.137.76"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 165c7bc8-656c-4430-a247-9ff43a8263a9 0xc0089de0c7 0xc0089de0c8}] []  [{kube-controller-manager Update v1 2021-09-27 19:58:48 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"165c7bc8-656c-4430-a247-9ff43a8263a9\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-09-27 19:58:50 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {multus Update v1 2021-09-27 19:58:50 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}}} {kubelet Update v1 2021-09-27 19:58:51 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.137.76\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-vlt2w,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-vlt2w,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-vlt2w,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.177.248.114,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c39,c19,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-w9gxb,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-27 19:58:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-27 19:58:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-27 19:58:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-27 19:58:48 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.177.248.114,PodIP:172.30.137.76,StartTime:2021-09-27 19:58:48 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-09-27 19:58:50 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:6feb0ea7b0967367da66e8d58ba813fde32bdb92f63bfc21a9e170d211539db4,ContainerID:cri-o://f0165abd4a2a5472eacff34399d1698e07fd6b699c37333497366a22f96f46de,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.137.76,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 27 19:58:55.092: INFO: Pod "webserver-deployment-dd94f59b7-t9sr9" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-t9sr9 webserver-deployment-dd94f59b7- deployment-9182 /api/v1/namespaces/deployment-9182/pods/webserver-deployment-dd94f59b7-t9sr9 c309bbfb-962e-4786-93ca-c967f72dd177 70135 0 2021-09-27 19:58:48 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[cni.projectcalico.org/podIP:172.30.137.90/32 cni.projectcalico.org/podIPs:172.30.137.90/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "",
    "ips": [
        "172.30.137.90"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "",
    "ips": [
        "172.30.137.90"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 165c7bc8-656c-4430-a247-9ff43a8263a9 0xc0089de447 0xc0089de448}] []  [{kube-controller-manager Update v1 2021-09-27 19:58:48 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"165c7bc8-656c-4430-a247-9ff43a8263a9\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-09-27 19:58:50 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {multus Update v1 2021-09-27 19:58:50 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}}} {kubelet Update v1 2021-09-27 19:58:51 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.137.90\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-vlt2w,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-vlt2w,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-vlt2w,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.177.248.114,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c39,c19,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-w9gxb,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-27 19:58:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-27 19:58:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-27 19:58:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-27 19:58:48 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.177.248.114,PodIP:172.30.137.90,StartTime:2021-09-27 19:58:48 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-09-27 19:58:50 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:6feb0ea7b0967367da66e8d58ba813fde32bdb92f63bfc21a9e170d211539db4,ContainerID:cri-o://dcb440b77c205a9732b201200bf6e550dd015b88f1c97ce56bf1c8852ae26909,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.137.90,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 27 19:58:55.094: INFO: Pod "webserver-deployment-dd94f59b7-wjqmd" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-wjqmd webserver-deployment-dd94f59b7- deployment-9182 /api/v1/namespaces/deployment-9182/pods/webserver-deployment-dd94f59b7-wjqmd b543f185-9b21-4a9e-a78e-1493938b0e49 70153 0 2021-09-27 19:58:48 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[cni.projectcalico.org/podIP:172.30.75.49/32 cni.projectcalico.org/podIPs:172.30.75.49/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "",
    "ips": [
        "172.30.75.49"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "",
    "ips": [
        "172.30.75.49"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 165c7bc8-656c-4430-a247-9ff43a8263a9 0xc0089de727 0xc0089de728}] []  [{kube-controller-manager Update v1 2021-09-27 19:58:48 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"165c7bc8-656c-4430-a247-9ff43a8263a9\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-09-27 19:58:50 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {multus Update v1 2021-09-27 19:58:50 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}}} {kubelet Update v1 2021-09-27 19:58:51 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.75.49\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-vlt2w,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-vlt2w,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-vlt2w,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.177.248.117,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c39,c19,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-w9gxb,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-27 19:58:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-27 19:58:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-27 19:58:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-27 19:58:48 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.177.248.117,PodIP:172.30.75.49,StartTime:2021-09-27 19:58:48 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-09-27 19:58:50 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:6feb0ea7b0967367da66e8d58ba813fde32bdb92f63bfc21a9e170d211539db4,ContainerID:cri-o://849d9e3afee31f5ffdb76c21e5ca54cfe2dbfcdb317b30da02a11b2caa1b9a92,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.75.49,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 19:58:55.095: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-9182" for this suite.

• [SLOW TEST:6.780 seconds]
[sig-apps] Deployment
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should support proportional scaling [Conformance]","total":305,"completed":67,"skipped":1044,"failed":0}
[sig-cli] Kubectl client Update Demo 
  should scale a replication controller  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 19:58:55.154: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[BeforeEach] Update Demo
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:308
[It] should scale a replication controller  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a replication controller
Sep 27 19:58:55.339: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-679324930 create -f - --namespace=kubectl-1895'
Sep 27 19:58:56.216: INFO: stderr: ""
Sep 27 19:58:56.216: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Sep 27 19:58:56.217: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-679324930 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-1895'
Sep 27 19:58:56.534: INFO: stderr: ""
Sep 27 19:58:56.534: INFO: stdout: "update-demo-nautilus-2kkq9 update-demo-nautilus-7k7lw "
Sep 27 19:58:56.534: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-679324930 get pods update-demo-nautilus-2kkq9 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-1895'
Sep 27 19:58:56.822: INFO: stderr: ""
Sep 27 19:58:56.822: INFO: stdout: ""
Sep 27 19:58:56.822: INFO: update-demo-nautilus-2kkq9 is created but not running
Sep 27 19:59:01.822: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-679324930 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-1895'
Sep 27 19:59:02.055: INFO: stderr: ""
Sep 27 19:59:02.055: INFO: stdout: "update-demo-nautilus-2kkq9 update-demo-nautilus-7k7lw "
Sep 27 19:59:02.056: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-679324930 get pods update-demo-nautilus-2kkq9 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-1895'
Sep 27 19:59:02.238: INFO: stderr: ""
Sep 27 19:59:02.239: INFO: stdout: "true"
Sep 27 19:59:02.239: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-679324930 get pods update-demo-nautilus-2kkq9 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-1895'
Sep 27 19:59:02.404: INFO: stderr: ""
Sep 27 19:59:02.405: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Sep 27 19:59:02.405: INFO: validating pod update-demo-nautilus-2kkq9
Sep 27 19:59:02.444: INFO: got data: {
  "image": "nautilus.jpg"
}

Sep 27 19:59:02.444: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Sep 27 19:59:02.444: INFO: update-demo-nautilus-2kkq9 is verified up and running
Sep 27 19:59:02.444: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-679324930 get pods update-demo-nautilus-7k7lw -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-1895'
Sep 27 19:59:02.577: INFO: stderr: ""
Sep 27 19:59:02.577: INFO: stdout: "true"
Sep 27 19:59:02.577: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-679324930 get pods update-demo-nautilus-7k7lw -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-1895'
Sep 27 19:59:02.726: INFO: stderr: ""
Sep 27 19:59:02.726: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Sep 27 19:59:02.726: INFO: validating pod update-demo-nautilus-7k7lw
Sep 27 19:59:02.763: INFO: got data: {
  "image": "nautilus.jpg"
}

Sep 27 19:59:02.763: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Sep 27 19:59:02.763: INFO: update-demo-nautilus-7k7lw is verified up and running
STEP: scaling down the replication controller
Sep 27 19:59:02.768: INFO: scanned /root for discovery docs: <nil>
Sep 27 19:59:02.768: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-679324930 scale rc update-demo-nautilus --replicas=1 --timeout=5m --namespace=kubectl-1895'
Sep 27 19:59:04.046: INFO: stderr: ""
Sep 27 19:59:04.046: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Sep 27 19:59:04.046: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-679324930 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-1895'
Sep 27 19:59:04.478: INFO: stderr: ""
Sep 27 19:59:04.478: INFO: stdout: "update-demo-nautilus-2kkq9 update-demo-nautilus-7k7lw "
STEP: Replicas for name=update-demo: expected=1 actual=2
Sep 27 19:59:09.478: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-679324930 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-1895'
Sep 27 19:59:09.655: INFO: stderr: ""
Sep 27 19:59:09.655: INFO: stdout: "update-demo-nautilus-7k7lw "
Sep 27 19:59:09.655: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-679324930 get pods update-demo-nautilus-7k7lw -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-1895'
Sep 27 19:59:09.805: INFO: stderr: ""
Sep 27 19:59:09.805: INFO: stdout: "true"
Sep 27 19:59:09.805: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-679324930 get pods update-demo-nautilus-7k7lw -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-1895'
Sep 27 19:59:09.968: INFO: stderr: ""
Sep 27 19:59:09.968: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Sep 27 19:59:09.968: INFO: validating pod update-demo-nautilus-7k7lw
Sep 27 19:59:09.993: INFO: got data: {
  "image": "nautilus.jpg"
}

Sep 27 19:59:09.993: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Sep 27 19:59:09.993: INFO: update-demo-nautilus-7k7lw is verified up and running
STEP: scaling up the replication controller
Sep 27 19:59:09.999: INFO: scanned /root for discovery docs: <nil>
Sep 27 19:59:09.999: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-679324930 scale rc update-demo-nautilus --replicas=2 --timeout=5m --namespace=kubectl-1895'
Sep 27 19:59:11.274: INFO: stderr: ""
Sep 27 19:59:11.274: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Sep 27 19:59:11.274: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-679324930 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-1895'
Sep 27 19:59:11.476: INFO: stderr: ""
Sep 27 19:59:11.476: INFO: stdout: "update-demo-nautilus-7k7lw update-demo-nautilus-gf9cn "
Sep 27 19:59:11.476: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-679324930 get pods update-demo-nautilus-7k7lw -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-1895'
Sep 27 19:59:11.835: INFO: stderr: ""
Sep 27 19:59:11.835: INFO: stdout: "true"
Sep 27 19:59:11.835: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-679324930 get pods update-demo-nautilus-7k7lw -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-1895'
Sep 27 19:59:12.023: INFO: stderr: ""
Sep 27 19:59:12.023: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Sep 27 19:59:12.023: INFO: validating pod update-demo-nautilus-7k7lw
Sep 27 19:59:12.048: INFO: got data: {
  "image": "nautilus.jpg"
}

Sep 27 19:59:12.048: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Sep 27 19:59:12.048: INFO: update-demo-nautilus-7k7lw is verified up and running
Sep 27 19:59:12.049: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-679324930 get pods update-demo-nautilus-gf9cn -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-1895'
Sep 27 19:59:12.215: INFO: stderr: ""
Sep 27 19:59:12.215: INFO: stdout: ""
Sep 27 19:59:12.215: INFO: update-demo-nautilus-gf9cn is created but not running
Sep 27 19:59:17.215: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-679324930 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-1895'
Sep 27 19:59:17.388: INFO: stderr: ""
Sep 27 19:59:17.388: INFO: stdout: "update-demo-nautilus-7k7lw update-demo-nautilus-gf9cn "
Sep 27 19:59:17.388: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-679324930 get pods update-demo-nautilus-7k7lw -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-1895'
Sep 27 19:59:17.569: INFO: stderr: ""
Sep 27 19:59:17.569: INFO: stdout: "true"
Sep 27 19:59:17.569: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-679324930 get pods update-demo-nautilus-7k7lw -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-1895'
Sep 27 19:59:17.745: INFO: stderr: ""
Sep 27 19:59:17.745: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Sep 27 19:59:17.745: INFO: validating pod update-demo-nautilus-7k7lw
Sep 27 19:59:17.777: INFO: got data: {
  "image": "nautilus.jpg"
}

Sep 27 19:59:17.778: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Sep 27 19:59:17.778: INFO: update-demo-nautilus-7k7lw is verified up and running
Sep 27 19:59:17.778: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-679324930 get pods update-demo-nautilus-gf9cn -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-1895'
Sep 27 19:59:17.942: INFO: stderr: ""
Sep 27 19:59:17.942: INFO: stdout: "true"
Sep 27 19:59:17.942: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-679324930 get pods update-demo-nautilus-gf9cn -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-1895'
Sep 27 19:59:18.100: INFO: stderr: ""
Sep 27 19:59:18.100: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Sep 27 19:59:18.100: INFO: validating pod update-demo-nautilus-gf9cn
Sep 27 19:59:18.135: INFO: got data: {
  "image": "nautilus.jpg"
}

Sep 27 19:59:18.135: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Sep 27 19:59:18.135: INFO: update-demo-nautilus-gf9cn is verified up and running
STEP: using delete to clean up resources
Sep 27 19:59:18.135: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-679324930 delete --grace-period=0 --force -f - --namespace=kubectl-1895'
Sep 27 19:59:18.333: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep 27 19:59:18.333: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Sep 27 19:59:18.333: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-679324930 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-1895'
Sep 27 19:59:18.536: INFO: stderr: "No resources found in kubectl-1895 namespace.\n"
Sep 27 19:59:18.536: INFO: stdout: ""
Sep 27 19:59:18.537: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-679324930 get pods -l name=update-demo --namespace=kubectl-1895 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Sep 27 19:59:18.737: INFO: stderr: ""
Sep 27 19:59:18.737: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 19:59:18.737: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1895" for this suite.

• [SLOW TEST:23.624 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:306
    should scale a replication controller  [Conformance]
    /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should scale a replication controller  [Conformance]","total":305,"completed":68,"skipped":1044,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 19:59:18.779: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0777 on tmpfs
Sep 27 19:59:18.982: INFO: Waiting up to 5m0s for pod "pod-4bebc797-4f5b-4431-9771-a4fbf54da021" in namespace "emptydir-2712" to be "Succeeded or Failed"
Sep 27 19:59:18.995: INFO: Pod "pod-4bebc797-4f5b-4431-9771-a4fbf54da021": Phase="Pending", Reason="", readiness=false. Elapsed: 12.910262ms
Sep 27 19:59:21.008: INFO: Pod "pod-4bebc797-4f5b-4431-9771-a4fbf54da021": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026078836s
Sep 27 19:59:23.021: INFO: Pod "pod-4bebc797-4f5b-4431-9771-a4fbf54da021": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.039746721s
STEP: Saw pod success
Sep 27 19:59:23.021: INFO: Pod "pod-4bebc797-4f5b-4431-9771-a4fbf54da021" satisfied condition "Succeeded or Failed"
Sep 27 19:59:23.033: INFO: Trying to get logs from node 10.177.248.117 pod pod-4bebc797-4f5b-4431-9771-a4fbf54da021 container test-container: <nil>
STEP: delete the pod
Sep 27 19:59:23.128: INFO: Waiting for pod pod-4bebc797-4f5b-4431-9771-a4fbf54da021 to disappear
Sep 27 19:59:23.141: INFO: Pod pod-4bebc797-4f5b-4431-9771-a4fbf54da021 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 19:59:23.141: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2712" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":69,"skipped":1085,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] 
  validates lower priority pod preemption by critical pod [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 19:59:23.213: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename sched-preemption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:89
Sep 27 19:59:23.401: INFO: Waiting up to 1m0s for all nodes to be ready
Sep 27 20:00:23.592: INFO: Waiting for terminating namespaces to be deleted...
[It] validates lower priority pod preemption by critical pod [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Create pods that use 2/3 of node resources.
Sep 27 20:00:23.721: INFO: Created pod: pod0-sched-preemption-low-priority
Sep 27 20:00:24.066: INFO: Created pod: pod1-sched-preemption-medium-priority
Sep 27 20:00:24.135: INFO: Created pod: pod2-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled.
STEP: Run a critical pod that use same resources as that of a lower priority pod
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 20:00:56.402: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-6818" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:77

• [SLOW TEST:93.412 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates lower priority pod preemption by critical pod [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]","total":305,"completed":70,"skipped":1097,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 20:00:56.627: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test env composition
Sep 27 20:00:56.819: INFO: Waiting up to 5m0s for pod "var-expansion-51b88bab-e141-4bab-8e10-1bffc56380b8" in namespace "var-expansion-2864" to be "Succeeded or Failed"
Sep 27 20:00:56.831: INFO: Pod "var-expansion-51b88bab-e141-4bab-8e10-1bffc56380b8": Phase="Pending", Reason="", readiness=false. Elapsed: 12.37632ms
Sep 27 20:00:58.853: INFO: Pod "var-expansion-51b88bab-e141-4bab-8e10-1bffc56380b8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.03405111s
Sep 27 20:01:00.869: INFO: Pod "var-expansion-51b88bab-e141-4bab-8e10-1bffc56380b8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.050390997s
STEP: Saw pod success
Sep 27 20:01:00.869: INFO: Pod "var-expansion-51b88bab-e141-4bab-8e10-1bffc56380b8" satisfied condition "Succeeded or Failed"
Sep 27 20:01:00.885: INFO: Trying to get logs from node 10.177.248.117 pod var-expansion-51b88bab-e141-4bab-8e10-1bffc56380b8 container dapi-container: <nil>
STEP: delete the pod
Sep 27 20:01:01.020: INFO: Waiting for pod var-expansion-51b88bab-e141-4bab-8e10-1bffc56380b8 to disappear
Sep 27 20:01:01.034: INFO: Pod var-expansion-51b88bab-e141-4bab-8e10-1bffc56380b8 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 20:01:01.034: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-2864" for this suite.
•{"msg":"PASSED [k8s.io] Variable Expansion should allow composing env vars into new env vars [NodeConformance] [Conformance]","total":305,"completed":71,"skipped":1115,"failed":0}
SSSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 20:01:01.081: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[BeforeEach] Update Demo
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:308
[It] should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a replication controller
Sep 27 20:01:01.223: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-679324930 create -f - --namespace=kubectl-4207'
Sep 27 20:01:02.237: INFO: stderr: ""
Sep 27 20:01:02.237: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Sep 27 20:01:02.237: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-679324930 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-4207'
Sep 27 20:01:02.403: INFO: stderr: ""
Sep 27 20:01:02.403: INFO: stdout: "update-demo-nautilus-jgbdx update-demo-nautilus-l2kq9 "
Sep 27 20:01:02.403: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-679324930 get pods update-demo-nautilus-jgbdx -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-4207'
Sep 27 20:01:02.569: INFO: stderr: ""
Sep 27 20:01:02.569: INFO: stdout: ""
Sep 27 20:01:02.569: INFO: update-demo-nautilus-jgbdx is created but not running
Sep 27 20:01:07.569: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-679324930 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-4207'
Sep 27 20:01:07.764: INFO: stderr: ""
Sep 27 20:01:07.764: INFO: stdout: "update-demo-nautilus-jgbdx update-demo-nautilus-l2kq9 "
Sep 27 20:01:07.764: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-679324930 get pods update-demo-nautilus-jgbdx -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-4207'
Sep 27 20:01:07.915: INFO: stderr: ""
Sep 27 20:01:07.915: INFO: stdout: "true"
Sep 27 20:01:07.915: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-679324930 get pods update-demo-nautilus-jgbdx -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-4207'
Sep 27 20:01:08.072: INFO: stderr: ""
Sep 27 20:01:08.072: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Sep 27 20:01:08.072: INFO: validating pod update-demo-nautilus-jgbdx
Sep 27 20:01:08.122: INFO: got data: {
  "image": "nautilus.jpg"
}

Sep 27 20:01:08.123: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Sep 27 20:01:08.123: INFO: update-demo-nautilus-jgbdx is verified up and running
Sep 27 20:01:08.123: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-679324930 get pods update-demo-nautilus-l2kq9 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-4207'
Sep 27 20:01:08.290: INFO: stderr: ""
Sep 27 20:01:08.290: INFO: stdout: "true"
Sep 27 20:01:08.290: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-679324930 get pods update-demo-nautilus-l2kq9 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-4207'
Sep 27 20:01:08.459: INFO: stderr: ""
Sep 27 20:01:08.459: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Sep 27 20:01:08.459: INFO: validating pod update-demo-nautilus-l2kq9
Sep 27 20:01:08.494: INFO: got data: {
  "image": "nautilus.jpg"
}

Sep 27 20:01:08.494: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Sep 27 20:01:08.494: INFO: update-demo-nautilus-l2kq9 is verified up and running
STEP: using delete to clean up resources
Sep 27 20:01:08.494: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-679324930 delete --grace-period=0 --force -f - --namespace=kubectl-4207'
Sep 27 20:01:08.695: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep 27 20:01:08.695: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Sep 27 20:01:08.695: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-679324930 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-4207'
Sep 27 20:01:08.904: INFO: stderr: "No resources found in kubectl-4207 namespace.\n"
Sep 27 20:01:08.904: INFO: stdout: ""
Sep 27 20:01:08.904: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-679324930 get pods -l name=update-demo --namespace=kubectl-4207 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Sep 27 20:01:09.078: INFO: stderr: ""
Sep 27 20:01:09.078: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 20:01:09.078: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4207" for this suite.

• [SLOW TEST:8.048 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:306
    should create and stop a replication controller  [Conformance]
    /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should create and stop a replication controller  [Conformance]","total":305,"completed":72,"skipped":1119,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 20:01:09.131: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename crd-webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:126
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
Sep 27 20:01:10.237: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
Sep 27 20:01:12.281: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63768369670, loc:(*time.Location)(0x7702840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63768369670, loc:(*time.Location)(0x7702840)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63768369670, loc:(*time.Location)(0x7702840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63768369670, loc:(*time.Location)(0x7702840)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-85d57b96d6\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep 27 20:01:15.331: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Sep 27 20:01:15.350: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Creating a v1 custom resource
STEP: Create a v2 custom resource
STEP: List CRs in v1
STEP: List CRs in v2
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 20:01:16.933: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-9720" for this suite.
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:137

• [SLOW TEST:8.098 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert a non homogeneous list of CRs [Conformance]","total":305,"completed":73,"skipped":1180,"failed":0}
[sig-cli] Kubectl client Guestbook application 
  should create and stop a working application  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 20:01:17.229: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[It] should create and stop a working application  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating all guestbook components
Sep 27 20:01:17.393: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-replica
  labels:
    app: agnhost
    role: replica
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: agnhost
    role: replica
    tier: backend

Sep 27 20:01:17.393: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-679324930 create -f - --namespace=kubectl-4925'
Sep 27 20:01:17.992: INFO: stderr: ""
Sep 27 20:01:17.992: INFO: stdout: "service/agnhost-replica created\n"
Sep 27 20:01:17.992: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-primary
  labels:
    app: agnhost
    role: primary
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: agnhost
    role: primary
    tier: backend

Sep 27 20:01:17.992: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-679324930 create -f - --namespace=kubectl-4925'
Sep 27 20:01:18.781: INFO: stderr: ""
Sep 27 20:01:18.781: INFO: stdout: "service/agnhost-primary created\n"
Sep 27 20:01:18.782: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Sep 27 20:01:18.782: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-679324930 create -f - --namespace=kubectl-4925'
Sep 27 20:01:19.588: INFO: stderr: ""
Sep 27 20:01:19.588: INFO: stdout: "service/frontend created\n"
Sep 27 20:01:19.589: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: guestbook-frontend
        image: k8s.gcr.io/e2e-test-images/agnhost:2.20
        args: [ "guestbook", "--backend-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 80

Sep 27 20:01:19.589: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-679324930 create -f - --namespace=kubectl-4925'
Sep 27 20:01:20.455: INFO: stderr: ""
Sep 27 20:01:20.455: INFO: stdout: "deployment.apps/frontend created\n"
Sep 27 20:01:20.456: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-primary
spec:
  replicas: 1
  selector:
    matchLabels:
      app: agnhost
      role: primary
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: primary
        tier: backend
    spec:
      containers:
      - name: primary
        image: k8s.gcr.io/e2e-test-images/agnhost:2.20
        args: [ "guestbook", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Sep 27 20:01:20.456: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-679324930 create -f - --namespace=kubectl-4925'
Sep 27 20:01:21.230: INFO: stderr: ""
Sep 27 20:01:21.230: INFO: stdout: "deployment.apps/agnhost-primary created\n"
Sep 27 20:01:21.230: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-replica
spec:
  replicas: 2
  selector:
    matchLabels:
      app: agnhost
      role: replica
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: replica
        tier: backend
    spec:
      containers:
      - name: replica
        image: k8s.gcr.io/e2e-test-images/agnhost:2.20
        args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Sep 27 20:01:21.230: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-679324930 create -f - --namespace=kubectl-4925'
Sep 27 20:01:22.055: INFO: stderr: ""
Sep 27 20:01:22.055: INFO: stdout: "deployment.apps/agnhost-replica created\n"
STEP: validating guestbook app
Sep 27 20:01:22.055: INFO: Waiting for all frontend pods to be Running.
Sep 27 20:01:27.106: INFO: Waiting for frontend to serve content.
Sep 27 20:01:27.154: INFO: Trying to add a new entry to the guestbook.
Sep 27 20:01:27.212: INFO: Verifying that added entry can be retrieved.
STEP: using delete to clean up resources
Sep 27 20:01:27.256: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-679324930 delete --grace-period=0 --force -f - --namespace=kubectl-4925'
Sep 27 20:01:27.514: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep 27 20:01:27.514: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
STEP: using delete to clean up resources
Sep 27 20:01:27.514: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-679324930 delete --grace-period=0 --force -f - --namespace=kubectl-4925'
Sep 27 20:01:27.809: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep 27 20:01:27.809: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources
Sep 27 20:01:27.809: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-679324930 delete --grace-period=0 --force -f - --namespace=kubectl-4925'
Sep 27 20:01:28.076: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep 27 20:01:28.077: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Sep 27 20:01:28.077: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-679324930 delete --grace-period=0 --force -f - --namespace=kubectl-4925'
Sep 27 20:01:28.291: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep 27 20:01:28.291: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Sep 27 20:01:28.291: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-679324930 delete --grace-period=0 --force -f - --namespace=kubectl-4925'
Sep 27 20:01:28.460: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep 27 20:01:28.460: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources
Sep 27 20:01:28.461: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-679324930 delete --grace-period=0 --force -f - --namespace=kubectl-4925'
Sep 27 20:01:28.638: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep 27 20:01:28.638: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 20:01:28.638: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4925" for this suite.

• [SLOW TEST:11.448 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Guestbook application
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:351
    should create and stop a working application  [Conformance]
    /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Guestbook application should create and stop a working application  [Conformance]","total":305,"completed":74,"skipped":1180,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 20:01:28.679: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-7756.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-7756.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-7756.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-7756.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-7756.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-7756.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe /etc/hosts
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Sep 27 20:01:33.254: INFO: DNS probes using dns-7756/dns-test-b404fc45-3e72-43f4-90d2-3f380d580808 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 20:01:33.295: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-7756" for this suite.
•{"msg":"PASSED [sig-network] DNS should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]","total":305,"completed":75,"skipped":1201,"failed":0}

------------------------------
[k8s.io] Kubelet when scheduling a busybox Pod with hostAliases 
  should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 20:01:33.339: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[It] should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 20:01:35.619: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-6790" for this suite.
•{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox Pod with hostAliases should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":76,"skipped":1201,"failed":0}
SSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should patch a secret [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 20:01:35.660: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should patch a secret [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a secret
STEP: listing secrets in all namespaces to ensure that there are more than zero
STEP: patching the secret
STEP: deleting the secret using a LabelSelector
STEP: listing secrets in all namespaces, searching for label name and value in patch
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 20:01:36.414: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1866" for this suite.
•{"msg":"PASSED [sig-api-machinery] Secrets should patch a secret [Conformance]","total":305,"completed":77,"skipped":1208,"failed":0}
SSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 20:01:36.469: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:162
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod
Sep 27 20:01:36.607: INFO: PodSpec: initContainers in spec.initContainers
Sep 27 20:02:20.523: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-f0483e99-2ebf-4c46-a0ae-431d5daf2639", GenerateName:"", Namespace:"init-container-6391", SelfLink:"/api/v1/namespaces/init-container-6391/pods/pod-init-f0483e99-2ebf-4c46-a0ae-431d5daf2639", UID:"7d03ae5c-7f45-4c0b-bf0e-2aa21d4aa591", ResourceVersion:"72953", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63768369696, loc:(*time.Location)(0x7702840)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"607721031"}, Annotations:map[string]string{"cni.projectcalico.org/podIP":"172.30.75.16/32", "cni.projectcalico.org/podIPs":"172.30.75.16/32", "k8s.v1.cni.cncf.io/network-status":"[{\n    \"name\": \"\",\n    \"ips\": [\n        \"172.30.75.16\"\n    ],\n    \"default\": true,\n    \"dns\": {}\n}]", "k8s.v1.cni.cncf.io/networks-status":"[{\n    \"name\": \"\",\n    \"ips\": [\n        \"172.30.75.16\"\n    ],\n    \"default\": true,\n    \"dns\": {}\n}]", "openshift.io/scc":"anyuid"}, OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:(*v1.Time)(0xc002c95040), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc002c95060)}, v1.ManagedFieldsEntry{Manager:"calico", Operation:"Update", APIVersion:"v1", Time:(*v1.Time)(0xc002c95080), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc002c950a0)}, v1.ManagedFieldsEntry{Manager:"multus", Operation:"Update", APIVersion:"v1", Time:(*v1.Time)(0xc002c950c0), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc002c950e0)}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:(*v1.Time)(0xc002c95100), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc002c95120)}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"default-token-gch6l", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(0xc004a5fe40), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-gch6l", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(0xc0040f9e00), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-gch6l", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(0xc0040f9e60), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"k8s.gcr.io/pause:3.2", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-gch6l", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(0xc0040f9da0), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc003277b18), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"10.177.248.117", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc000ae3b20), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc003277bd0)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc003277bf0)}, v1.Toleration{Key:"node.kubernetes.io/memory-pressure", Operator:"Exists", Value:"", Effect:"NoSchedule", TolerationSeconds:(*int64)(nil)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc003277c0c), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc003277c10), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc002121530), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63768369696, loc:(*time.Location)(0x7702840)}}, Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63768369696, loc:(*time.Location)(0x7702840)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63768369696, loc:(*time.Location)(0x7702840)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63768369696, loc:(*time.Location)(0x7702840)}}, Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"10.177.248.117", PodIP:"172.30.75.16", PodIPs:[]v1.PodIP{v1.PodIP{IP:"172.30.75.16"}}, StartTime:(*v1.Time)(0xc002c95140), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc000ae3c00)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc000ae3d50)}, Ready:false, RestartCount:3, Image:"docker.io/library/busybox:1.29", ImageID:"docker.io/library/busybox@sha256:8ccbac733d19c0dd4d70b4f0c1e12245b5fa3ad24758a11035ee505c629c0796", ContainerID:"cri-o://b6b4a97c5cf6526e132020f2fe0182eaa01c87d94569cfbb68dfb1103505dee1", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc002c95180), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"docker.io/library/busybox:1.29", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc002c95160), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/pause:3.2", ImageID:"", ContainerID:"", Started:(*bool)(0xc003277c8f)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 20:02:20.529: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-6391" for this suite.

• [SLOW TEST:44.103 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should not start app containers if init containers fail on a RestartAlways pod [Conformance]","total":305,"completed":78,"skipped":1211,"failed":0}
[k8s.io] Security Context When creating a pod with privileged 
  should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 20:02:20.572: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:41
[It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Sep 27 20:02:20.795: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-e6857fb6-45c8-4b41-b871-d74c0ad8d492" in namespace "security-context-test-2986" to be "Succeeded or Failed"
Sep 27 20:02:20.808: INFO: Pod "busybox-privileged-false-e6857fb6-45c8-4b41-b871-d74c0ad8d492": Phase="Pending", Reason="", readiness=false. Elapsed: 12.866872ms
Sep 27 20:02:22.825: INFO: Pod "busybox-privileged-false-e6857fb6-45c8-4b41-b871-d74c0ad8d492": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.030417966s
Sep 27 20:02:22.825: INFO: Pod "busybox-privileged-false-e6857fb6-45c8-4b41-b871-d74c0ad8d492" satisfied condition "Succeeded or Failed"
Sep 27 20:02:22.897: INFO: Got logs for pod "busybox-privileged-false-e6857fb6-45c8-4b41-b871-d74c0ad8d492": "ip: RTNETLINK answers: Operation not permitted\n"
[AfterEach] [k8s.io] Security Context
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 20:02:22.897: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-2986" for this suite.
•{"msg":"PASSED [k8s.io] Security Context When creating a pod with privileged should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":79,"skipped":1211,"failed":0}
SSSSSS
------------------------------
[k8s.io] Pods 
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 20:02:22.994: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:181
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Sep 27 20:02:23.129: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 20:02:27.301: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-7629" for this suite.
•{"msg":"PASSED [k8s.io] Pods should support retrieving logs from the container over websockets [NodeConformance] [Conformance]","total":305,"completed":80,"skipped":1217,"failed":0}

------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not conflict [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 20:02:27.347: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not conflict [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Cleaning up the secret
STEP: Cleaning up the configmap
STEP: Cleaning up the pod
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 20:02:29.755: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-1394" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not conflict [Conformance]","total":305,"completed":81,"skipped":1217,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 20:02:29.799: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating service in namespace services-9024
STEP: creating service affinity-nodeport-transition in namespace services-9024
STEP: creating replication controller affinity-nodeport-transition in namespace services-9024
I0927 20:02:30.064095      22 runners.go:190] Created replication controller with name: affinity-nodeport-transition, namespace: services-9024, replica count: 3
I0927 20:02:33.114703      22 runners.go:190] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Sep 27 20:02:33.177: INFO: Creating new exec pod
Sep 27 20:02:38.288: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-679324930 exec --namespace=services-9024 execpod-affinityhc7mk -- /bin/sh -x -c nc -zv -t -w 2 affinity-nodeport-transition 80'
Sep 27 20:02:38.807: INFO: stderr: "+ nc -zv -t -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
Sep 27 20:02:38.807: INFO: stdout: ""
Sep 27 20:02:38.808: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-679324930 exec --namespace=services-9024 execpod-affinityhc7mk -- /bin/sh -x -c nc -zv -t -w 2 172.21.70.251 80'
Sep 27 20:02:39.221: INFO: stderr: "+ nc -zv -t -w 2 172.21.70.251 80\nConnection to 172.21.70.251 80 port [tcp/http] succeeded!\n"
Sep 27 20:02:39.221: INFO: stdout: ""
Sep 27 20:02:39.221: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-679324930 exec --namespace=services-9024 execpod-affinityhc7mk -- /bin/sh -x -c nc -zv -t -w 2 10.177.248.117 32048'
Sep 27 20:02:39.638: INFO: stderr: "+ nc -zv -t -w 2 10.177.248.117 32048\nConnection to 10.177.248.117 32048 port [tcp/32048] succeeded!\n"
Sep 27 20:02:39.638: INFO: stdout: ""
Sep 27 20:02:39.638: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-679324930 exec --namespace=services-9024 execpod-affinityhc7mk -- /bin/sh -x -c nc -zv -t -w 2 10.177.248.114 32048'
Sep 27 20:02:40.076: INFO: stderr: "+ nc -zv -t -w 2 10.177.248.114 32048\nConnection to 10.177.248.114 32048 port [tcp/32048] succeeded!\n"
Sep 27 20:02:40.076: INFO: stdout: ""
Sep 27 20:02:40.076: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-679324930 exec --namespace=services-9024 execpod-affinityhc7mk -- /bin/sh -x -c nc -zv -t -w 2 169.61.243.3 32048'
Sep 27 20:02:40.550: INFO: stderr: "+ nc -zv -t -w 2 169.61.243.3 32048\nConnection to 169.61.243.3 32048 port [tcp/32048] succeeded!\n"
Sep 27 20:02:40.550: INFO: stdout: ""
Sep 27 20:02:40.550: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-679324930 exec --namespace=services-9024 execpod-affinityhc7mk -- /bin/sh -x -c nc -zv -t -w 2 169.61.243.2 32048'
Sep 27 20:02:41.006: INFO: stderr: "+ nc -zv -t -w 2 169.61.243.2 32048\nConnection to 169.61.243.2 32048 port [tcp/32048] succeeded!\n"
Sep 27 20:02:41.006: INFO: stdout: ""
Sep 27 20:02:41.042: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-679324930 exec --namespace=services-9024 execpod-affinityhc7mk -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.177.248.114:32048/ ; done'
Sep 27 20:02:41.650: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.177.248.114:32048/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.177.248.114:32048/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.177.248.114:32048/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.177.248.114:32048/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.177.248.114:32048/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.177.248.114:32048/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.177.248.114:32048/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.177.248.114:32048/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.177.248.114:32048/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.177.248.114:32048/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.177.248.114:32048/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.177.248.114:32048/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.177.248.114:32048/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.177.248.114:32048/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.177.248.114:32048/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.177.248.114:32048/\n"
Sep 27 20:02:41.650: INFO: stdout: "\naffinity-nodeport-transition-z6dxw\naffinity-nodeport-transition-sjr7b\naffinity-nodeport-transition-sjr7b\naffinity-nodeport-transition-z6dxw\naffinity-nodeport-transition-6fhkj\naffinity-nodeport-transition-sjr7b\naffinity-nodeport-transition-sjr7b\naffinity-nodeport-transition-6fhkj\naffinity-nodeport-transition-z6dxw\naffinity-nodeport-transition-6fhkj\naffinity-nodeport-transition-6fhkj\naffinity-nodeport-transition-z6dxw\naffinity-nodeport-transition-6fhkj\naffinity-nodeport-transition-6fhkj\naffinity-nodeport-transition-z6dxw\naffinity-nodeport-transition-sjr7b"
Sep 27 20:02:41.650: INFO: Received response from host: affinity-nodeport-transition-z6dxw
Sep 27 20:02:41.650: INFO: Received response from host: affinity-nodeport-transition-sjr7b
Sep 27 20:02:41.650: INFO: Received response from host: affinity-nodeport-transition-sjr7b
Sep 27 20:02:41.650: INFO: Received response from host: affinity-nodeport-transition-z6dxw
Sep 27 20:02:41.650: INFO: Received response from host: affinity-nodeport-transition-6fhkj
Sep 27 20:02:41.650: INFO: Received response from host: affinity-nodeport-transition-sjr7b
Sep 27 20:02:41.650: INFO: Received response from host: affinity-nodeport-transition-sjr7b
Sep 27 20:02:41.650: INFO: Received response from host: affinity-nodeport-transition-6fhkj
Sep 27 20:02:41.650: INFO: Received response from host: affinity-nodeport-transition-z6dxw
Sep 27 20:02:41.650: INFO: Received response from host: affinity-nodeport-transition-6fhkj
Sep 27 20:02:41.650: INFO: Received response from host: affinity-nodeport-transition-6fhkj
Sep 27 20:02:41.650: INFO: Received response from host: affinity-nodeport-transition-z6dxw
Sep 27 20:02:41.650: INFO: Received response from host: affinity-nodeport-transition-6fhkj
Sep 27 20:02:41.650: INFO: Received response from host: affinity-nodeport-transition-6fhkj
Sep 27 20:02:41.650: INFO: Received response from host: affinity-nodeport-transition-z6dxw
Sep 27 20:02:41.650: INFO: Received response from host: affinity-nodeport-transition-sjr7b
Sep 27 20:02:41.705: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-679324930 exec --namespace=services-9024 execpod-affinityhc7mk -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.177.248.114:32048/ ; done'
Sep 27 20:02:42.277: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.177.248.114:32048/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.177.248.114:32048/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.177.248.114:32048/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.177.248.114:32048/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.177.248.114:32048/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.177.248.114:32048/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.177.248.114:32048/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.177.248.114:32048/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.177.248.114:32048/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.177.248.114:32048/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.177.248.114:32048/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.177.248.114:32048/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.177.248.114:32048/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.177.248.114:32048/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.177.248.114:32048/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.177.248.114:32048/\n"
Sep 27 20:02:42.277: INFO: stdout: "\naffinity-nodeport-transition-sjr7b\naffinity-nodeport-transition-sjr7b\naffinity-nodeport-transition-sjr7b\naffinity-nodeport-transition-sjr7b\naffinity-nodeport-transition-sjr7b\naffinity-nodeport-transition-sjr7b\naffinity-nodeport-transition-sjr7b\naffinity-nodeport-transition-sjr7b\naffinity-nodeport-transition-sjr7b\naffinity-nodeport-transition-sjr7b\naffinity-nodeport-transition-sjr7b\naffinity-nodeport-transition-sjr7b\naffinity-nodeport-transition-sjr7b\naffinity-nodeport-transition-sjr7b\naffinity-nodeport-transition-sjr7b\naffinity-nodeport-transition-sjr7b"
Sep 27 20:02:42.277: INFO: Received response from host: affinity-nodeport-transition-sjr7b
Sep 27 20:02:42.277: INFO: Received response from host: affinity-nodeport-transition-sjr7b
Sep 27 20:02:42.277: INFO: Received response from host: affinity-nodeport-transition-sjr7b
Sep 27 20:02:42.277: INFO: Received response from host: affinity-nodeport-transition-sjr7b
Sep 27 20:02:42.277: INFO: Received response from host: affinity-nodeport-transition-sjr7b
Sep 27 20:02:42.277: INFO: Received response from host: affinity-nodeport-transition-sjr7b
Sep 27 20:02:42.277: INFO: Received response from host: affinity-nodeport-transition-sjr7b
Sep 27 20:02:42.277: INFO: Received response from host: affinity-nodeport-transition-sjr7b
Sep 27 20:02:42.277: INFO: Received response from host: affinity-nodeport-transition-sjr7b
Sep 27 20:02:42.277: INFO: Received response from host: affinity-nodeport-transition-sjr7b
Sep 27 20:02:42.277: INFO: Received response from host: affinity-nodeport-transition-sjr7b
Sep 27 20:02:42.277: INFO: Received response from host: affinity-nodeport-transition-sjr7b
Sep 27 20:02:42.277: INFO: Received response from host: affinity-nodeport-transition-sjr7b
Sep 27 20:02:42.277: INFO: Received response from host: affinity-nodeport-transition-sjr7b
Sep 27 20:02:42.277: INFO: Received response from host: affinity-nodeport-transition-sjr7b
Sep 27 20:02:42.277: INFO: Received response from host: affinity-nodeport-transition-sjr7b
Sep 27 20:02:42.277: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-9024, will wait for the garbage collector to delete the pods
Sep 27 20:02:42.407: INFO: Deleting ReplicationController affinity-nodeport-transition took: 24.923964ms
Sep 27 20:02:42.507: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 100.321607ms
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 20:02:59.393: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-9024" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

• [SLOW TEST:29.634 seconds]
[sig-network] Services
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]","total":305,"completed":82,"skipped":1270,"failed":0}
SS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 20:02:59.433: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name configmap-test-volume-map-f4ef15e4-f62b-4efd-adff-16ebbd592e6e
STEP: Creating a pod to test consume configMaps
Sep 27 20:02:59.658: INFO: Waiting up to 5m0s for pod "pod-configmaps-44be6ea2-16b7-448e-8098-4c6734389cfb" in namespace "configmap-5960" to be "Succeeded or Failed"
Sep 27 20:02:59.672: INFO: Pod "pod-configmaps-44be6ea2-16b7-448e-8098-4c6734389cfb": Phase="Pending", Reason="", readiness=false. Elapsed: 13.361909ms
Sep 27 20:03:01.688: INFO: Pod "pod-configmaps-44be6ea2-16b7-448e-8098-4c6734389cfb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029360184s
Sep 27 20:03:03.705: INFO: Pod "pod-configmaps-44be6ea2-16b7-448e-8098-4c6734389cfb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.046651896s
STEP: Saw pod success
Sep 27 20:03:03.705: INFO: Pod "pod-configmaps-44be6ea2-16b7-448e-8098-4c6734389cfb" satisfied condition "Succeeded or Failed"
Sep 27 20:03:03.725: INFO: Trying to get logs from node 10.177.248.117 pod pod-configmaps-44be6ea2-16b7-448e-8098-4c6734389cfb container configmap-volume-test: <nil>
STEP: delete the pod
Sep 27 20:03:03.814: INFO: Waiting for pod pod-configmaps-44be6ea2-16b7-448e-8098-4c6734389cfb to disappear
Sep 27 20:03:03.831: INFO: Pod pod-configmaps-44be6ea2-16b7-448e-8098-4c6734389cfb no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 20:03:03.832: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5960" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":83,"skipped":1272,"failed":0}
SSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 20:03:03.878: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a ResourceQuota with best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a best-effort pod
STEP: Ensuring resource quota with best effort scope captures the pod usage
STEP: Ensuring resource quota with not best effort ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a not best-effort pod
STEP: Ensuring resource quota with not best effort scope captures the pod usage
STEP: Ensuring resource quota with best effort scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 20:03:20.356: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-6590" for this suite.

• [SLOW TEST:16.533 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with best effort scope. [Conformance]","total":305,"completed":84,"skipped":1279,"failed":0}
SSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 20:03:20.411: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Sep 27 20:03:28.881: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Sep 27 20:03:28.900: INFO: Pod pod-with-prestop-exec-hook still exists
Sep 27 20:03:30.900: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Sep 27 20:03:30.913: INFO: Pod pod-with-prestop-exec-hook still exists
Sep 27 20:03:32.900: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Sep 27 20:03:32.915: INFO: Pod pod-with-prestop-exec-hook still exists
Sep 27 20:03:34.900: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Sep 27 20:03:34.914: INFO: Pod pod-with-prestop-exec-hook still exists
Sep 27 20:03:36.901: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Sep 27 20:03:36.925: INFO: Pod pod-with-prestop-exec-hook still exists
Sep 27 20:03:38.900: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Sep 27 20:03:38.915: INFO: Pod pod-with-prestop-exec-hook still exists
Sep 27 20:03:40.900: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Sep 27 20:03:40.915: INFO: Pod pod-with-prestop-exec-hook still exists
Sep 27 20:03:42.900: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Sep 27 20:03:42.914: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 20:03:42.949: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-8962" for this suite.

• [SLOW TEST:22.578 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  when create a pod with lifecycle hook
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop exec hook properly [NodeConformance] [Conformance]","total":305,"completed":85,"skipped":1282,"failed":0}
S
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 20:03:42.991: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name configmap-test-volume-map-74281293-bcb6-43ec-b47c-220c9b7da539
STEP: Creating a pod to test consume configMaps
Sep 27 20:03:43.204: INFO: Waiting up to 5m0s for pod "pod-configmaps-f6476fa6-64bc-4520-8e48-e9cefee5ed27" in namespace "configmap-2146" to be "Succeeded or Failed"
Sep 27 20:03:43.220: INFO: Pod "pod-configmaps-f6476fa6-64bc-4520-8e48-e9cefee5ed27": Phase="Pending", Reason="", readiness=false. Elapsed: 15.1454ms
Sep 27 20:03:45.239: INFO: Pod "pod-configmaps-f6476fa6-64bc-4520-8e48-e9cefee5ed27": Phase="Pending", Reason="", readiness=false. Elapsed: 2.034414927s
Sep 27 20:03:47.253: INFO: Pod "pod-configmaps-f6476fa6-64bc-4520-8e48-e9cefee5ed27": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.048771769s
STEP: Saw pod success
Sep 27 20:03:47.253: INFO: Pod "pod-configmaps-f6476fa6-64bc-4520-8e48-e9cefee5ed27" satisfied condition "Succeeded or Failed"
Sep 27 20:03:47.267: INFO: Trying to get logs from node 10.177.248.117 pod pod-configmaps-f6476fa6-64bc-4520-8e48-e9cefee5ed27 container configmap-volume-test: <nil>
STEP: delete the pod
Sep 27 20:03:47.352: INFO: Waiting for pod pod-configmaps-f6476fa6-64bc-4520-8e48-e9cefee5ed27 to disappear
Sep 27 20:03:47.366: INFO: Pod pod-configmaps-f6476fa6-64bc-4520-8e48-e9cefee5ed27 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 20:03:47.366: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2146" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","total":305,"completed":86,"skipped":1283,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 20:03:47.416: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a watch on configmaps
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: closing the watch once it receives two notifications
Sep 27 20:03:47.700: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-3013 /api/v1/namespaces/watch-3013/configmaps/e2e-watch-test-watch-closed 3cb8fe9a-a011-47c0-a51d-6f55599f09b3 74282 0 2021-09-27 20:03:47 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2021-09-27 20:03:47 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Sep 27 20:03:47.700: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-3013 /api/v1/namespaces/watch-3013/configmaps/e2e-watch-test-watch-closed 3cb8fe9a-a011-47c0-a51d-6f55599f09b3 74285 0 2021-09-27 20:03:47 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2021-09-27 20:03:47 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time, while the watch is closed
STEP: creating a new watch on configmaps from the last resource version observed by the first watch
STEP: deleting the configmap
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed
Sep 27 20:03:47.777: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-3013 /api/v1/namespaces/watch-3013/configmaps/e2e-watch-test-watch-closed 3cb8fe9a-a011-47c0-a51d-6f55599f09b3 74286 0 2021-09-27 20:03:47 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2021-09-27 20:03:47 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Sep 27 20:03:47.778: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-3013 /api/v1/namespaces/watch-3013/configmaps/e2e-watch-test-watch-closed 3cb8fe9a-a011-47c0-a51d-6f55599f09b3 74288 0 2021-09-27 20:03:47 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2021-09-27 20:03:47 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 20:03:47.778: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-3013" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should be able to restart watching from the last resource version observed by the previous watch [Conformance]","total":305,"completed":87,"skipped":1341,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 20:03:47.829: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating the pod
Sep 27 20:03:50.665: INFO: Successfully updated pod "labelsupdate6a081e52-90a8-450a-a85f-25256dc2faf3"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 20:03:52.739: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8295" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should update labels on modification [NodeConformance] [Conformance]","total":305,"completed":88,"skipped":1358,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 20:03:52.780: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a service externalname-service with the type=ExternalName in namespace services-9624
STEP: changing the ExternalName service to type=NodePort
STEP: creating replication controller externalname-service in namespace services-9624
I0927 20:03:53.076038      22 runners.go:190] Created replication controller with name: externalname-service, namespace: services-9624, replica count: 2
I0927 20:03:56.126488      22 runners.go:190] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Sep 27 20:03:56.126: INFO: Creating new exec pod
Sep 27 20:03:59.219: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-679324930 exec --namespace=services-9624 execpod8lc5t -- /bin/sh -x -c nc -zv -t -w 2 externalname-service 80'
Sep 27 20:03:59.629: INFO: stderr: "+ nc -zv -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Sep 27 20:03:59.629: INFO: stdout: ""
Sep 27 20:03:59.630: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-679324930 exec --namespace=services-9624 execpod8lc5t -- /bin/sh -x -c nc -zv -t -w 2 172.21.108.24 80'
Sep 27 20:04:00.099: INFO: stderr: "+ nc -zv -t -w 2 172.21.108.24 80\nConnection to 172.21.108.24 80 port [tcp/http] succeeded!\n"
Sep 27 20:04:00.099: INFO: stdout: ""
Sep 27 20:04:00.099: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-679324930 exec --namespace=services-9624 execpod8lc5t -- /bin/sh -x -c nc -zv -t -w 2 10.177.248.126 32254'
Sep 27 20:04:00.737: INFO: stderr: "+ nc -zv -t -w 2 10.177.248.126 32254\nConnection to 10.177.248.126 32254 port [tcp/32254] succeeded!\n"
Sep 27 20:04:00.737: INFO: stdout: ""
Sep 27 20:04:00.737: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-679324930 exec --namespace=services-9624 execpod8lc5t -- /bin/sh -x -c nc -zv -t -w 2 10.177.248.114 32254'
Sep 27 20:04:01.628: INFO: stderr: "+ nc -zv -t -w 2 10.177.248.114 32254\nConnection to 10.177.248.114 32254 port [tcp/32254] succeeded!\n"
Sep 27 20:04:01.629: INFO: stdout: ""
Sep 27 20:04:01.629: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-679324930 exec --namespace=services-9624 execpod8lc5t -- /bin/sh -x -c nc -zv -t -w 2 169.61.243.7 32254'
Sep 27 20:04:02.199: INFO: stderr: "+ nc -zv -t -w 2 169.61.243.7 32254\nConnection to 169.61.243.7 32254 port [tcp/32254] succeeded!\n"
Sep 27 20:04:02.199: INFO: stdout: ""
Sep 27 20:04:02.199: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-679324930 exec --namespace=services-9624 execpod8lc5t -- /bin/sh -x -c nc -zv -t -w 2 169.61.243.2 32254'
Sep 27 20:04:02.632: INFO: stderr: "+ nc -zv -t -w 2 169.61.243.2 32254\nConnection to 169.61.243.2 32254 port [tcp/32254] succeeded!\n"
Sep 27 20:04:02.632: INFO: stdout: ""
Sep 27 20:04:02.632: INFO: Cleaning up the ExternalName to NodePort test service
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 20:04:02.782: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-9624" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

• [SLOW TEST:10.043 seconds]
[sig-network] Services
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to NodePort [Conformance]","total":305,"completed":89,"skipped":1378,"failed":0}
S
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 20:04:02.824: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ConfigMap
STEP: Ensuring resource quota status captures configMap creation
STEP: Deleting a ConfigMap
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 20:04:19.123: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-117" for this suite.

• [SLOW TEST:16.342 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a configMap. [Conformance]","total":305,"completed":90,"skipped":1379,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to create a functioning NodePort service [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 20:04:19.166: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should be able to create a functioning NodePort service [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating service nodeport-test with type=NodePort in namespace services-4688
STEP: creating replication controller nodeport-test in namespace services-4688
I0927 20:04:19.397456      22 runners.go:190] Created replication controller with name: nodeport-test, namespace: services-4688, replica count: 2
Sep 27 20:04:22.447: INFO: Creating new exec pod
I0927 20:04:22.447892      22 runners.go:190] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Sep 27 20:04:25.542: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-679324930 exec --namespace=services-4688 execpod58lw4 -- /bin/sh -x -c nc -zv -t -w 2 nodeport-test 80'
Sep 27 20:04:25.971: INFO: stderr: "+ nc -zv -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Sep 27 20:04:25.971: INFO: stdout: ""
Sep 27 20:04:25.971: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-679324930 exec --namespace=services-4688 execpod58lw4 -- /bin/sh -x -c nc -zv -t -w 2 172.21.98.189 80'
Sep 27 20:04:26.422: INFO: stderr: "+ nc -zv -t -w 2 172.21.98.189 80\nConnection to 172.21.98.189 80 port [tcp/http] succeeded!\n"
Sep 27 20:04:26.423: INFO: stdout: ""
Sep 27 20:04:26.423: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-679324930 exec --namespace=services-4688 execpod58lw4 -- /bin/sh -x -c nc -zv -t -w 2 10.177.248.114 32091'
Sep 27 20:04:26.859: INFO: stderr: "+ nc -zv -t -w 2 10.177.248.114 32091\nConnection to 10.177.248.114 32091 port [tcp/32091] succeeded!\n"
Sep 27 20:04:26.860: INFO: stdout: ""
Sep 27 20:04:26.860: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-679324930 exec --namespace=services-4688 execpod58lw4 -- /bin/sh -x -c nc -zv -t -w 2 10.177.248.126 32091'
Sep 27 20:04:27.280: INFO: stderr: "+ nc -zv -t -w 2 10.177.248.126 32091\nConnection to 10.177.248.126 32091 port [tcp/32091] succeeded!\n"
Sep 27 20:04:27.280: INFO: stdout: ""
Sep 27 20:04:27.281: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-679324930 exec --namespace=services-4688 execpod58lw4 -- /bin/sh -x -c nc -zv -t -w 2 169.61.243.2 32091'
Sep 27 20:04:27.691: INFO: stderr: "+ nc -zv -t -w 2 169.61.243.2 32091\nConnection to 169.61.243.2 32091 port [tcp/32091] succeeded!\n"
Sep 27 20:04:27.691: INFO: stdout: ""
Sep 27 20:04:27.691: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-679324930 exec --namespace=services-4688 execpod58lw4 -- /bin/sh -x -c nc -zv -t -w 2 169.61.243.7 32091'
Sep 27 20:04:28.192: INFO: stderr: "+ nc -zv -t -w 2 169.61.243.7 32091\nConnection to 169.61.243.7 32091 port [tcp/32091] succeeded!\n"
Sep 27 20:04:28.192: INFO: stdout: ""
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 20:04:28.193: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4688" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

• [SLOW TEST:9.067 seconds]
[sig-network] Services
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to create a functioning NodePort service [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should be able to create a functioning NodePort service [Conformance]","total":305,"completed":91,"skipped":1394,"failed":0}
SSSSSSSS
------------------------------
[sig-auth] Certificates API [Privileged:ClusterAdmin] 
  should support CSR API operations [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 20:04:28.234: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename certificates
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support CSR API operations [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: getting /apis
STEP: getting /apis/certificates.k8s.io
STEP: getting /apis/certificates.k8s.io/v1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Sep 27 20:04:29.330: INFO: starting watch
STEP: patching
STEP: updating
Sep 27 20:04:29.373: INFO: waiting for watch events with expected annotations
Sep 27 20:04:29.373: INFO: saw patched and updated annotations
STEP: getting /approval
STEP: patching /approval
STEP: updating /approval
STEP: getting /status
STEP: patching /status
STEP: updating /status
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 20:04:29.594: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "certificates-1806" for this suite.
•{"msg":"PASSED [sig-auth] Certificates API [Privileged:ClusterAdmin] should support CSR API operations [Conformance]","total":305,"completed":92,"skipped":1402,"failed":0}
SS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 20:04:29.638: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating secret with name secret-test-27b8e226-84ca-4ff8-bbea-3b0bae9c03f0
STEP: Creating a pod to test consume secrets
Sep 27 20:04:29.868: INFO: Waiting up to 5m0s for pod "pod-secrets-7d5dc48a-6df0-4274-8b92-d82a4e518733" in namespace "secrets-8380" to be "Succeeded or Failed"
Sep 27 20:04:29.881: INFO: Pod "pod-secrets-7d5dc48a-6df0-4274-8b92-d82a4e518733": Phase="Pending", Reason="", readiness=false. Elapsed: 13.108836ms
Sep 27 20:04:31.897: INFO: Pod "pod-secrets-7d5dc48a-6df0-4274-8b92-d82a4e518733": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028815426s
Sep 27 20:04:33.913: INFO: Pod "pod-secrets-7d5dc48a-6df0-4274-8b92-d82a4e518733": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.045616595s
STEP: Saw pod success
Sep 27 20:04:33.914: INFO: Pod "pod-secrets-7d5dc48a-6df0-4274-8b92-d82a4e518733" satisfied condition "Succeeded or Failed"
Sep 27 20:04:33.927: INFO: Trying to get logs from node 10.177.248.117 pod pod-secrets-7d5dc48a-6df0-4274-8b92-d82a4e518733 container secret-volume-test: <nil>
STEP: delete the pod
Sep 27 20:04:34.009: INFO: Waiting for pod pod-secrets-7d5dc48a-6df0-4274-8b92-d82a4e518733 to disappear
Sep 27 20:04:34.023: INFO: Pod pod-secrets-7d5dc48a-6df0-4274-8b92-d82a4e518733 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 20:04:34.023: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8380" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":93,"skipped":1404,"failed":0}
SSSS
------------------------------
[sig-cli] Kubectl client Kubectl cluster-info 
  should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 20:04:34.066: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[It] should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: validating cluster-info
Sep 27 20:04:34.214: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-679324930 cluster-info'
Sep 27 20:04:34.381: INFO: stderr: ""
Sep 27 20:04:34.381: INFO: stdout: "\x1b[0;32mKubernetes master\x1b[0m is running at \x1b[0;33mhttps://172.21.0.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 20:04:34.381: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4481" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]","total":305,"completed":94,"skipped":1408,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 20:04:34.428: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:181
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Sep 27 20:04:34.572: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 20:04:36.882: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-7334" for this suite.
•{"msg":"PASSED [k8s.io] Pods should support remote command execution over websockets [NodeConformance] [Conformance]","total":305,"completed":95,"skipped":1449,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 20:04:36.931: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating the pod
Sep 27 20:04:41.805: INFO: Successfully updated pod "annotationupdatea1d12a66-3b3e-4a11-94ac-c5363cc671fc"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 20:04:43.876: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9841" for this suite.

• [SLOW TEST:7.001 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:37
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should update annotations on modification [NodeConformance] [Conformance]","total":305,"completed":96,"skipped":1464,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 20:04:43.935: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:134
[It] should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Sep 27 20:04:44.354: INFO: Create a RollingUpdate DaemonSet
Sep 27 20:04:44.374: INFO: Check that daemon pods launch on every node of the cluster
Sep 27 20:04:44.423: INFO: Number of nodes with available pods: 0
Sep 27 20:04:44.423: INFO: Node 10.177.248.114 is running more than one daemon pod
Sep 27 20:04:45.463: INFO: Number of nodes with available pods: 0
Sep 27 20:04:45.463: INFO: Node 10.177.248.114 is running more than one daemon pod
Sep 27 20:04:46.462: INFO: Number of nodes with available pods: 1
Sep 27 20:04:46.463: INFO: Node 10.177.248.114 is running more than one daemon pod
Sep 27 20:04:47.460: INFO: Number of nodes with available pods: 3
Sep 27 20:04:47.460: INFO: Number of running nodes: 3, number of available pods: 3
Sep 27 20:04:47.460: INFO: Update the DaemonSet to trigger a rollout
Sep 27 20:04:47.545: INFO: Updating DaemonSet daemon-set
Sep 27 20:05:02.615: INFO: Roll back the DaemonSet before rollout is complete
Sep 27 20:05:02.657: INFO: Updating DaemonSet daemon-set
Sep 27 20:05:02.658: INFO: Make sure DaemonSet rollback is complete
Sep 27 20:05:02.673: INFO: Wrong image for pod: daemon-set-92bd9. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Sep 27 20:05:02.674: INFO: Pod daemon-set-92bd9 is not available
Sep 27 20:05:03.715: INFO: Wrong image for pod: daemon-set-92bd9. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Sep 27 20:05:03.715: INFO: Pod daemon-set-92bd9 is not available
Sep 27 20:05:04.726: INFO: Wrong image for pod: daemon-set-92bd9. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Sep 27 20:05:04.726: INFO: Pod daemon-set-92bd9 is not available
Sep 27 20:05:05.715: INFO: Wrong image for pod: daemon-set-92bd9. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Sep 27 20:05:05.715: INFO: Pod daemon-set-92bd9 is not available
Sep 27 20:05:06.724: INFO: Wrong image for pod: daemon-set-92bd9. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Sep 27 20:05:06.724: INFO: Pod daemon-set-92bd9 is not available
Sep 27 20:05:07.720: INFO: Wrong image for pod: daemon-set-92bd9. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Sep 27 20:05:07.720: INFO: Pod daemon-set-92bd9 is not available
Sep 27 20:05:08.713: INFO: Wrong image for pod: daemon-set-92bd9. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Sep 27 20:05:08.713: INFO: Pod daemon-set-92bd9 is not available
Sep 27 20:05:09.714: INFO: Wrong image for pod: daemon-set-92bd9. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Sep 27 20:05:09.714: INFO: Pod daemon-set-92bd9 is not available
Sep 27 20:05:10.722: INFO: Wrong image for pod: daemon-set-92bd9. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Sep 27 20:05:10.722: INFO: Pod daemon-set-92bd9 is not available
Sep 27 20:05:11.721: INFO: Wrong image for pod: daemon-set-92bd9. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Sep 27 20:05:11.721: INFO: Pod daemon-set-92bd9 is not available
Sep 27 20:05:12.716: INFO: Pod daemon-set-hd7jm is not available
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:100
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-8535, will wait for the garbage collector to delete the pods
Sep 27 20:05:12.869: INFO: Deleting DaemonSet.extensions daemon-set took: 39.67101ms
Sep 27 20:05:12.969: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.506239ms
Sep 27 20:05:22.088: INFO: Number of nodes with available pods: 0
Sep 27 20:05:22.088: INFO: Number of running nodes: 0, number of available pods: 0
Sep 27 20:05:22.103: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-8535/daemonsets","resourceVersion":"75711"},"items":null}

Sep 27 20:05:22.116: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-8535/pods","resourceVersion":"75711"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 20:05:22.191: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-8535" for this suite.

• [SLOW TEST:38.308 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","total":305,"completed":97,"skipped":1515,"failed":0}
SSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 20:05:22.244: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: getting the auto-created API token
Sep 27 20:05:23.029: INFO: created pod pod-service-account-defaultsa
Sep 27 20:05:23.030: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Sep 27 20:05:23.077: INFO: created pod pod-service-account-mountsa
Sep 27 20:05:23.077: INFO: pod pod-service-account-mountsa service account token volume mount: true
Sep 27 20:05:23.117: INFO: created pod pod-service-account-nomountsa
Sep 27 20:05:23.117: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Sep 27 20:05:23.154: INFO: created pod pod-service-account-defaultsa-mountspec
Sep 27 20:05:23.154: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Sep 27 20:05:23.193: INFO: created pod pod-service-account-mountsa-mountspec
Sep 27 20:05:23.193: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Sep 27 20:05:23.267: INFO: created pod pod-service-account-nomountsa-mountspec
Sep 27 20:05:23.267: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Sep 27 20:05:23.320: INFO: created pod pod-service-account-defaultsa-nomountspec
Sep 27 20:05:23.320: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Sep 27 20:05:23.363: INFO: created pod pod-service-account-mountsa-nomountspec
Sep 27 20:05:23.363: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Sep 27 20:05:23.429: INFO: created pod pod-service-account-nomountsa-nomountspec
Sep 27 20:05:23.431: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 20:05:23.432: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-1799" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should allow opting out of API token automount  [Conformance]","total":305,"completed":98,"skipped":1524,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should patch a Namespace [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 20:05:23.500: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should patch a Namespace [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a Namespace
STEP: patching the Namespace
STEP: get the Namespace and ensuring it has the label
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 20:05:23.868: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-980" for this suite.
STEP: Destroying namespace "nspatchtest-39e0f5e0-5fc8-496c-82ea-49e0825b1890-7703" for this suite.
•{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should patch a Namespace [Conformance]","total":305,"completed":99,"skipped":1540,"failed":0}
SSSSS
------------------------------
[sig-storage] Projected configMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 20:05:23.945: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Sep 27 20:05:24.208: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
STEP: Creating configMap with name cm-test-opt-del-edbe686d-e2ae-42a6-b314-6ac4d3205668
STEP: Creating configMap with name cm-test-opt-upd-3d5e7acc-27a3-45d3-b494-4f8691963ad1
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-edbe686d-e2ae-42a6-b314-6ac4d3205668
STEP: Updating configmap cm-test-opt-upd-3d5e7acc-27a3-45d3-b494-4f8691963ad1
STEP: Creating configMap with name cm-test-opt-create-c20887af-0fbf-495b-972b-54e964c77bac
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 20:06:38.453: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3106" for this suite.

• [SLOW TEST:74.554 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:36
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap optional updates should be reflected in volume [NodeConformance] [Conformance]","total":305,"completed":100,"skipped":1545,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 20:06:38.502: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0666 on tmpfs
Sep 27 20:06:38.721: INFO: Waiting up to 5m0s for pod "pod-e85241a6-4c00-4575-9848-589aa6d992d6" in namespace "emptydir-2043" to be "Succeeded or Failed"
Sep 27 20:06:38.737: INFO: Pod "pod-e85241a6-4c00-4575-9848-589aa6d992d6": Phase="Pending", Reason="", readiness=false. Elapsed: 15.73966ms
Sep 27 20:06:40.763: INFO: Pod "pod-e85241a6-4c00-4575-9848-589aa6d992d6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.04112756s
Sep 27 20:06:42.777: INFO: Pod "pod-e85241a6-4c00-4575-9848-589aa6d992d6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.054972718s
STEP: Saw pod success
Sep 27 20:06:42.777: INFO: Pod "pod-e85241a6-4c00-4575-9848-589aa6d992d6" satisfied condition "Succeeded or Failed"
Sep 27 20:06:42.794: INFO: Trying to get logs from node 10.177.248.117 pod pod-e85241a6-4c00-4575-9848-589aa6d992d6 container test-container: <nil>
STEP: delete the pod
Sep 27 20:06:42.911: INFO: Waiting for pod pod-e85241a6-4c00-4575-9848-589aa6d992d6 to disappear
Sep 27 20:06:42.925: INFO: Pod pod-e85241a6-4c00-4575-9848-589aa6d992d6 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 20:06:42.925: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2043" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":101,"skipped":1566,"failed":0}

------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 20:06:42.977: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating projection with secret that has name projected-secret-test-map-b6bcc098-45ad-42e7-be08-bcbc9be066db
STEP: Creating a pod to test consume secrets
Sep 27 20:06:43.212: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-08015d16-b3a3-4015-b8a6-fbf0cc34c11d" in namespace "projected-6757" to be "Succeeded or Failed"
Sep 27 20:06:43.232: INFO: Pod "pod-projected-secrets-08015d16-b3a3-4015-b8a6-fbf0cc34c11d": Phase="Pending", Reason="", readiness=false. Elapsed: 19.140995ms
Sep 27 20:06:45.249: INFO: Pod "pod-projected-secrets-08015d16-b3a3-4015-b8a6-fbf0cc34c11d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.036440067s
Sep 27 20:06:47.270: INFO: Pod "pod-projected-secrets-08015d16-b3a3-4015-b8a6-fbf0cc34c11d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.057170289s
STEP: Saw pod success
Sep 27 20:06:47.270: INFO: Pod "pod-projected-secrets-08015d16-b3a3-4015-b8a6-fbf0cc34c11d" satisfied condition "Succeeded or Failed"
Sep 27 20:06:47.284: INFO: Trying to get logs from node 10.177.248.117 pod pod-projected-secrets-08015d16-b3a3-4015-b8a6-fbf0cc34c11d container projected-secret-volume-test: <nil>
STEP: delete the pod
Sep 27 20:06:47.361: INFO: Waiting for pod pod-projected-secrets-08015d16-b3a3-4015-b8a6-fbf0cc34c11d to disappear
Sep 27 20:06:47.374: INFO: Pod pod-projected-secrets-08015d16-b3a3-4015-b8a6-fbf0cc34c11d no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 20:06:47.374: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6757" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":102,"skipped":1566,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should succeed in writing subpaths in container [sig-storage][Slow] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 20:06:47.423: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should succeed in writing subpaths in container [sig-storage][Slow] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod
STEP: waiting for pod running
STEP: creating a file in subpath
Sep 27 20:06:51.698: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-2620 PodName:var-expansion-01334f61-d879-47f7-a395-b97e9c6070b4 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep 27 20:06:51.699: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: test for file in mounted path
Sep 27 20:06:51.972: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-2620 PodName:var-expansion-01334f61-d879-47f7-a395-b97e9c6070b4 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep 27 20:06:51.974: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: updating the annotation value
Sep 27 20:06:52.757: INFO: Successfully updated pod "var-expansion-01334f61-d879-47f7-a395-b97e9c6070b4"
STEP: waiting for annotated pod running
STEP: deleting the pod gracefully
Sep 27 20:06:52.771: INFO: Deleting pod "var-expansion-01334f61-d879-47f7-a395-b97e9c6070b4" in namespace "var-expansion-2620"
Sep 27 20:06:52.796: INFO: Wait up to 5m0s for pod "var-expansion-01334f61-d879-47f7-a395-b97e9c6070b4" to be fully deleted
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 20:07:26.826: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-2620" for this suite.

• [SLOW TEST:39.446 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should succeed in writing subpaths in container [sig-storage][Slow] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Variable Expansion should succeed in writing subpaths in container [sig-storage][Slow] [Conformance]","total":305,"completed":103,"skipped":1580,"failed":0}
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 20:07:26.869: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Sep 27 20:07:31.323: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Sep 27 20:07:31.341: INFO: Pod pod-with-poststart-exec-hook still exists
Sep 27 20:07:33.341: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Sep 27 20:07:33.357: INFO: Pod pod-with-poststart-exec-hook still exists
Sep 27 20:07:35.341: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Sep 27 20:07:35.356: INFO: Pod pod-with-poststart-exec-hook still exists
Sep 27 20:07:37.341: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Sep 27 20:07:37.355: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 20:07:37.355: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-7276" for this suite.

• [SLOW TEST:10.527 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  when create a pod with lifecycle hook
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart exec hook properly [NodeConformance] [Conformance]","total":305,"completed":104,"skipped":1580,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 20:07:37.396: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Sep 27 20:07:37.604: INFO: Waiting up to 5m0s for pod "downwardapi-volume-3a54ed29-3b03-45d9-90c5-3c8701443c6e" in namespace "downward-api-956" to be "Succeeded or Failed"
Sep 27 20:07:37.618: INFO: Pod "downwardapi-volume-3a54ed29-3b03-45d9-90c5-3c8701443c6e": Phase="Pending", Reason="", readiness=false. Elapsed: 13.608772ms
Sep 27 20:07:39.633: INFO: Pod "downwardapi-volume-3a54ed29-3b03-45d9-90c5-3c8701443c6e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028641059s
Sep 27 20:07:41.653: INFO: Pod "downwardapi-volume-3a54ed29-3b03-45d9-90c5-3c8701443c6e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.049167964s
STEP: Saw pod success
Sep 27 20:07:41.653: INFO: Pod "downwardapi-volume-3a54ed29-3b03-45d9-90c5-3c8701443c6e" satisfied condition "Succeeded or Failed"
Sep 27 20:07:41.675: INFO: Trying to get logs from node 10.177.248.117 pod downwardapi-volume-3a54ed29-3b03-45d9-90c5-3c8701443c6e container client-container: <nil>
STEP: delete the pod
Sep 27 20:07:41.769: INFO: Waiting for pod downwardapi-volume-3a54ed29-3b03-45d9-90c5-3c8701443c6e to disappear
Sep 27 20:07:41.782: INFO: Pod downwardapi-volume-3a54ed29-3b03-45d9-90c5-3c8701443c6e no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 20:07:41.782: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-956" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":105,"skipped":1591,"failed":0}
S
------------------------------
[sig-apps] Job 
  should delete a job [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Job
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 20:07:41.827: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a job [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: delete a job
STEP: deleting Job.batch foo in namespace job-1441, will wait for the garbage collector to delete the pods
Sep 27 20:07:44.109: INFO: Deleting Job.batch foo took: 26.618044ms
Sep 27 20:07:44.210: INFO: Terminating Job.batch foo pods took: 100.378087ms
STEP: Ensuring job was deleted
[AfterEach] [sig-apps] Job
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 20:08:18.223: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-1441" for this suite.

• [SLOW TEST:36.439 seconds]
[sig-apps] Job
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should delete a job [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Job should delete a job [Conformance]","total":305,"completed":106,"skipped":1592,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 20:08:18.268: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Sep 27 20:08:18.477: INFO: Waiting up to 5m0s for pod "downwardapi-volume-26cdca77-c58a-4290-aaf0-dbc3be15b39d" in namespace "projected-2381" to be "Succeeded or Failed"
Sep 27 20:08:18.490: INFO: Pod "downwardapi-volume-26cdca77-c58a-4290-aaf0-dbc3be15b39d": Phase="Pending", Reason="", readiness=false. Elapsed: 13.113772ms
Sep 27 20:08:20.507: INFO: Pod "downwardapi-volume-26cdca77-c58a-4290-aaf0-dbc3be15b39d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.030884958s
STEP: Saw pod success
Sep 27 20:08:20.508: INFO: Pod "downwardapi-volume-26cdca77-c58a-4290-aaf0-dbc3be15b39d" satisfied condition "Succeeded or Failed"
Sep 27 20:08:20.548: INFO: Trying to get logs from node 10.177.248.117 pod downwardapi-volume-26cdca77-c58a-4290-aaf0-dbc3be15b39d container client-container: <nil>
STEP: delete the pod
Sep 27 20:08:20.623: INFO: Waiting for pod downwardapi-volume-26cdca77-c58a-4290-aaf0-dbc3be15b39d to disappear
Sep 27 20:08:20.638: INFO: Pod downwardapi-volume-26cdca77-c58a-4290-aaf0-dbc3be15b39d no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 20:08:20.638: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2381" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide podname only [NodeConformance] [Conformance]","total":305,"completed":107,"skipped":1610,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] IngressClass API 
   should support creating IngressClass API operations [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] IngressClass API
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 20:08:20.692: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename ingressclass
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] IngressClass API
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/ingressclass.go:148
[It]  should support creating IngressClass API operations [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: getting /apis
STEP: getting /apis/networking.k8s.io
STEP: getting /apis/networking.k8s.iov1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Sep 27 20:08:20.949: INFO: starting watch
STEP: patching
STEP: updating
Sep 27 20:08:20.989: INFO: waiting for watch events with expected annotations
Sep 27 20:08:20.989: INFO: saw patched and updated annotations
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-network] IngressClass API
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 20:08:21.118: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ingressclass-7470" for this suite.
•{"msg":"PASSED [sig-network] IngressClass API  should support creating IngressClass API operations [Conformance]","total":305,"completed":108,"skipped":1648,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 20:08:21.179: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Sep 27 20:08:21.341: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 20:08:22.422: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-138" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition creating/deleting custom resource definition objects works  [Conformance]","total":305,"completed":109,"skipped":1656,"failed":0}
SSSSS
------------------------------
[k8s.io] Variable Expansion 
  should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 20:08:22.469: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Sep 27 20:10:22.734: INFO: Deleting pod "var-expansion-323f65bc-b7bc-4de2-a7cc-6d2f5a4f251c" in namespace "var-expansion-6649"
Sep 27 20:10:22.761: INFO: Wait up to 5m0s for pod "var-expansion-323f65bc-b7bc-4de2-a7cc-6d2f5a4f251c" to be fully deleted
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 20:10:32.790: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-6649" for this suite.

• [SLOW TEST:130.369 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Variable Expansion should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]","total":305,"completed":110,"skipped":1661,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate pod and apply defaults after mutation [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 20:10:32.842: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Sep 27 20:10:33.578: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Sep 27 20:10:35.624: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63768370233, loc:(*time.Location)(0x7702840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63768370233, loc:(*time.Location)(0x7702840)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63768370233, loc:(*time.Location)(0x7702840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63768370233, loc:(*time.Location)(0x7702840)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep 27 20:10:38.674: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate pod and apply defaults after mutation [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Registering the mutating pod webhook via the AdmissionRegistration API
STEP: create a pod that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 20:10:38.926: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6159" for this suite.
STEP: Destroying namespace "webhook-6159-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:6.352 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate pod and apply defaults after mutation [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate pod and apply defaults after mutation [Conformance]","total":305,"completed":111,"skipped":1683,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 20:10:39.194: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:162
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod
Sep 27 20:10:39.353: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 20:10:42.502: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-6335" for this suite.
•{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]","total":305,"completed":112,"skipped":1730,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 20:10:42.547: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:134
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Sep 27 20:10:42.825: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster.
Sep 27 20:10:42.910: INFO: Number of nodes with available pods: 0
Sep 27 20:10:42.910: INFO: Node 10.177.248.114 is running more than one daemon pod
Sep 27 20:10:44.097: INFO: Number of nodes with available pods: 0
Sep 27 20:10:44.097: INFO: Node 10.177.248.114 is running more than one daemon pod
Sep 27 20:10:45.073: INFO: Number of nodes with available pods: 1
Sep 27 20:10:45.121: INFO: Node 10.177.248.114 is running more than one daemon pod
Sep 27 20:10:45.977: INFO: Number of nodes with available pods: 2
Sep 27 20:10:45.977: INFO: Node 10.177.248.114 is running more than one daemon pod
Sep 27 20:10:47.015: INFO: Number of nodes with available pods: 2
Sep 27 20:10:47.015: INFO: Node 10.177.248.114 is running more than one daemon pod
Sep 27 20:10:47.952: INFO: Number of nodes with available pods: 3
Sep 27 20:10:47.952: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Update daemon pods image.
STEP: Check that daemon pods images are updated.
Sep 27 20:10:48.313: INFO: Wrong image for pod: daemon-set-5mwn2. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Sep 27 20:10:48.313: INFO: Wrong image for pod: daemon-set-q9hmv. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Sep 27 20:10:48.313: INFO: Wrong image for pod: daemon-set-r65tb. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Sep 27 20:10:49.358: INFO: Wrong image for pod: daemon-set-5mwn2. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Sep 27 20:10:49.358: INFO: Wrong image for pod: daemon-set-q9hmv. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Sep 27 20:10:49.358: INFO: Wrong image for pod: daemon-set-r65tb. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Sep 27 20:10:50.354: INFO: Wrong image for pod: daemon-set-5mwn2. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Sep 27 20:10:50.356: INFO: Wrong image for pod: daemon-set-q9hmv. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Sep 27 20:10:50.356: INFO: Pod daemon-set-q9hmv is not available
Sep 27 20:10:50.356: INFO: Wrong image for pod: daemon-set-r65tb. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Sep 27 20:10:51.352: INFO: Wrong image for pod: daemon-set-5mwn2. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Sep 27 20:10:51.352: INFO: Pod daemon-set-l5x6g is not available
Sep 27 20:10:51.352: INFO: Wrong image for pod: daemon-set-r65tb. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Sep 27 20:10:52.351: INFO: Wrong image for pod: daemon-set-5mwn2. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Sep 27 20:10:52.351: INFO: Pod daemon-set-l5x6g is not available
Sep 27 20:10:52.351: INFO: Wrong image for pod: daemon-set-r65tb. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Sep 27 20:10:53.351: INFO: Wrong image for pod: daemon-set-5mwn2. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Sep 27 20:10:53.351: INFO: Wrong image for pod: daemon-set-r65tb. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Sep 27 20:10:54.351: INFO: Wrong image for pod: daemon-set-5mwn2. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Sep 27 20:10:54.351: INFO: Wrong image for pod: daemon-set-r65tb. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Sep 27 20:10:55.351: INFO: Wrong image for pod: daemon-set-5mwn2. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Sep 27 20:10:55.351: INFO: Wrong image for pod: daemon-set-r65tb. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Sep 27 20:10:55.351: INFO: Pod daemon-set-r65tb is not available
Sep 27 20:10:56.358: INFO: Wrong image for pod: daemon-set-5mwn2. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Sep 27 20:10:56.358: INFO: Wrong image for pod: daemon-set-r65tb. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Sep 27 20:10:56.358: INFO: Pod daemon-set-r65tb is not available
Sep 27 20:10:57.350: INFO: Wrong image for pod: daemon-set-5mwn2. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Sep 27 20:10:57.350: INFO: Wrong image for pod: daemon-set-r65tb. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Sep 27 20:10:57.350: INFO: Pod daemon-set-r65tb is not available
Sep 27 20:10:58.357: INFO: Wrong image for pod: daemon-set-5mwn2. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Sep 27 20:10:58.357: INFO: Wrong image for pod: daemon-set-r65tb. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Sep 27 20:10:58.357: INFO: Pod daemon-set-r65tb is not available
Sep 27 20:10:59.352: INFO: Wrong image for pod: daemon-set-5mwn2. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Sep 27 20:10:59.352: INFO: Wrong image for pod: daemon-set-r65tb. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Sep 27 20:10:59.352: INFO: Pod daemon-set-r65tb is not available
Sep 27 20:11:00.351: INFO: Wrong image for pod: daemon-set-5mwn2. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Sep 27 20:11:00.351: INFO: Wrong image for pod: daemon-set-r65tb. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Sep 27 20:11:00.351: INFO: Pod daemon-set-r65tb is not available
Sep 27 20:11:01.356: INFO: Wrong image for pod: daemon-set-5mwn2. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Sep 27 20:11:01.356: INFO: Wrong image for pod: daemon-set-r65tb. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Sep 27 20:11:01.356: INFO: Pod daemon-set-r65tb is not available
Sep 27 20:11:02.358: INFO: Wrong image for pod: daemon-set-5mwn2. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Sep 27 20:11:02.358: INFO: Pod daemon-set-vp7pr is not available
Sep 27 20:11:03.370: INFO: Wrong image for pod: daemon-set-5mwn2. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Sep 27 20:11:03.370: INFO: Pod daemon-set-vp7pr is not available
Sep 27 20:11:04.356: INFO: Wrong image for pod: daemon-set-5mwn2. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Sep 27 20:11:04.356: INFO: Pod daemon-set-vp7pr is not available
Sep 27 20:11:05.351: INFO: Wrong image for pod: daemon-set-5mwn2. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Sep 27 20:11:06.359: INFO: Wrong image for pod: daemon-set-5mwn2. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Sep 27 20:11:07.352: INFO: Wrong image for pod: daemon-set-5mwn2. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Sep 27 20:11:07.352: INFO: Pod daemon-set-5mwn2 is not available
Sep 27 20:11:08.350: INFO: Pod daemon-set-dd5hg is not available
STEP: Check that daemon pods are still running on every node of the cluster.
Sep 27 20:11:08.417: INFO: Number of nodes with available pods: 2
Sep 27 20:11:08.417: INFO: Node 10.177.248.126 is running more than one daemon pod
Sep 27 20:11:09.449: INFO: Number of nodes with available pods: 2
Sep 27 20:11:09.450: INFO: Node 10.177.248.126 is running more than one daemon pod
Sep 27 20:11:10.458: INFO: Number of nodes with available pods: 3
Sep 27 20:11:10.458: INFO: Number of running nodes: 3, number of available pods: 3
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:100
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-1656, will wait for the garbage collector to delete the pods
Sep 27 20:11:10.625: INFO: Deleting DaemonSet.extensions daemon-set took: 29.350578ms
Sep 27 20:11:10.725: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.405996ms
Sep 27 20:11:22.140: INFO: Number of nodes with available pods: 0
Sep 27 20:11:22.140: INFO: Number of running nodes: 0, number of available pods: 0
Sep 27 20:11:22.155: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-1656/daemonsets","resourceVersion":"79312"},"items":null}

Sep 27 20:11:22.171: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-1656/pods","resourceVersion":"79312"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 20:11:22.243: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-1656" for this suite.

• [SLOW TEST:39.745 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]","total":305,"completed":113,"skipped":1754,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] KubeletManagedEtcHosts 
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 20:11:22.292: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Setting up the test
STEP: Creating hostNetwork=false pod
STEP: Creating hostNetwork=true pod
STEP: Running the test
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false
Sep 27 20:11:30.672: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-5235 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep 27 20:11:30.672: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
Sep 27 20:11:30.918: INFO: Exec stderr: ""
Sep 27 20:11:30.918: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-5235 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep 27 20:11:30.918: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
Sep 27 20:11:31.134: INFO: Exec stderr: ""
Sep 27 20:11:31.134: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-5235 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep 27 20:11:31.134: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
Sep 27 20:11:31.379: INFO: Exec stderr: ""
Sep 27 20:11:31.379: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-5235 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep 27 20:11:31.379: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
Sep 27 20:11:31.601: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount
Sep 27 20:11:31.602: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-5235 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep 27 20:11:31.602: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
Sep 27 20:11:31.840: INFO: Exec stderr: ""
Sep 27 20:11:31.841: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-5235 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep 27 20:11:31.841: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
Sep 27 20:11:32.078: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true
Sep 27 20:11:32.079: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-5235 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep 27 20:11:32.079: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
Sep 27 20:11:32.350: INFO: Exec stderr: ""
Sep 27 20:11:32.351: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-5235 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep 27 20:11:32.351: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
Sep 27 20:11:32.608: INFO: Exec stderr: ""
Sep 27 20:11:32.609: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-5235 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep 27 20:11:32.609: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
Sep 27 20:11:32.845: INFO: Exec stderr: ""
Sep 27 20:11:32.845: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-5235 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep 27 20:11:32.845: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
Sep 27 20:11:33.114: INFO: Exec stderr: ""
[AfterEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 20:11:33.114: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-kubelet-etc-hosts-5235" for this suite.

• [SLOW TEST:10.866 seconds]
[k8s.io] KubeletManagedEtcHosts
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] KubeletManagedEtcHosts should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":114,"skipped":1773,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 20:11:33.158: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Sep 27 20:11:33.317: INFO: Creating ReplicaSet my-hostname-basic-72b2de33-9937-447d-b16e-2c7412c85564
Sep 27 20:11:33.349: INFO: Pod name my-hostname-basic-72b2de33-9937-447d-b16e-2c7412c85564: Found 0 pods out of 1
Sep 27 20:11:38.364: INFO: Pod name my-hostname-basic-72b2de33-9937-447d-b16e-2c7412c85564: Found 1 pods out of 1
Sep 27 20:11:38.364: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-72b2de33-9937-447d-b16e-2c7412c85564" is running
Sep 27 20:11:38.382: INFO: Pod "my-hostname-basic-72b2de33-9937-447d-b16e-2c7412c85564-46d8w" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-09-27 20:11:33 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-09-27 20:11:35 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-09-27 20:11:35 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-09-27 20:11:33 +0000 UTC Reason: Message:}])
Sep 27 20:11:38.383: INFO: Trying to dial the pod
Sep 27 20:11:43.450: INFO: Controller my-hostname-basic-72b2de33-9937-447d-b16e-2c7412c85564: Got expected result from replica 1 [my-hostname-basic-72b2de33-9937-447d-b16e-2c7412c85564-46d8w]: "my-hostname-basic-72b2de33-9937-447d-b16e-2c7412c85564-46d8w", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 20:11:43.450: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-9239" for this suite.

• [SLOW TEST:10.339 seconds]
[sig-apps] ReplicaSet
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet should serve a basic image on each replica with a public image  [Conformance]","total":305,"completed":115,"skipped":1794,"failed":0}
[sig-storage] ConfigMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 20:11:43.498: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Sep 27 20:11:43.688: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
STEP: Creating configMap with name cm-test-opt-del-3628ca9a-ee3d-4a30-9dc2-3adce481fa0d
STEP: Creating configMap with name cm-test-opt-upd-29710569-3032-415c-8c58-6fc344e644eb
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-3628ca9a-ee3d-4a30-9dc2-3adce481fa0d
STEP: Updating configmap cm-test-opt-upd-29710569-3032-415c-8c58-6fc344e644eb
STEP: Creating configMap with name cm-test-opt-create-04258ed0-497d-417d-8b70-220b2853554c
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 20:12:59.734: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5698" for this suite.

• [SLOW TEST:76.278 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:36
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap optional updates should be reflected in volume [NodeConformance] [Conformance]","total":305,"completed":116,"skipped":1794,"failed":0}
SSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD without validation schema [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 20:12:59.779: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD without validation schema [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Sep 27 20:12:59.938: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Sep 27 20:13:08.484: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-679324930 --namespace=crd-publish-openapi-5005 create -f -'
Sep 27 20:13:09.423: INFO: stderr: ""
Sep 27 20:13:09.423: INFO: stdout: "e2e-test-crd-publish-openapi-6067-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Sep 27 20:13:09.423: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-679324930 --namespace=crd-publish-openapi-5005 delete e2e-test-crd-publish-openapi-6067-crds test-cr'
Sep 27 20:13:09.676: INFO: stderr: ""
Sep 27 20:13:09.676: INFO: stdout: "e2e-test-crd-publish-openapi-6067-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
Sep 27 20:13:09.676: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-679324930 --namespace=crd-publish-openapi-5005 apply -f -'
Sep 27 20:13:10.528: INFO: stderr: ""
Sep 27 20:13:10.528: INFO: stdout: "e2e-test-crd-publish-openapi-6067-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Sep 27 20:13:10.528: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-679324930 --namespace=crd-publish-openapi-5005 delete e2e-test-crd-publish-openapi-6067-crds test-cr'
Sep 27 20:13:10.750: INFO: stderr: ""
Sep 27 20:13:10.750: INFO: stdout: "e2e-test-crd-publish-openapi-6067-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR without validation schema
Sep 27 20:13:10.750: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-679324930 explain e2e-test-crd-publish-openapi-6067-crds'
Sep 27 20:13:11.287: INFO: stderr: ""
Sep 27 20:13:11.287: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-6067-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 20:13:20.191: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-5005" for this suite.

• [SLOW TEST:20.460 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD without validation schema [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD without validation schema [Conformance]","total":305,"completed":117,"skipped":1801,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 20:13:20.239: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:181
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Sep 27 20:13:23.072: INFO: Successfully updated pod "pod-update-activedeadlineseconds-713e7db0-62d2-4d8c-9920-c67e34256069"
Sep 27 20:13:23.072: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-713e7db0-62d2-4d8c-9920-c67e34256069" in namespace "pods-6260" to be "terminated due to deadline exceeded"
Sep 27 20:13:23.086: INFO: Pod "pod-update-activedeadlineseconds-713e7db0-62d2-4d8c-9920-c67e34256069": Phase="Running", Reason="", readiness=true. Elapsed: 13.903749ms
Sep 27 20:13:25.101: INFO: Pod "pod-update-activedeadlineseconds-713e7db0-62d2-4d8c-9920-c67e34256069": Phase="Running", Reason="", readiness=true. Elapsed: 2.029080322s
Sep 27 20:13:27.115: INFO: Pod "pod-update-activedeadlineseconds-713e7db0-62d2-4d8c-9920-c67e34256069": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 4.043214894s
Sep 27 20:13:27.115: INFO: Pod "pod-update-activedeadlineseconds-713e7db0-62d2-4d8c-9920-c67e34256069" satisfied condition "terminated due to deadline exceeded"
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 20:13:27.115: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-6260" for this suite.

• [SLOW TEST:6.916 seconds]
[k8s.io] Pods
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Pods should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]","total":305,"completed":118,"skipped":1824,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should honor timeout [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 20:13:27.156: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Sep 27 20:13:27.888: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Sep 27 20:13:29.927: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63768370407, loc:(*time.Location)(0x7702840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63768370407, loc:(*time.Location)(0x7702840)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63768370407, loc:(*time.Location)(0x7702840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63768370407, loc:(*time.Location)(0x7702840)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep 27 20:13:32.980: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should honor timeout [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Setting timeout (1s) shorter than webhook latency (5s)
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s)
STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is longer than webhook latency
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is empty (defaulted to 10s in v1)
STEP: Registering slow webhook via the AdmissionRegistration API
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 20:13:45.572: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5017" for this suite.
STEP: Destroying namespace "webhook-5017-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:18.675 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should honor timeout [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should honor timeout [Conformance]","total":305,"completed":119,"skipped":1853,"failed":0}
S
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 20:13:45.831: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name configmap-test-volume-c182226e-4cc5-4248-a46f-85bd9f89a308
STEP: Creating a pod to test consume configMaps
Sep 27 20:13:46.122: INFO: Waiting up to 5m0s for pod "pod-configmaps-483ab3fb-bdc6-4d79-b321-4747837a2302" in namespace "configmap-2264" to be "Succeeded or Failed"
Sep 27 20:13:46.136: INFO: Pod "pod-configmaps-483ab3fb-bdc6-4d79-b321-4747837a2302": Phase="Pending", Reason="", readiness=false. Elapsed: 14.575852ms
Sep 27 20:13:48.155: INFO: Pod "pod-configmaps-483ab3fb-bdc6-4d79-b321-4747837a2302": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.033558304s
STEP: Saw pod success
Sep 27 20:13:48.155: INFO: Pod "pod-configmaps-483ab3fb-bdc6-4d79-b321-4747837a2302" satisfied condition "Succeeded or Failed"
Sep 27 20:13:48.170: INFO: Trying to get logs from node 10.177.248.117 pod pod-configmaps-483ab3fb-bdc6-4d79-b321-4747837a2302 container configmap-volume-test: <nil>
STEP: delete the pod
Sep 27 20:13:48.399: INFO: Waiting for pod pod-configmaps-483ab3fb-bdc6-4d79-b321-4747837a2302 to disappear
Sep 27 20:13:48.414: INFO: Pod pod-configmaps-483ab3fb-bdc6-4d79-b321-4747837a2302 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 20:13:48.414: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2264" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":120,"skipped":1854,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl describe 
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 20:13:48.457: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Sep 27 20:13:48.654: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-679324930 create -f - --namespace=kubectl-2634'
Sep 27 20:13:49.400: INFO: stderr: ""
Sep 27 20:13:49.400: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
Sep 27 20:13:49.400: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-679324930 create -f - --namespace=kubectl-2634'
Sep 27 20:13:49.909: INFO: stderr: ""
Sep 27 20:13:49.909: INFO: stdout: "service/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
Sep 27 20:13:50.925: INFO: Selector matched 1 pods for map[app:agnhost]
Sep 27 20:13:50.925: INFO: Found 0 / 1
Sep 27 20:13:51.929: INFO: Selector matched 1 pods for map[app:agnhost]
Sep 27 20:13:51.929: INFO: Found 1 / 1
Sep 27 20:13:51.929: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Sep 27 20:13:51.945: INFO: Selector matched 1 pods for map[app:agnhost]
Sep 27 20:13:51.945: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Sep 27 20:13:51.945: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-679324930 describe pod agnhost-primary-c6vdt --namespace=kubectl-2634'
Sep 27 20:13:52.157: INFO: stderr: ""
Sep 27 20:13:52.157: INFO: stdout: "Name:         agnhost-primary-c6vdt\nNamespace:    kubectl-2634\nPriority:     0\nNode:         10.177.248.117/10.177.248.117\nStart Time:   Mon, 27 Sep 2021 20:13:49 +0000\nLabels:       app=agnhost\n              role=primary\nAnnotations:  cni.projectcalico.org/podIP: 172.30.75.59/32\n              cni.projectcalico.org/podIPs: 172.30.75.59/32\n              k8s.v1.cni.cncf.io/network-status:\n                [{\n                    \"name\": \"\",\n                    \"ips\": [\n                        \"172.30.75.59\"\n                    ],\n                    \"default\": true,\n                    \"dns\": {}\n                }]\n              k8s.v1.cni.cncf.io/networks-status:\n                [{\n                    \"name\": \"\",\n                    \"ips\": [\n                        \"172.30.75.59\"\n                    ],\n                    \"default\": true,\n                    \"dns\": {}\n                }]\n              openshift.io/scc: anyuid\nStatus:       Running\nIP:           172.30.75.59\nIPs:\n  IP:           172.30.75.59\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   cri-o://1dfe33db5778d714057e87cd1bfe63deccb5cf1f5ef8468db3051377a6f8baa8\n    Image:          k8s.gcr.io/e2e-test-images/agnhost:2.20\n    Image ID:       k8s.gcr.io/e2e-test-images/agnhost@sha256:17e61a0b9e498b6c73ed97670906be3d5a3ae394739c1bd5b619e1a004885cf0\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Mon, 27 Sep 2021 20:13:50 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from default-token-v5f82 (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  default-token-v5f82:\n    Type:        Secret (a volume populated by a Secret)\n    SecretName:  default-token-v5f82\n    Optional:    false\nQoS Class:       BestEffort\nNode-Selectors:  <none>\nTolerations:     node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                 node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason          Age   From                     Message\n  ----    ------          ----  ----                     -------\n  Normal  Scheduled       3s    default-scheduler        Successfully assigned kubectl-2634/agnhost-primary-c6vdt to 10.177.248.117\n  Normal  AddedInterface  2s    multus                   Add eth0 [172.30.75.59/32]\n  Normal  Pulled          2s    kubelet, 10.177.248.117  Container image \"k8s.gcr.io/e2e-test-images/agnhost:2.20\" already present on machine\n  Normal  Created         2s    kubelet, 10.177.248.117  Created container agnhost-primary\n  Normal  Started         1s    kubelet, 10.177.248.117  Started container agnhost-primary\n"
Sep 27 20:13:52.157: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-679324930 describe rc agnhost-primary --namespace=kubectl-2634'
Sep 27 20:13:52.383: INFO: stderr: ""
Sep 27 20:13:52.383: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-2634\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        k8s.gcr.io/e2e-test-images/agnhost:2.20\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  3s    replication-controller  Created pod: agnhost-primary-c6vdt\n"
Sep 27 20:13:52.384: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-679324930 describe service agnhost-primary --namespace=kubectl-2634'
Sep 27 20:13:52.591: INFO: stderr: ""
Sep 27 20:13:52.591: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-2634\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP:                172.21.71.114\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         172.30.75.59:6379\nSession Affinity:  None\nEvents:            <none>\n"
Sep 27 20:13:52.632: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-679324930 describe node 10.177.248.114'
Sep 27 20:13:53.090: INFO: stderr: ""
Sep 27 20:13:53.091: INFO: stdout: "Name:               10.177.248.114\nRoles:              master,worker\nLabels:             arch=amd64\n                    beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/instance-type=b3c.4x16.encrypted\n                    beta.kubernetes.io/os=linux\n                    failure-domain.beta.kubernetes.io/region=us-south\n                    failure-domain.beta.kubernetes.io/zone=dal10\n                    ibm-cloud.kubernetes.io/encrypted-docker-data=true\n                    ibm-cloud.kubernetes.io/external-ip=169.61.243.2\n                    ibm-cloud.kubernetes.io/iaas-provider=softlayer\n                    ibm-cloud.kubernetes.io/internal-ip=10.177.248.114\n                    ibm-cloud.kubernetes.io/machine-type=b3c.4x16.encrypted\n                    ibm-cloud.kubernetes.io/os=REDHAT_7_64\n                    ibm-cloud.kubernetes.io/region=us-south\n                    ibm-cloud.kubernetes.io/sgx-enabled=false\n                    ibm-cloud.kubernetes.io/worker-id=kube-c58vqdcd0gtsghlgpd0g-kubee2epvgp-default-000002bd\n                    ibm-cloud.kubernetes.io/worker-pool-id=c58vqdcd0gtsghlgpd0g-5d1414f\n                    ibm-cloud.kubernetes.io/worker-pool-name=default\n                    ibm-cloud.kubernetes.io/worker-version=4.6.44_1555_openshift\n                    ibm-cloud.kubernetes.io/zone=dal10\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=10.177.248.114\n                    kubernetes.io/os=linux\n                    node-role.kubernetes.io/master=\n                    node-role.kubernetes.io/worker=\n                    node.kubernetes.io/instance-type=b3c.4x16.encrypted\n                    node.openshift.io/os_id=rhel\n                    privateVLAN=2722932\n                    publicVLAN=2722926\n                    topology.kubernetes.io/region=us-south\n                    topology.kubernetes.io/zone=dal10\nAnnotations:        projectcalico.org/IPv4Address: 10.177.248.114/26\n                    projectcalico.org/IPv4IPIPTunnelAddr: 172.30.137.64\nCreationTimestamp:  Mon, 27 Sep 2021 17:49:49 +0000\nTaints:             <none>\nUnschedulable:      false\nLease:\n  HolderIdentity:  10.177.248.114\n  AcquireTime:     <unset>\n  RenewTime:       Mon, 27 Sep 2021 20:13:50 +0000\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable   False   Mon, 27 Sep 2021 17:51:04 +0000   Mon, 27 Sep 2021 17:51:04 +0000   CalicoIsUp                   Calico is running on this node\n  MemoryPressure       False   Mon, 27 Sep 2021 20:10:36 +0000   Mon, 27 Sep 2021 17:49:49 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Mon, 27 Sep 2021 20:10:36 +0000   Mon, 27 Sep 2021 17:49:49 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Mon, 27 Sep 2021 20:10:36 +0000   Mon, 27 Sep 2021 17:49:49 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Mon, 27 Sep 2021 20:10:36 +0000   Mon, 27 Sep 2021 17:51:20 +0000   KubeletReady                 kubelet is posting ready status\nAddresses:\n  InternalIP:  10.177.248.114\n  ExternalIP:  169.61.243.2\n  Hostname:    10.177.248.114\nCapacity:\n  cpu:                    4\n  ephemeral-storage:      103078840Ki\n  hugepages-1Gi:          0\n  hugepages-2Mi:          0\n  memory:                 16260856Ki\n  pods:                   110\n  scheduling.k8s.io/foo:  3\nAllocatable:\n  cpu:                    3910m\n  ephemeral-storage:      94369515442\n  hugepages-1Gi:          0\n  hugepages-2Mi:          0\n  memory:                 13484792Ki\n  pods:                   110\n  scheduling.k8s.io/foo:  3\nSystem Info:\n  Machine ID:                             8470a585d1864966931159039ee7697b\n  System UUID:                            860EFC14-D830-E844-2734-1DE1A8143E6F\n  Boot ID:                                4631a65b-4a0d-477f-9a66-7067c87f38a5\n  Kernel Version:                         3.10.0-1160.42.2.el7.x86_64\n  OS Image:                               Red Hat\n  Operating System:                       linux\n  Architecture:                           amd64\n  Container Runtime Version:              cri-o://1.19.3-11.rhaos4.6.git66a69b8.el7\n  Kubelet Version:                        v1.19.0+4c3480d\n  Kube-Proxy Version:                     v1.19.0+4c3480d\nPodCIDR:                                  172.30.2.0/24\nPodCIDRs:                                 172.30.2.0/24\nProviderID:                               ibm://fee034388aa6435883a1f720010ab3a2///c58vqdcd0gtsghlgpd0g/kube-c58vqdcd0gtsghlgpd0g-kubee2epvgp-default-000002bd\nNon-terminated Pods:                      (36 in total)\n  Namespace                               Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  AGE\n  ---------                               ----                                                       ------------  ----------  ---------------  -------------  ---\n  calico-system                           calico-node-kxvx5                                          250m (6%)     0 (0%)      80Mi (0%)        0 (0%)         143m\n  calico-system                           calico-typha-7d789bfc7c-8ftcw                              250m (6%)     0 (0%)      80Mi (0%)        0 (0%)         141m\n  default                                 test-k8s-e2e-pvg-master-verification                       0 (0%)        0 (0%)      0 (0%)           0 (0%)         141m\n  ibm-system                              ibm-cloud-provider-ip-169-61-239-242-59b4964694-ngqgl      5m (0%)       0 (0%)      10Mi (0%)        0 (0%)         130m\n  kube-system                             ibm-keepalived-watcher-8lj4c                               5m (0%)       0 (0%)      10Mi (0%)        0 (0%)         144m\n  kube-system                             ibm-master-proxy-static-10.177.248.114                     25m (0%)      300m (7%)   32M (0%)         512M (3%)      143m\n  kube-system                             ibmcloud-block-storage-driver-mxb6q                        50m (1%)      300m (7%)   100Mi (0%)       300Mi (2%)     143m\n  kube-system                             vpn-bc979587-kglsg                                         5m (0%)       0 (0%)      5Mi (0%)         0 (0%)         140m\n  openshift-cluster-node-tuning-operator  tuned-dsf6r                                                10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         142m\n  openshift-console                       console-5b57dff4ff-2hr57                                   10m (0%)      0 (0%)      100Mi (0%)       0 (0%)         138m\n  openshift-dns                           dns-default-ktd8w                                          65m (1%)      0 (0%)      110Mi (0%)       512Mi (3%)     140m\n  openshift-image-registry                node-ca-bv4wj                                              10m (0%)      0 (0%)      10Mi (0%)        0 (0%)         140m\n  openshift-ingress                       router-default-768f4875db-n8g9c                            100m (2%)     0 (0%)      256Mi (1%)       0 (0%)         140m\n  openshift-kube-proxy                    openshift-kube-proxy-lxdz4                                 100m (2%)     0 (0%)      200Mi (1%)       0 (0%)         144m\n  openshift-marketplace                   certified-operators-qvnmv                                  10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         139m\n  openshift-marketplace                   community-operators-mc4rj                                  10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         139m\n  openshift-marketplace                   redhat-marketplace-tf75q                                   10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         139m\n  openshift-marketplace                   redhat-operators-2sd6b                                     10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         139m\n  openshift-monitoring                    alertmanager-main-0                                        8m (0%)       0 (0%)      270Mi (2%)       0 (0%)         139m\n  openshift-monitoring                    grafana-6f7d589b46-r88hj                                   5m (0%)       0 (0%)      120Mi (0%)       0 (0%)         140m\n  openshift-monitoring                    kube-state-metrics-5d4985f6b7-jx87q                        4m (0%)       0 (0%)      120Mi (0%)       0 (0%)         142m\n  openshift-monitoring                    node-exporter-r84r5                                        9m (0%)       0 (0%)      210Mi (1%)       0 (0%)         142m\n  openshift-monitoring                    openshift-state-metrics-58bfb7bff-qxd59                    3m (0%)       0 (0%)      190Mi (1%)       0 (0%)         142m\n  openshift-monitoring                    prometheus-adapter-66469d976-gl6wl                         1m (0%)       0 (0%)      25Mi (0%)        0 (0%)         139m\n  openshift-monitoring                    prometheus-k8s-0                                           75m (1%)      0 (0%)      1194Mi (9%)      0 (0%)         137m\n  openshift-monitoring                    prometheus-operator-856d6cddd4-blncm                       6m (0%)       0 (0%)      100Mi (0%)       0 (0%)         139m\n  openshift-monitoring                    telemeter-client-68cfc9967c-sr2hj                          3m (0%)       0 (0%)      20Mi (0%)        0 (0%)         141m\n  openshift-monitoring                    thanos-querier-6b5789bbbd-qwc9q                            9m (0%)       0 (0%)      92Mi (0%)        0 (0%)         140m\n  openshift-multus                        multus-admission-controller-pp4c5                          20m (0%)      0 (0%)      20Mi (0%)        0 (0%)         142m\n  openshift-multus                        multus-qwrqb                                               10m (0%)      0 (0%)      150Mi (1%)       0 (0%)         144m\n  openshift-multus                        network-metrics-daemon-zdwnk                               20m (0%)      0 (0%)      120Mi (0%)       0 (0%)         144m\n  openshift-operator-lifecycle-manager    packageserver-67b4ccc984-2lht2                             10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         140m\n  openshift-service-ca                    service-ca-5655fcb96b-ntfqm                                10m (0%)      0 (0%)      120Mi (0%)       0 (0%)         142m\n  sonobuoy                                sonobuoy                                                   0 (0%)        0 (0%)      0 (0%)           0 (0%)         52m\n  sonobuoy                                sonobuoy-e2e-job-f7e56101f16c459b                          0 (0%)        0 (0%)      0 (0%)           0 (0%)         52m\n  sonobuoy                                sonobuoy-systemd-logs-daemon-set-a625d2c9d0384434-sl5dj    0 (0%)        0 (0%)      0 (0%)           0 (0%)         52m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource               Requests         Limits\n  --------               --------         ------\n  cpu                    1118m (28%)      600m (15%)\n  memory                 4139538Ki (30%)  1363443712 (9%)\n  ephemeral-storage      0 (0%)           0 (0%)\n  hugepages-1Gi          0 (0%)           0 (0%)\n  hugepages-2Mi          0 (0%)           0 (0%)\n  scheduling.k8s.io/foo  0                0\nEvents:\n  Type    Reason                   Age                  From                        Message\n  ----    ------                   ----                 ----                        -------\n  Normal  Starting                 144m                 kubelet, 10.177.248.114     Starting kubelet.\n  Normal  NodeHasSufficientPID     144m (x7 over 144m)  kubelet, 10.177.248.114     Node 10.177.248.114 status is now: NodeHasSufficientPID\n  Normal  NodeAllocatableEnforced  144m                 kubelet, 10.177.248.114     Updated Node Allocatable limit across pods\n  Normal  NodeHasSufficientMemory  144m (x8 over 144m)  kubelet, 10.177.248.114     Node 10.177.248.114 status is now: NodeHasSufficientMemory\n  Normal  NodeHasNoDiskPressure    144m (x8 over 144m)  kubelet, 10.177.248.114     Node 10.177.248.114 status is now: NodeHasNoDiskPressure\n  Normal  Starting                 143m                 kube-proxy, 10.177.248.114  Starting kube-proxy.\n"
Sep 27 20:13:53.091: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-679324930 describe namespace kubectl-2634'
Sep 27 20:13:53.310: INFO: stderr: ""
Sep 27 20:13:53.310: INFO: stdout: "Name:         kubectl-2634\nLabels:       e2e-framework=kubectl\n              e2e-run=94d08252-a62c-43eb-a6d0-b17ed5f2fc00\nAnnotations:  openshift.io/sa.scc.mcs: s0:c46,c10\n              openshift.io/sa.scc.supplemental-groups: 1002090000/10000\n              openshift.io/sa.scc.uid-range: 1002090000/10000\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 20:13:53.310: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2634" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl describe should check if kubectl describe prints relevant information for rc and pods  [Conformance]","total":305,"completed":121,"skipped":1866,"failed":0}
SSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 20:13:53.352: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating secret with name secret-test-22f466a6-5659-4b6f-829d-624ac2c81b3d
STEP: Creating a pod to test consume secrets
Sep 27 20:13:53.715: INFO: Waiting up to 5m0s for pod "pod-secrets-07bedad6-bf11-4363-85f5-997c0b2081c2" in namespace "secrets-6325" to be "Succeeded or Failed"
Sep 27 20:13:53.732: INFO: Pod "pod-secrets-07bedad6-bf11-4363-85f5-997c0b2081c2": Phase="Pending", Reason="", readiness=false. Elapsed: 16.962045ms
Sep 27 20:13:55.750: INFO: Pod "pod-secrets-07bedad6-bf11-4363-85f5-997c0b2081c2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.034293221s
Sep 27 20:13:57.762: INFO: Pod "pod-secrets-07bedad6-bf11-4363-85f5-997c0b2081c2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.046441061s
STEP: Saw pod success
Sep 27 20:13:57.762: INFO: Pod "pod-secrets-07bedad6-bf11-4363-85f5-997c0b2081c2" satisfied condition "Succeeded or Failed"
Sep 27 20:13:57.774: INFO: Trying to get logs from node 10.177.248.117 pod pod-secrets-07bedad6-bf11-4363-85f5-997c0b2081c2 container secret-env-test: <nil>
STEP: delete the pod
Sep 27 20:13:57.854: INFO: Waiting for pod pod-secrets-07bedad6-bf11-4363-85f5-997c0b2081c2 to disappear
Sep 27 20:13:57.868: INFO: Pod pod-secrets-07bedad6-bf11-4363-85f5-997c0b2081c2 no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 20:13:57.869: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6325" for this suite.
•{"msg":"PASSED [sig-api-machinery] Secrets should be consumable from pods in env vars [NodeConformance] [Conformance]","total":305,"completed":122,"skipped":1875,"failed":0}
SSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 20:13:57.907: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Sep 27 20:13:58.121: INFO: Waiting up to 5m0s for pod "downwardapi-volume-356d085a-b107-4c92-b8f5-56b03573b858" in namespace "projected-4867" to be "Succeeded or Failed"
Sep 27 20:13:58.134: INFO: Pod "downwardapi-volume-356d085a-b107-4c92-b8f5-56b03573b858": Phase="Pending", Reason="", readiness=false. Elapsed: 13.191932ms
Sep 27 20:14:00.148: INFO: Pod "downwardapi-volume-356d085a-b107-4c92-b8f5-56b03573b858": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027332849s
Sep 27 20:14:02.162: INFO: Pod "downwardapi-volume-356d085a-b107-4c92-b8f5-56b03573b858": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.041640289s
STEP: Saw pod success
Sep 27 20:14:02.163: INFO: Pod "downwardapi-volume-356d085a-b107-4c92-b8f5-56b03573b858" satisfied condition "Succeeded or Failed"
Sep 27 20:14:02.179: INFO: Trying to get logs from node 10.177.248.117 pod downwardapi-volume-356d085a-b107-4c92-b8f5-56b03573b858 container client-container: <nil>
STEP: delete the pod
Sep 27 20:14:02.252: INFO: Waiting for pod downwardapi-volume-356d085a-b107-4c92-b8f5-56b03573b858 to disappear
Sep 27 20:14:02.267: INFO: Pod downwardapi-volume-356d085a-b107-4c92-b8f5-56b03573b858 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 20:14:02.268: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4867" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory limit [NodeConformance] [Conformance]","total":305,"completed":123,"skipped":1882,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 20:14:02.311: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Discovering how many secrets are in namespace by default
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Secret
STEP: Ensuring resource quota status captures secret creation
STEP: Deleting a secret
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 20:14:19.801: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-9338" for this suite.

• [SLOW TEST:17.532 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a secret. [Conformance]","total":305,"completed":124,"skipped":1897,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should run through the lifecycle of a ServiceAccount [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 20:14:19.846: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run through the lifecycle of a ServiceAccount [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a ServiceAccount
STEP: watching for the ServiceAccount to be added
STEP: patching the ServiceAccount
STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector)
STEP: deleting the ServiceAccount
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 20:14:20.197: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-5959" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should run through the lifecycle of a ServiceAccount [Conformance]","total":305,"completed":125,"skipped":1925,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing validating webhooks should work [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 20:14:20.259: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Sep 27 20:14:21.277: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Sep 27 20:14:23.345: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63768370461, loc:(*time.Location)(0x7702840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63768370461, loc:(*time.Location)(0x7702840)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63768370461, loc:(*time.Location)(0x7702840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63768370461, loc:(*time.Location)(0x7702840)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep 27 20:14:26.404: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing validating webhooks should work [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 20:14:27.623: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1392" for this suite.
STEP: Destroying namespace "webhook-1392-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:7.758 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  listing validating webhooks should work [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing validating webhooks should work [Conformance]","total":305,"completed":126,"skipped":1931,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 20:14:28.017: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0666 on node default medium
Sep 27 20:14:28.261: INFO: Waiting up to 5m0s for pod "pod-0bbbc27f-ecd3-4e49-ad32-7d5d7faf807d" in namespace "emptydir-6183" to be "Succeeded or Failed"
Sep 27 20:14:28.298: INFO: Pod "pod-0bbbc27f-ecd3-4e49-ad32-7d5d7faf807d": Phase="Pending", Reason="", readiness=false. Elapsed: 32.082222ms
Sep 27 20:14:30.356: INFO: Pod "pod-0bbbc27f-ecd3-4e49-ad32-7d5d7faf807d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.089408739s
STEP: Saw pod success
Sep 27 20:14:30.356: INFO: Pod "pod-0bbbc27f-ecd3-4e49-ad32-7d5d7faf807d" satisfied condition "Succeeded or Failed"
Sep 27 20:14:30.483: INFO: Trying to get logs from node 10.177.248.117 pod pod-0bbbc27f-ecd3-4e49-ad32-7d5d7faf807d container test-container: <nil>
STEP: delete the pod
Sep 27 20:14:30.627: INFO: Waiting for pod pod-0bbbc27f-ecd3-4e49-ad32-7d5d7faf807d to disappear
Sep 27 20:14:30.660: INFO: Pod pod-0bbbc27f-ecd3-4e49-ad32-7d5d7faf807d no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 20:14:30.665: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6183" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":127,"skipped":1944,"failed":0}
SS
------------------------------
[sig-node] Downward API 
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 20:14:30.771: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward api env vars
Sep 27 20:14:31.114: INFO: Waiting up to 5m0s for pod "downward-api-b295f024-b847-4b7a-9663-43247f8b06e1" in namespace "downward-api-7326" to be "Succeeded or Failed"
Sep 27 20:14:31.206: INFO: Pod "downward-api-b295f024-b847-4b7a-9663-43247f8b06e1": Phase="Pending", Reason="", readiness=false. Elapsed: 91.957552ms
Sep 27 20:14:33.242: INFO: Pod "downward-api-b295f024-b847-4b7a-9663-43247f8b06e1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.128494973s
Sep 27 20:14:35.258: INFO: Pod "downward-api-b295f024-b847-4b7a-9663-43247f8b06e1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.144128241s
STEP: Saw pod success
Sep 27 20:14:35.258: INFO: Pod "downward-api-b295f024-b847-4b7a-9663-43247f8b06e1" satisfied condition "Succeeded or Failed"
Sep 27 20:14:35.272: INFO: Trying to get logs from node 10.177.248.117 pod downward-api-b295f024-b847-4b7a-9663-43247f8b06e1 container dapi-container: <nil>
STEP: delete the pod
Sep 27 20:14:35.348: INFO: Waiting for pod downward-api-b295f024-b847-4b7a-9663-43247f8b06e1 to disappear
Sep 27 20:14:35.362: INFO: Pod downward-api-b295f024-b847-4b7a-9663-43247f8b06e1 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 20:14:35.362: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7326" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]","total":305,"completed":128,"skipped":1946,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should delete a collection of pods [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 20:14:35.414: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:181
[It] should delete a collection of pods [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Create set of pods
Sep 27 20:14:35.598: INFO: created test-pod-1
Sep 27 20:14:35.634: INFO: created test-pod-2
Sep 27 20:14:35.670: INFO: created test-pod-3
STEP: waiting for all 3 pods to be located
STEP: waiting for all pods to be deleted
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 20:14:35.818: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-7872" for this suite.
•{"msg":"PASSED [k8s.io] Pods should delete a collection of pods [Conformance]","total":305,"completed":129,"skipped":1996,"failed":0}
SSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 20:14:35.860: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:134
[It] should run and stop simple daemon [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Sep 27 20:14:36.179: INFO: Number of nodes with available pods: 0
Sep 27 20:14:36.179: INFO: Node 10.177.248.114 is running more than one daemon pod
Sep 27 20:14:37.212: INFO: Number of nodes with available pods: 0
Sep 27 20:14:37.212: INFO: Node 10.177.248.114 is running more than one daemon pod
Sep 27 20:14:38.218: INFO: Number of nodes with available pods: 0
Sep 27 20:14:38.218: INFO: Node 10.177.248.114 is running more than one daemon pod
Sep 27 20:14:39.218: INFO: Number of nodes with available pods: 3
Sep 27 20:14:39.218: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Stop a daemon pod, check that the daemon pod is revived.
Sep 27 20:14:39.316: INFO: Number of nodes with available pods: 2
Sep 27 20:14:39.316: INFO: Node 10.177.248.117 is running more than one daemon pod
Sep 27 20:14:40.371: INFO: Number of nodes with available pods: 2
Sep 27 20:14:40.371: INFO: Node 10.177.248.117 is running more than one daemon pod
Sep 27 20:14:41.351: INFO: Number of nodes with available pods: 2
Sep 27 20:14:41.351: INFO: Node 10.177.248.117 is running more than one daemon pod
Sep 27 20:14:42.358: INFO: Number of nodes with available pods: 2
Sep 27 20:14:42.358: INFO: Node 10.177.248.117 is running more than one daemon pod
Sep 27 20:14:43.352: INFO: Number of nodes with available pods: 2
Sep 27 20:14:43.352: INFO: Node 10.177.248.117 is running more than one daemon pod
Sep 27 20:14:44.359: INFO: Number of nodes with available pods: 2
Sep 27 20:14:44.359: INFO: Node 10.177.248.117 is running more than one daemon pod
Sep 27 20:14:45.356: INFO: Number of nodes with available pods: 2
Sep 27 20:14:45.356: INFO: Node 10.177.248.117 is running more than one daemon pod
Sep 27 20:14:46.351: INFO: Number of nodes with available pods: 2
Sep 27 20:14:46.351: INFO: Node 10.177.248.117 is running more than one daemon pod
Sep 27 20:14:47.349: INFO: Number of nodes with available pods: 2
Sep 27 20:14:47.349: INFO: Node 10.177.248.117 is running more than one daemon pod
Sep 27 20:14:48.357: INFO: Number of nodes with available pods: 2
Sep 27 20:14:48.357: INFO: Node 10.177.248.117 is running more than one daemon pod
Sep 27 20:14:49.352: INFO: Number of nodes with available pods: 2
Sep 27 20:14:49.352: INFO: Node 10.177.248.117 is running more than one daemon pod
Sep 27 20:14:50.360: INFO: Number of nodes with available pods: 2
Sep 27 20:14:50.360: INFO: Node 10.177.248.117 is running more than one daemon pod
Sep 27 20:14:51.350: INFO: Number of nodes with available pods: 2
Sep 27 20:14:51.350: INFO: Node 10.177.248.117 is running more than one daemon pod
Sep 27 20:14:52.360: INFO: Number of nodes with available pods: 2
Sep 27 20:14:52.360: INFO: Node 10.177.248.117 is running more than one daemon pod
Sep 27 20:14:53.351: INFO: Number of nodes with available pods: 2
Sep 27 20:14:53.351: INFO: Node 10.177.248.117 is running more than one daemon pod
Sep 27 20:14:54.354: INFO: Number of nodes with available pods: 3
Sep 27 20:14:54.354: INFO: Number of running nodes: 3, number of available pods: 3
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:100
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-9767, will wait for the garbage collector to delete the pods
Sep 27 20:14:54.461: INFO: Deleting DaemonSet.extensions daemon-set took: 29.285184ms
Sep 27 20:14:54.561: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.387014ms
Sep 27 20:15:02.077: INFO: Number of nodes with available pods: 0
Sep 27 20:15:02.077: INFO: Number of running nodes: 0, number of available pods: 0
Sep 27 20:15:02.091: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-9767/daemonsets","resourceVersion":"82232"},"items":null}

Sep 27 20:15:02.105: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-9767/pods","resourceVersion":"82232"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 20:15:02.187: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-9767" for this suite.

• [SLOW TEST:26.383 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop simple daemon [Conformance]","total":305,"completed":130,"skipped":2000,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 20:15:02.245: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating projection with secret that has name projected-secret-test-b0d20906-cf8e-4d73-a339-a343f22c4ed3
STEP: Creating a pod to test consume secrets
Sep 27 20:15:02.555: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-31ad244d-c925-46b8-9e60-4e254239de90" in namespace "projected-3787" to be "Succeeded or Failed"
Sep 27 20:15:02.569: INFO: Pod "pod-projected-secrets-31ad244d-c925-46b8-9e60-4e254239de90": Phase="Pending", Reason="", readiness=false. Elapsed: 14.406127ms
Sep 27 20:15:04.587: INFO: Pod "pod-projected-secrets-31ad244d-c925-46b8-9e60-4e254239de90": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032078948s
Sep 27 20:15:06.602: INFO: Pod "pod-projected-secrets-31ad244d-c925-46b8-9e60-4e254239de90": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.046989083s
STEP: Saw pod success
Sep 27 20:15:06.602: INFO: Pod "pod-projected-secrets-31ad244d-c925-46b8-9e60-4e254239de90" satisfied condition "Succeeded or Failed"
Sep 27 20:15:06.617: INFO: Trying to get logs from node 10.177.248.117 pod pod-projected-secrets-31ad244d-c925-46b8-9e60-4e254239de90 container projected-secret-volume-test: <nil>
STEP: delete the pod
Sep 27 20:15:06.701: INFO: Waiting for pod pod-projected-secrets-31ad244d-c925-46b8-9e60-4e254239de90 to disappear
Sep 27 20:15:06.721: INFO: Pod pod-projected-secrets-31ad244d-c925-46b8-9e60-4e254239de90 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 20:15:06.721: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3787" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":131,"skipped":2041,"failed":0}
SSSSSSS
------------------------------
[sig-network] Services 
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 20:15:06.778: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating service in namespace services-6046
STEP: creating service affinity-clusterip-transition in namespace services-6046
STEP: creating replication controller affinity-clusterip-transition in namespace services-6046
I0927 20:15:07.032910      22 runners.go:190] Created replication controller with name: affinity-clusterip-transition, namespace: services-6046, replica count: 3
I0927 20:15:10.084105      22 runners.go:190] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Sep 27 20:15:10.111: INFO: Creating new exec pod
Sep 27 20:15:15.185: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-679324930 exec --namespace=services-6046 execpod-affinity7577v -- /bin/sh -x -c nc -zv -t -w 2 affinity-clusterip-transition 80'
Sep 27 20:15:16.083: INFO: stderr: "+ nc -zv -t -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
Sep 27 20:15:16.087: INFO: stdout: ""
Sep 27 20:15:16.089: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-679324930 exec --namespace=services-6046 execpod-affinity7577v -- /bin/sh -x -c nc -zv -t -w 2 172.21.75.177 80'
Sep 27 20:15:17.390: INFO: stderr: "+ nc -zv -t -w 2 172.21.75.177 80\nConnection to 172.21.75.177 80 port [tcp/http] succeeded!\n"
Sep 27 20:15:17.390: INFO: stdout: ""
Sep 27 20:15:17.452: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-679324930 exec --namespace=services-6046 execpod-affinity7577v -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.21.75.177:80/ ; done'
Sep 27 20:15:18.415: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.75.177:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.75.177:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.75.177:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.75.177:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.75.177:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.75.177:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.75.177:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.75.177:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.75.177:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.75.177:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.75.177:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.75.177:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.75.177:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.75.177:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.75.177:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.75.177:80/\n"
Sep 27 20:15:18.422: INFO: stdout: "\naffinity-clusterip-transition-2hrjj\naffinity-clusterip-transition-2hrjj\naffinity-clusterip-transition-r9d6d\naffinity-clusterip-transition-2hrjj\naffinity-clusterip-transition-h7vcx\naffinity-clusterip-transition-r9d6d\naffinity-clusterip-transition-h7vcx\naffinity-clusterip-transition-h7vcx\naffinity-clusterip-transition-r9d6d\naffinity-clusterip-transition-r9d6d\naffinity-clusterip-transition-r9d6d\naffinity-clusterip-transition-2hrjj\naffinity-clusterip-transition-h7vcx\naffinity-clusterip-transition-r9d6d\naffinity-clusterip-transition-r9d6d\naffinity-clusterip-transition-2hrjj"
Sep 27 20:15:18.422: INFO: Received response from host: affinity-clusterip-transition-2hrjj
Sep 27 20:15:18.422: INFO: Received response from host: affinity-clusterip-transition-2hrjj
Sep 27 20:15:18.422: INFO: Received response from host: affinity-clusterip-transition-r9d6d
Sep 27 20:15:18.422: INFO: Received response from host: affinity-clusterip-transition-2hrjj
Sep 27 20:15:18.422: INFO: Received response from host: affinity-clusterip-transition-h7vcx
Sep 27 20:15:18.422: INFO: Received response from host: affinity-clusterip-transition-r9d6d
Sep 27 20:15:18.422: INFO: Received response from host: affinity-clusterip-transition-h7vcx
Sep 27 20:15:18.422: INFO: Received response from host: affinity-clusterip-transition-h7vcx
Sep 27 20:15:18.422: INFO: Received response from host: affinity-clusterip-transition-r9d6d
Sep 27 20:15:18.422: INFO: Received response from host: affinity-clusterip-transition-r9d6d
Sep 27 20:15:18.422: INFO: Received response from host: affinity-clusterip-transition-r9d6d
Sep 27 20:15:18.422: INFO: Received response from host: affinity-clusterip-transition-2hrjj
Sep 27 20:15:18.422: INFO: Received response from host: affinity-clusterip-transition-h7vcx
Sep 27 20:15:18.422: INFO: Received response from host: affinity-clusterip-transition-r9d6d
Sep 27 20:15:18.422: INFO: Received response from host: affinity-clusterip-transition-r9d6d
Sep 27 20:15:18.422: INFO: Received response from host: affinity-clusterip-transition-2hrjj
Sep 27 20:15:18.476: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-679324930 exec --namespace=services-6046 execpod-affinity7577v -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.21.75.177:80/ ; done'
Sep 27 20:15:19.233: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.75.177:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.75.177:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.75.177:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.75.177:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.75.177:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.75.177:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.75.177:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.75.177:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.75.177:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.75.177:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.75.177:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.75.177:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.75.177:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.75.177:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.75.177:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.75.177:80/\n"
Sep 27 20:15:19.233: INFO: stdout: "\naffinity-clusterip-transition-h7vcx\naffinity-clusterip-transition-h7vcx\naffinity-clusterip-transition-h7vcx\naffinity-clusterip-transition-h7vcx\naffinity-clusterip-transition-h7vcx\naffinity-clusterip-transition-h7vcx\naffinity-clusterip-transition-h7vcx\naffinity-clusterip-transition-h7vcx\naffinity-clusterip-transition-h7vcx\naffinity-clusterip-transition-h7vcx\naffinity-clusterip-transition-h7vcx\naffinity-clusterip-transition-h7vcx\naffinity-clusterip-transition-h7vcx\naffinity-clusterip-transition-h7vcx\naffinity-clusterip-transition-h7vcx\naffinity-clusterip-transition-h7vcx"
Sep 27 20:15:19.233: INFO: Received response from host: affinity-clusterip-transition-h7vcx
Sep 27 20:15:19.233: INFO: Received response from host: affinity-clusterip-transition-h7vcx
Sep 27 20:15:19.233: INFO: Received response from host: affinity-clusterip-transition-h7vcx
Sep 27 20:15:19.233: INFO: Received response from host: affinity-clusterip-transition-h7vcx
Sep 27 20:15:19.233: INFO: Received response from host: affinity-clusterip-transition-h7vcx
Sep 27 20:15:19.233: INFO: Received response from host: affinity-clusterip-transition-h7vcx
Sep 27 20:15:19.233: INFO: Received response from host: affinity-clusterip-transition-h7vcx
Sep 27 20:15:19.233: INFO: Received response from host: affinity-clusterip-transition-h7vcx
Sep 27 20:15:19.233: INFO: Received response from host: affinity-clusterip-transition-h7vcx
Sep 27 20:15:19.234: INFO: Received response from host: affinity-clusterip-transition-h7vcx
Sep 27 20:15:19.234: INFO: Received response from host: affinity-clusterip-transition-h7vcx
Sep 27 20:15:19.234: INFO: Received response from host: affinity-clusterip-transition-h7vcx
Sep 27 20:15:19.234: INFO: Received response from host: affinity-clusterip-transition-h7vcx
Sep 27 20:15:19.234: INFO: Received response from host: affinity-clusterip-transition-h7vcx
Sep 27 20:15:19.234: INFO: Received response from host: affinity-clusterip-transition-h7vcx
Sep 27 20:15:19.234: INFO: Received response from host: affinity-clusterip-transition-h7vcx
Sep 27 20:15:19.234: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-6046, will wait for the garbage collector to delete the pods
Sep 27 20:15:19.372: INFO: Deleting ReplicationController affinity-clusterip-transition took: 29.098217ms
Sep 27 20:15:19.472: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 100.146529ms
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 20:15:32.164: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6046" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

• [SLOW TEST:25.438 seconds]
[sig-network] Services
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","total":305,"completed":132,"skipped":2048,"failed":0}
SS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 20:15:32.218: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:90
Sep 27 20:15:32.392: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Sep 27 20:15:32.438: INFO: Waiting for terminating namespaces to be deleted...
Sep 27 20:15:32.464: INFO: 
Logging pods the apiserver thinks is on node 10.177.248.114 before test
Sep 27 20:15:32.505: INFO: calico-node-kxvx5 from calico-system started at 2021-09-27 17:50:17 +0000 UTC (1 container statuses recorded)
Sep 27 20:15:32.505: INFO: 	Container calico-node ready: true, restart count 0
Sep 27 20:15:32.505: INFO: calico-typha-7d789bfc7c-8ftcw from calico-system started at 2021-09-27 17:52:15 +0000 UTC (1 container statuses recorded)
Sep 27 20:15:32.505: INFO: 	Container calico-typha ready: true, restart count 0
Sep 27 20:15:32.505: INFO: test-k8s-e2e-pvg-master-verification from default started at 2021-09-27 17:52:52 +0000 UTC (1 container statuses recorded)
Sep 27 20:15:32.506: INFO: 	Container test-k8s-e2e-pvg-master-verification ready: true, restart count 0
Sep 27 20:15:32.506: INFO: ibm-cloud-provider-ip-169-61-239-242-59b4964694-ngqgl from ibm-system started at 2021-09-27 18:03:32 +0000 UTC (1 container statuses recorded)
Sep 27 20:15:32.506: INFO: 	Container ibm-cloud-provider-ip-169-61-239-242 ready: true, restart count 0
Sep 27 20:15:32.506: INFO: ibm-keepalived-watcher-8lj4c from kube-system started at 2021-09-27 17:50:13 +0000 UTC (1 container statuses recorded)
Sep 27 20:15:32.506: INFO: 	Container keepalived-watcher ready: true, restart count 0
Sep 27 20:15:32.506: INFO: ibm-master-proxy-static-10.177.248.114 from kube-system started at 2021-09-27 17:49:33 +0000 UTC (2 container statuses recorded)
Sep 27 20:15:32.506: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Sep 27 20:15:32.506: INFO: 	Container pause ready: true, restart count 0
Sep 27 20:15:32.506: INFO: ibmcloud-block-storage-driver-mxb6q from kube-system started at 2021-09-27 17:50:13 +0000 UTC (1 container statuses recorded)
Sep 27 20:15:32.506: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Sep 27 20:15:32.506: INFO: vpn-bc979587-kglsg from kube-system started at 2021-09-27 17:53:53 +0000 UTC (1 container statuses recorded)
Sep 27 20:15:32.506: INFO: 	Container vpn ready: true, restart count 0
Sep 27 20:15:32.506: INFO: tuned-dsf6r from openshift-cluster-node-tuning-operator started at 2021-09-27 17:51:40 +0000 UTC (1 container statuses recorded)
Sep 27 20:15:32.506: INFO: 	Container tuned ready: true, restart count 0
Sep 27 20:15:32.506: INFO: console-5b57dff4ff-2hr57 from openshift-console started at 2021-09-27 17:54:58 +0000 UTC (1 container statuses recorded)
Sep 27 20:15:32.506: INFO: 	Container console ready: true, restart count 0
Sep 27 20:15:32.507: INFO: dns-default-ktd8w from openshift-dns started at 2021-09-27 17:52:55 +0000 UTC (3 container statuses recorded)
Sep 27 20:15:32.507: INFO: 	Container dns ready: true, restart count 0
Sep 27 20:15:32.507: INFO: 	Container dns-node-resolver ready: true, restart count 0
Sep 27 20:15:32.507: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep 27 20:15:32.507: INFO: node-ca-bv4wj from openshift-image-registry started at 2021-09-27 17:53:17 +0000 UTC (1 container statuses recorded)
Sep 27 20:15:32.507: INFO: 	Container node-ca ready: true, restart count 0
Sep 27 20:15:32.507: INFO: router-default-768f4875db-n8g9c from openshift-ingress started at 2021-09-27 17:53:15 +0000 UTC (1 container statuses recorded)
Sep 27 20:15:32.507: INFO: 	Container router ready: true, restart count 0
Sep 27 20:15:32.507: INFO: openshift-kube-proxy-lxdz4 from openshift-kube-proxy started at 2021-09-27 17:50:13 +0000 UTC (1 container statuses recorded)
Sep 27 20:15:32.507: INFO: 	Container kube-proxy ready: true, restart count 0
Sep 27 20:15:32.507: INFO: certified-operators-qvnmv from openshift-marketplace started at 2021-09-27 17:54:49 +0000 UTC (1 container statuses recorded)
Sep 27 20:15:32.508: INFO: 	Container registry-server ready: true, restart count 0
Sep 27 20:15:32.508: INFO: community-operators-mc4rj from openshift-marketplace started at 2021-09-27 17:54:50 +0000 UTC (1 container statuses recorded)
Sep 27 20:15:32.508: INFO: 	Container registry-server ready: true, restart count 0
Sep 27 20:15:32.508: INFO: redhat-marketplace-tf75q from openshift-marketplace started at 2021-09-27 17:54:48 +0000 UTC (1 container statuses recorded)
Sep 27 20:15:32.508: INFO: 	Container registry-server ready: true, restart count 0
Sep 27 20:15:32.508: INFO: redhat-operators-2sd6b from openshift-marketplace started at 2021-09-27 17:54:49 +0000 UTC (1 container statuses recorded)
Sep 27 20:15:32.508: INFO: 	Container registry-server ready: true, restart count 0
Sep 27 20:15:32.508: INFO: alertmanager-main-0 from openshift-monitoring started at 2021-09-27 17:53:55 +0000 UTC (5 container statuses recorded)
Sep 27 20:15:32.508: INFO: 	Container alertmanager ready: true, restart count 0
Sep 27 20:15:32.508: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Sep 27 20:15:32.508: INFO: 	Container config-reloader ready: true, restart count 0
Sep 27 20:15:32.508: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep 27 20:15:32.508: INFO: 	Container prom-label-proxy ready: true, restart count 0
Sep 27 20:15:32.508: INFO: grafana-6f7d589b46-r88hj from openshift-monitoring started at 2021-09-27 17:53:53 +0000 UTC (2 container statuses recorded)
Sep 27 20:15:32.508: INFO: 	Container grafana ready: true, restart count 0
Sep 27 20:15:32.508: INFO: 	Container grafana-proxy ready: true, restart count 0
Sep 27 20:15:32.508: INFO: kube-state-metrics-5d4985f6b7-jx87q from openshift-monitoring started at 2021-09-27 17:51:49 +0000 UTC (3 container statuses recorded)
Sep 27 20:15:32.509: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Sep 27 20:15:32.509: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Sep 27 20:15:32.509: INFO: 	Container kube-state-metrics ready: true, restart count 0
Sep 27 20:15:32.509: INFO: node-exporter-r84r5 from openshift-monitoring started at 2021-09-27 17:51:50 +0000 UTC (2 container statuses recorded)
Sep 27 20:15:32.509: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep 27 20:15:32.509: INFO: 	Container node-exporter ready: true, restart count 0
Sep 27 20:15:32.509: INFO: openshift-state-metrics-58bfb7bff-qxd59 from openshift-monitoring started at 2021-09-27 17:51:50 +0000 UTC (3 container statuses recorded)
Sep 27 20:15:32.509: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Sep 27 20:15:32.509: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Sep 27 20:15:32.509: INFO: 	Container openshift-state-metrics ready: true, restart count 0
Sep 27 20:15:32.509: INFO: prometheus-adapter-66469d976-gl6wl from openshift-monitoring started at 2021-09-27 17:54:45 +0000 UTC (1 container statuses recorded)
Sep 27 20:15:32.509: INFO: 	Container prometheus-adapter ready: true, restart count 0
Sep 27 20:15:32.509: INFO: prometheus-k8s-0 from openshift-monitoring started at 2021-09-27 17:56:49 +0000 UTC (6 container statuses recorded)
Sep 27 20:15:32.509: INFO: 	Container config-reloader ready: true, restart count 0
Sep 27 20:15:32.509: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep 27 20:15:32.509: INFO: 	Container prom-label-proxy ready: true, restart count 0
Sep 27 20:15:32.509: INFO: 	Container prometheus ready: true, restart count 1
Sep 27 20:15:32.509: INFO: 	Container prometheus-proxy ready: true, restart count 0
Sep 27 20:15:32.509: INFO: 	Container thanos-sidecar ready: true, restart count 0
Sep 27 20:15:32.510: INFO: prometheus-operator-856d6cddd4-blncm from openshift-monitoring started at 2021-09-27 17:54:31 +0000 UTC (2 container statuses recorded)
Sep 27 20:15:32.510: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep 27 20:15:32.510: INFO: 	Container prometheus-operator ready: true, restart count 0
Sep 27 20:15:32.510: INFO: telemeter-client-68cfc9967c-sr2hj from openshift-monitoring started at 2021-09-27 17:51:59 +0000 UTC (3 container statuses recorded)
Sep 27 20:15:32.510: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep 27 20:15:32.510: INFO: 	Container reload ready: true, restart count 0
Sep 27 20:15:32.510: INFO: 	Container telemeter-client ready: true, restart count 0
Sep 27 20:15:32.510: INFO: thanos-querier-6b5789bbbd-qwc9q from openshift-monitoring started at 2021-09-27 17:53:50 +0000 UTC (5 container statuses recorded)
Sep 27 20:15:32.510: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep 27 20:15:32.510: INFO: 	Container kube-rbac-proxy-rules ready: true, restart count 0
Sep 27 20:15:32.510: INFO: 	Container oauth-proxy ready: true, restart count 0
Sep 27 20:15:32.510: INFO: 	Container prom-label-proxy ready: true, restart count 0
Sep 27 20:15:32.510: INFO: 	Container thanos-query ready: true, restart count 0
Sep 27 20:15:32.510: INFO: multus-admission-controller-pp4c5 from openshift-multus started at 2021-09-27 17:51:20 +0000 UTC (2 container statuses recorded)
Sep 27 20:15:32.510: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep 27 20:15:32.510: INFO: 	Container multus-admission-controller ready: true, restart count 0
Sep 27 20:15:32.511: INFO: multus-qwrqb from openshift-multus started at 2021-09-27 17:50:13 +0000 UTC (1 container statuses recorded)
Sep 27 20:15:32.511: INFO: 	Container kube-multus ready: true, restart count 0
Sep 27 20:15:32.511: INFO: network-metrics-daemon-zdwnk from openshift-multus started at 2021-09-27 17:50:13 +0000 UTC (2 container statuses recorded)
Sep 27 20:15:32.511: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep 27 20:15:32.511: INFO: 	Container network-metrics-daemon ready: true, restart count 0
Sep 27 20:15:32.511: INFO: packageserver-67b4ccc984-2lht2 from openshift-operator-lifecycle-manager started at 2021-09-27 17:53:13 +0000 UTC (1 container statuses recorded)
Sep 27 20:15:32.511: INFO: 	Container packageserver ready: true, restart count 0
Sep 27 20:15:32.511: INFO: service-ca-5655fcb96b-ntfqm from openshift-service-ca started at 2021-09-27 17:51:48 +0000 UTC (1 container statuses recorded)
Sep 27 20:15:32.511: INFO: 	Container service-ca-controller ready: true, restart count 0
Sep 27 20:15:32.511: INFO: sonobuoy from sonobuoy started at 2021-09-27 19:21:37 +0000 UTC (1 container statuses recorded)
Sep 27 20:15:32.511: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Sep 27 20:15:32.511: INFO: sonobuoy-e2e-job-f7e56101f16c459b from sonobuoy started at 2021-09-27 19:21:44 +0000 UTC (2 container statuses recorded)
Sep 27 20:15:32.511: INFO: 	Container e2e ready: true, restart count 0
Sep 27 20:15:32.511: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep 27 20:15:32.511: INFO: sonobuoy-systemd-logs-daemon-set-a625d2c9d0384434-sl5dj from sonobuoy started at 2021-09-27 19:21:44 +0000 UTC (2 container statuses recorded)
Sep 27 20:15:32.511: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep 27 20:15:32.511: INFO: 	Container systemd-logs ready: true, restart count 0
Sep 27 20:15:32.512: INFO: 
Logging pods the apiserver thinks is on node 10.177.248.117 before test
Sep 27 20:15:32.544: INFO: calico-node-d4rns from calico-system started at 2021-09-27 17:50:17 +0000 UTC (1 container statuses recorded)
Sep 27 20:15:32.544: INFO: 	Container calico-node ready: true, restart count 0
Sep 27 20:15:32.544: INFO: calico-typha-7d789bfc7c-79bn2 from calico-system started at 2021-09-27 17:52:15 +0000 UTC (1 container statuses recorded)
Sep 27 20:15:32.544: INFO: 	Container calico-typha ready: true, restart count 0
Sep 27 20:15:32.544: INFO: ibm-keepalived-watcher-9fxhn from kube-system started at 2021-09-27 17:48:31 +0000 UTC (1 container statuses recorded)
Sep 27 20:15:32.544: INFO: 	Container keepalived-watcher ready: true, restart count 0
Sep 27 20:15:32.544: INFO: ibm-master-proxy-static-10.177.248.117 from kube-system started at 2021-09-27 17:47:53 +0000 UTC (2 container statuses recorded)
Sep 27 20:15:32.544: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Sep 27 20:15:32.544: INFO: 	Container pause ready: true, restart count 0
Sep 27 20:15:32.544: INFO: ibm-storage-watcher-7cd75f8d4f-kdzj2 from kube-system started at 2021-09-27 19:57:16 +0000 UTC (1 container statuses recorded)
Sep 27 20:15:32.544: INFO: 	Container ibm-storage-watcher-container ready: true, restart count 0
Sep 27 20:15:32.544: INFO: ibmcloud-block-storage-driver-4ldjm from kube-system started at 2021-09-27 17:48:31 +0000 UTC (1 container statuses recorded)
Sep 27 20:15:32.544: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Sep 27 20:15:32.544: INFO: tuned-k9qlw from openshift-cluster-node-tuning-operator started at 2021-09-27 17:51:40 +0000 UTC (1 container statuses recorded)
Sep 27 20:15:32.545: INFO: 	Container tuned ready: true, restart count 0
Sep 27 20:15:32.545: INFO: console-operator-d6cf9dd7d-76zv4 from openshift-console-operator started at 2021-09-27 19:57:16 +0000 UTC (1 container statuses recorded)
Sep 27 20:15:32.545: INFO: 	Container console-operator ready: true, restart count 0
Sep 27 20:15:32.545: INFO: console-5b57dff4ff-d7kjb from openshift-console started at 2021-09-27 19:57:16 +0000 UTC (1 container statuses recorded)
Sep 27 20:15:32.545: INFO: 	Container console ready: true, restart count 0
Sep 27 20:15:32.545: INFO: dns-default-cb2zk from openshift-dns started at 2021-09-27 17:52:55 +0000 UTC (3 container statuses recorded)
Sep 27 20:15:32.545: INFO: 	Container dns ready: true, restart count 0
Sep 27 20:15:32.545: INFO: 	Container dns-node-resolver ready: true, restart count 0
Sep 27 20:15:32.545: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep 27 20:15:32.545: INFO: node-ca-82tbs from openshift-image-registry started at 2021-09-27 17:53:17 +0000 UTC (1 container statuses recorded)
Sep 27 20:15:32.545: INFO: 	Container node-ca ready: true, restart count 0
Sep 27 20:15:32.545: INFO: registry-pvc-permissions-vncj6 from openshift-image-registry started at 2021-09-27 17:55:54 +0000 UTC (1 container statuses recorded)
Sep 27 20:15:32.545: INFO: 	Container pvc-permissions ready: false, restart count 0
Sep 27 20:15:32.545: INFO: openshift-kube-proxy-2dc5b from openshift-kube-proxy started at 2021-09-27 17:49:34 +0000 UTC (1 container statuses recorded)
Sep 27 20:15:32.545: INFO: 	Container kube-proxy ready: true, restart count 0
Sep 27 20:15:32.545: INFO: migrator-6656c87b46-pnhl2 from openshift-kube-storage-version-migrator started at 2021-09-27 19:57:16 +0000 UTC (1 container statuses recorded)
Sep 27 20:15:32.545: INFO: 	Container migrator ready: true, restart count 0
Sep 27 20:15:32.545: INFO: alertmanager-main-2 from openshift-monitoring started at 2021-09-27 19:57:22 +0000 UTC (5 container statuses recorded)
Sep 27 20:15:32.545: INFO: 	Container alertmanager ready: true, restart count 0
Sep 27 20:15:32.545: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Sep 27 20:15:32.545: INFO: 	Container config-reloader ready: true, restart count 0
Sep 27 20:15:32.545: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep 27 20:15:32.545: INFO: 	Container prom-label-proxy ready: true, restart count 0
Sep 27 20:15:32.545: INFO: node-exporter-r6mdq from openshift-monitoring started at 2021-09-27 17:51:50 +0000 UTC (2 container statuses recorded)
Sep 27 20:15:32.545: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep 27 20:15:32.545: INFO: 	Container node-exporter ready: true, restart count 0
Sep 27 20:15:32.545: INFO: prometheus-k8s-1 from openshift-monitoring started at 2021-09-27 19:57:17 +0000 UTC (6 container statuses recorded)
Sep 27 20:15:32.545: INFO: 	Container config-reloader ready: true, restart count 0
Sep 27 20:15:32.545: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep 27 20:15:32.545: INFO: 	Container prom-label-proxy ready: true, restart count 0
Sep 27 20:15:32.545: INFO: 	Container prometheus ready: true, restart count 1
Sep 27 20:15:32.545: INFO: 	Container prometheus-proxy ready: true, restart count 0
Sep 27 20:15:32.545: INFO: 	Container thanos-sidecar ready: true, restart count 0
Sep 27 20:15:32.545: INFO: multus-admission-controller-rtg5z from openshift-multus started at 2021-09-27 19:57:16 +0000 UTC (2 container statuses recorded)
Sep 27 20:15:32.545: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep 27 20:15:32.545: INFO: 	Container multus-admission-controller ready: true, restart count 0
Sep 27 20:15:32.545: INFO: multus-vjt2q from openshift-multus started at 2021-09-27 17:49:24 +0000 UTC (1 container statuses recorded)
Sep 27 20:15:32.545: INFO: 	Container kube-multus ready: true, restart count 0
Sep 27 20:15:32.545: INFO: network-metrics-daemon-4t2kd from openshift-multus started at 2021-09-27 17:49:26 +0000 UTC (2 container statuses recorded)
Sep 27 20:15:32.546: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep 27 20:15:32.546: INFO: 	Container network-metrics-daemon ready: true, restart count 0
Sep 27 20:15:32.546: INFO: network-operator-dbdd595f7-dwsbv from openshift-network-operator started at 2021-09-27 19:57:16 +0000 UTC (1 container statuses recorded)
Sep 27 20:15:32.546: INFO: 	Container network-operator ready: true, restart count 0
Sep 27 20:15:32.546: INFO: sonobuoy-systemd-logs-daemon-set-a625d2c9d0384434-z8xz7 from sonobuoy started at 2021-09-27 19:21:44 +0000 UTC (2 container statuses recorded)
Sep 27 20:15:32.546: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep 27 20:15:32.546: INFO: 	Container systemd-logs ready: true, restart count 0
Sep 27 20:15:32.546: INFO: 
Logging pods the apiserver thinks is on node 10.177.248.126 before test
Sep 27 20:15:32.585: INFO: calico-kube-controllers-5465c95dd-7hlj7 from calico-system started at 2021-09-27 17:50:29 +0000 UTC (1 container statuses recorded)
Sep 27 20:15:32.586: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Sep 27 20:15:32.586: INFO: calico-node-dhz9l from calico-system started at 2021-09-27 17:50:17 +0000 UTC (1 container statuses recorded)
Sep 27 20:15:32.586: INFO: 	Container calico-node ready: true, restart count 0
Sep 27 20:15:32.586: INFO: calico-typha-7d789bfc7c-7669x from calico-system started at 2021-09-27 17:50:17 +0000 UTC (1 container statuses recorded)
Sep 27 20:15:32.586: INFO: 	Container calico-typha ready: true, restart count 0
Sep 27 20:15:32.586: INFO: ibm-cloud-provider-ip-169-61-239-242-59b4964694-mc2b8 from ibm-system started at 2021-09-27 19:52:27 +0000 UTC (1 container statuses recorded)
Sep 27 20:15:32.586: INFO: 	Container ibm-cloud-provider-ip-169-61-239-242 ready: true, restart count 0
Sep 27 20:15:32.586: INFO: ibm-file-plugin-fcdf5f569-x8hfq from kube-system started at 2021-09-27 17:50:29 +0000 UTC (1 container statuses recorded)
Sep 27 20:15:32.586: INFO: 	Container ibm-file-plugin-container ready: true, restart count 0
Sep 27 20:15:32.586: INFO: ibm-keepalived-watcher-rgwfm from kube-system started at 2021-09-27 17:49:04 +0000 UTC (1 container statuses recorded)
Sep 27 20:15:32.586: INFO: 	Container keepalived-watcher ready: true, restart count 0
Sep 27 20:15:32.587: INFO: ibm-master-proxy-static-10.177.248.126 from kube-system started at 2021-09-27 17:48:20 +0000 UTC (2 container statuses recorded)
Sep 27 20:15:32.587: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Sep 27 20:15:32.587: INFO: 	Container pause ready: true, restart count 0
Sep 27 20:15:32.587: INFO: ibmcloud-block-storage-driver-kbd9g from kube-system started at 2021-09-27 17:49:04 +0000 UTC (1 container statuses recorded)
Sep 27 20:15:32.587: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Sep 27 20:15:32.587: INFO: ibmcloud-block-storage-plugin-74d6877898-zqtnb from kube-system started at 2021-09-27 17:50:29 +0000 UTC (1 container statuses recorded)
Sep 27 20:15:32.587: INFO: 	Container ibmcloud-block-storage-plugin-container ready: true, restart count 0
Sep 27 20:15:32.587: INFO: cluster-node-tuning-operator-67b4b4fbf5-zslfs from openshift-cluster-node-tuning-operator started at 2021-09-27 17:50:29 +0000 UTC (1 container statuses recorded)
Sep 27 20:15:32.587: INFO: 	Container cluster-node-tuning-operator ready: true, restart count 0
Sep 27 20:15:32.587: INFO: tuned-qc5dt from openshift-cluster-node-tuning-operator started at 2021-09-27 17:51:41 +0000 UTC (1 container statuses recorded)
Sep 27 20:15:32.587: INFO: 	Container tuned ready: true, restart count 0
Sep 27 20:15:32.587: INFO: cluster-samples-operator-97cc95ff8-nmc7q from openshift-cluster-samples-operator started at 2021-09-27 17:50:29 +0000 UTC (2 container statuses recorded)
Sep 27 20:15:32.587: INFO: 	Container cluster-samples-operator ready: true, restart count 0
Sep 27 20:15:32.588: INFO: 	Container cluster-samples-operator-watch ready: true, restart count 0
Sep 27 20:15:32.588: INFO: cluster-storage-operator-59698ddbdf-w86gw from openshift-cluster-storage-operator started at 2021-09-27 17:50:29 +0000 UTC (1 container statuses recorded)
Sep 27 20:15:32.588: INFO: 	Container cluster-storage-operator ready: true, restart count 0
Sep 27 20:15:32.588: INFO: downloads-c785794b6-q92tc from openshift-console started at 2021-09-27 17:50:29 +0000 UTC (1 container statuses recorded)
Sep 27 20:15:32.588: INFO: 	Container download-server ready: true, restart count 0
Sep 27 20:15:32.588: INFO: downloads-c785794b6-w28kb from openshift-console started at 2021-09-27 17:50:29 +0000 UTC (1 container statuses recorded)
Sep 27 20:15:32.588: INFO: 	Container download-server ready: true, restart count 0
Sep 27 20:15:32.588: INFO: dns-operator-7489bbc67f-9ztr4 from openshift-dns-operator started at 2021-09-27 17:50:29 +0000 UTC (2 container statuses recorded)
Sep 27 20:15:32.589: INFO: 	Container dns-operator ready: true, restart count 0
Sep 27 20:15:32.589: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep 27 20:15:32.589: INFO: dns-default-msjgz from openshift-dns started at 2021-09-27 17:52:55 +0000 UTC (3 container statuses recorded)
Sep 27 20:15:32.589: INFO: 	Container dns ready: true, restart count 0
Sep 27 20:15:32.589: INFO: 	Container dns-node-resolver ready: true, restart count 0
Sep 27 20:15:32.589: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep 27 20:15:32.589: INFO: cluster-image-registry-operator-675674456f-k2fc9 from openshift-image-registry started at 2021-09-27 17:50:29 +0000 UTC (1 container statuses recorded)
Sep 27 20:15:32.589: INFO: 	Container cluster-image-registry-operator ready: true, restart count 0
Sep 27 20:15:32.589: INFO: image-registry-ffbbf6ddb-xl9b9 from openshift-image-registry started at 2021-09-27 17:55:54 +0000 UTC (1 container statuses recorded)
Sep 27 20:15:32.589: INFO: 	Container registry ready: true, restart count 0
Sep 27 20:15:32.590: INFO: node-ca-2rrgl from openshift-image-registry started at 2021-09-27 17:53:17 +0000 UTC (1 container statuses recorded)
Sep 27 20:15:32.590: INFO: 	Container node-ca ready: true, restart count 0
Sep 27 20:15:32.590: INFO: ingress-operator-8469759c95-4mm9m from openshift-ingress-operator started at 2021-09-27 17:50:29 +0000 UTC (2 container statuses recorded)
Sep 27 20:15:32.590: INFO: 	Container ingress-operator ready: true, restart count 0
Sep 27 20:15:32.590: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep 27 20:15:32.590: INFO: router-default-768f4875db-8qx4w from openshift-ingress started at 2021-09-27 17:53:15 +0000 UTC (1 container statuses recorded)
Sep 27 20:15:32.590: INFO: 	Container router ready: true, restart count 0
Sep 27 20:15:32.590: INFO: openshift-kube-proxy-7gx28 from openshift-kube-proxy started at 2021-09-27 17:49:34 +0000 UTC (1 container statuses recorded)
Sep 27 20:15:32.590: INFO: 	Container kube-proxy ready: true, restart count 0
Sep 27 20:15:32.590: INFO: kube-storage-version-migrator-operator-55c7c8f996-xr7ds from openshift-kube-storage-version-migrator-operator started at 2021-09-27 17:50:29 +0000 UTC (1 container statuses recorded)
Sep 27 20:15:32.590: INFO: 	Container kube-storage-version-migrator-operator ready: true, restart count 1
Sep 27 20:15:32.590: INFO: marketplace-operator-7cd49bbf56-nggpg from openshift-marketplace started at 2021-09-27 17:50:29 +0000 UTC (1 container statuses recorded)
Sep 27 20:15:32.591: INFO: 	Container marketplace-operator ready: true, restart count 0
Sep 27 20:15:32.591: INFO: alertmanager-main-1 from openshift-monitoring started at 2021-09-27 17:53:55 +0000 UTC (5 container statuses recorded)
Sep 27 20:15:32.591: INFO: 	Container alertmanager ready: true, restart count 0
Sep 27 20:15:32.591: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Sep 27 20:15:32.591: INFO: 	Container config-reloader ready: true, restart count 0
Sep 27 20:15:32.591: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep 27 20:15:32.591: INFO: 	Container prom-label-proxy ready: true, restart count 0
Sep 27 20:15:32.591: INFO: cluster-monitoring-operator-7656b489c8-z2wmx from openshift-monitoring started at 2021-09-27 17:50:29 +0000 UTC (2 container statuses recorded)
Sep 27 20:15:32.591: INFO: 	Container cluster-monitoring-operator ready: true, restart count 0
Sep 27 20:15:32.591: INFO: 	Container kube-rbac-proxy ready: true, restart count 3
Sep 27 20:15:32.591: INFO: node-exporter-42nr4 from openshift-monitoring started at 2021-09-27 17:51:50 +0000 UTC (2 container statuses recorded)
Sep 27 20:15:32.591: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep 27 20:15:32.591: INFO: 	Container node-exporter ready: true, restart count 0
Sep 27 20:15:32.591: INFO: prometheus-adapter-66469d976-t6rf2 from openshift-monitoring started at 2021-09-27 17:54:46 +0000 UTC (1 container statuses recorded)
Sep 27 20:15:32.591: INFO: 	Container prometheus-adapter ready: true, restart count 0
Sep 27 20:15:32.591: INFO: thanos-querier-6b5789bbbd-l8dmh from openshift-monitoring started at 2021-09-27 17:53:51 +0000 UTC (5 container statuses recorded)
Sep 27 20:15:32.591: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep 27 20:15:32.591: INFO: 	Container kube-rbac-proxy-rules ready: true, restart count 0
Sep 27 20:15:32.592: INFO: 	Container oauth-proxy ready: true, restart count 0
Sep 27 20:15:32.592: INFO: 	Container prom-label-proxy ready: true, restart count 0
Sep 27 20:15:32.592: INFO: 	Container thanos-query ready: true, restart count 0
Sep 27 20:15:32.592: INFO: multus-admission-controller-fgtsq from openshift-multus started at 2021-09-27 17:50:29 +0000 UTC (2 container statuses recorded)
Sep 27 20:15:32.592: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep 27 20:15:32.592: INFO: 	Container multus-admission-controller ready: true, restart count 0
Sep 27 20:15:32.592: INFO: multus-fzpdl from openshift-multus started at 2021-09-27 17:49:24 +0000 UTC (1 container statuses recorded)
Sep 27 20:15:32.592: INFO: 	Container kube-multus ready: true, restart count 0
Sep 27 20:15:32.592: INFO: network-metrics-daemon-5wdz5 from openshift-multus started at 2021-09-27 17:49:26 +0000 UTC (2 container statuses recorded)
Sep 27 20:15:32.592: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep 27 20:15:32.592: INFO: 	Container network-metrics-daemon ready: true, restart count 0
Sep 27 20:15:32.592: INFO: catalog-operator-69c4599997-btc8l from openshift-operator-lifecycle-manager started at 2021-09-27 17:50:29 +0000 UTC (1 container statuses recorded)
Sep 27 20:15:32.592: INFO: 	Container catalog-operator ready: true, restart count 0
Sep 27 20:15:32.592: INFO: olm-operator-db7cd6974-2gxjx from openshift-operator-lifecycle-manager started at 2021-09-27 17:50:29 +0000 UTC (1 container statuses recorded)
Sep 27 20:15:32.592: INFO: 	Container olm-operator ready: true, restart count 0
Sep 27 20:15:32.592: INFO: packageserver-67b4ccc984-w6sdn from openshift-operator-lifecycle-manager started at 2021-09-27 17:53:13 +0000 UTC (1 container statuses recorded)
Sep 27 20:15:32.593: INFO: 	Container packageserver ready: true, restart count 0
Sep 27 20:15:32.593: INFO: metrics-666cbf9545-t95w7 from openshift-roks-metrics started at 2021-09-27 17:50:29 +0000 UTC (1 container statuses recorded)
Sep 27 20:15:32.593: INFO: 	Container metrics ready: true, restart count 3
Sep 27 20:15:32.593: INFO: push-gateway-6f6b5cb7f7-k6lpz from openshift-roks-metrics started at 2021-09-27 19:52:28 +0000 UTC (1 container statuses recorded)
Sep 27 20:15:32.593: INFO: 	Container push-gateway ready: false, restart count 0
Sep 27 20:15:32.593: INFO: service-ca-operator-6ccc7cff9-l9wfj from openshift-service-ca-operator started at 2021-09-27 17:50:29 +0000 UTC (1 container statuses recorded)
Sep 27 20:15:32.593: INFO: 	Container service-ca-operator ready: true, restart count 1
Sep 27 20:15:32.593: INFO: sonobuoy-systemd-logs-daemon-set-a625d2c9d0384434-6w8vw from sonobuoy started at 2021-09-27 19:21:44 +0000 UTC (2 container statuses recorded)
Sep 27 20:15:32.593: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep 27 20:15:32.593: INFO: 	Container systemd-logs ready: true, restart count 0
Sep 27 20:15:32.593: INFO: tigera-operator-db8ddcc79-mf5vc from tigera-operator started at 2021-09-27 19:52:27 +0000 UTC (1 container statuses recorded)
Sep 27 20:15:32.593: INFO: 	Container tigera-operator ready: true, restart count 0
[It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-ff2f6c37-2176-401b-8254-b2db45ca4fa2 90
STEP: Trying to create a pod(pod1) with hostport 54321 and hostIP 127.0.0.1 and expect scheduled
STEP: Trying to create another pod(pod2) with hostport 54321 but hostIP 127.0.0.2 on the node which pod1 resides and expect scheduled
STEP: Trying to create a third pod(pod3) with hostport 54321, hostIP 127.0.0.2 but use UDP protocol on the node which pod2 resides
STEP: removing the label kubernetes.io/e2e-ff2f6c37-2176-401b-8254-b2db45ca4fa2 off the node 10.177.248.117
STEP: verifying the node doesn't have the label kubernetes.io/e2e-ff2f6c37-2176-401b-8254-b2db45ca4fa2
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 20:15:41.156: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-7743" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81

• [SLOW TEST:8.982 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]","total":305,"completed":133,"skipped":2050,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 20:15:41.204: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a ResourceQuota with terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a long running pod
STEP: Ensuring resource quota with not terminating scope captures the pod usage
STEP: Ensuring resource quota with terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a terminating pod
STEP: Ensuring resource quota with terminating scope captures the pod usage
STEP: Ensuring resource quota with not terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 20:15:57.890: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-4088" for this suite.

• [SLOW TEST:16.733 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with terminating scopes. [Conformance]","total":305,"completed":134,"skipped":2115,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 20:15:57.938: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicationController
STEP: Ensuring resource quota status captures replication controller creation
STEP: Deleting a ReplicationController
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 20:16:09.314: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-2769" for this suite.

• [SLOW TEST:11.432 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replication controller. [Conformance]","total":305,"completed":135,"skipped":2157,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 20:16:09.385: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Sep 27 20:16:09.606: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ced1a1b7-1e74-4f64-b667-75e670ef48c2" in namespace "projected-1447" to be "Succeeded or Failed"
Sep 27 20:16:09.626: INFO: Pod "downwardapi-volume-ced1a1b7-1e74-4f64-b667-75e670ef48c2": Phase="Pending", Reason="", readiness=false. Elapsed: 19.98354ms
Sep 27 20:16:11.679: INFO: Pod "downwardapi-volume-ced1a1b7-1e74-4f64-b667-75e670ef48c2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.073222189s
Sep 27 20:16:13.693: INFO: Pod "downwardapi-volume-ced1a1b7-1e74-4f64-b667-75e670ef48c2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.08745192s
STEP: Saw pod success
Sep 27 20:16:13.694: INFO: Pod "downwardapi-volume-ced1a1b7-1e74-4f64-b667-75e670ef48c2" satisfied condition "Succeeded or Failed"
Sep 27 20:16:13.706: INFO: Trying to get logs from node 10.177.248.117 pod downwardapi-volume-ced1a1b7-1e74-4f64-b667-75e670ef48c2 container client-container: <nil>
STEP: delete the pod
Sep 27 20:16:13.779: INFO: Waiting for pod downwardapi-volume-ced1a1b7-1e74-4f64-b667-75e670ef48c2 to disappear
Sep 27 20:16:13.793: INFO: Pod downwardapi-volume-ced1a1b7-1e74-4f64-b667-75e670ef48c2 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 20:16:13.793: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1447" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","total":305,"completed":136,"skipped":2187,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 20:16:13.842: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating projection with secret that has name projected-secret-test-map-507a944b-550a-454c-9a9a-319583c564ac
STEP: Creating a pod to test consume secrets
Sep 27 20:16:14.088: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-4b39950d-ac75-4513-a5ab-c2ebddedd16d" in namespace "projected-1353" to be "Succeeded or Failed"
Sep 27 20:16:14.104: INFO: Pod "pod-projected-secrets-4b39950d-ac75-4513-a5ab-c2ebddedd16d": Phase="Pending", Reason="", readiness=false. Elapsed: 15.626208ms
Sep 27 20:16:16.119: INFO: Pod "pod-projected-secrets-4b39950d-ac75-4513-a5ab-c2ebddedd16d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.03080157s
STEP: Saw pod success
Sep 27 20:16:16.119: INFO: Pod "pod-projected-secrets-4b39950d-ac75-4513-a5ab-c2ebddedd16d" satisfied condition "Succeeded or Failed"
Sep 27 20:16:16.132: INFO: Trying to get logs from node 10.177.248.117 pod pod-projected-secrets-4b39950d-ac75-4513-a5ab-c2ebddedd16d container projected-secret-volume-test: <nil>
STEP: delete the pod
Sep 27 20:16:16.210: INFO: Waiting for pod pod-projected-secrets-4b39950d-ac75-4513-a5ab-c2ebddedd16d to disappear
Sep 27 20:16:16.228: INFO: Pod pod-projected-secrets-4b39950d-ac75-4513-a5ab-c2ebddedd16d no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 20:16:16.228: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1353" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":305,"completed":137,"skipped":2217,"failed":0}
S
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 20:16:16.271: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[It] should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: starting the proxy server
Sep 27 20:16:16.454: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-679324930 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 20:16:16.732: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-462" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support proxy with --port 0  [Conformance]","total":305,"completed":138,"skipped":2218,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Events 
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 20:16:16.829: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: retrieving the pod
Sep 27 20:16:19.163: INFO: &Pod{ObjectMeta:{send-events-255970b4-177d-472c-b359-9399d205df77  events-8544 /api/v1/namespaces/events-8544/pods/send-events-255970b4-177d-472c-b359-9399d205df77 ac352440-a0d4-4934-90b0-f1f676931a6b 83417 0 2021-09-27 20:16:17 +0000 UTC <nil> <nil> map[name:foo time:48031855] map[cni.projectcalico.org/podIP:172.30.75.22/32 cni.projectcalico.org/podIPs:172.30.75.22/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "",
    "ips": [
        "172.30.75.22"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "",
    "ips": [
        "172.30.75.22"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [] []  [{e2e.test Update v1 2021-09-27 20:16:17 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:time":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"p\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":80,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-09-27 20:16:18 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2021-09-27 20:16:18 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.75.22\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}} {multus Update v1 2021-09-27 20:16:18 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-99r4q,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-99r4q,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:p,Image:k8s.gcr.io/e2e-test-images/agnhost:2.20,Command:[],Args:[serve-hostname],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:80,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-99r4q,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.177.248.117,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c48,c12,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-wphrg,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-27 20:16:17 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-27 20:16:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-27 20:16:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-27 20:16:17 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.177.248.117,PodIP:172.30.75.22,StartTime:2021-09-27 20:16:17 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:p,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-09-27 20:16:18 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.20,ImageID:k8s.gcr.io/e2e-test-images/agnhost@sha256:17e61a0b9e498b6c73ed97670906be3d5a3ae394739c1bd5b619e1a004885cf0,ContainerID:cri-o://dd20bb263733e3ca7056b294191ab1a7c7913f43feae6a141133896ebad09f26,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.75.22,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

STEP: checking for scheduler event about the pod
Sep 27 20:16:21.208: INFO: Saw scheduler event for our pod.
STEP: checking for kubelet event about the pod
Sep 27 20:16:23.240: INFO: Saw kubelet event for our pod.
STEP: deleting the pod
[AfterEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 20:16:23.263: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-8544" for this suite.

• [SLOW TEST:6.478 seconds]
[k8s.io] [sig-node] Events
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] [sig-node] Events should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]","total":305,"completed":139,"skipped":2235,"failed":0}
[sig-network] Services 
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 20:16:23.308: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a service externalname-service with the type=ExternalName in namespace services-8705
STEP: changing the ExternalName service to type=ClusterIP
STEP: creating replication controller externalname-service in namespace services-8705
I0927 20:16:23.586550      22 runners.go:190] Created replication controller with name: externalname-service, namespace: services-8705, replica count: 2
I0927 20:16:26.637276      22 runners.go:190] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Sep 27 20:16:26.637: INFO: Creating new exec pod
Sep 27 20:16:31.742: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-679324930 exec --namespace=services-8705 execpodrwdtk -- /bin/sh -x -c nc -zv -t -w 2 externalname-service 80'
Sep 27 20:16:32.195: INFO: stderr: "+ nc -zv -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Sep 27 20:16:32.195: INFO: stdout: ""
Sep 27 20:16:32.197: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-679324930 exec --namespace=services-8705 execpodrwdtk -- /bin/sh -x -c nc -zv -t -w 2 172.21.146.49 80'
Sep 27 20:16:32.725: INFO: stderr: "+ nc -zv -t -w 2 172.21.146.49 80\nConnection to 172.21.146.49 80 port [tcp/http] succeeded!\n"
Sep 27 20:16:32.725: INFO: stdout: ""
Sep 27 20:16:32.725: INFO: Cleaning up the ExternalName to ClusterIP test service
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 20:16:32.816: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8705" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

• [SLOW TEST:9.562 seconds]
[sig-network] Services
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to ClusterIP [Conformance]","total":305,"completed":140,"skipped":2235,"failed":0}
SSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 20:16:32.871: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:162
[It] should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod
Sep 27 20:16:33.095: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 20:16:38.005: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-5843" for this suite.

• [SLOW TEST:5.184 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should invoke init containers on a RestartAlways pod [Conformance]","total":305,"completed":141,"skipped":2238,"failed":0}
[sig-api-machinery] Garbage collector 
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 20:16:38.055: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W0927 20:16:44.363349      22 metrics_grabber.go:98] Can't find kube-scheduler pod. Grabbing metrics from kube-scheduler is disabled.
W0927 20:16:44.363601      22 metrics_grabber.go:102] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
W0927 20:16:44.363711      22 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
Sep 27 20:16:44.363: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 20:16:44.364: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-7929" for this suite.

• [SLOW TEST:6.350 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]","total":305,"completed":142,"skipped":2238,"failed":0}
SSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 20:16:44.405: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:78
[It] deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Sep 27 20:16:44.612: INFO: Pod name cleanup-pod: Found 0 pods out of 1
Sep 27 20:16:49.633: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Sep 27 20:16:49.633: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:72
Sep 27 20:16:49.706: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:{test-cleanup-deployment  deployment-2379 /apis/apps/v1/namespaces/deployment-2379/deployments/test-cleanup-deployment d23314bf-deaa-4e3a-bf4a-106f2981da37 84198 1 2021-09-27 20:16:49 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] []  [{e2e.test Update apps/v1 2021-09-27 20:16:49 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{}}},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.20 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0012fcb38 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[]DeploymentCondition{},ReadyReplicas:0,CollisionCount:nil,},}

Sep 27 20:16:49.725: INFO: New ReplicaSet "test-cleanup-deployment-5d446bdd47" of Deployment "test-cleanup-deployment":
&ReplicaSet{ObjectMeta:{test-cleanup-deployment-5d446bdd47  deployment-2379 /apis/apps/v1/namespaces/deployment-2379/replicasets/test-cleanup-deployment-5d446bdd47 eb06bda3-b60e-46e4-b2d7-16862431e6ac 84205 1 2021-09-27 20:16:49 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:5d446bdd47] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-cleanup-deployment d23314bf-deaa-4e3a-bf4a-106f2981da37 0xc0012fd047 0xc0012fd048}] []  [{kube-controller-manager Update apps/v1 2021-09-27 20:16:49 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d23314bf-deaa-4e3a-bf4a-106f2981da37\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 5d446bdd47,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:5d446bdd47] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.20 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0012fd0d8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:0,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Sep 27 20:16:49.726: INFO: All old ReplicaSets of Deployment "test-cleanup-deployment":
Sep 27 20:16:49.726: INFO: &ReplicaSet{ObjectMeta:{test-cleanup-controller  deployment-2379 /apis/apps/v1/namespaces/deployment-2379/replicasets/test-cleanup-controller 00852505-e24d-4b28-af14-a3d2ed979fd8 84203 1 2021-09-27 20:16:44 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 Deployment test-cleanup-deployment d23314bf-deaa-4e3a-bf4a-106f2981da37 0xc0012fcf37 0xc0012fcf38}] []  [{e2e.test Update apps/v1 2021-09-27 20:16:44 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:replicas":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{},"f:pod":{}}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-09-27 20:16:49 +0000 UTC FieldsV1 {"f:metadata":{"f:ownerReferences":{".":{},"k:{\"uid\":\"d23314bf-deaa-4e3a-bf4a-106f2981da37\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0012fcfd8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Sep 27 20:16:49.742: INFO: Pod "test-cleanup-controller-65gx9" is available:
&Pod{ObjectMeta:{test-cleanup-controller-65gx9 test-cleanup-controller- deployment-2379 /api/v1/namespaces/deployment-2379/pods/test-cleanup-controller-65gx9 da1ecbc9-4d1f-4458-acdf-b03cc9ba3062 84173 0 2021-09-27 20:16:44 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[cni.projectcalico.org/podIP:172.30.75.10/32 cni.projectcalico.org/podIPs:172.30.75.10/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "",
    "ips": [
        "172.30.75.10"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "",
    "ips": [
        "172.30.75.10"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet test-cleanup-controller 00852505-e24d-4b28-af14-a3d2ed979fd8 0xc0012fd5f7 0xc0012fd5f8}] []  [{kube-controller-manager Update v1 2021-09-27 20:16:44 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"00852505-e24d-4b28-af14-a3d2ed979fd8\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-09-27 20:16:45 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {multus Update v1 2021-09-27 20:16:45 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}}} {kubelet Update v1 2021-09-27 20:16:46 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.75.10\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-b6pjh,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-b6pjh,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-b6pjh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.177.248.117,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c48,c32,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-pkw5t,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-27 20:16:44 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-27 20:16:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-27 20:16:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-27 20:16:44 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.177.248.117,PodIP:172.30.75.10,StartTime:2021-09-27 20:16:44 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-09-27 20:16:46 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:6feb0ea7b0967367da66e8d58ba813fde32bdb92f63bfc21a9e170d211539db4,ContainerID:cri-o://ab52001dfe44227dbea97fc71eef968eca0060ee8de851f4127321dfc2e0d896,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.75.10,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 20:16:49.743: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-2379" for this suite.

• [SLOW TEST:5.378 seconds]
[sig-apps] Deployment
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should delete old replica sets [Conformance]","total":305,"completed":143,"skipped":2245,"failed":0}
SSS
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class 
  should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 20:16:49.783: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods Set QOS Class
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:161
[It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying QOS class is set on the pod
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 20:16:50.084: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-7886" for this suite.
•{"msg":"PASSED [k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]","total":305,"completed":144,"skipped":2248,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 20:16:50.129: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward api env vars
Sep 27 20:16:50.356: INFO: Waiting up to 5m0s for pod "downward-api-121be5dc-9171-46ac-8e1e-186e9b67c2e3" in namespace "downward-api-2520" to be "Succeeded or Failed"
Sep 27 20:16:50.369: INFO: Pod "downward-api-121be5dc-9171-46ac-8e1e-186e9b67c2e3": Phase="Pending", Reason="", readiness=false. Elapsed: 13.018228ms
Sep 27 20:16:52.386: INFO: Pod "downward-api-121be5dc-9171-46ac-8e1e-186e9b67c2e3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029765011s
Sep 27 20:16:54.402: INFO: Pod "downward-api-121be5dc-9171-46ac-8e1e-186e9b67c2e3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.046160502s
STEP: Saw pod success
Sep 27 20:16:54.402: INFO: Pod "downward-api-121be5dc-9171-46ac-8e1e-186e9b67c2e3" satisfied condition "Succeeded or Failed"
Sep 27 20:16:54.416: INFO: Trying to get logs from node 10.177.248.126 pod downward-api-121be5dc-9171-46ac-8e1e-186e9b67c2e3 container dapi-container: <nil>
STEP: delete the pod
Sep 27 20:16:54.557: INFO: Waiting for pod downward-api-121be5dc-9171-46ac-8e1e-186e9b67c2e3 to disappear
Sep 27 20:16:54.571: INFO: Pod downward-api-121be5dc-9171-46ac-8e1e-186e9b67c2e3 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 20:16:54.572: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2520" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide host IP as an env var [NodeConformance] [Conformance]","total":305,"completed":145,"skipped":2276,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 20:16:54.615: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating secret with name projected-secret-test-84575f65-a660-421a-8163-4c7d5e3e60cb
STEP: Creating a pod to test consume secrets
Sep 27 20:16:54.905: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-9543072c-75a2-46a3-8637-545df6d4bd26" in namespace "projected-9366" to be "Succeeded or Failed"
Sep 27 20:16:54.919: INFO: Pod "pod-projected-secrets-9543072c-75a2-46a3-8637-545df6d4bd26": Phase="Pending", Reason="", readiness=false. Elapsed: 13.528472ms
Sep 27 20:16:56.941: INFO: Pod "pod-projected-secrets-9543072c-75a2-46a3-8637-545df6d4bd26": Phase="Pending", Reason="", readiness=false. Elapsed: 2.035466981s
Sep 27 20:16:58.956: INFO: Pod "pod-projected-secrets-9543072c-75a2-46a3-8637-545df6d4bd26": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.050850423s
STEP: Saw pod success
Sep 27 20:16:58.956: INFO: Pod "pod-projected-secrets-9543072c-75a2-46a3-8637-545df6d4bd26" satisfied condition "Succeeded or Failed"
Sep 27 20:16:58.970: INFO: Trying to get logs from node 10.177.248.117 pod pod-projected-secrets-9543072c-75a2-46a3-8637-545df6d4bd26 container secret-volume-test: <nil>
STEP: delete the pod
Sep 27 20:16:59.052: INFO: Waiting for pod pod-projected-secrets-9543072c-75a2-46a3-8637-545df6d4bd26 to disappear
Sep 27 20:16:59.066: INFO: Pod pod-projected-secrets-9543072c-75a2-46a3-8637-545df6d4bd26 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 20:16:59.067: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9366" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","total":305,"completed":146,"skipped":2287,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 20:16:59.115: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:134
[It] should run and stop complex daemon [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Sep 27 20:16:59.399: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes.
Sep 27 20:16:59.432: INFO: Number of nodes with available pods: 0
Sep 27 20:16:59.432: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Change node label to blue, check that daemon pod is launched.
Sep 27 20:16:59.523: INFO: Number of nodes with available pods: 0
Sep 27 20:16:59.523: INFO: Node 10.177.248.114 is running more than one daemon pod
Sep 27 20:17:00.538: INFO: Number of nodes with available pods: 0
Sep 27 20:17:00.538: INFO: Node 10.177.248.114 is running more than one daemon pod
Sep 27 20:17:01.542: INFO: Number of nodes with available pods: 0
Sep 27 20:17:01.542: INFO: Node 10.177.248.114 is running more than one daemon pod
Sep 27 20:17:02.541: INFO: Number of nodes with available pods: 1
Sep 27 20:17:02.541: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Update the node label to green, and wait for daemons to be unscheduled
Sep 27 20:17:02.640: INFO: Number of nodes with available pods: 0
Sep 27 20:17:02.640: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate
Sep 27 20:17:02.696: INFO: Number of nodes with available pods: 0
Sep 27 20:17:02.696: INFO: Node 10.177.248.114 is running more than one daemon pod
Sep 27 20:17:03.712: INFO: Number of nodes with available pods: 0
Sep 27 20:17:03.712: INFO: Node 10.177.248.114 is running more than one daemon pod
Sep 27 20:17:04.710: INFO: Number of nodes with available pods: 0
Sep 27 20:17:04.710: INFO: Node 10.177.248.114 is running more than one daemon pod
Sep 27 20:17:05.711: INFO: Number of nodes with available pods: 0
Sep 27 20:17:05.711: INFO: Node 10.177.248.114 is running more than one daemon pod
Sep 27 20:17:06.725: INFO: Number of nodes with available pods: 0
Sep 27 20:17:06.725: INFO: Node 10.177.248.114 is running more than one daemon pod
Sep 27 20:17:07.713: INFO: Number of nodes with available pods: 0
Sep 27 20:17:07.714: INFO: Node 10.177.248.114 is running more than one daemon pod
Sep 27 20:17:08.711: INFO: Number of nodes with available pods: 0
Sep 27 20:17:08.711: INFO: Node 10.177.248.114 is running more than one daemon pod
Sep 27 20:17:09.711: INFO: Number of nodes with available pods: 0
Sep 27 20:17:09.711: INFO: Node 10.177.248.114 is running more than one daemon pod
Sep 27 20:17:10.712: INFO: Number of nodes with available pods: 0
Sep 27 20:17:10.713: INFO: Node 10.177.248.114 is running more than one daemon pod
Sep 27 20:17:11.711: INFO: Number of nodes with available pods: 0
Sep 27 20:17:11.711: INFO: Node 10.177.248.114 is running more than one daemon pod
Sep 27 20:17:12.713: INFO: Number of nodes with available pods: 0
Sep 27 20:17:12.713: INFO: Node 10.177.248.114 is running more than one daemon pod
Sep 27 20:17:13.711: INFO: Number of nodes with available pods: 0
Sep 27 20:17:13.712: INFO: Node 10.177.248.114 is running more than one daemon pod
Sep 27 20:17:14.715: INFO: Number of nodes with available pods: 0
Sep 27 20:17:14.716: INFO: Node 10.177.248.114 is running more than one daemon pod
Sep 27 20:17:15.713: INFO: Number of nodes with available pods: 0
Sep 27 20:17:15.713: INFO: Node 10.177.248.114 is running more than one daemon pod
Sep 27 20:17:16.711: INFO: Number of nodes with available pods: 1
Sep 27 20:17:16.711: INFO: Number of running nodes: 1, number of available pods: 1
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:100
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-6746, will wait for the garbage collector to delete the pods
Sep 27 20:17:16.842: INFO: Deleting DaemonSet.extensions daemon-set took: 31.243472ms
Sep 27 20:17:16.942: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.141183ms
Sep 27 20:17:32.157: INFO: Number of nodes with available pods: 0
Sep 27 20:17:32.157: INFO: Number of running nodes: 0, number of available pods: 0
Sep 27 20:17:32.174: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-6746/daemonsets","resourceVersion":"84879"},"items":null}

Sep 27 20:17:32.188: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-6746/pods","resourceVersion":"84879"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 20:17:32.322: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-6746" for this suite.

• [SLOW TEST:33.257 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop complex daemon [Conformance]","total":305,"completed":147,"skipped":2301,"failed":0}
SSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 20:17:32.372: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod pod-subpath-test-downwardapi-2hbp
STEP: Creating a pod to test atomic-volume-subpath
Sep 27 20:17:32.644: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-2hbp" in namespace "subpath-3693" to be "Succeeded or Failed"
Sep 27 20:17:32.665: INFO: Pod "pod-subpath-test-downwardapi-2hbp": Phase="Pending", Reason="", readiness=false. Elapsed: 20.652135ms
Sep 27 20:17:34.680: INFO: Pod "pod-subpath-test-downwardapi-2hbp": Phase="Pending", Reason="", readiness=false. Elapsed: 2.035855258s
Sep 27 20:17:36.703: INFO: Pod "pod-subpath-test-downwardapi-2hbp": Phase="Running", Reason="", readiness=true. Elapsed: 4.05958418s
Sep 27 20:17:38.722: INFO: Pod "pod-subpath-test-downwardapi-2hbp": Phase="Running", Reason="", readiness=true. Elapsed: 6.078548761s
Sep 27 20:17:40.738: INFO: Pod "pod-subpath-test-downwardapi-2hbp": Phase="Running", Reason="", readiness=true. Elapsed: 8.093723225s
Sep 27 20:17:42.752: INFO: Pod "pod-subpath-test-downwardapi-2hbp": Phase="Running", Reason="", readiness=true. Elapsed: 10.107983173s
Sep 27 20:17:44.767: INFO: Pod "pod-subpath-test-downwardapi-2hbp": Phase="Running", Reason="", readiness=true. Elapsed: 12.122990606s
Sep 27 20:17:46.782: INFO: Pod "pod-subpath-test-downwardapi-2hbp": Phase="Running", Reason="", readiness=true. Elapsed: 14.137701922s
Sep 27 20:17:48.797: INFO: Pod "pod-subpath-test-downwardapi-2hbp": Phase="Running", Reason="", readiness=true. Elapsed: 16.152899792s
Sep 27 20:17:50.811: INFO: Pod "pod-subpath-test-downwardapi-2hbp": Phase="Running", Reason="", readiness=true. Elapsed: 18.167250382s
Sep 27 20:17:52.826: INFO: Pod "pod-subpath-test-downwardapi-2hbp": Phase="Running", Reason="", readiness=true. Elapsed: 20.181637487s
Sep 27 20:17:54.838: INFO: Pod "pod-subpath-test-downwardapi-2hbp": Phase="Running", Reason="", readiness=true. Elapsed: 22.194480623s
Sep 27 20:17:56.853: INFO: Pod "pod-subpath-test-downwardapi-2hbp": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.208837453s
STEP: Saw pod success
Sep 27 20:17:56.853: INFO: Pod "pod-subpath-test-downwardapi-2hbp" satisfied condition "Succeeded or Failed"
Sep 27 20:17:56.868: INFO: Trying to get logs from node 10.177.248.117 pod pod-subpath-test-downwardapi-2hbp container test-container-subpath-downwardapi-2hbp: <nil>
STEP: delete the pod
Sep 27 20:17:56.946: INFO: Waiting for pod pod-subpath-test-downwardapi-2hbp to disappear
Sep 27 20:17:56.959: INFO: Pod pod-subpath-test-downwardapi-2hbp no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-2hbp
Sep 27 20:17:56.960: INFO: Deleting pod "pod-subpath-test-downwardapi-2hbp" in namespace "subpath-3693"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 20:17:56.972: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-3693" for this suite.

• [SLOW TEST:24.644 seconds]
[sig-storage] Subpath
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with downward pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with downward pod [LinuxOnly] [Conformance]","total":305,"completed":148,"skipped":2305,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 20:17:57.019: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod pod-subpath-test-secret-5jxk
STEP: Creating a pod to test atomic-volume-subpath
Sep 27 20:17:57.256: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-5jxk" in namespace "subpath-8168" to be "Succeeded or Failed"
Sep 27 20:17:57.268: INFO: Pod "pod-subpath-test-secret-5jxk": Phase="Pending", Reason="", readiness=false. Elapsed: 12.599092ms
Sep 27 20:17:59.283: INFO: Pod "pod-subpath-test-secret-5jxk": Phase="Running", Reason="", readiness=true. Elapsed: 2.027581324s
Sep 27 20:18:01.299: INFO: Pod "pod-subpath-test-secret-5jxk": Phase="Running", Reason="", readiness=true. Elapsed: 4.043238831s
Sep 27 20:18:03.316: INFO: Pod "pod-subpath-test-secret-5jxk": Phase="Running", Reason="", readiness=true. Elapsed: 6.060531357s
Sep 27 20:18:05.333: INFO: Pod "pod-subpath-test-secret-5jxk": Phase="Running", Reason="", readiness=true. Elapsed: 8.077463663s
Sep 27 20:18:07.347: INFO: Pod "pod-subpath-test-secret-5jxk": Phase="Running", Reason="", readiness=true. Elapsed: 10.091507123s
Sep 27 20:18:09.361: INFO: Pod "pod-subpath-test-secret-5jxk": Phase="Running", Reason="", readiness=true. Elapsed: 12.105296078s
Sep 27 20:18:11.376: INFO: Pod "pod-subpath-test-secret-5jxk": Phase="Running", Reason="", readiness=true. Elapsed: 14.120191407s
Sep 27 20:18:13.391: INFO: Pod "pod-subpath-test-secret-5jxk": Phase="Running", Reason="", readiness=true. Elapsed: 16.134933917s
Sep 27 20:18:15.404: INFO: Pod "pod-subpath-test-secret-5jxk": Phase="Running", Reason="", readiness=true. Elapsed: 18.148669542s
Sep 27 20:18:17.425: INFO: Pod "pod-subpath-test-secret-5jxk": Phase="Running", Reason="", readiness=true. Elapsed: 20.169409915s
Sep 27 20:18:19.444: INFO: Pod "pod-subpath-test-secret-5jxk": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.188845041s
STEP: Saw pod success
Sep 27 20:18:19.445: INFO: Pod "pod-subpath-test-secret-5jxk" satisfied condition "Succeeded or Failed"
Sep 27 20:18:19.458: INFO: Trying to get logs from node 10.177.248.117 pod pod-subpath-test-secret-5jxk container test-container-subpath-secret-5jxk: <nil>
STEP: delete the pod
Sep 27 20:18:19.547: INFO: Waiting for pod pod-subpath-test-secret-5jxk to disappear
Sep 27 20:18:19.562: INFO: Pod pod-subpath-test-secret-5jxk no longer exists
STEP: Deleting pod pod-subpath-test-secret-5jxk
Sep 27 20:18:19.562: INFO: Deleting pod "pod-subpath-test-secret-5jxk" in namespace "subpath-8168"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 20:18:19.578: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-8168" for this suite.

• [SLOW TEST:22.614 seconds]
[sig-storage] Subpath
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with secret pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with secret pod [LinuxOnly] [Conformance]","total":305,"completed":149,"skipped":2317,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 20:18:19.639: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods
STEP: Gathering metrics
W0927 20:18:59.984585      22 metrics_grabber.go:98] Can't find kube-scheduler pod. Grabbing metrics from kube-scheduler is disabled.
W0927 20:18:59.985067      22 metrics_grabber.go:102] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
W0927 20:18:59.985227      22 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
Sep 27 20:18:59.985: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

Sep 27 20:18:59.985: INFO: Deleting pod "simpletest.rc-2bplt" in namespace "gc-1563"
Sep 27 20:19:00.036: INFO: Deleting pod "simpletest.rc-5qc2t" in namespace "gc-1563"
Sep 27 20:19:00.076: INFO: Deleting pod "simpletest.rc-6fskc" in namespace "gc-1563"
Sep 27 20:19:00.114: INFO: Deleting pod "simpletest.rc-7nrd6" in namespace "gc-1563"
Sep 27 20:19:00.161: INFO: Deleting pod "simpletest.rc-9mccs" in namespace "gc-1563"
Sep 27 20:19:00.214: INFO: Deleting pod "simpletest.rc-f2qld" in namespace "gc-1563"
Sep 27 20:19:00.258: INFO: Deleting pod "simpletest.rc-klgkr" in namespace "gc-1563"
Sep 27 20:19:00.307: INFO: Deleting pod "simpletest.rc-pdlb4" in namespace "gc-1563"
Sep 27 20:19:00.347: INFO: Deleting pod "simpletest.rc-z29kd" in namespace "gc-1563"
Sep 27 20:19:00.388: INFO: Deleting pod "simpletest.rc-z4r87" in namespace "gc-1563"
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 20:19:00.445: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-1563" for this suite.

• [SLOW TEST:40.862 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan pods created by rc if delete options say so [Conformance]","total":305,"completed":150,"skipped":2357,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 20:19:00.503: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating secret with name secret-test-map-860c689c-5024-44a2-a195-56ba787457d6
STEP: Creating a pod to test consume secrets
Sep 27 20:19:00.834: INFO: Waiting up to 5m0s for pod "pod-secrets-7bae9ec7-c577-4791-9c25-e9f3fd125f30" in namespace "secrets-9593" to be "Succeeded or Failed"
Sep 27 20:19:00.852: INFO: Pod "pod-secrets-7bae9ec7-c577-4791-9c25-e9f3fd125f30": Phase="Pending", Reason="", readiness=false. Elapsed: 17.941441ms
Sep 27 20:19:02.892: INFO: Pod "pod-secrets-7bae9ec7-c577-4791-9c25-e9f3fd125f30": Phase="Pending", Reason="", readiness=false. Elapsed: 2.057747258s
Sep 27 20:19:04.907: INFO: Pod "pod-secrets-7bae9ec7-c577-4791-9c25-e9f3fd125f30": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.07303119s
STEP: Saw pod success
Sep 27 20:19:04.907: INFO: Pod "pod-secrets-7bae9ec7-c577-4791-9c25-e9f3fd125f30" satisfied condition "Succeeded or Failed"
Sep 27 20:19:04.921: INFO: Trying to get logs from node 10.177.248.117 pod pod-secrets-7bae9ec7-c577-4791-9c25-e9f3fd125f30 container secret-volume-test: <nil>
STEP: delete the pod
Sep 27 20:19:05.033: INFO: Waiting for pod pod-secrets-7bae9ec7-c577-4791-9c25-e9f3fd125f30 to disappear
Sep 27 20:19:05.049: INFO: Pod pod-secrets-7bae9ec7-c577-4791-9c25-e9f3fd125f30 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 20:19:05.049: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9593" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":151,"skipped":2373,"failed":0}
SSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl label 
  should update the label on a resource  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 20:19:05.103: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[BeforeEach] Kubectl label
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1333
STEP: creating the pod
Sep 27 20:19:05.252: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-679324930 create -f - --namespace=kubectl-9696'
Sep 27 20:19:05.864: INFO: stderr: ""
Sep 27 20:19:05.864: INFO: stdout: "pod/pause created\n"
Sep 27 20:19:05.864: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Sep 27 20:19:05.864: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-9696" to be "running and ready"
Sep 27 20:19:05.878: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 13.691615ms
Sep 27 20:19:07.897: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.033029179s
Sep 27 20:19:07.897: INFO: Pod "pause" satisfied condition "running and ready"
Sep 27 20:19:07.897: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: adding the label testing-label with value testing-label-value to a pod
Sep 27 20:19:07.897: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-679324930 label pods pause testing-label=testing-label-value --namespace=kubectl-9696'
Sep 27 20:19:08.137: INFO: stderr: ""
Sep 27 20:19:08.137: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value
Sep 27 20:19:08.137: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-679324930 get pod pause -L testing-label --namespace=kubectl-9696'
Sep 27 20:19:08.304: INFO: stderr: ""
Sep 27 20:19:08.304: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          3s    testing-label-value\n"
STEP: removing the label testing-label of a pod
Sep 27 20:19:08.305: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-679324930 label pods pause testing-label- --namespace=kubectl-9696'
Sep 27 20:19:08.505: INFO: stderr: ""
Sep 27 20:19:08.506: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod doesn't have the label testing-label
Sep 27 20:19:08.506: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-679324930 get pod pause -L testing-label --namespace=kubectl-9696'
Sep 27 20:19:08.646: INFO: stderr: ""
Sep 27 20:19:08.646: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          3s    \n"
[AfterEach] Kubectl label
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1340
STEP: using delete to clean up resources
Sep 27 20:19:08.646: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-679324930 delete --grace-period=0 --force -f - --namespace=kubectl-9696'
Sep 27 20:19:08.851: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep 27 20:19:08.851: INFO: stdout: "pod \"pause\" force deleted\n"
Sep 27 20:19:08.851: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-679324930 get rc,svc -l name=pause --no-headers --namespace=kubectl-9696'
Sep 27 20:19:09.029: INFO: stderr: "No resources found in kubectl-9696 namespace.\n"
Sep 27 20:19:09.029: INFO: stdout: ""
Sep 27 20:19:09.029: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-679324930 get pods -l name=pause --namespace=kubectl-9696 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Sep 27 20:19:09.172: INFO: stderr: ""
Sep 27 20:19:09.172: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 20:19:09.172: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9696" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl label should update the label on a resource  [Conformance]","total":305,"completed":152,"skipped":2380,"failed":0}
SSS
------------------------------
[sig-api-machinery] Secrets 
  should fail to create secret due to empty secret key [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 20:19:09.238: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create secret due to empty secret key [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating projection with secret that has name secret-emptykey-test-7ef7b5d7-0bd2-40a9-9bec-1e307eef6713
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 20:19:09.394: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3250" for this suite.
•{"msg":"PASSED [sig-api-machinery] Secrets should fail to create secret due to empty secret key [Conformance]","total":305,"completed":153,"skipped":2383,"failed":0}
SSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 20:19:09.443: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod liveness-9d0eac46-84a5-4b52-891e-a5de5fb04596 in namespace container-probe-332
Sep 27 20:19:11.685: INFO: Started pod liveness-9d0eac46-84a5-4b52-891e-a5de5fb04596 in namespace container-probe-332
STEP: checking the pod's current state and verifying that restartCount is present
Sep 27 20:19:11.701: INFO: Initial restart count of pod liveness-9d0eac46-84a5-4b52-891e-a5de5fb04596 is 0
Sep 27 20:19:27.868: INFO: Restart count of pod container-probe-332/liveness-9d0eac46-84a5-4b52-891e-a5de5fb04596 is now 1 (16.166749141s elapsed)
Sep 27 20:19:48.026: INFO: Restart count of pod container-probe-332/liveness-9d0eac46-84a5-4b52-891e-a5de5fb04596 is now 2 (36.325629566s elapsed)
Sep 27 20:20:08.215: INFO: Restart count of pod container-probe-332/liveness-9d0eac46-84a5-4b52-891e-a5de5fb04596 is now 3 (56.51449963s elapsed)
Sep 27 20:20:28.388: INFO: Restart count of pod container-probe-332/liveness-9d0eac46-84a5-4b52-891e-a5de5fb04596 is now 4 (1m16.687092234s elapsed)
Sep 27 20:21:26.900: INFO: Restart count of pod container-probe-332/liveness-9d0eac46-84a5-4b52-891e-a5de5fb04596 is now 5 (2m15.198710002s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 20:21:26.941: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-332" for this suite.

• [SLOW TEST:137.576 seconds]
[k8s.io] Probing container
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Probing container should have monotonically increasing restart count [NodeConformance] [Conformance]","total":305,"completed":154,"skipped":2392,"failed":0}
SSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 20:21:27.019: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod test-webserver-0c1d5a07-b82c-452c-b46e-34c761f3d88b in namespace container-probe-8456
Sep 27 20:21:29.253: INFO: Started pod test-webserver-0c1d5a07-b82c-452c-b46e-34c761f3d88b in namespace container-probe-8456
STEP: checking the pod's current state and verifying that restartCount is present
Sep 27 20:21:29.271: INFO: Initial restart count of pod test-webserver-0c1d5a07-b82c-452c-b46e-34c761f3d88b is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 20:25:31.249: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-8456" for this suite.

• [SLOW TEST:244.305 seconds]
[k8s.io] Probing container
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Probing container should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","total":305,"completed":155,"skipped":2398,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 20:25:31.335: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating service in namespace services-2499
Sep 27 20:25:33.605: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-679324930 exec --namespace=services-2499 kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
Sep 27 20:25:34.299: INFO: rc: 7
Sep 27 20:25:34.327: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Sep 27 20:25:34.341: INFO: Pod kube-proxy-mode-detector still exists
Sep 27 20:25:36.341: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Sep 27 20:25:36.373: INFO: Pod kube-proxy-mode-detector still exists
Sep 27 20:25:38.341: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Sep 27 20:25:38.355: INFO: Pod kube-proxy-mode-detector no longer exists
Sep 27 20:25:38.355: INFO: Couldn't detect KubeProxy mode - test failure may be expected: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-679324930 exec --namespace=services-2499 kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode:
Command stdout:

stderr:
+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode
command terminated with exit code 7

error:
exit status 7
STEP: creating service affinity-clusterip-timeout in namespace services-2499
STEP: creating replication controller affinity-clusterip-timeout in namespace services-2499
I0927 20:25:38.417381      22 runners.go:190] Created replication controller with name: affinity-clusterip-timeout, namespace: services-2499, replica count: 3
I0927 20:25:41.468215      22 runners.go:190] affinity-clusterip-timeout Pods: 3 out of 3 created, 1 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0927 20:25:44.468842      22 runners.go:190] affinity-clusterip-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Sep 27 20:25:44.511: INFO: Creating new exec pod
Sep 27 20:25:47.674: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-679324930 exec --namespace=services-2499 execpod-affinitygwjsg -- /bin/sh -x -c nc -zv -t -w 2 affinity-clusterip-timeout 80'
Sep 27 20:25:48.158: INFO: stderr: "+ nc -zv -t -w 2 affinity-clusterip-timeout 80\nConnection to affinity-clusterip-timeout 80 port [tcp/http] succeeded!\n"
Sep 27 20:25:48.158: INFO: stdout: ""
Sep 27 20:25:48.159: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-679324930 exec --namespace=services-2499 execpod-affinitygwjsg -- /bin/sh -x -c nc -zv -t -w 2 172.21.195.149 80'
Sep 27 20:25:48.580: INFO: stderr: "+ nc -zv -t -w 2 172.21.195.149 80\nConnection to 172.21.195.149 80 port [tcp/http] succeeded!\n"
Sep 27 20:25:48.580: INFO: stdout: ""
Sep 27 20:25:48.580: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-679324930 exec --namespace=services-2499 execpod-affinitygwjsg -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.21.195.149:80/ ; done'
Sep 27 20:25:49.306: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.195.149:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.195.149:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.195.149:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.195.149:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.195.149:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.195.149:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.195.149:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.195.149:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.195.149:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.195.149:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.195.149:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.195.149:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.195.149:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.195.149:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.195.149:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.195.149:80/\n"
Sep 27 20:25:49.306: INFO: stdout: "\naffinity-clusterip-timeout-nnmtg\naffinity-clusterip-timeout-nnmtg\naffinity-clusterip-timeout-nnmtg\naffinity-clusterip-timeout-nnmtg\naffinity-clusterip-timeout-nnmtg\naffinity-clusterip-timeout-nnmtg\naffinity-clusterip-timeout-nnmtg\naffinity-clusterip-timeout-nnmtg\naffinity-clusterip-timeout-nnmtg\naffinity-clusterip-timeout-nnmtg\naffinity-clusterip-timeout-nnmtg\naffinity-clusterip-timeout-nnmtg\naffinity-clusterip-timeout-nnmtg\naffinity-clusterip-timeout-nnmtg\naffinity-clusterip-timeout-nnmtg\naffinity-clusterip-timeout-nnmtg"
Sep 27 20:25:49.306: INFO: Received response from host: affinity-clusterip-timeout-nnmtg
Sep 27 20:25:49.306: INFO: Received response from host: affinity-clusterip-timeout-nnmtg
Sep 27 20:25:49.306: INFO: Received response from host: affinity-clusterip-timeout-nnmtg
Sep 27 20:25:49.306: INFO: Received response from host: affinity-clusterip-timeout-nnmtg
Sep 27 20:25:49.306: INFO: Received response from host: affinity-clusterip-timeout-nnmtg
Sep 27 20:25:49.306: INFO: Received response from host: affinity-clusterip-timeout-nnmtg
Sep 27 20:25:49.306: INFO: Received response from host: affinity-clusterip-timeout-nnmtg
Sep 27 20:25:49.306: INFO: Received response from host: affinity-clusterip-timeout-nnmtg
Sep 27 20:25:49.306: INFO: Received response from host: affinity-clusterip-timeout-nnmtg
Sep 27 20:25:49.306: INFO: Received response from host: affinity-clusterip-timeout-nnmtg
Sep 27 20:25:49.306: INFO: Received response from host: affinity-clusterip-timeout-nnmtg
Sep 27 20:25:49.306: INFO: Received response from host: affinity-clusterip-timeout-nnmtg
Sep 27 20:25:49.306: INFO: Received response from host: affinity-clusterip-timeout-nnmtg
Sep 27 20:25:49.306: INFO: Received response from host: affinity-clusterip-timeout-nnmtg
Sep 27 20:25:49.306: INFO: Received response from host: affinity-clusterip-timeout-nnmtg
Sep 27 20:25:49.306: INFO: Received response from host: affinity-clusterip-timeout-nnmtg
Sep 27 20:25:49.306: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-679324930 exec --namespace=services-2499 execpod-affinitygwjsg -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://172.21.195.149:80/'
Sep 27 20:25:49.786: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://172.21.195.149:80/\n"
Sep 27 20:25:49.786: INFO: stdout: "affinity-clusterip-timeout-nnmtg"
Sep 27 20:26:04.786: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-679324930 exec --namespace=services-2499 execpod-affinitygwjsg -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://172.21.195.149:80/'
Sep 27 20:26:05.253: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://172.21.195.149:80/\n"
Sep 27 20:26:05.253: INFO: stdout: "affinity-clusterip-timeout-n8hl7"
Sep 27 20:26:05.253: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-timeout in namespace services-2499, will wait for the garbage collector to delete the pods
Sep 27 20:26:05.386: INFO: Deleting ReplicationController affinity-clusterip-timeout took: 30.097721ms
Sep 27 20:26:05.590: INFO: Terminating ReplicationController affinity-clusterip-timeout pods took: 203.173286ms
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 20:26:22.162: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-2499" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

• [SLOW TEST:50.893 seconds]
[sig-network] Services
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]","total":305,"completed":156,"skipped":2482,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate configmap [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 20:26:22.233: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Sep 27 20:26:23.129: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Sep 27 20:26:25.169: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63768371183, loc:(*time.Location)(0x7702840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63768371183, loc:(*time.Location)(0x7702840)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63768371183, loc:(*time.Location)(0x7702840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63768371183, loc:(*time.Location)(0x7702840)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep 27 20:26:28.214: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate configmap [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Registering the mutating configmap webhook via the AdmissionRegistration API
STEP: create a configmap that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 20:26:28.410: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5838" for this suite.
STEP: Destroying namespace "webhook-5838-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:6.398 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate configmap [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate configmap [Conformance]","total":305,"completed":157,"skipped":2515,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 20:26:28.632: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-7442
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-7442
STEP: creating replication controller externalsvc in namespace services-7442
I0927 20:26:28.857774      22 runners.go:190] Created replication controller with name: externalsvc, namespace: services-7442, replica count: 2
I0927 20:26:31.908405      22 runners.go:190] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the ClusterIP service to type=ExternalName
Sep 27 20:26:31.986: INFO: Creating new exec pod
Sep 27 20:26:34.051: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-679324930 exec --namespace=services-7442 execpodz6vvh -- /bin/sh -x -c nslookup clusterip-service.services-7442.svc.cluster.local'
Sep 27 20:26:34.553: INFO: stderr: "+ nslookup clusterip-service.services-7442.svc.cluster.local\n"
Sep 27 20:26:34.553: INFO: stdout: "Server:\t\t172.21.0.10\nAddress:\t172.21.0.10#53\n\nclusterip-service.services-7442.svc.cluster.local\tcanonical name = externalsvc.services-7442.svc.cluster.local.\nName:\texternalsvc.services-7442.svc.cluster.local\nAddress: 172.21.198.101\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-7442, will wait for the garbage collector to delete the pods
Sep 27 20:26:34.641: INFO: Deleting ReplicationController externalsvc took: 23.75867ms
Sep 27 20:26:34.742: INFO: Terminating ReplicationController externalsvc pods took: 100.662287ms
Sep 27 20:26:43.316: INFO: Cleaning up the ClusterIP to ExternalName test service
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 20:26:43.372: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7442" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

• [SLOW TEST:14.788 seconds]
[sig-network] Services
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ClusterIP to ExternalName [Conformance]","total":305,"completed":158,"skipped":2536,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should find a service from listing all namespaces [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 20:26:43.421: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should find a service from listing all namespaces [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: fetching services
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 20:26:43.607: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-716" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786
•{"msg":"PASSED [sig-network] Services should find a service from listing all namespaces [Conformance]","total":305,"completed":159,"skipped":2558,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 20:26:43.657: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:90
Sep 27 20:26:43.802: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Sep 27 20:26:43.856: INFO: Waiting for terminating namespaces to be deleted...
Sep 27 20:26:43.885: INFO: 
Logging pods the apiserver thinks is on node 10.177.248.114 before test
Sep 27 20:26:43.928: INFO: calico-node-kxvx5 from calico-system started at 2021-09-27 17:50:17 +0000 UTC (1 container statuses recorded)
Sep 27 20:26:43.928: INFO: 	Container calico-node ready: true, restart count 0
Sep 27 20:26:43.928: INFO: calico-typha-7d789bfc7c-8ftcw from calico-system started at 2021-09-27 17:52:15 +0000 UTC (1 container statuses recorded)
Sep 27 20:26:43.928: INFO: 	Container calico-typha ready: true, restart count 0
Sep 27 20:26:43.928: INFO: test-k8s-e2e-pvg-master-verification from default started at 2021-09-27 17:52:52 +0000 UTC (1 container statuses recorded)
Sep 27 20:26:43.928: INFO: 	Container test-k8s-e2e-pvg-master-verification ready: true, restart count 0
Sep 27 20:26:43.928: INFO: ibm-cloud-provider-ip-169-61-239-242-59b4964694-ngqgl from ibm-system started at 2021-09-27 18:03:32 +0000 UTC (1 container statuses recorded)
Sep 27 20:26:43.928: INFO: 	Container ibm-cloud-provider-ip-169-61-239-242 ready: true, restart count 0
Sep 27 20:26:43.928: INFO: ibm-keepalived-watcher-8lj4c from kube-system started at 2021-09-27 17:50:13 +0000 UTC (1 container statuses recorded)
Sep 27 20:26:43.928: INFO: 	Container keepalived-watcher ready: true, restart count 0
Sep 27 20:26:43.928: INFO: ibm-master-proxy-static-10.177.248.114 from kube-system started at 2021-09-27 17:49:33 +0000 UTC (2 container statuses recorded)
Sep 27 20:26:43.928: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Sep 27 20:26:43.928: INFO: 	Container pause ready: true, restart count 0
Sep 27 20:26:43.928: INFO: ibmcloud-block-storage-driver-mxb6q from kube-system started at 2021-09-27 17:50:13 +0000 UTC (1 container statuses recorded)
Sep 27 20:26:43.928: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Sep 27 20:26:43.928: INFO: vpn-bc979587-kglsg from kube-system started at 2021-09-27 17:53:53 +0000 UTC (1 container statuses recorded)
Sep 27 20:26:43.928: INFO: 	Container vpn ready: true, restart count 0
Sep 27 20:26:43.928: INFO: tuned-dsf6r from openshift-cluster-node-tuning-operator started at 2021-09-27 17:51:40 +0000 UTC (1 container statuses recorded)
Sep 27 20:26:43.928: INFO: 	Container tuned ready: true, restart count 0
Sep 27 20:26:43.928: INFO: console-5b57dff4ff-2hr57 from openshift-console started at 2021-09-27 17:54:58 +0000 UTC (1 container statuses recorded)
Sep 27 20:26:43.928: INFO: 	Container console ready: true, restart count 0
Sep 27 20:26:43.928: INFO: dns-default-ktd8w from openshift-dns started at 2021-09-27 17:52:55 +0000 UTC (3 container statuses recorded)
Sep 27 20:26:43.928: INFO: 	Container dns ready: true, restart count 0
Sep 27 20:26:43.928: INFO: 	Container dns-node-resolver ready: true, restart count 0
Sep 27 20:26:43.928: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep 27 20:26:43.928: INFO: node-ca-bv4wj from openshift-image-registry started at 2021-09-27 17:53:17 +0000 UTC (1 container statuses recorded)
Sep 27 20:26:43.928: INFO: 	Container node-ca ready: true, restart count 0
Sep 27 20:26:43.928: INFO: router-default-768f4875db-n8g9c from openshift-ingress started at 2021-09-27 17:53:15 +0000 UTC (1 container statuses recorded)
Sep 27 20:26:43.928: INFO: 	Container router ready: true, restart count 0
Sep 27 20:26:43.928: INFO: openshift-kube-proxy-lxdz4 from openshift-kube-proxy started at 2021-09-27 17:50:13 +0000 UTC (1 container statuses recorded)
Sep 27 20:26:43.928: INFO: 	Container kube-proxy ready: true, restart count 0
Sep 27 20:26:43.928: INFO: certified-operators-qvnmv from openshift-marketplace started at 2021-09-27 17:54:49 +0000 UTC (1 container statuses recorded)
Sep 27 20:26:43.928: INFO: 	Container registry-server ready: true, restart count 0
Sep 27 20:26:43.928: INFO: community-operators-ghzk6 from openshift-marketplace started at 2021-09-27 20:26:37 +0000 UTC (1 container statuses recorded)
Sep 27 20:26:43.928: INFO: 	Container registry-server ready: false, restart count 0
Sep 27 20:26:43.928: INFO: community-operators-mc4rj from openshift-marketplace started at 2021-09-27 17:54:50 +0000 UTC (1 container statuses recorded)
Sep 27 20:26:43.928: INFO: 	Container registry-server ready: true, restart count 0
Sep 27 20:26:43.928: INFO: redhat-marketplace-tf75q from openshift-marketplace started at 2021-09-27 17:54:48 +0000 UTC (1 container statuses recorded)
Sep 27 20:26:43.928: INFO: 	Container registry-server ready: true, restart count 0
Sep 27 20:26:43.928: INFO: redhat-operators-2sd6b from openshift-marketplace started at 2021-09-27 17:54:49 +0000 UTC (1 container statuses recorded)
Sep 27 20:26:43.928: INFO: 	Container registry-server ready: true, restart count 0
Sep 27 20:26:43.928: INFO: alertmanager-main-0 from openshift-monitoring started at 2021-09-27 17:53:55 +0000 UTC (5 container statuses recorded)
Sep 27 20:26:43.928: INFO: 	Container alertmanager ready: true, restart count 0
Sep 27 20:26:43.928: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Sep 27 20:26:43.928: INFO: 	Container config-reloader ready: true, restart count 0
Sep 27 20:26:43.928: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep 27 20:26:43.928: INFO: 	Container prom-label-proxy ready: true, restart count 0
Sep 27 20:26:43.928: INFO: grafana-6f7d589b46-r88hj from openshift-monitoring started at 2021-09-27 17:53:53 +0000 UTC (2 container statuses recorded)
Sep 27 20:26:43.928: INFO: 	Container grafana ready: true, restart count 0
Sep 27 20:26:43.928: INFO: 	Container grafana-proxy ready: true, restart count 0
Sep 27 20:26:43.928: INFO: kube-state-metrics-5d4985f6b7-jx87q from openshift-monitoring started at 2021-09-27 17:51:49 +0000 UTC (3 container statuses recorded)
Sep 27 20:26:43.928: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Sep 27 20:26:43.928: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Sep 27 20:26:43.928: INFO: 	Container kube-state-metrics ready: true, restart count 0
Sep 27 20:26:43.928: INFO: node-exporter-r84r5 from openshift-monitoring started at 2021-09-27 17:51:50 +0000 UTC (2 container statuses recorded)
Sep 27 20:26:43.928: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep 27 20:26:43.928: INFO: 	Container node-exporter ready: true, restart count 0
Sep 27 20:26:43.928: INFO: openshift-state-metrics-58bfb7bff-qxd59 from openshift-monitoring started at 2021-09-27 17:51:50 +0000 UTC (3 container statuses recorded)
Sep 27 20:26:43.928: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Sep 27 20:26:43.928: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Sep 27 20:26:43.928: INFO: 	Container openshift-state-metrics ready: true, restart count 0
Sep 27 20:26:43.928: INFO: prometheus-adapter-66469d976-gl6wl from openshift-monitoring started at 2021-09-27 17:54:45 +0000 UTC (1 container statuses recorded)
Sep 27 20:26:43.928: INFO: 	Container prometheus-adapter ready: true, restart count 0
Sep 27 20:26:43.928: INFO: prometheus-k8s-0 from openshift-monitoring started at 2021-09-27 17:56:49 +0000 UTC (6 container statuses recorded)
Sep 27 20:26:43.928: INFO: 	Container config-reloader ready: true, restart count 0
Sep 27 20:26:43.928: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep 27 20:26:43.928: INFO: 	Container prom-label-proxy ready: true, restart count 0
Sep 27 20:26:43.928: INFO: 	Container prometheus ready: true, restart count 1
Sep 27 20:26:43.928: INFO: 	Container prometheus-proxy ready: true, restart count 0
Sep 27 20:26:43.928: INFO: 	Container thanos-sidecar ready: true, restart count 0
Sep 27 20:26:43.928: INFO: prometheus-operator-856d6cddd4-blncm from openshift-monitoring started at 2021-09-27 17:54:31 +0000 UTC (2 container statuses recorded)
Sep 27 20:26:43.928: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep 27 20:26:43.928: INFO: 	Container prometheus-operator ready: true, restart count 0
Sep 27 20:26:43.928: INFO: telemeter-client-68cfc9967c-sr2hj from openshift-monitoring started at 2021-09-27 17:51:59 +0000 UTC (3 container statuses recorded)
Sep 27 20:26:43.928: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep 27 20:26:43.928: INFO: 	Container reload ready: true, restart count 0
Sep 27 20:26:43.928: INFO: 	Container telemeter-client ready: true, restart count 0
Sep 27 20:26:43.928: INFO: thanos-querier-6b5789bbbd-qwc9q from openshift-monitoring started at 2021-09-27 17:53:50 +0000 UTC (5 container statuses recorded)
Sep 27 20:26:43.928: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep 27 20:26:43.928: INFO: 	Container kube-rbac-proxy-rules ready: true, restart count 0
Sep 27 20:26:43.928: INFO: 	Container oauth-proxy ready: true, restart count 0
Sep 27 20:26:43.928: INFO: 	Container prom-label-proxy ready: true, restart count 0
Sep 27 20:26:43.928: INFO: 	Container thanos-query ready: true, restart count 0
Sep 27 20:26:43.928: INFO: multus-admission-controller-pp4c5 from openshift-multus started at 2021-09-27 17:51:20 +0000 UTC (2 container statuses recorded)
Sep 27 20:26:43.928: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep 27 20:26:43.928: INFO: 	Container multus-admission-controller ready: true, restart count 0
Sep 27 20:26:43.928: INFO: multus-qwrqb from openshift-multus started at 2021-09-27 17:50:13 +0000 UTC (1 container statuses recorded)
Sep 27 20:26:43.928: INFO: 	Container kube-multus ready: true, restart count 0
Sep 27 20:26:43.928: INFO: network-metrics-daemon-zdwnk from openshift-multus started at 2021-09-27 17:50:13 +0000 UTC (2 container statuses recorded)
Sep 27 20:26:43.928: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep 27 20:26:43.928: INFO: 	Container network-metrics-daemon ready: true, restart count 0
Sep 27 20:26:43.928: INFO: packageserver-67b4ccc984-2lht2 from openshift-operator-lifecycle-manager started at 2021-09-27 17:53:13 +0000 UTC (1 container statuses recorded)
Sep 27 20:26:43.928: INFO: 	Container packageserver ready: true, restart count 0
Sep 27 20:26:43.928: INFO: service-ca-5655fcb96b-ntfqm from openshift-service-ca started at 2021-09-27 17:51:48 +0000 UTC (1 container statuses recorded)
Sep 27 20:26:43.928: INFO: 	Container service-ca-controller ready: true, restart count 0
Sep 27 20:26:43.928: INFO: sonobuoy from sonobuoy started at 2021-09-27 19:21:37 +0000 UTC (1 container statuses recorded)
Sep 27 20:26:43.928: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Sep 27 20:26:43.929: INFO: sonobuoy-e2e-job-f7e56101f16c459b from sonobuoy started at 2021-09-27 19:21:44 +0000 UTC (2 container statuses recorded)
Sep 27 20:26:43.929: INFO: 	Container e2e ready: true, restart count 0
Sep 27 20:26:43.929: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep 27 20:26:43.929: INFO: sonobuoy-systemd-logs-daemon-set-a625d2c9d0384434-sl5dj from sonobuoy started at 2021-09-27 19:21:44 +0000 UTC (2 container statuses recorded)
Sep 27 20:26:43.929: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep 27 20:26:43.929: INFO: 	Container systemd-logs ready: true, restart count 0
Sep 27 20:26:43.929: INFO: 
Logging pods the apiserver thinks is on node 10.177.248.117 before test
Sep 27 20:26:43.963: INFO: calico-node-d4rns from calico-system started at 2021-09-27 17:50:17 +0000 UTC (1 container statuses recorded)
Sep 27 20:26:43.963: INFO: 	Container calico-node ready: true, restart count 0
Sep 27 20:26:43.963: INFO: calico-typha-7d789bfc7c-79bn2 from calico-system started at 2021-09-27 17:52:15 +0000 UTC (1 container statuses recorded)
Sep 27 20:26:43.963: INFO: 	Container calico-typha ready: true, restart count 0
Sep 27 20:26:43.963: INFO: ibm-keepalived-watcher-9fxhn from kube-system started at 2021-09-27 17:48:31 +0000 UTC (1 container statuses recorded)
Sep 27 20:26:43.963: INFO: 	Container keepalived-watcher ready: true, restart count 0
Sep 27 20:26:43.963: INFO: ibm-master-proxy-static-10.177.248.117 from kube-system started at 2021-09-27 17:47:53 +0000 UTC (2 container statuses recorded)
Sep 27 20:26:43.963: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Sep 27 20:26:43.963: INFO: 	Container pause ready: true, restart count 0
Sep 27 20:26:43.963: INFO: ibm-storage-watcher-7cd75f8d4f-kdzj2 from kube-system started at 2021-09-27 19:57:16 +0000 UTC (1 container statuses recorded)
Sep 27 20:26:43.963: INFO: 	Container ibm-storage-watcher-container ready: true, restart count 0
Sep 27 20:26:43.963: INFO: ibmcloud-block-storage-driver-4ldjm from kube-system started at 2021-09-27 17:48:31 +0000 UTC (1 container statuses recorded)
Sep 27 20:26:43.963: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Sep 27 20:26:43.963: INFO: tuned-k9qlw from openshift-cluster-node-tuning-operator started at 2021-09-27 17:51:40 +0000 UTC (1 container statuses recorded)
Sep 27 20:26:43.963: INFO: 	Container tuned ready: true, restart count 0
Sep 27 20:26:43.963: INFO: console-operator-d6cf9dd7d-76zv4 from openshift-console-operator started at 2021-09-27 19:57:16 +0000 UTC (1 container statuses recorded)
Sep 27 20:26:43.963: INFO: 	Container console-operator ready: true, restart count 0
Sep 27 20:26:43.963: INFO: console-5b57dff4ff-d7kjb from openshift-console started at 2021-09-27 19:57:16 +0000 UTC (1 container statuses recorded)
Sep 27 20:26:43.963: INFO: 	Container console ready: true, restart count 0
Sep 27 20:26:43.963: INFO: dns-default-cb2zk from openshift-dns started at 2021-09-27 17:52:55 +0000 UTC (3 container statuses recorded)
Sep 27 20:26:43.963: INFO: 	Container dns ready: true, restart count 0
Sep 27 20:26:43.963: INFO: 	Container dns-node-resolver ready: true, restart count 0
Sep 27 20:26:43.963: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep 27 20:26:43.963: INFO: node-ca-82tbs from openshift-image-registry started at 2021-09-27 17:53:17 +0000 UTC (1 container statuses recorded)
Sep 27 20:26:43.963: INFO: 	Container node-ca ready: true, restart count 0
Sep 27 20:26:43.963: INFO: registry-pvc-permissions-vncj6 from openshift-image-registry started at 2021-09-27 17:55:54 +0000 UTC (1 container statuses recorded)
Sep 27 20:26:43.963: INFO: 	Container pvc-permissions ready: false, restart count 0
Sep 27 20:26:43.963: INFO: openshift-kube-proxy-2dc5b from openshift-kube-proxy started at 2021-09-27 17:49:34 +0000 UTC (1 container statuses recorded)
Sep 27 20:26:43.963: INFO: 	Container kube-proxy ready: true, restart count 0
Sep 27 20:26:43.964: INFO: migrator-6656c87b46-pnhl2 from openshift-kube-storage-version-migrator started at 2021-09-27 19:57:16 +0000 UTC (1 container statuses recorded)
Sep 27 20:26:43.964: INFO: 	Container migrator ready: true, restart count 0
Sep 27 20:26:43.964: INFO: alertmanager-main-2 from openshift-monitoring started at 2021-09-27 19:57:22 +0000 UTC (5 container statuses recorded)
Sep 27 20:26:43.964: INFO: 	Container alertmanager ready: true, restart count 0
Sep 27 20:26:43.964: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Sep 27 20:26:43.964: INFO: 	Container config-reloader ready: true, restart count 0
Sep 27 20:26:43.964: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep 27 20:26:43.964: INFO: 	Container prom-label-proxy ready: true, restart count 0
Sep 27 20:26:43.964: INFO: node-exporter-r6mdq from openshift-monitoring started at 2021-09-27 17:51:50 +0000 UTC (2 container statuses recorded)
Sep 27 20:26:43.964: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep 27 20:26:43.964: INFO: 	Container node-exporter ready: true, restart count 0
Sep 27 20:26:43.964: INFO: prometheus-k8s-1 from openshift-monitoring started at 2021-09-27 19:57:17 +0000 UTC (6 container statuses recorded)
Sep 27 20:26:43.964: INFO: 	Container config-reloader ready: true, restart count 0
Sep 27 20:26:43.964: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep 27 20:26:43.964: INFO: 	Container prom-label-proxy ready: true, restart count 0
Sep 27 20:26:43.964: INFO: 	Container prometheus ready: true, restart count 1
Sep 27 20:26:43.964: INFO: 	Container prometheus-proxy ready: true, restart count 0
Sep 27 20:26:43.964: INFO: 	Container thanos-sidecar ready: true, restart count 0
Sep 27 20:26:43.964: INFO: multus-admission-controller-rtg5z from openshift-multus started at 2021-09-27 19:57:16 +0000 UTC (2 container statuses recorded)
Sep 27 20:26:43.964: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep 27 20:26:43.964: INFO: 	Container multus-admission-controller ready: true, restart count 0
Sep 27 20:26:43.964: INFO: multus-vjt2q from openshift-multus started at 2021-09-27 17:49:24 +0000 UTC (1 container statuses recorded)
Sep 27 20:26:43.964: INFO: 	Container kube-multus ready: true, restart count 0
Sep 27 20:26:43.964: INFO: network-metrics-daemon-4t2kd from openshift-multus started at 2021-09-27 17:49:26 +0000 UTC (2 container statuses recorded)
Sep 27 20:26:43.964: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep 27 20:26:43.964: INFO: 	Container network-metrics-daemon ready: true, restart count 0
Sep 27 20:26:43.964: INFO: network-operator-dbdd595f7-dwsbv from openshift-network-operator started at 2021-09-27 19:57:16 +0000 UTC (1 container statuses recorded)
Sep 27 20:26:43.964: INFO: 	Container network-operator ready: true, restart count 0
Sep 27 20:26:43.964: INFO: execpodz6vvh from services-7442 started at 2021-09-27 20:26:32 +0000 UTC (1 container statuses recorded)
Sep 27 20:26:43.964: INFO: 	Container agnhost-pause ready: true, restart count 0
Sep 27 20:26:43.964: INFO: sonobuoy-systemd-logs-daemon-set-a625d2c9d0384434-z8xz7 from sonobuoy started at 2021-09-27 19:21:44 +0000 UTC (2 container statuses recorded)
Sep 27 20:26:43.964: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep 27 20:26:43.964: INFO: 	Container systemd-logs ready: true, restart count 0
Sep 27 20:26:43.964: INFO: 
Logging pods the apiserver thinks is on node 10.177.248.126 before test
Sep 27 20:26:44.004: INFO: calico-kube-controllers-5465c95dd-7hlj7 from calico-system started at 2021-09-27 17:50:29 +0000 UTC (1 container statuses recorded)
Sep 27 20:26:44.004: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Sep 27 20:26:44.004: INFO: calico-node-dhz9l from calico-system started at 2021-09-27 17:50:17 +0000 UTC (1 container statuses recorded)
Sep 27 20:26:44.004: INFO: 	Container calico-node ready: true, restart count 0
Sep 27 20:26:44.004: INFO: calico-typha-7d789bfc7c-7669x from calico-system started at 2021-09-27 17:50:17 +0000 UTC (1 container statuses recorded)
Sep 27 20:26:44.004: INFO: 	Container calico-typha ready: true, restart count 0
Sep 27 20:26:44.004: INFO: ibm-cloud-provider-ip-169-61-239-242-59b4964694-mc2b8 from ibm-system started at 2021-09-27 19:52:27 +0000 UTC (1 container statuses recorded)
Sep 27 20:26:44.004: INFO: 	Container ibm-cloud-provider-ip-169-61-239-242 ready: true, restart count 0
Sep 27 20:26:44.004: INFO: ibm-file-plugin-fcdf5f569-x8hfq from kube-system started at 2021-09-27 17:50:29 +0000 UTC (1 container statuses recorded)
Sep 27 20:26:44.004: INFO: 	Container ibm-file-plugin-container ready: true, restart count 0
Sep 27 20:26:44.004: INFO: ibm-keepalived-watcher-rgwfm from kube-system started at 2021-09-27 17:49:04 +0000 UTC (1 container statuses recorded)
Sep 27 20:26:44.004: INFO: 	Container keepalived-watcher ready: true, restart count 0
Sep 27 20:26:44.004: INFO: ibm-master-proxy-static-10.177.248.126 from kube-system started at 2021-09-27 17:48:20 +0000 UTC (2 container statuses recorded)
Sep 27 20:26:44.004: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Sep 27 20:26:44.004: INFO: 	Container pause ready: true, restart count 0
Sep 27 20:26:44.004: INFO: ibmcloud-block-storage-driver-kbd9g from kube-system started at 2021-09-27 17:49:04 +0000 UTC (1 container statuses recorded)
Sep 27 20:26:44.004: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Sep 27 20:26:44.004: INFO: ibmcloud-block-storage-plugin-74d6877898-zqtnb from kube-system started at 2021-09-27 17:50:29 +0000 UTC (1 container statuses recorded)
Sep 27 20:26:44.004: INFO: 	Container ibmcloud-block-storage-plugin-container ready: true, restart count 0
Sep 27 20:26:44.004: INFO: cluster-node-tuning-operator-67b4b4fbf5-zslfs from openshift-cluster-node-tuning-operator started at 2021-09-27 17:50:29 +0000 UTC (1 container statuses recorded)
Sep 27 20:26:44.004: INFO: 	Container cluster-node-tuning-operator ready: true, restart count 0
Sep 27 20:26:44.004: INFO: tuned-qc5dt from openshift-cluster-node-tuning-operator started at 2021-09-27 17:51:41 +0000 UTC (1 container statuses recorded)
Sep 27 20:26:44.004: INFO: 	Container tuned ready: true, restart count 0
Sep 27 20:26:44.004: INFO: cluster-samples-operator-97cc95ff8-nmc7q from openshift-cluster-samples-operator started at 2021-09-27 17:50:29 +0000 UTC (2 container statuses recorded)
Sep 27 20:26:44.004: INFO: 	Container cluster-samples-operator ready: true, restart count 0
Sep 27 20:26:44.004: INFO: 	Container cluster-samples-operator-watch ready: true, restart count 0
Sep 27 20:26:44.004: INFO: cluster-storage-operator-59698ddbdf-w86gw from openshift-cluster-storage-operator started at 2021-09-27 17:50:29 +0000 UTC (1 container statuses recorded)
Sep 27 20:26:44.004: INFO: 	Container cluster-storage-operator ready: true, restart count 0
Sep 27 20:26:44.004: INFO: downloads-c785794b6-q92tc from openshift-console started at 2021-09-27 17:50:29 +0000 UTC (1 container statuses recorded)
Sep 27 20:26:44.004: INFO: 	Container download-server ready: true, restart count 0
Sep 27 20:26:44.004: INFO: downloads-c785794b6-w28kb from openshift-console started at 2021-09-27 17:50:29 +0000 UTC (1 container statuses recorded)
Sep 27 20:26:44.004: INFO: 	Container download-server ready: true, restart count 0
Sep 27 20:26:44.004: INFO: dns-operator-7489bbc67f-9ztr4 from openshift-dns-operator started at 2021-09-27 17:50:29 +0000 UTC (2 container statuses recorded)
Sep 27 20:26:44.004: INFO: 	Container dns-operator ready: true, restart count 0
Sep 27 20:26:44.004: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep 27 20:26:44.004: INFO: dns-default-msjgz from openshift-dns started at 2021-09-27 17:52:55 +0000 UTC (3 container statuses recorded)
Sep 27 20:26:44.004: INFO: 	Container dns ready: true, restart count 0
Sep 27 20:26:44.004: INFO: 	Container dns-node-resolver ready: true, restart count 0
Sep 27 20:26:44.004: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep 27 20:26:44.004: INFO: cluster-image-registry-operator-675674456f-k2fc9 from openshift-image-registry started at 2021-09-27 17:50:29 +0000 UTC (1 container statuses recorded)
Sep 27 20:26:44.004: INFO: 	Container cluster-image-registry-operator ready: true, restart count 0
Sep 27 20:26:44.004: INFO: image-registry-ffbbf6ddb-xl9b9 from openshift-image-registry started at 2021-09-27 17:55:54 +0000 UTC (1 container statuses recorded)
Sep 27 20:26:44.004: INFO: 	Container registry ready: true, restart count 0
Sep 27 20:26:44.004: INFO: node-ca-2rrgl from openshift-image-registry started at 2021-09-27 17:53:17 +0000 UTC (1 container statuses recorded)
Sep 27 20:26:44.004: INFO: 	Container node-ca ready: true, restart count 0
Sep 27 20:26:44.004: INFO: ingress-operator-8469759c95-4mm9m from openshift-ingress-operator started at 2021-09-27 17:50:29 +0000 UTC (2 container statuses recorded)
Sep 27 20:26:44.004: INFO: 	Container ingress-operator ready: true, restart count 0
Sep 27 20:26:44.005: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep 27 20:26:44.005: INFO: router-default-768f4875db-8qx4w from openshift-ingress started at 2021-09-27 17:53:15 +0000 UTC (1 container statuses recorded)
Sep 27 20:26:44.005: INFO: 	Container router ready: true, restart count 0
Sep 27 20:26:44.005: INFO: openshift-kube-proxy-7gx28 from openshift-kube-proxy started at 2021-09-27 17:49:34 +0000 UTC (1 container statuses recorded)
Sep 27 20:26:44.005: INFO: 	Container kube-proxy ready: true, restart count 0
Sep 27 20:26:44.005: INFO: kube-storage-version-migrator-operator-55c7c8f996-xr7ds from openshift-kube-storage-version-migrator-operator started at 2021-09-27 17:50:29 +0000 UTC (1 container statuses recorded)
Sep 27 20:26:44.005: INFO: 	Container kube-storage-version-migrator-operator ready: true, restart count 1
Sep 27 20:26:44.005: INFO: marketplace-operator-7cd49bbf56-nggpg from openshift-marketplace started at 2021-09-27 17:50:29 +0000 UTC (1 container statuses recorded)
Sep 27 20:26:44.005: INFO: 	Container marketplace-operator ready: true, restart count 0
Sep 27 20:26:44.005: INFO: alertmanager-main-1 from openshift-monitoring started at 2021-09-27 17:53:55 +0000 UTC (5 container statuses recorded)
Sep 27 20:26:44.005: INFO: 	Container alertmanager ready: true, restart count 0
Sep 27 20:26:44.005: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Sep 27 20:26:44.005: INFO: 	Container config-reloader ready: true, restart count 0
Sep 27 20:26:44.005: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep 27 20:26:44.005: INFO: 	Container prom-label-proxy ready: true, restart count 0
Sep 27 20:26:44.005: INFO: cluster-monitoring-operator-7656b489c8-z2wmx from openshift-monitoring started at 2021-09-27 17:50:29 +0000 UTC (2 container statuses recorded)
Sep 27 20:26:44.005: INFO: 	Container cluster-monitoring-operator ready: true, restart count 0
Sep 27 20:26:44.005: INFO: 	Container kube-rbac-proxy ready: true, restart count 3
Sep 27 20:26:44.005: INFO: node-exporter-42nr4 from openshift-monitoring started at 2021-09-27 17:51:50 +0000 UTC (2 container statuses recorded)
Sep 27 20:26:44.005: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep 27 20:26:44.005: INFO: 	Container node-exporter ready: true, restart count 0
Sep 27 20:26:44.005: INFO: prometheus-adapter-66469d976-t6rf2 from openshift-monitoring started at 2021-09-27 17:54:46 +0000 UTC (1 container statuses recorded)
Sep 27 20:26:44.005: INFO: 	Container prometheus-adapter ready: true, restart count 0
Sep 27 20:26:44.005: INFO: thanos-querier-6b5789bbbd-l8dmh from openshift-monitoring started at 2021-09-27 17:53:51 +0000 UTC (5 container statuses recorded)
Sep 27 20:26:44.005: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep 27 20:26:44.005: INFO: 	Container kube-rbac-proxy-rules ready: true, restart count 0
Sep 27 20:26:44.005: INFO: 	Container oauth-proxy ready: true, restart count 0
Sep 27 20:26:44.005: INFO: 	Container prom-label-proxy ready: true, restart count 0
Sep 27 20:26:44.005: INFO: 	Container thanos-query ready: true, restart count 0
Sep 27 20:26:44.005: INFO: multus-admission-controller-fgtsq from openshift-multus started at 2021-09-27 17:50:29 +0000 UTC (2 container statuses recorded)
Sep 27 20:26:44.005: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep 27 20:26:44.005: INFO: 	Container multus-admission-controller ready: true, restart count 0
Sep 27 20:26:44.005: INFO: multus-fzpdl from openshift-multus started at 2021-09-27 17:49:24 +0000 UTC (1 container statuses recorded)
Sep 27 20:26:44.005: INFO: 	Container kube-multus ready: true, restart count 0
Sep 27 20:26:44.005: INFO: network-metrics-daemon-5wdz5 from openshift-multus started at 2021-09-27 17:49:26 +0000 UTC (2 container statuses recorded)
Sep 27 20:26:44.005: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep 27 20:26:44.005: INFO: 	Container network-metrics-daemon ready: true, restart count 0
Sep 27 20:26:44.005: INFO: catalog-operator-69c4599997-btc8l from openshift-operator-lifecycle-manager started at 2021-09-27 17:50:29 +0000 UTC (1 container statuses recorded)
Sep 27 20:26:44.005: INFO: 	Container catalog-operator ready: true, restart count 0
Sep 27 20:26:44.005: INFO: olm-operator-db7cd6974-2gxjx from openshift-operator-lifecycle-manager started at 2021-09-27 17:50:29 +0000 UTC (1 container statuses recorded)
Sep 27 20:26:44.005: INFO: 	Container olm-operator ready: true, restart count 0
Sep 27 20:26:44.005: INFO: packageserver-67b4ccc984-w6sdn from openshift-operator-lifecycle-manager started at 2021-09-27 17:53:13 +0000 UTC (1 container statuses recorded)
Sep 27 20:26:44.005: INFO: 	Container packageserver ready: true, restart count 0
Sep 27 20:26:44.005: INFO: metrics-666cbf9545-t95w7 from openshift-roks-metrics started at 2021-09-27 17:50:29 +0000 UTC (1 container statuses recorded)
Sep 27 20:26:44.005: INFO: 	Container metrics ready: true, restart count 3
Sep 27 20:26:44.005: INFO: push-gateway-6f6b5cb7f7-k6lpz from openshift-roks-metrics started at 2021-09-27 19:52:28 +0000 UTC (1 container statuses recorded)
Sep 27 20:26:44.005: INFO: 	Container push-gateway ready: false, restart count 0
Sep 27 20:26:44.005: INFO: service-ca-operator-6ccc7cff9-l9wfj from openshift-service-ca-operator started at 2021-09-27 17:50:29 +0000 UTC (1 container statuses recorded)
Sep 27 20:26:44.005: INFO: 	Container service-ca-operator ready: true, restart count 1
Sep 27 20:26:44.005: INFO: sonobuoy-systemd-logs-daemon-set-a625d2c9d0384434-6w8vw from sonobuoy started at 2021-09-27 19:21:44 +0000 UTC (2 container statuses recorded)
Sep 27 20:26:44.005: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep 27 20:26:44.005: INFO: 	Container systemd-logs ready: true, restart count 0
Sep 27 20:26:44.005: INFO: tigera-operator-db8ddcc79-mf5vc from tigera-operator started at 2021-09-27 19:52:27 +0000 UTC (1 container statuses recorded)
Sep 27 20:26:44.005: INFO: 	Container tigera-operator ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Trying to schedule Pod with nonempty NodeSelector.
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.16a8c7ab7600d939], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 node(s) didn't match node selector.]
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 20:26:45.150: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-5996" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81
•{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if not matching  [Conformance]","total":305,"completed":160,"skipped":2573,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 20:26:45.197: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod liveness-0008c7e9-b138-421e-8129-2b72b2b2e720 in namespace container-probe-3261
Sep 27 20:26:51.436: INFO: Started pod liveness-0008c7e9-b138-421e-8129-2b72b2b2e720 in namespace container-probe-3261
STEP: checking the pod's current state and verifying that restartCount is present
Sep 27 20:26:51.450: INFO: Initial restart count of pod liveness-0008c7e9-b138-421e-8129-2b72b2b2e720 is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 20:30:53.479: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-3261" for this suite.

• [SLOW TEST:248.342 seconds]
[k8s.io] Probing container
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Probing container should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]","total":305,"completed":161,"skipped":2587,"failed":0}
[sig-api-machinery] server version 
  should find the server version [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] server version
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 20:30:53.539: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename server-version
STEP: Waiting for a default service account to be provisioned in namespace
[It] should find the server version [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Request ServerVersion
STEP: Confirm major version
Sep 27 20:30:53.689: INFO: Major version: 1
STEP: Confirm minor version
Sep 27 20:30:53.689: INFO: cleanMinorVersion: 19
Sep 27 20:30:53.689: INFO: Minor version: 19
[AfterEach] [sig-api-machinery] server version
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 20:30:53.690: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "server-version-7767" for this suite.
•{"msg":"PASSED [sig-api-machinery] server version should find the server version [Conformance]","total":305,"completed":162,"skipped":2587,"failed":0}
SSS
------------------------------
[sig-network] Services 
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 20:30:53.737: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating service multi-endpoint-test in namespace services-7022
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-7022 to expose endpoints map[]
Sep 27 20:30:54.004: INFO: successfully validated that service multi-endpoint-test in namespace services-7022 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-7022
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-7022 to expose endpoints map[pod1:[100]]
Sep 27 20:30:57.128: INFO: successfully validated that service multi-endpoint-test in namespace services-7022 exposes endpoints map[pod1:[100]]
STEP: Creating pod pod2 in namespace services-7022
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-7022 to expose endpoints map[pod1:[100] pod2:[101]]
Sep 27 20:31:00.248: INFO: successfully validated that service multi-endpoint-test in namespace services-7022 exposes endpoints map[pod1:[100] pod2:[101]]
STEP: Deleting pod pod1 in namespace services-7022
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-7022 to expose endpoints map[pod2:[101]]
Sep 27 20:31:00.348: INFO: successfully validated that service multi-endpoint-test in namespace services-7022 exposes endpoints map[pod2:[101]]
STEP: Deleting pod pod2 in namespace services-7022
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-7022 to expose endpoints map[]
Sep 27 20:31:00.422: INFO: successfully validated that service multi-endpoint-test in namespace services-7022 exposes endpoints map[]
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 20:31:00.502: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7022" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

• [SLOW TEST:6.815 seconds]
[sig-network] Services
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should serve multiport endpoints from pods  [Conformance]","total":305,"completed":163,"skipped":2590,"failed":0}
SSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 20:31:00.553: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating secret with name secret-test-27932c29-ee83-46c2-aec5-f6ec182eb048
STEP: Creating a pod to test consume secrets
Sep 27 20:31:00.767: INFO: Waiting up to 5m0s for pod "pod-secrets-d43fc1c1-1dda-4da7-b61c-727d4305b86e" in namespace "secrets-7813" to be "Succeeded or Failed"
Sep 27 20:31:00.780: INFO: Pod "pod-secrets-d43fc1c1-1dda-4da7-b61c-727d4305b86e": Phase="Pending", Reason="", readiness=false. Elapsed: 12.923104ms
Sep 27 20:31:02.798: INFO: Pod "pod-secrets-d43fc1c1-1dda-4da7-b61c-727d4305b86e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.030927131s
STEP: Saw pod success
Sep 27 20:31:02.798: INFO: Pod "pod-secrets-d43fc1c1-1dda-4da7-b61c-727d4305b86e" satisfied condition "Succeeded or Failed"
Sep 27 20:31:02.814: INFO: Trying to get logs from node 10.177.248.117 pod pod-secrets-d43fc1c1-1dda-4da7-b61c-727d4305b86e container secret-volume-test: <nil>
STEP: delete the pod
Sep 27 20:31:02.957: INFO: Waiting for pod pod-secrets-d43fc1c1-1dda-4da7-b61c-727d4305b86e to disappear
Sep 27 20:31:02.970: INFO: Pod pod-secrets-d43fc1c1-1dda-4da7-b61c-727d4305b86e no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 20:31:02.970: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-7813" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":164,"skipped":2595,"failed":0}
SSSSSSSSS
------------------------------
[sig-node] PodTemplates 
  should delete a collection of pod templates [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-node] PodTemplates
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 20:31:03.017: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename podtemplate
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a collection of pod templates [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Create set of pod templates
Sep 27 20:31:03.183: INFO: created test-podtemplate-1
Sep 27 20:31:03.204: INFO: created test-podtemplate-2
Sep 27 20:31:03.230: INFO: created test-podtemplate-3
STEP: get a list of pod templates with a label in the current namespace
STEP: delete collection of pod templates
Sep 27 20:31:03.245: INFO: requesting DeleteCollection of pod templates
STEP: check that the list of pod templates matches the requested quantity
Sep 27 20:31:03.356: INFO: requesting list of pod templates to confirm quantity
[AfterEach] [sig-node] PodTemplates
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 20:31:03.373: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-455" for this suite.
•{"msg":"PASSED [sig-node] PodTemplates should delete a collection of pod templates [Conformance]","total":305,"completed":165,"skipped":2604,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 20:31:03.423: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:90
Sep 27 20:31:03.588: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Sep 27 20:31:03.637: INFO: Waiting for terminating namespaces to be deleted...
Sep 27 20:31:03.668: INFO: 
Logging pods the apiserver thinks is on node 10.177.248.114 before test
Sep 27 20:31:03.719: INFO: calico-node-kxvx5 from calico-system started at 2021-09-27 17:50:17 +0000 UTC (1 container statuses recorded)
Sep 27 20:31:03.719: INFO: 	Container calico-node ready: true, restart count 0
Sep 27 20:31:03.719: INFO: calico-typha-7d789bfc7c-8ftcw from calico-system started at 2021-09-27 17:52:15 +0000 UTC (1 container statuses recorded)
Sep 27 20:31:03.719: INFO: 	Container calico-typha ready: true, restart count 0
Sep 27 20:31:03.719: INFO: test-k8s-e2e-pvg-master-verification from default started at 2021-09-27 17:52:52 +0000 UTC (1 container statuses recorded)
Sep 27 20:31:03.719: INFO: 	Container test-k8s-e2e-pvg-master-verification ready: true, restart count 0
Sep 27 20:31:03.719: INFO: ibm-cloud-provider-ip-169-61-239-242-59b4964694-ngqgl from ibm-system started at 2021-09-27 18:03:32 +0000 UTC (1 container statuses recorded)
Sep 27 20:31:03.719: INFO: 	Container ibm-cloud-provider-ip-169-61-239-242 ready: true, restart count 0
Sep 27 20:31:03.719: INFO: ibm-keepalived-watcher-8lj4c from kube-system started at 2021-09-27 17:50:13 +0000 UTC (1 container statuses recorded)
Sep 27 20:31:03.719: INFO: 	Container keepalived-watcher ready: true, restart count 0
Sep 27 20:31:03.719: INFO: ibm-master-proxy-static-10.177.248.114 from kube-system started at 2021-09-27 17:49:33 +0000 UTC (2 container statuses recorded)
Sep 27 20:31:03.719: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Sep 27 20:31:03.719: INFO: 	Container pause ready: true, restart count 0
Sep 27 20:31:03.719: INFO: ibmcloud-block-storage-driver-mxb6q from kube-system started at 2021-09-27 17:50:13 +0000 UTC (1 container statuses recorded)
Sep 27 20:31:03.719: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Sep 27 20:31:03.719: INFO: vpn-bc979587-kglsg from kube-system started at 2021-09-27 17:53:53 +0000 UTC (1 container statuses recorded)
Sep 27 20:31:03.719: INFO: 	Container vpn ready: true, restart count 0
Sep 27 20:31:03.719: INFO: tuned-dsf6r from openshift-cluster-node-tuning-operator started at 2021-09-27 17:51:40 +0000 UTC (1 container statuses recorded)
Sep 27 20:31:03.719: INFO: 	Container tuned ready: true, restart count 0
Sep 27 20:31:03.719: INFO: console-5b57dff4ff-2hr57 from openshift-console started at 2021-09-27 17:54:58 +0000 UTC (1 container statuses recorded)
Sep 27 20:31:03.719: INFO: 	Container console ready: true, restart count 0
Sep 27 20:31:03.719: INFO: dns-default-ktd8w from openshift-dns started at 2021-09-27 17:52:55 +0000 UTC (3 container statuses recorded)
Sep 27 20:31:03.719: INFO: 	Container dns ready: true, restart count 0
Sep 27 20:31:03.719: INFO: 	Container dns-node-resolver ready: true, restart count 0
Sep 27 20:31:03.719: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep 27 20:31:03.719: INFO: node-ca-bv4wj from openshift-image-registry started at 2021-09-27 17:53:17 +0000 UTC (1 container statuses recorded)
Sep 27 20:31:03.719: INFO: 	Container node-ca ready: true, restart count 0
Sep 27 20:31:03.719: INFO: router-default-768f4875db-n8g9c from openshift-ingress started at 2021-09-27 17:53:15 +0000 UTC (1 container statuses recorded)
Sep 27 20:31:03.719: INFO: 	Container router ready: true, restart count 0
Sep 27 20:31:03.719: INFO: openshift-kube-proxy-lxdz4 from openshift-kube-proxy started at 2021-09-27 17:50:13 +0000 UTC (1 container statuses recorded)
Sep 27 20:31:03.719: INFO: 	Container kube-proxy ready: true, restart count 0
Sep 27 20:31:03.719: INFO: certified-operators-qvnmv from openshift-marketplace started at 2021-09-27 17:54:49 +0000 UTC (1 container statuses recorded)
Sep 27 20:31:03.719: INFO: 	Container registry-server ready: true, restart count 0
Sep 27 20:31:03.719: INFO: community-operators-mc4rj from openshift-marketplace started at 2021-09-27 17:54:50 +0000 UTC (1 container statuses recorded)
Sep 27 20:31:03.719: INFO: 	Container registry-server ready: true, restart count 0
Sep 27 20:31:03.719: INFO: redhat-marketplace-tf75q from openshift-marketplace started at 2021-09-27 17:54:48 +0000 UTC (1 container statuses recorded)
Sep 27 20:31:03.719: INFO: 	Container registry-server ready: true, restart count 0
Sep 27 20:31:03.719: INFO: redhat-operators-2sd6b from openshift-marketplace started at 2021-09-27 17:54:49 +0000 UTC (1 container statuses recorded)
Sep 27 20:31:03.719: INFO: 	Container registry-server ready: true, restart count 0
Sep 27 20:31:03.719: INFO: alertmanager-main-0 from openshift-monitoring started at 2021-09-27 17:53:55 +0000 UTC (5 container statuses recorded)
Sep 27 20:31:03.719: INFO: 	Container alertmanager ready: true, restart count 0
Sep 27 20:31:03.719: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Sep 27 20:31:03.719: INFO: 	Container config-reloader ready: true, restart count 0
Sep 27 20:31:03.719: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep 27 20:31:03.719: INFO: 	Container prom-label-proxy ready: true, restart count 0
Sep 27 20:31:03.719: INFO: grafana-6f7d589b46-r88hj from openshift-monitoring started at 2021-09-27 17:53:53 +0000 UTC (2 container statuses recorded)
Sep 27 20:31:03.719: INFO: 	Container grafana ready: true, restart count 0
Sep 27 20:31:03.719: INFO: 	Container grafana-proxy ready: true, restart count 0
Sep 27 20:31:03.719: INFO: kube-state-metrics-5d4985f6b7-jx87q from openshift-monitoring started at 2021-09-27 17:51:49 +0000 UTC (3 container statuses recorded)
Sep 27 20:31:03.719: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Sep 27 20:31:03.719: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Sep 27 20:31:03.719: INFO: 	Container kube-state-metrics ready: true, restart count 0
Sep 27 20:31:03.719: INFO: node-exporter-r84r5 from openshift-monitoring started at 2021-09-27 17:51:50 +0000 UTC (2 container statuses recorded)
Sep 27 20:31:03.719: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep 27 20:31:03.719: INFO: 	Container node-exporter ready: true, restart count 0
Sep 27 20:31:03.719: INFO: openshift-state-metrics-58bfb7bff-qxd59 from openshift-monitoring started at 2021-09-27 17:51:50 +0000 UTC (3 container statuses recorded)
Sep 27 20:31:03.719: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Sep 27 20:31:03.719: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Sep 27 20:31:03.719: INFO: 	Container openshift-state-metrics ready: true, restart count 0
Sep 27 20:31:03.719: INFO: prometheus-adapter-66469d976-gl6wl from openshift-monitoring started at 2021-09-27 17:54:45 +0000 UTC (1 container statuses recorded)
Sep 27 20:31:03.719: INFO: 	Container prometheus-adapter ready: true, restart count 0
Sep 27 20:31:03.720: INFO: prometheus-k8s-0 from openshift-monitoring started at 2021-09-27 17:56:49 +0000 UTC (6 container statuses recorded)
Sep 27 20:31:03.720: INFO: 	Container config-reloader ready: true, restart count 0
Sep 27 20:31:03.720: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep 27 20:31:03.720: INFO: 	Container prom-label-proxy ready: true, restart count 0
Sep 27 20:31:03.720: INFO: 	Container prometheus ready: true, restart count 1
Sep 27 20:31:03.720: INFO: 	Container prometheus-proxy ready: true, restart count 0
Sep 27 20:31:03.720: INFO: 	Container thanos-sidecar ready: true, restart count 0
Sep 27 20:31:03.720: INFO: prometheus-operator-856d6cddd4-blncm from openshift-monitoring started at 2021-09-27 17:54:31 +0000 UTC (2 container statuses recorded)
Sep 27 20:31:03.720: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep 27 20:31:03.720: INFO: 	Container prometheus-operator ready: true, restart count 0
Sep 27 20:31:03.720: INFO: telemeter-client-68cfc9967c-sr2hj from openshift-monitoring started at 2021-09-27 17:51:59 +0000 UTC (3 container statuses recorded)
Sep 27 20:31:03.720: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep 27 20:31:03.720: INFO: 	Container reload ready: true, restart count 0
Sep 27 20:31:03.720: INFO: 	Container telemeter-client ready: true, restart count 0
Sep 27 20:31:03.720: INFO: thanos-querier-6b5789bbbd-qwc9q from openshift-monitoring started at 2021-09-27 17:53:50 +0000 UTC (5 container statuses recorded)
Sep 27 20:31:03.720: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep 27 20:31:03.720: INFO: 	Container kube-rbac-proxy-rules ready: true, restart count 0
Sep 27 20:31:03.720: INFO: 	Container oauth-proxy ready: true, restart count 0
Sep 27 20:31:03.720: INFO: 	Container prom-label-proxy ready: true, restart count 0
Sep 27 20:31:03.720: INFO: 	Container thanos-query ready: true, restart count 0
Sep 27 20:31:03.720: INFO: multus-admission-controller-pp4c5 from openshift-multus started at 2021-09-27 17:51:20 +0000 UTC (2 container statuses recorded)
Sep 27 20:31:03.720: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep 27 20:31:03.720: INFO: 	Container multus-admission-controller ready: true, restart count 0
Sep 27 20:31:03.720: INFO: multus-qwrqb from openshift-multus started at 2021-09-27 17:50:13 +0000 UTC (1 container statuses recorded)
Sep 27 20:31:03.720: INFO: 	Container kube-multus ready: true, restart count 0
Sep 27 20:31:03.720: INFO: network-metrics-daemon-zdwnk from openshift-multus started at 2021-09-27 17:50:13 +0000 UTC (2 container statuses recorded)
Sep 27 20:31:03.720: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep 27 20:31:03.720: INFO: 	Container network-metrics-daemon ready: true, restart count 0
Sep 27 20:31:03.720: INFO: packageserver-67b4ccc984-2lht2 from openshift-operator-lifecycle-manager started at 2021-09-27 17:53:13 +0000 UTC (1 container statuses recorded)
Sep 27 20:31:03.720: INFO: 	Container packageserver ready: true, restart count 0
Sep 27 20:31:03.720: INFO: service-ca-5655fcb96b-ntfqm from openshift-service-ca started at 2021-09-27 17:51:48 +0000 UTC (1 container statuses recorded)
Sep 27 20:31:03.720: INFO: 	Container service-ca-controller ready: true, restart count 0
Sep 27 20:31:03.720: INFO: pod2 from services-7022 started at 2021-09-27 20:30:57 +0000 UTC (1 container statuses recorded)
Sep 27 20:31:03.720: INFO: 	Container pause ready: true, restart count 0
Sep 27 20:31:03.720: INFO: sonobuoy from sonobuoy started at 2021-09-27 19:21:37 +0000 UTC (1 container statuses recorded)
Sep 27 20:31:03.720: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Sep 27 20:31:03.720: INFO: sonobuoy-e2e-job-f7e56101f16c459b from sonobuoy started at 2021-09-27 19:21:44 +0000 UTC (2 container statuses recorded)
Sep 27 20:31:03.720: INFO: 	Container e2e ready: true, restart count 0
Sep 27 20:31:03.720: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep 27 20:31:03.720: INFO: sonobuoy-systemd-logs-daemon-set-a625d2c9d0384434-sl5dj from sonobuoy started at 2021-09-27 19:21:44 +0000 UTC (2 container statuses recorded)
Sep 27 20:31:03.720: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep 27 20:31:03.720: INFO: 	Container systemd-logs ready: true, restart count 0
Sep 27 20:31:03.720: INFO: 
Logging pods the apiserver thinks is on node 10.177.248.117 before test
Sep 27 20:31:03.755: INFO: calico-node-d4rns from calico-system started at 2021-09-27 17:50:17 +0000 UTC (1 container statuses recorded)
Sep 27 20:31:03.755: INFO: 	Container calico-node ready: true, restart count 0
Sep 27 20:31:03.755: INFO: calico-typha-7d789bfc7c-79bn2 from calico-system started at 2021-09-27 17:52:15 +0000 UTC (1 container statuses recorded)
Sep 27 20:31:03.755: INFO: 	Container calico-typha ready: true, restart count 0
Sep 27 20:31:03.755: INFO: ibm-keepalived-watcher-9fxhn from kube-system started at 2021-09-27 17:48:31 +0000 UTC (1 container statuses recorded)
Sep 27 20:31:03.755: INFO: 	Container keepalived-watcher ready: true, restart count 0
Sep 27 20:31:03.755: INFO: ibm-master-proxy-static-10.177.248.117 from kube-system started at 2021-09-27 17:47:53 +0000 UTC (2 container statuses recorded)
Sep 27 20:31:03.755: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Sep 27 20:31:03.755: INFO: 	Container pause ready: true, restart count 0
Sep 27 20:31:03.755: INFO: ibm-storage-watcher-7cd75f8d4f-kdzj2 from kube-system started at 2021-09-27 19:57:16 +0000 UTC (1 container statuses recorded)
Sep 27 20:31:03.755: INFO: 	Container ibm-storage-watcher-container ready: true, restart count 0
Sep 27 20:31:03.755: INFO: ibmcloud-block-storage-driver-4ldjm from kube-system started at 2021-09-27 17:48:31 +0000 UTC (1 container statuses recorded)
Sep 27 20:31:03.755: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Sep 27 20:31:03.755: INFO: tuned-k9qlw from openshift-cluster-node-tuning-operator started at 2021-09-27 17:51:40 +0000 UTC (1 container statuses recorded)
Sep 27 20:31:03.755: INFO: 	Container tuned ready: true, restart count 0
Sep 27 20:31:03.755: INFO: console-operator-d6cf9dd7d-76zv4 from openshift-console-operator started at 2021-09-27 19:57:16 +0000 UTC (1 container statuses recorded)
Sep 27 20:31:03.755: INFO: 	Container console-operator ready: true, restart count 0
Sep 27 20:31:03.755: INFO: console-5b57dff4ff-d7kjb from openshift-console started at 2021-09-27 19:57:16 +0000 UTC (1 container statuses recorded)
Sep 27 20:31:03.755: INFO: 	Container console ready: true, restart count 0
Sep 27 20:31:03.755: INFO: dns-default-cb2zk from openshift-dns started at 2021-09-27 17:52:55 +0000 UTC (3 container statuses recorded)
Sep 27 20:31:03.755: INFO: 	Container dns ready: true, restart count 0
Sep 27 20:31:03.755: INFO: 	Container dns-node-resolver ready: true, restart count 0
Sep 27 20:31:03.755: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep 27 20:31:03.755: INFO: node-ca-82tbs from openshift-image-registry started at 2021-09-27 17:53:17 +0000 UTC (1 container statuses recorded)
Sep 27 20:31:03.755: INFO: 	Container node-ca ready: true, restart count 0
Sep 27 20:31:03.755: INFO: registry-pvc-permissions-vncj6 from openshift-image-registry started at 2021-09-27 17:55:54 +0000 UTC (1 container statuses recorded)
Sep 27 20:31:03.755: INFO: 	Container pvc-permissions ready: false, restart count 0
Sep 27 20:31:03.755: INFO: openshift-kube-proxy-2dc5b from openshift-kube-proxy started at 2021-09-27 17:49:34 +0000 UTC (1 container statuses recorded)
Sep 27 20:31:03.756: INFO: 	Container kube-proxy ready: true, restart count 0
Sep 27 20:31:03.756: INFO: migrator-6656c87b46-pnhl2 from openshift-kube-storage-version-migrator started at 2021-09-27 19:57:16 +0000 UTC (1 container statuses recorded)
Sep 27 20:31:03.756: INFO: 	Container migrator ready: true, restart count 0
Sep 27 20:31:03.756: INFO: alertmanager-main-2 from openshift-monitoring started at 2021-09-27 19:57:22 +0000 UTC (5 container statuses recorded)
Sep 27 20:31:03.756: INFO: 	Container alertmanager ready: true, restart count 0
Sep 27 20:31:03.756: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Sep 27 20:31:03.756: INFO: 	Container config-reloader ready: true, restart count 0
Sep 27 20:31:03.756: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep 27 20:31:03.756: INFO: 	Container prom-label-proxy ready: true, restart count 0
Sep 27 20:31:03.756: INFO: node-exporter-r6mdq from openshift-monitoring started at 2021-09-27 17:51:50 +0000 UTC (2 container statuses recorded)
Sep 27 20:31:03.756: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep 27 20:31:03.756: INFO: 	Container node-exporter ready: true, restart count 0
Sep 27 20:31:03.756: INFO: prometheus-k8s-1 from openshift-monitoring started at 2021-09-27 19:57:17 +0000 UTC (6 container statuses recorded)
Sep 27 20:31:03.756: INFO: 	Container config-reloader ready: true, restart count 0
Sep 27 20:31:03.756: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep 27 20:31:03.756: INFO: 	Container prom-label-proxy ready: true, restart count 0
Sep 27 20:31:03.756: INFO: 	Container prometheus ready: true, restart count 1
Sep 27 20:31:03.756: INFO: 	Container prometheus-proxy ready: true, restart count 0
Sep 27 20:31:03.756: INFO: 	Container thanos-sidecar ready: true, restart count 0
Sep 27 20:31:03.756: INFO: multus-admission-controller-rtg5z from openshift-multus started at 2021-09-27 19:57:16 +0000 UTC (2 container statuses recorded)
Sep 27 20:31:03.756: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep 27 20:31:03.756: INFO: 	Container multus-admission-controller ready: true, restart count 0
Sep 27 20:31:03.756: INFO: multus-vjt2q from openshift-multus started at 2021-09-27 17:49:24 +0000 UTC (1 container statuses recorded)
Sep 27 20:31:03.756: INFO: 	Container kube-multus ready: true, restart count 0
Sep 27 20:31:03.756: INFO: network-metrics-daemon-4t2kd from openshift-multus started at 2021-09-27 17:49:26 +0000 UTC (2 container statuses recorded)
Sep 27 20:31:03.756: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep 27 20:31:03.756: INFO: 	Container network-metrics-daemon ready: true, restart count 0
Sep 27 20:31:03.756: INFO: network-operator-dbdd595f7-dwsbv from openshift-network-operator started at 2021-09-27 19:57:16 +0000 UTC (1 container statuses recorded)
Sep 27 20:31:03.756: INFO: 	Container network-operator ready: true, restart count 0
Sep 27 20:31:03.756: INFO: pod1 from services-7022 started at 2021-09-27 20:30:54 +0000 UTC (1 container statuses recorded)
Sep 27 20:31:03.756: INFO: 	Container pause ready: false, restart count 0
Sep 27 20:31:03.756: INFO: sonobuoy-systemd-logs-daemon-set-a625d2c9d0384434-z8xz7 from sonobuoy started at 2021-09-27 19:21:44 +0000 UTC (2 container statuses recorded)
Sep 27 20:31:03.756: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep 27 20:31:03.756: INFO: 	Container systemd-logs ready: true, restart count 0
Sep 27 20:31:03.756: INFO: 
Logging pods the apiserver thinks is on node 10.177.248.126 before test
Sep 27 20:31:03.812: INFO: calico-kube-controllers-5465c95dd-7hlj7 from calico-system started at 2021-09-27 17:50:29 +0000 UTC (1 container statuses recorded)
Sep 27 20:31:03.812: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Sep 27 20:31:03.812: INFO: calico-node-dhz9l from calico-system started at 2021-09-27 17:50:17 +0000 UTC (1 container statuses recorded)
Sep 27 20:31:03.812: INFO: 	Container calico-node ready: true, restart count 0
Sep 27 20:31:03.812: INFO: calico-typha-7d789bfc7c-7669x from calico-system started at 2021-09-27 17:50:17 +0000 UTC (1 container statuses recorded)
Sep 27 20:31:03.812: INFO: 	Container calico-typha ready: true, restart count 0
Sep 27 20:31:03.812: INFO: ibm-cloud-provider-ip-169-61-239-242-59b4964694-mc2b8 from ibm-system started at 2021-09-27 19:52:27 +0000 UTC (1 container statuses recorded)
Sep 27 20:31:03.813: INFO: 	Container ibm-cloud-provider-ip-169-61-239-242 ready: true, restart count 0
Sep 27 20:31:03.813: INFO: ibm-file-plugin-fcdf5f569-x8hfq from kube-system started at 2021-09-27 17:50:29 +0000 UTC (1 container statuses recorded)
Sep 27 20:31:03.813: INFO: 	Container ibm-file-plugin-container ready: true, restart count 0
Sep 27 20:31:03.813: INFO: ibm-keepalived-watcher-rgwfm from kube-system started at 2021-09-27 17:49:04 +0000 UTC (1 container statuses recorded)
Sep 27 20:31:03.813: INFO: 	Container keepalived-watcher ready: true, restart count 0
Sep 27 20:31:03.813: INFO: ibm-master-proxy-static-10.177.248.126 from kube-system started at 2021-09-27 17:48:20 +0000 UTC (2 container statuses recorded)
Sep 27 20:31:03.813: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Sep 27 20:31:03.813: INFO: 	Container pause ready: true, restart count 0
Sep 27 20:31:03.813: INFO: ibmcloud-block-storage-driver-kbd9g from kube-system started at 2021-09-27 17:49:04 +0000 UTC (1 container statuses recorded)
Sep 27 20:31:03.813: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Sep 27 20:31:03.813: INFO: ibmcloud-block-storage-plugin-74d6877898-zqtnb from kube-system started at 2021-09-27 17:50:29 +0000 UTC (1 container statuses recorded)
Sep 27 20:31:03.813: INFO: 	Container ibmcloud-block-storage-plugin-container ready: true, restart count 0
Sep 27 20:31:03.813: INFO: cluster-node-tuning-operator-67b4b4fbf5-zslfs from openshift-cluster-node-tuning-operator started at 2021-09-27 17:50:29 +0000 UTC (1 container statuses recorded)
Sep 27 20:31:03.813: INFO: 	Container cluster-node-tuning-operator ready: true, restart count 0
Sep 27 20:31:03.813: INFO: tuned-qc5dt from openshift-cluster-node-tuning-operator started at 2021-09-27 17:51:41 +0000 UTC (1 container statuses recorded)
Sep 27 20:31:03.813: INFO: 	Container tuned ready: true, restart count 0
Sep 27 20:31:03.813: INFO: cluster-samples-operator-97cc95ff8-nmc7q from openshift-cluster-samples-operator started at 2021-09-27 17:50:29 +0000 UTC (2 container statuses recorded)
Sep 27 20:31:03.814: INFO: 	Container cluster-samples-operator ready: true, restart count 0
Sep 27 20:31:03.814: INFO: 	Container cluster-samples-operator-watch ready: true, restart count 0
Sep 27 20:31:03.814: INFO: cluster-storage-operator-59698ddbdf-w86gw from openshift-cluster-storage-operator started at 2021-09-27 17:50:29 +0000 UTC (1 container statuses recorded)
Sep 27 20:31:03.814: INFO: 	Container cluster-storage-operator ready: true, restart count 0
Sep 27 20:31:03.814: INFO: downloads-c785794b6-q92tc from openshift-console started at 2021-09-27 17:50:29 +0000 UTC (1 container statuses recorded)
Sep 27 20:31:03.814: INFO: 	Container download-server ready: true, restart count 0
Sep 27 20:31:03.814: INFO: downloads-c785794b6-w28kb from openshift-console started at 2021-09-27 17:50:29 +0000 UTC (1 container statuses recorded)
Sep 27 20:31:03.814: INFO: 	Container download-server ready: true, restart count 0
Sep 27 20:31:03.814: INFO: dns-operator-7489bbc67f-9ztr4 from openshift-dns-operator started at 2021-09-27 17:50:29 +0000 UTC (2 container statuses recorded)
Sep 27 20:31:03.814: INFO: 	Container dns-operator ready: true, restart count 0
Sep 27 20:31:03.814: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep 27 20:31:03.814: INFO: dns-default-msjgz from openshift-dns started at 2021-09-27 17:52:55 +0000 UTC (3 container statuses recorded)
Sep 27 20:31:03.814: INFO: 	Container dns ready: true, restart count 0
Sep 27 20:31:03.814: INFO: 	Container dns-node-resolver ready: true, restart count 0
Sep 27 20:31:03.814: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep 27 20:31:03.814: INFO: cluster-image-registry-operator-675674456f-k2fc9 from openshift-image-registry started at 2021-09-27 17:50:29 +0000 UTC (1 container statuses recorded)
Sep 27 20:31:03.814: INFO: 	Container cluster-image-registry-operator ready: true, restart count 0
Sep 27 20:31:03.814: INFO: image-registry-ffbbf6ddb-xl9b9 from openshift-image-registry started at 2021-09-27 17:55:54 +0000 UTC (1 container statuses recorded)
Sep 27 20:31:03.814: INFO: 	Container registry ready: true, restart count 0
Sep 27 20:31:03.814: INFO: node-ca-2rrgl from openshift-image-registry started at 2021-09-27 17:53:17 +0000 UTC (1 container statuses recorded)
Sep 27 20:31:03.815: INFO: 	Container node-ca ready: true, restart count 0
Sep 27 20:31:03.815: INFO: ingress-operator-8469759c95-4mm9m from openshift-ingress-operator started at 2021-09-27 17:50:29 +0000 UTC (2 container statuses recorded)
Sep 27 20:31:03.817: INFO: 	Container ingress-operator ready: true, restart count 0
Sep 27 20:31:03.817: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep 27 20:31:03.817: INFO: router-default-768f4875db-8qx4w from openshift-ingress started at 2021-09-27 17:53:15 +0000 UTC (1 container statuses recorded)
Sep 27 20:31:03.817: INFO: 	Container router ready: true, restart count 0
Sep 27 20:31:03.817: INFO: openshift-kube-proxy-7gx28 from openshift-kube-proxy started at 2021-09-27 17:49:34 +0000 UTC (1 container statuses recorded)
Sep 27 20:31:03.818: INFO: 	Container kube-proxy ready: true, restart count 0
Sep 27 20:31:03.818: INFO: kube-storage-version-migrator-operator-55c7c8f996-xr7ds from openshift-kube-storage-version-migrator-operator started at 2021-09-27 17:50:29 +0000 UTC (1 container statuses recorded)
Sep 27 20:31:03.818: INFO: 	Container kube-storage-version-migrator-operator ready: true, restart count 1
Sep 27 20:31:03.818: INFO: marketplace-operator-7cd49bbf56-nggpg from openshift-marketplace started at 2021-09-27 17:50:29 +0000 UTC (1 container statuses recorded)
Sep 27 20:31:03.818: INFO: 	Container marketplace-operator ready: true, restart count 0
Sep 27 20:31:03.818: INFO: alertmanager-main-1 from openshift-monitoring started at 2021-09-27 17:53:55 +0000 UTC (5 container statuses recorded)
Sep 27 20:31:03.818: INFO: 	Container alertmanager ready: true, restart count 0
Sep 27 20:31:03.818: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Sep 27 20:31:03.818: INFO: 	Container config-reloader ready: true, restart count 0
Sep 27 20:31:03.818: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep 27 20:31:03.818: INFO: 	Container prom-label-proxy ready: true, restart count 0
Sep 27 20:31:03.818: INFO: cluster-monitoring-operator-7656b489c8-z2wmx from openshift-monitoring started at 2021-09-27 17:50:29 +0000 UTC (2 container statuses recorded)
Sep 27 20:31:03.818: INFO: 	Container cluster-monitoring-operator ready: true, restart count 0
Sep 27 20:31:03.818: INFO: 	Container kube-rbac-proxy ready: true, restart count 3
Sep 27 20:31:03.818: INFO: node-exporter-42nr4 from openshift-monitoring started at 2021-09-27 17:51:50 +0000 UTC (2 container statuses recorded)
Sep 27 20:31:03.818: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep 27 20:31:03.818: INFO: 	Container node-exporter ready: true, restart count 0
Sep 27 20:31:03.818: INFO: prometheus-adapter-66469d976-t6rf2 from openshift-monitoring started at 2021-09-27 17:54:46 +0000 UTC (1 container statuses recorded)
Sep 27 20:31:03.818: INFO: 	Container prometheus-adapter ready: true, restart count 0
Sep 27 20:31:03.818: INFO: thanos-querier-6b5789bbbd-l8dmh from openshift-monitoring started at 2021-09-27 17:53:51 +0000 UTC (5 container statuses recorded)
Sep 27 20:31:03.818: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep 27 20:31:03.818: INFO: 	Container kube-rbac-proxy-rules ready: true, restart count 0
Sep 27 20:31:03.819: INFO: 	Container oauth-proxy ready: true, restart count 0
Sep 27 20:31:03.819: INFO: 	Container prom-label-proxy ready: true, restart count 0
Sep 27 20:31:03.819: INFO: 	Container thanos-query ready: true, restart count 0
Sep 27 20:31:03.819: INFO: multus-admission-controller-fgtsq from openshift-multus started at 2021-09-27 17:50:29 +0000 UTC (2 container statuses recorded)
Sep 27 20:31:03.819: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep 27 20:31:03.819: INFO: 	Container multus-admission-controller ready: true, restart count 0
Sep 27 20:31:03.819: INFO: multus-fzpdl from openshift-multus started at 2021-09-27 17:49:24 +0000 UTC (1 container statuses recorded)
Sep 27 20:31:03.819: INFO: 	Container kube-multus ready: true, restart count 0
Sep 27 20:31:03.819: INFO: network-metrics-daemon-5wdz5 from openshift-multus started at 2021-09-27 17:49:26 +0000 UTC (2 container statuses recorded)
Sep 27 20:31:03.819: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep 27 20:31:03.819: INFO: 	Container network-metrics-daemon ready: true, restart count 0
Sep 27 20:31:03.819: INFO: catalog-operator-69c4599997-btc8l from openshift-operator-lifecycle-manager started at 2021-09-27 17:50:29 +0000 UTC (1 container statuses recorded)
Sep 27 20:31:03.819: INFO: 	Container catalog-operator ready: true, restart count 0
Sep 27 20:31:03.819: INFO: olm-operator-db7cd6974-2gxjx from openshift-operator-lifecycle-manager started at 2021-09-27 17:50:29 +0000 UTC (1 container statuses recorded)
Sep 27 20:31:03.819: INFO: 	Container olm-operator ready: true, restart count 0
Sep 27 20:31:03.819: INFO: packageserver-67b4ccc984-w6sdn from openshift-operator-lifecycle-manager started at 2021-09-27 17:53:13 +0000 UTC (1 container statuses recorded)
Sep 27 20:31:03.819: INFO: 	Container packageserver ready: true, restart count 0
Sep 27 20:31:03.819: INFO: metrics-666cbf9545-t95w7 from openshift-roks-metrics started at 2021-09-27 17:50:29 +0000 UTC (1 container statuses recorded)
Sep 27 20:31:03.819: INFO: 	Container metrics ready: true, restart count 3
Sep 27 20:31:03.819: INFO: push-gateway-6f6b5cb7f7-k6lpz from openshift-roks-metrics started at 2021-09-27 19:52:28 +0000 UTC (1 container statuses recorded)
Sep 27 20:31:03.819: INFO: 	Container push-gateway ready: false, restart count 0
Sep 27 20:31:03.819: INFO: service-ca-operator-6ccc7cff9-l9wfj from openshift-service-ca-operator started at 2021-09-27 17:50:29 +0000 UTC (1 container statuses recorded)
Sep 27 20:31:03.820: INFO: 	Container service-ca-operator ready: true, restart count 1
Sep 27 20:31:03.820: INFO: sonobuoy-systemd-logs-daemon-set-a625d2c9d0384434-6w8vw from sonobuoy started at 2021-09-27 19:21:44 +0000 UTC (2 container statuses recorded)
Sep 27 20:31:03.820: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep 27 20:31:03.820: INFO: 	Container systemd-logs ready: true, restart count 0
Sep 27 20:31:03.820: INFO: tigera-operator-db8ddcc79-mf5vc from tigera-operator started at 2021-09-27 19:52:27 +0000 UTC (1 container statuses recorded)
Sep 27 20:31:03.820: INFO: 	Container tigera-operator ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: verifying the node has the label node 10.177.248.114
STEP: verifying the node has the label node 10.177.248.117
STEP: verifying the node has the label node 10.177.248.126
Sep 27 20:31:04.054: INFO: Pod calico-kube-controllers-5465c95dd-7hlj7 requesting resource cpu=10m on Node 10.177.248.126
Sep 27 20:31:04.055: INFO: Pod calico-node-d4rns requesting resource cpu=250m on Node 10.177.248.117
Sep 27 20:31:04.055: INFO: Pod calico-node-dhz9l requesting resource cpu=250m on Node 10.177.248.126
Sep 27 20:31:04.055: INFO: Pod calico-node-kxvx5 requesting resource cpu=250m on Node 10.177.248.114
Sep 27 20:31:04.055: INFO: Pod calico-typha-7d789bfc7c-7669x requesting resource cpu=250m on Node 10.177.248.126
Sep 27 20:31:04.055: INFO: Pod calico-typha-7d789bfc7c-79bn2 requesting resource cpu=250m on Node 10.177.248.117
Sep 27 20:31:04.055: INFO: Pod calico-typha-7d789bfc7c-8ftcw requesting resource cpu=250m on Node 10.177.248.114
Sep 27 20:31:04.055: INFO: Pod test-k8s-e2e-pvg-master-verification requesting resource cpu=0m on Node 10.177.248.114
Sep 27 20:31:04.055: INFO: Pod ibm-cloud-provider-ip-169-61-239-242-59b4964694-mc2b8 requesting resource cpu=5m on Node 10.177.248.126
Sep 27 20:31:04.055: INFO: Pod ibm-cloud-provider-ip-169-61-239-242-59b4964694-ngqgl requesting resource cpu=5m on Node 10.177.248.114
Sep 27 20:31:04.055: INFO: Pod ibm-file-plugin-fcdf5f569-x8hfq requesting resource cpu=50m on Node 10.177.248.126
Sep 27 20:31:04.055: INFO: Pod ibm-keepalived-watcher-8lj4c requesting resource cpu=5m on Node 10.177.248.114
Sep 27 20:31:04.055: INFO: Pod ibm-keepalived-watcher-9fxhn requesting resource cpu=5m on Node 10.177.248.117
Sep 27 20:31:04.055: INFO: Pod ibm-keepalived-watcher-rgwfm requesting resource cpu=5m on Node 10.177.248.126
Sep 27 20:31:04.055: INFO: Pod ibm-master-proxy-static-10.177.248.114 requesting resource cpu=25m on Node 10.177.248.114
Sep 27 20:31:04.055: INFO: Pod ibm-master-proxy-static-10.177.248.117 requesting resource cpu=25m on Node 10.177.248.117
Sep 27 20:31:04.055: INFO: Pod ibm-master-proxy-static-10.177.248.126 requesting resource cpu=25m on Node 10.177.248.126
Sep 27 20:31:04.055: INFO: Pod ibm-storage-watcher-7cd75f8d4f-kdzj2 requesting resource cpu=50m on Node 10.177.248.117
Sep 27 20:31:04.055: INFO: Pod ibmcloud-block-storage-driver-4ldjm requesting resource cpu=50m on Node 10.177.248.117
Sep 27 20:31:04.055: INFO: Pod ibmcloud-block-storage-driver-kbd9g requesting resource cpu=50m on Node 10.177.248.126
Sep 27 20:31:04.055: INFO: Pod ibmcloud-block-storage-driver-mxb6q requesting resource cpu=50m on Node 10.177.248.114
Sep 27 20:31:04.055: INFO: Pod ibmcloud-block-storage-plugin-74d6877898-zqtnb requesting resource cpu=50m on Node 10.177.248.126
Sep 27 20:31:04.056: INFO: Pod vpn-bc979587-kglsg requesting resource cpu=5m on Node 10.177.248.114
Sep 27 20:31:04.056: INFO: Pod cluster-node-tuning-operator-67b4b4fbf5-zslfs requesting resource cpu=10m on Node 10.177.248.126
Sep 27 20:31:04.056: INFO: Pod tuned-dsf6r requesting resource cpu=10m on Node 10.177.248.114
Sep 27 20:31:04.056: INFO: Pod tuned-k9qlw requesting resource cpu=10m on Node 10.177.248.117
Sep 27 20:31:04.056: INFO: Pod tuned-qc5dt requesting resource cpu=10m on Node 10.177.248.126
Sep 27 20:31:04.056: INFO: Pod cluster-samples-operator-97cc95ff8-nmc7q requesting resource cpu=20m on Node 10.177.248.126
Sep 27 20:31:04.056: INFO: Pod cluster-storage-operator-59698ddbdf-w86gw requesting resource cpu=10m on Node 10.177.248.126
Sep 27 20:31:04.056: INFO: Pod console-operator-d6cf9dd7d-76zv4 requesting resource cpu=10m on Node 10.177.248.117
Sep 27 20:31:04.056: INFO: Pod console-5b57dff4ff-2hr57 requesting resource cpu=10m on Node 10.177.248.114
Sep 27 20:31:04.056: INFO: Pod console-5b57dff4ff-d7kjb requesting resource cpu=10m on Node 10.177.248.117
Sep 27 20:31:04.056: INFO: Pod downloads-c785794b6-q92tc requesting resource cpu=10m on Node 10.177.248.126
Sep 27 20:31:04.056: INFO: Pod downloads-c785794b6-w28kb requesting resource cpu=10m on Node 10.177.248.126
Sep 27 20:31:04.056: INFO: Pod dns-operator-7489bbc67f-9ztr4 requesting resource cpu=20m on Node 10.177.248.126
Sep 27 20:31:04.056: INFO: Pod dns-default-cb2zk requesting resource cpu=65m on Node 10.177.248.117
Sep 27 20:31:04.056: INFO: Pod dns-default-ktd8w requesting resource cpu=65m on Node 10.177.248.114
Sep 27 20:31:04.056: INFO: Pod dns-default-msjgz requesting resource cpu=65m on Node 10.177.248.126
Sep 27 20:31:04.056: INFO: Pod cluster-image-registry-operator-675674456f-k2fc9 requesting resource cpu=10m on Node 10.177.248.126
Sep 27 20:31:04.056: INFO: Pod image-registry-ffbbf6ddb-xl9b9 requesting resource cpu=100m on Node 10.177.248.126
Sep 27 20:31:04.056: INFO: Pod node-ca-2rrgl requesting resource cpu=10m on Node 10.177.248.126
Sep 27 20:31:04.056: INFO: Pod node-ca-82tbs requesting resource cpu=10m on Node 10.177.248.117
Sep 27 20:31:04.056: INFO: Pod node-ca-bv4wj requesting resource cpu=10m on Node 10.177.248.114
Sep 27 20:31:04.057: INFO: Pod ingress-operator-8469759c95-4mm9m requesting resource cpu=20m on Node 10.177.248.126
Sep 27 20:31:04.057: INFO: Pod router-default-768f4875db-8qx4w requesting resource cpu=100m on Node 10.177.248.126
Sep 27 20:31:04.057: INFO: Pod router-default-768f4875db-n8g9c requesting resource cpu=100m on Node 10.177.248.114
Sep 27 20:31:04.057: INFO: Pod openshift-kube-proxy-2dc5b requesting resource cpu=100m on Node 10.177.248.117
Sep 27 20:31:04.057: INFO: Pod openshift-kube-proxy-7gx28 requesting resource cpu=100m on Node 10.177.248.126
Sep 27 20:31:04.057: INFO: Pod openshift-kube-proxy-lxdz4 requesting resource cpu=100m on Node 10.177.248.114
Sep 27 20:31:04.057: INFO: Pod kube-storage-version-migrator-operator-55c7c8f996-xr7ds requesting resource cpu=10m on Node 10.177.248.126
Sep 27 20:31:04.057: INFO: Pod migrator-6656c87b46-pnhl2 requesting resource cpu=100m on Node 10.177.248.117
Sep 27 20:31:04.057: INFO: Pod certified-operators-qvnmv requesting resource cpu=10m on Node 10.177.248.114
Sep 27 20:31:04.057: INFO: Pod community-operators-mc4rj requesting resource cpu=10m on Node 10.177.248.114
Sep 27 20:31:04.057: INFO: Pod marketplace-operator-7cd49bbf56-nggpg requesting resource cpu=10m on Node 10.177.248.126
Sep 27 20:31:04.057: INFO: Pod redhat-marketplace-tf75q requesting resource cpu=10m on Node 10.177.248.114
Sep 27 20:31:04.057: INFO: Pod redhat-operators-2sd6b requesting resource cpu=10m on Node 10.177.248.114
Sep 27 20:31:04.057: INFO: Pod alertmanager-main-0 requesting resource cpu=8m on Node 10.177.248.114
Sep 27 20:31:04.057: INFO: Pod alertmanager-main-1 requesting resource cpu=8m on Node 10.177.248.126
Sep 27 20:31:04.057: INFO: Pod alertmanager-main-2 requesting resource cpu=8m on Node 10.177.248.117
Sep 27 20:31:04.057: INFO: Pod cluster-monitoring-operator-7656b489c8-z2wmx requesting resource cpu=11m on Node 10.177.248.126
Sep 27 20:31:04.057: INFO: Pod grafana-6f7d589b46-r88hj requesting resource cpu=5m on Node 10.177.248.114
Sep 27 20:31:04.057: INFO: Pod kube-state-metrics-5d4985f6b7-jx87q requesting resource cpu=4m on Node 10.177.248.114
Sep 27 20:31:04.057: INFO: Pod node-exporter-42nr4 requesting resource cpu=9m on Node 10.177.248.126
Sep 27 20:31:04.057: INFO: Pod node-exporter-r6mdq requesting resource cpu=9m on Node 10.177.248.117
Sep 27 20:31:04.057: INFO: Pod node-exporter-r84r5 requesting resource cpu=9m on Node 10.177.248.114
Sep 27 20:31:04.057: INFO: Pod openshift-state-metrics-58bfb7bff-qxd59 requesting resource cpu=3m on Node 10.177.248.114
Sep 27 20:31:04.057: INFO: Pod prometheus-adapter-66469d976-gl6wl requesting resource cpu=1m on Node 10.177.248.114
Sep 27 20:31:04.058: INFO: Pod prometheus-adapter-66469d976-t6rf2 requesting resource cpu=1m on Node 10.177.248.126
Sep 27 20:31:04.058: INFO: Pod prometheus-k8s-0 requesting resource cpu=75m on Node 10.177.248.114
Sep 27 20:31:04.058: INFO: Pod prometheus-k8s-1 requesting resource cpu=75m on Node 10.177.248.117
Sep 27 20:31:04.058: INFO: Pod prometheus-operator-856d6cddd4-blncm requesting resource cpu=6m on Node 10.177.248.114
Sep 27 20:31:04.058: INFO: Pod telemeter-client-68cfc9967c-sr2hj requesting resource cpu=3m on Node 10.177.248.114
Sep 27 20:31:04.058: INFO: Pod thanos-querier-6b5789bbbd-l8dmh requesting resource cpu=9m on Node 10.177.248.126
Sep 27 20:31:04.058: INFO: Pod thanos-querier-6b5789bbbd-qwc9q requesting resource cpu=9m on Node 10.177.248.114
Sep 27 20:31:04.058: INFO: Pod multus-admission-controller-fgtsq requesting resource cpu=20m on Node 10.177.248.126
Sep 27 20:31:04.058: INFO: Pod multus-admission-controller-pp4c5 requesting resource cpu=20m on Node 10.177.248.114
Sep 27 20:31:04.058: INFO: Pod multus-admission-controller-rtg5z requesting resource cpu=20m on Node 10.177.248.117
Sep 27 20:31:04.058: INFO: Pod multus-fzpdl requesting resource cpu=10m on Node 10.177.248.126
Sep 27 20:31:04.058: INFO: Pod multus-qwrqb requesting resource cpu=10m on Node 10.177.248.114
Sep 27 20:31:04.058: INFO: Pod multus-vjt2q requesting resource cpu=10m on Node 10.177.248.117
Sep 27 20:31:04.058: INFO: Pod network-metrics-daemon-4t2kd requesting resource cpu=20m on Node 10.177.248.117
Sep 27 20:31:04.058: INFO: Pod network-metrics-daemon-5wdz5 requesting resource cpu=20m on Node 10.177.248.126
Sep 27 20:31:04.058: INFO: Pod network-metrics-daemon-zdwnk requesting resource cpu=20m on Node 10.177.248.114
Sep 27 20:31:04.058: INFO: Pod network-operator-dbdd595f7-dwsbv requesting resource cpu=10m on Node 10.177.248.117
Sep 27 20:31:04.058: INFO: Pod catalog-operator-69c4599997-btc8l requesting resource cpu=10m on Node 10.177.248.126
Sep 27 20:31:04.058: INFO: Pod olm-operator-db7cd6974-2gxjx requesting resource cpu=10m on Node 10.177.248.126
Sep 27 20:31:04.058: INFO: Pod packageserver-67b4ccc984-2lht2 requesting resource cpu=10m on Node 10.177.248.114
Sep 27 20:31:04.058: INFO: Pod packageserver-67b4ccc984-w6sdn requesting resource cpu=10m on Node 10.177.248.126
Sep 27 20:31:04.058: INFO: Pod metrics-666cbf9545-t95w7 requesting resource cpu=10m on Node 10.177.248.126
Sep 27 20:31:04.059: INFO: Pod push-gateway-6f6b5cb7f7-k6lpz requesting resource cpu=10m on Node 10.177.248.126
Sep 27 20:31:04.059: INFO: Pod service-ca-operator-6ccc7cff9-l9wfj requesting resource cpu=10m on Node 10.177.248.126
Sep 27 20:31:04.059: INFO: Pod service-ca-5655fcb96b-ntfqm requesting resource cpu=10m on Node 10.177.248.114
Sep 27 20:31:04.059: INFO: Pod pod1 requesting resource cpu=0m on Node 10.177.248.117
Sep 27 20:31:04.059: INFO: Pod pod2 requesting resource cpu=0m on Node 10.177.248.114
Sep 27 20:31:04.059: INFO: Pod sonobuoy requesting resource cpu=0m on Node 10.177.248.114
Sep 27 20:31:04.059: INFO: Pod sonobuoy-e2e-job-f7e56101f16c459b requesting resource cpu=0m on Node 10.177.248.114
Sep 27 20:31:04.059: INFO: Pod sonobuoy-systemd-logs-daemon-set-a625d2c9d0384434-6w8vw requesting resource cpu=0m on Node 10.177.248.126
Sep 27 20:31:04.059: INFO: Pod sonobuoy-systemd-logs-daemon-set-a625d2c9d0384434-sl5dj requesting resource cpu=0m on Node 10.177.248.114
Sep 27 20:31:04.059: INFO: Pod sonobuoy-systemd-logs-daemon-set-a625d2c9d0384434-z8xz7 requesting resource cpu=0m on Node 10.177.248.117
Sep 27 20:31:04.059: INFO: Pod tigera-operator-db8ddcc79-mf5vc requesting resource cpu=100m on Node 10.177.248.126
STEP: Starting Pods to consume most of the cluster CPU.
Sep 27 20:31:04.059: INFO: Creating a pod which consumes cpu=1954m on Node 10.177.248.114
Sep 27 20:31:04.104: INFO: Creating a pod which consumes cpu=1976m on Node 10.177.248.117
Sep 27 20:31:04.139: INFO: Creating a pod which consumes cpu=1716m on Node 10.177.248.126
STEP: Creating another pod that requires unavailable amount of CPU.
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-1ed7ac5c-d86d-453b-b77f-7aab97691a13.16a8c7e7fdb9f25e], Reason = [Scheduled], Message = [Successfully assigned sched-pred-4354/filler-pod-1ed7ac5c-d86d-453b-b77f-7aab97691a13 to 10.177.248.114]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-1ed7ac5c-d86d-453b-b77f-7aab97691a13.16a8c7e8485d88c9], Reason = [AddedInterface], Message = [Add eth0 [172.30.137.93/32]]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-1ed7ac5c-d86d-453b-b77f-7aab97691a13.16a8c7e84badccd1], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.2" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-1ed7ac5c-d86d-453b-b77f-7aab97691a13.16a8c7e85b1cef9c], Reason = [Created], Message = [Created container filler-pod-1ed7ac5c-d86d-453b-b77f-7aab97691a13]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-1ed7ac5c-d86d-453b-b77f-7aab97691a13.16a8c7e85e8831d9], Reason = [Started], Message = [Started container filler-pod-1ed7ac5c-d86d-453b-b77f-7aab97691a13]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-3ef63e31-9543-4003-8a1e-d26e08acc553.16a8c7e7ff84ad6d], Reason = [Scheduled], Message = [Successfully assigned sched-pred-4354/filler-pod-3ef63e31-9543-4003-8a1e-d26e08acc553 to 10.177.248.117]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-3ef63e31-9543-4003-8a1e-d26e08acc553.16a8c7e83c2ef069], Reason = [AddedInterface], Message = [Add eth0 [172.30.75.47/32]]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-3ef63e31-9543-4003-8a1e-d26e08acc553.16a8c7e83f1ba1ed], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.2" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-3ef63e31-9543-4003-8a1e-d26e08acc553.16a8c7e84bbb2ad7], Reason = [Created], Message = [Created container filler-pod-3ef63e31-9543-4003-8a1e-d26e08acc553]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-3ef63e31-9543-4003-8a1e-d26e08acc553.16a8c7e84e51528f], Reason = [Started], Message = [Started container filler-pod-3ef63e31-9543-4003-8a1e-d26e08acc553]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-7efa8e51-2325-4fd2-9bce-0d43fb0b0f66.16a8c7e801993551], Reason = [Scheduled], Message = [Successfully assigned sched-pred-4354/filler-pod-7efa8e51-2325-4fd2-9bce-0d43fb0b0f66 to 10.177.248.126]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-7efa8e51-2325-4fd2-9bce-0d43fb0b0f66.16a8c7e848115176], Reason = [AddedInterface], Message = [Add eth0 [172.30.85.160/32]]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-7efa8e51-2325-4fd2-9bce-0d43fb0b0f66.16a8c7e84b8ea3db], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.2" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-7efa8e51-2325-4fd2-9bce-0d43fb0b0f66.16a8c7e85c24997d], Reason = [Created], Message = [Created container filler-pod-7efa8e51-2325-4fd2-9bce-0d43fb0b0f66]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-7efa8e51-2325-4fd2-9bce-0d43fb0b0f66.16a8c7e85fb90cd6], Reason = [Started], Message = [Started container filler-pod-7efa8e51-2325-4fd2-9bce-0d43fb0b0f66]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.16a8c7e8f7a37237], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 Insufficient cpu.]
STEP: removing the label node off the node 10.177.248.117
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node 10.177.248.126
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node 10.177.248.114
STEP: verifying the node doesn't have the label node
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 20:31:09.538: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-4354" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81

• [SLOW TEST:6.165 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates resource limits of pods that are allowed to run  [Conformance]","total":305,"completed":166,"skipped":2621,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 20:31:09.592: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name configmap-test-volume-c2a96470-3856-4d94-9997-d264589fb5cc
STEP: Creating a pod to test consume configMaps
Sep 27 20:31:09.833: INFO: Waiting up to 5m0s for pod "pod-configmaps-51182c8d-5239-465c-a2e0-86f09a6210e5" in namespace "configmap-7473" to be "Succeeded or Failed"
Sep 27 20:31:09.855: INFO: Pod "pod-configmaps-51182c8d-5239-465c-a2e0-86f09a6210e5": Phase="Pending", Reason="", readiness=false. Elapsed: 21.382706ms
Sep 27 20:31:11.873: INFO: Pod "pod-configmaps-51182c8d-5239-465c-a2e0-86f09a6210e5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.039352707s
STEP: Saw pod success
Sep 27 20:31:11.873: INFO: Pod "pod-configmaps-51182c8d-5239-465c-a2e0-86f09a6210e5" satisfied condition "Succeeded or Failed"
Sep 27 20:31:11.889: INFO: Trying to get logs from node 10.177.248.126 pod pod-configmaps-51182c8d-5239-465c-a2e0-86f09a6210e5 container configmap-volume-test: <nil>
STEP: delete the pod
Sep 27 20:31:12.124: INFO: Waiting for pod pod-configmaps-51182c8d-5239-465c-a2e0-86f09a6210e5 to disappear
Sep 27 20:31:12.144: INFO: Pod pod-configmaps-51182c8d-5239-465c-a2e0-86f09a6210e5 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 20:31:12.144: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7473" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","total":305,"completed":167,"skipped":2651,"failed":0}
SSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 20:31:12.203: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Sep 27 20:31:15.504: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 20:31:15.563: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-3649" for this suite.
•{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]","total":305,"completed":168,"skipped":2657,"failed":0}
SSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group but different versions [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 20:31:15.610: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of same group but different versions [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation
Sep 27 20:31:15.775: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation
Sep 27 20:31:46.625: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
Sep 27 20:31:56.824: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 20:32:34.907: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-284" for this suite.

• [SLOW TEST:79.343 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group but different versions [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group but different versions [Conformance]","total":305,"completed":169,"skipped":2664,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 20:32:34.953: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Sep 27 20:32:35.169: INFO: Waiting up to 5m0s for pod "downwardapi-volume-952e5afe-56d7-4012-adf0-ffac9641a7b3" in namespace "downward-api-2887" to be "Succeeded or Failed"
Sep 27 20:32:35.185: INFO: Pod "downwardapi-volume-952e5afe-56d7-4012-adf0-ffac9641a7b3": Phase="Pending", Reason="", readiness=false. Elapsed: 16.022992ms
Sep 27 20:32:37.201: INFO: Pod "downwardapi-volume-952e5afe-56d7-4012-adf0-ffac9641a7b3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.031347722s
STEP: Saw pod success
Sep 27 20:32:37.201: INFO: Pod "downwardapi-volume-952e5afe-56d7-4012-adf0-ffac9641a7b3" satisfied condition "Succeeded or Failed"
Sep 27 20:32:37.219: INFO: Trying to get logs from node 10.177.248.117 pod downwardapi-volume-952e5afe-56d7-4012-adf0-ffac9641a7b3 container client-container: <nil>
STEP: delete the pod
Sep 27 20:32:37.324: INFO: Waiting for pod downwardapi-volume-952e5afe-56d7-4012-adf0-ffac9641a7b3 to disappear
Sep 27 20:32:37.337: INFO: Pod downwardapi-volume-952e5afe-56d7-4012-adf0-ffac9641a7b3 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 20:32:37.337: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2887" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","total":305,"completed":170,"skipped":2682,"failed":0}
SS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 20:32:37.379: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Pod that fits quota
STEP: Ensuring ResourceQuota status captures the pod usage
STEP: Not allowing a pod to be created that exceeds remaining quota
STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources)
STEP: Ensuring a pod cannot update its resource requirements
STEP: Ensuring attempts to update pod resource requirements did not change quota usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 20:32:50.798: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-7542" for this suite.

• [SLOW TEST:13.462 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a pod. [Conformance]","total":305,"completed":171,"skipped":2684,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected combined 
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected combined
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 20:32:50.842: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name configmap-projected-all-test-volume-5d5a9415-cccb-4b3d-bc80-bb87a72054c0
STEP: Creating secret with name secret-projected-all-test-volume-896dc515-7772-4368-b0ca-d3b1c27c3441
STEP: Creating a pod to test Check all projections for projected volume plugin
Sep 27 20:32:51.072: INFO: Waiting up to 5m0s for pod "projected-volume-b8c74c28-09af-4ed8-b586-71dbf13d9737" in namespace "projected-6031" to be "Succeeded or Failed"
Sep 27 20:32:51.087: INFO: Pod "projected-volume-b8c74c28-09af-4ed8-b586-71dbf13d9737": Phase="Pending", Reason="", readiness=false. Elapsed: 15.518851ms
Sep 27 20:32:53.105: INFO: Pod "projected-volume-b8c74c28-09af-4ed8-b586-71dbf13d9737": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.033253617s
STEP: Saw pod success
Sep 27 20:32:53.105: INFO: Pod "projected-volume-b8c74c28-09af-4ed8-b586-71dbf13d9737" satisfied condition "Succeeded or Failed"
Sep 27 20:32:53.120: INFO: Trying to get logs from node 10.177.248.117 pod projected-volume-b8c74c28-09af-4ed8-b586-71dbf13d9737 container projected-all-volume-test: <nil>
STEP: delete the pod
Sep 27 20:32:53.193: INFO: Waiting for pod projected-volume-b8c74c28-09af-4ed8-b586-71dbf13d9737 to disappear
Sep 27 20:32:53.207: INFO: Pod projected-volume-b8c74c28-09af-4ed8-b586-71dbf13d9737 no longer exists
[AfterEach] [sig-storage] Projected combined
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 20:32:53.207: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6031" for this suite.
•{"msg":"PASSED [sig-storage] Projected combined should project all components that make up the projection API [Projection][NodeConformance] [Conformance]","total":305,"completed":172,"skipped":2704,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 20:32:53.254: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Sep 27 20:32:53.429: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
STEP: Creating secret with name s-test-opt-del-d7df022c-ab40-41a7-912f-cb8a6e4b558f
STEP: Creating secret with name s-test-opt-upd-d70f5997-50cd-48c1-b9eb-15d8761c9dc9
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-d7df022c-ab40-41a7-912f-cb8a6e4b558f
STEP: Updating secret s-test-opt-upd-d70f5997-50cd-48c1-b9eb-15d8761c9dc9
STEP: Creating secret with name s-test-opt-create-740cd8b9-6dce-4b13-a392-00d565f9113a
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 20:34:25.675: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3730" for this suite.

• [SLOW TEST:92.469 seconds]
[sig-storage] Secrets
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:36
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Secrets optional updates should be reflected in volume [NodeConformance] [Conformance]","total":305,"completed":173,"skipped":2719,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 20:34:25.733: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir volume type on node default medium
Sep 27 20:34:25.922: INFO: Waiting up to 5m0s for pod "pod-843d1a4f-9242-4a28-b9ab-849c5bb5692c" in namespace "emptydir-25" to be "Succeeded or Failed"
Sep 27 20:34:25.936: INFO: Pod "pod-843d1a4f-9242-4a28-b9ab-849c5bb5692c": Phase="Pending", Reason="", readiness=false. Elapsed: 13.313285ms
Sep 27 20:34:27.964: INFO: Pod "pod-843d1a4f-9242-4a28-b9ab-849c5bb5692c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.041475597s
Sep 27 20:34:29.981: INFO: Pod "pod-843d1a4f-9242-4a28-b9ab-849c5bb5692c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.058842396s
STEP: Saw pod success
Sep 27 20:34:29.982: INFO: Pod "pod-843d1a4f-9242-4a28-b9ab-849c5bb5692c" satisfied condition "Succeeded or Failed"
Sep 27 20:34:30.000: INFO: Trying to get logs from node 10.177.248.114 pod pod-843d1a4f-9242-4a28-b9ab-849c5bb5692c container test-container: <nil>
STEP: delete the pod
Sep 27 20:34:31.163: INFO: Waiting for pod pod-843d1a4f-9242-4a28-b9ab-849c5bb5692c to disappear
Sep 27 20:34:31.183: INFO: Pod pod-843d1a4f-9242-4a28-b9ab-849c5bb5692c no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 20:34:31.183: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-25" for this suite.

• [SLOW TEST:5.500 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:42
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":174,"skipped":2795,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 20:34:31.233: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0777 on tmpfs
Sep 27 20:34:31.443: INFO: Waiting up to 5m0s for pod "pod-c571ae14-5adc-44e4-8b46-5e0d15a5b808" in namespace "emptydir-8070" to be "Succeeded or Failed"
Sep 27 20:34:31.458: INFO: Pod "pod-c571ae14-5adc-44e4-8b46-5e0d15a5b808": Phase="Pending", Reason="", readiness=false. Elapsed: 14.961853ms
Sep 27 20:34:33.473: INFO: Pod "pod-c571ae14-5adc-44e4-8b46-5e0d15a5b808": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030311531s
Sep 27 20:34:35.490: INFO: Pod "pod-c571ae14-5adc-44e4-8b46-5e0d15a5b808": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.046958905s
STEP: Saw pod success
Sep 27 20:34:35.490: INFO: Pod "pod-c571ae14-5adc-44e4-8b46-5e0d15a5b808" satisfied condition "Succeeded or Failed"
Sep 27 20:34:35.510: INFO: Trying to get logs from node 10.177.248.114 pod pod-c571ae14-5adc-44e4-8b46-5e0d15a5b808 container test-container: <nil>
STEP: delete the pod
Sep 27 20:34:35.595: INFO: Waiting for pod pod-c571ae14-5adc-44e4-8b46-5e0d15a5b808 to disappear
Sep 27 20:34:35.608: INFO: Pod pod-c571ae14-5adc-44e4-8b46-5e0d15a5b808 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 20:34:35.608: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8070" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":175,"skipped":2816,"failed":0}
SSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 20:34:35.658: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:82
[It] should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 20:34:35.866: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-4799" for this suite.
•{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox command that always fails in a pod should be possible to delete [NodeConformance] [Conformance]","total":305,"completed":176,"skipped":2820,"failed":0}
SSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 20:34:35.925: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:162
[It] should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod
Sep 27 20:34:36.052: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 20:34:40.217: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-960" for this suite.
•{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should invoke init containers on a RestartNever pod [Conformance]","total":305,"completed":177,"skipped":2831,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 20:34:40.272: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount an API token into pods  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: getting the auto-created API token
STEP: reading a file in the container
Sep 27 20:34:45.066: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-4189 pod-service-account-1e373690-2d1e-41dd-a8b0-49c3e6dc4e61 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container
Sep 27 20:34:45.526: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-4189 pod-service-account-1e373690-2d1e-41dd-a8b0-49c3e6dc4e61 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container
Sep 27 20:34:45.929: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-4189 pod-service-account-1e373690-2d1e-41dd-a8b0-49c3e6dc4e61 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 20:34:46.374: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-4189" for this suite.

• [SLOW TEST:6.155 seconds]
[sig-auth] ServiceAccounts
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:23
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-auth] ServiceAccounts should mount an API token into pods  [Conformance]","total":305,"completed":178,"skipped":2844,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 20:34:46.427: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Sep 27 20:34:51.810: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Sep 27 20:34:51.828: INFO: Pod pod-with-prestop-http-hook still exists
Sep 27 20:34:53.828: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Sep 27 20:34:53.842: INFO: Pod pod-with-prestop-http-hook still exists
Sep 27 20:34:55.828: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Sep 27 20:34:55.844: INFO: Pod pod-with-prestop-http-hook still exists
Sep 27 20:34:57.828: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Sep 27 20:34:57.843: INFO: Pod pod-with-prestop-http-hook still exists
Sep 27 20:34:59.828: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Sep 27 20:34:59.844: INFO: Pod pod-with-prestop-http-hook still exists
Sep 27 20:35:01.828: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Sep 27 20:35:01.849: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 20:35:01.886: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-6554" for this suite.

• [SLOW TEST:15.513 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  when create a pod with lifecycle hook
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute prestop http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop http hook properly [NodeConformance] [Conformance]","total":305,"completed":179,"skipped":2858,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 20:35:01.940: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating secret secrets-6072/secret-test-81656bf1-45b3-4f92-897f-d0cdba1ebb37
STEP: Creating a pod to test consume secrets
Sep 27 20:35:02.200: INFO: Waiting up to 5m0s for pod "pod-configmaps-fcdb19e6-8ca6-4968-99bb-4c80131963e5" in namespace "secrets-6072" to be "Succeeded or Failed"
Sep 27 20:35:02.222: INFO: Pod "pod-configmaps-fcdb19e6-8ca6-4968-99bb-4c80131963e5": Phase="Pending", Reason="", readiness=false. Elapsed: 21.837634ms
Sep 27 20:35:04.237: INFO: Pod "pod-configmaps-fcdb19e6-8ca6-4968-99bb-4c80131963e5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.037325434s
Sep 27 20:35:06.252: INFO: Pod "pod-configmaps-fcdb19e6-8ca6-4968-99bb-4c80131963e5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.051697371s
STEP: Saw pod success
Sep 27 20:35:06.252: INFO: Pod "pod-configmaps-fcdb19e6-8ca6-4968-99bb-4c80131963e5" satisfied condition "Succeeded or Failed"
Sep 27 20:35:06.266: INFO: Trying to get logs from node 10.177.248.117 pod pod-configmaps-fcdb19e6-8ca6-4968-99bb-4c80131963e5 container env-test: <nil>
STEP: delete the pod
Sep 27 20:35:06.348: INFO: Waiting for pod pod-configmaps-fcdb19e6-8ca6-4968-99bb-4c80131963e5 to disappear
Sep 27 20:35:06.365: INFO: Pod pod-configmaps-fcdb19e6-8ca6-4968-99bb-4c80131963e5 no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 20:35:06.365: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6072" for this suite.
•{"msg":"PASSED [sig-api-machinery] Secrets should be consumable via the environment [NodeConformance] [Conformance]","total":305,"completed":180,"skipped":2892,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 20:35:06.411: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating the pod
Sep 27 20:35:09.225: INFO: Successfully updated pod "labelsupdate473f5677-f0e4-4dc0-95ed-4e6a8aba10ea"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 20:35:11.294: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8540" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should update labels on modification [NodeConformance] [Conformance]","total":305,"completed":181,"skipped":2927,"failed":0}

------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 20:35:11.339: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a service in the namespace
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there is no service in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 20:35:19.973: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-4145" for this suite.
STEP: Destroying namespace "nsdeletetest-7773" for this suite.
Sep 27 20:35:20.040: INFO: Namespace nsdeletetest-7773 was already deleted
STEP: Destroying namespace "nsdeletetest-583" for this suite.

• [SLOW TEST:8.723 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all services are removed when a namespace is deleted [Conformance]","total":305,"completed":182,"skipped":2927,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 20:35:20.062: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Sep 27 20:35:20.247: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
STEP: Creating projection with configMap that has name projected-configmap-test-upd-fa43434c-5e7f-491c-8fe8-4ceac36ae1ad
STEP: Creating the pod
STEP: Updating configmap projected-configmap-test-upd-fa43434c-5e7f-491c-8fe8-4ceac36ae1ad
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 20:36:33.864: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2621" for this suite.

• [SLOW TEST:73.844 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:36
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap updates should be reflected in volume [NodeConformance] [Conformance]","total":305,"completed":183,"skipped":2942,"failed":0}
SS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 20:36:33.906: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod pod-subpath-test-projected-c8w7
STEP: Creating a pod to test atomic-volume-subpath
Sep 27 20:36:34.149: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-c8w7" in namespace "subpath-2926" to be "Succeeded or Failed"
Sep 27 20:36:34.163: INFO: Pod "pod-subpath-test-projected-c8w7": Phase="Pending", Reason="", readiness=false. Elapsed: 13.297733ms
Sep 27 20:36:36.177: INFO: Pod "pod-subpath-test-projected-c8w7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027821029s
Sep 27 20:36:38.192: INFO: Pod "pod-subpath-test-projected-c8w7": Phase="Running", Reason="", readiness=true. Elapsed: 4.042373657s
Sep 27 20:36:40.207: INFO: Pod "pod-subpath-test-projected-c8w7": Phase="Running", Reason="", readiness=true. Elapsed: 6.057479637s
Sep 27 20:36:42.222: INFO: Pod "pod-subpath-test-projected-c8w7": Phase="Running", Reason="", readiness=true. Elapsed: 8.072700562s
Sep 27 20:36:44.241: INFO: Pod "pod-subpath-test-projected-c8w7": Phase="Running", Reason="", readiness=true. Elapsed: 10.091362738s
Sep 27 20:36:46.255: INFO: Pod "pod-subpath-test-projected-c8w7": Phase="Running", Reason="", readiness=true. Elapsed: 12.106101037s
Sep 27 20:36:48.273: INFO: Pod "pod-subpath-test-projected-c8w7": Phase="Running", Reason="", readiness=true. Elapsed: 14.1237915s
Sep 27 20:36:50.287: INFO: Pod "pod-subpath-test-projected-c8w7": Phase="Running", Reason="", readiness=true. Elapsed: 16.137583183s
Sep 27 20:36:52.303: INFO: Pod "pod-subpath-test-projected-c8w7": Phase="Running", Reason="", readiness=true. Elapsed: 18.153464364s
Sep 27 20:36:54.320: INFO: Pod "pod-subpath-test-projected-c8w7": Phase="Running", Reason="", readiness=true. Elapsed: 20.170896723s
Sep 27 20:36:56.333: INFO: Pod "pod-subpath-test-projected-c8w7": Phase="Running", Reason="", readiness=true. Elapsed: 22.184045671s
Sep 27 20:36:58.348: INFO: Pod "pod-subpath-test-projected-c8w7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.198701035s
STEP: Saw pod success
Sep 27 20:36:58.348: INFO: Pod "pod-subpath-test-projected-c8w7" satisfied condition "Succeeded or Failed"
Sep 27 20:36:58.367: INFO: Trying to get logs from node 10.177.248.117 pod pod-subpath-test-projected-c8w7 container test-container-subpath-projected-c8w7: <nil>
STEP: delete the pod
Sep 27 20:36:58.440: INFO: Waiting for pod pod-subpath-test-projected-c8w7 to disappear
Sep 27 20:36:58.453: INFO: Pod pod-subpath-test-projected-c8w7 no longer exists
STEP: Deleting pod pod-subpath-test-projected-c8w7
Sep 27 20:36:58.453: INFO: Deleting pod "pod-subpath-test-projected-c8w7" in namespace "subpath-2926"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 20:36:58.465: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-2926" for this suite.

• [SLOW TEST:24.608 seconds]
[sig-storage] Subpath
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with projected pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with projected pod [LinuxOnly] [Conformance]","total":305,"completed":184,"skipped":2944,"failed":0}
SSSSSS
------------------------------
[sig-network] Services 
  should test the lifecycle of an Endpoint [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 20:36:58.513: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should test the lifecycle of an Endpoint [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating an Endpoint
STEP: waiting for available Endpoint
STEP: listing all Endpoints
STEP: updating the Endpoint
STEP: fetching the Endpoint
STEP: patching the Endpoint
STEP: fetching the Endpoint
STEP: deleting the Endpoint by Collection
STEP: waiting for Endpoint deletion
STEP: fetching the Endpoint
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 20:36:58.913: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1889" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786
•{"msg":"PASSED [sig-network] Services should test the lifecycle of an Endpoint [Conformance]","total":305,"completed":185,"skipped":2950,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl diff 
  should check if kubectl diff finds a difference for Deployments [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 20:36:58.983: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[It] should check if kubectl diff finds a difference for Deployments [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create deployment with httpd image
Sep 27 20:36:59.210: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-679324930 create -f -'
Sep 27 20:37:00.146: INFO: stderr: ""
Sep 27 20:37:00.146: INFO: stdout: "deployment.apps/httpd-deployment created\n"
STEP: verify diff finds difference between live and declared image
Sep 27 20:37:00.146: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-679324930 diff -f -'
Sep 27 20:37:01.159: INFO: rc: 1
Sep 27 20:37:01.159: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-679324930 delete -f -'
Sep 27 20:37:01.315: INFO: stderr: ""
Sep 27 20:37:01.315: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 20:37:01.316: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7620" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl diff should check if kubectl diff finds a difference for Deployments [Conformance]","total":305,"completed":186,"skipped":2986,"failed":0}
S
------------------------------
[sig-scheduling] LimitRange 
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-scheduling] LimitRange
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 20:37:01.366: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename limitrange
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a LimitRange
STEP: Setting up watch
STEP: Submitting a LimitRange
Sep 27 20:37:01.561: INFO: observed the limitRanges list
STEP: Verifying LimitRange creation was observed
STEP: Fetching the LimitRange to ensure it has proper values
Sep 27 20:37:01.584: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Sep 27 20:37:01.584: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with no resource requirements
STEP: Ensuring Pod has resource requirements applied from LimitRange
Sep 27 20:37:01.648: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Sep 27 20:37:01.648: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with partial resource requirements
STEP: Ensuring Pod has merged resource requirements applied from LimitRange
Sep 27 20:37:01.702: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
Sep 27 20:37:01.702: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Failing to create a Pod with less than min resources
STEP: Failing to create a Pod with more than max resources
STEP: Updating a LimitRange
STEP: Verifying LimitRange updating is effective
STEP: Creating a Pod with less than former min resources
STEP: Failing to create a Pod with more than max resources
STEP: Deleting a LimitRange
STEP: Verifying the LimitRange was deleted
Sep 27 20:37:08.907: INFO: limitRange is already deleted
STEP: Creating a Pod with more than former max resources
[AfterEach] [sig-scheduling] LimitRange
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 20:37:08.966: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "limitrange-2736" for this suite.

• [SLOW TEST:7.643 seconds]
[sig-scheduling] LimitRange
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-scheduling] LimitRange should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]","total":305,"completed":187,"skipped":2987,"failed":0}
SS
------------------------------
[sig-apps] ReplicationController 
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 20:37:09.010: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should release no longer matching pods [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Given a ReplicationController is created
STEP: When the matched label of one of its pods change
Sep 27 20:37:09.289: INFO: Pod name pod-release: Found 0 pods out of 1
Sep 27 20:37:14.303: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 20:37:15.398: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-9585" for this suite.

• [SLOW TEST:6.438 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should release no longer matching pods [Conformance]","total":305,"completed":188,"skipped":2989,"failed":0}
SSSSSS
------------------------------
[sig-apps] ReplicationController 
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 20:37:15.452: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Given a Pod with a 'name' label pod-adoption is created
STEP: When a replication controller with a matching selector is created
STEP: Then the orphan pod is adopted
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 20:37:18.763: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-4382" for this suite.
•{"msg":"PASSED [sig-apps] ReplicationController should adopt matching pods on creation [Conformance]","total":305,"completed":189,"skipped":2995,"failed":0}
SSSS
------------------------------
[sig-storage] Projected secret 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 20:37:18.809: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Sep 27 20:37:18.998: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
STEP: Creating secret with name s-test-opt-del-e54d8554-0142-43b7-a8d7-55cfe7312e39
STEP: Creating secret with name s-test-opt-upd-2dc774eb-189e-4e4e-8979-400766bef118
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-e54d8554-0142-43b7-a8d7-55cfe7312e39
STEP: Updating secret s-test-opt-upd-2dc774eb-189e-4e4e-8979-400766bef118
STEP: Creating secret with name s-test-opt-create-a3682146-71b6-4b9f-9274-64fe193bcba2
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 20:37:27.522: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6846" for this suite.

• [SLOW TEST:8.773 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Projected secret optional updates should be reflected in volume [NodeConformance] [Conformance]","total":305,"completed":190,"skipped":2999,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  listing custom resource definition objects works  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 20:37:27.586: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] listing custom resource definition objects works  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Sep 27 20:37:27.735: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 20:37:35.174: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-7087" for this suite.

• [SLOW TEST:7.646 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:48
    listing custom resource definition objects works  [Conformance]
    /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition listing custom resource definition objects works  [Conformance]","total":305,"completed":191,"skipped":3013,"failed":0}
SSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 20:37:35.233: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test override command
Sep 27 20:37:35.440: INFO: Waiting up to 5m0s for pod "client-containers-1216ddb8-01d6-430c-961f-060b8592051c" in namespace "containers-387" to be "Succeeded or Failed"
Sep 27 20:37:35.453: INFO: Pod "client-containers-1216ddb8-01d6-430c-961f-060b8592051c": Phase="Pending", Reason="", readiness=false. Elapsed: 12.423826ms
Sep 27 20:37:37.470: INFO: Pod "client-containers-1216ddb8-01d6-430c-961f-060b8592051c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029110383s
Sep 27 20:37:39.501: INFO: Pod "client-containers-1216ddb8-01d6-430c-961f-060b8592051c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.060575667s
STEP: Saw pod success
Sep 27 20:37:39.501: INFO: Pod "client-containers-1216ddb8-01d6-430c-961f-060b8592051c" satisfied condition "Succeeded or Failed"
Sep 27 20:37:39.516: INFO: Trying to get logs from node 10.177.248.117 pod client-containers-1216ddb8-01d6-430c-961f-060b8592051c container test-container: <nil>
STEP: delete the pod
Sep 27 20:37:39.594: INFO: Waiting for pod client-containers-1216ddb8-01d6-430c-961f-060b8592051c to disappear
Sep 27 20:37:39.607: INFO: Pod client-containers-1216ddb8-01d6-430c-961f-060b8592051c no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 20:37:39.608: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-387" for this suite.
•{"msg":"PASSED [k8s.io] Docker Containers should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]","total":305,"completed":192,"skipped":3018,"failed":0}
SSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 20:37:39.654: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicaSet
STEP: Ensuring resource quota status captures replicaset creation
STEP: Deleting a ReplicaSet
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 20:37:51.003: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-9482" for this suite.

• [SLOW TEST:11.392 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replica set. [Conformance]","total":305,"completed":193,"skipped":3022,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 20:37:51.048: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Sep 27 20:37:51.201: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
STEP: Creating rc "condition-test" that asks for more than the allowed pod quota
STEP: Checking rc "condition-test" has the desired failure condition set
STEP: Scaling down rc "condition-test" to satisfy pod quota
Sep 27 20:37:53.321: INFO: Updating replication controller "condition-test"
STEP: Checking rc "condition-test" has no failure condition set
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 20:37:54.348: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-3394" for this suite.
•{"msg":"PASSED [sig-apps] ReplicationController should surface a failure condition on a common issue like exceeded quota [Conformance]","total":305,"completed":194,"skipped":3051,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 20:37:54.395: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating service in namespace services-8102
STEP: creating service affinity-nodeport in namespace services-8102
STEP: creating replication controller affinity-nodeport in namespace services-8102
I0927 20:37:54.624755      22 runners.go:190] Created replication controller with name: affinity-nodeport, namespace: services-8102, replica count: 3
I0927 20:37:57.677946      22 runners.go:190] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Sep 27 20:37:57.732: INFO: Creating new exec pod
Sep 27 20:38:02.828: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-679324930 exec --namespace=services-8102 execpod-affinityrxmf2 -- /bin/sh -x -c nc -zv -t -w 2 affinity-nodeport 80'
Sep 27 20:38:03.258: INFO: stderr: "+ nc -zv -t -w 2 affinity-nodeport 80\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
Sep 27 20:38:03.258: INFO: stdout: ""
Sep 27 20:38:03.258: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-679324930 exec --namespace=services-8102 execpod-affinityrxmf2 -- /bin/sh -x -c nc -zv -t -w 2 172.21.84.21 80'
Sep 27 20:38:03.659: INFO: stderr: "+ nc -zv -t -w 2 172.21.84.21 80\nConnection to 172.21.84.21 80 port [tcp/http] succeeded!\n"
Sep 27 20:38:03.659: INFO: stdout: ""
Sep 27 20:38:03.660: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-679324930 exec --namespace=services-8102 execpod-affinityrxmf2 -- /bin/sh -x -c nc -zv -t -w 2 10.177.248.114 30253'
Sep 27 20:38:04.083: INFO: stderr: "+ nc -zv -t -w 2 10.177.248.114 30253\nConnection to 10.177.248.114 30253 port [tcp/30253] succeeded!\n"
Sep 27 20:38:04.083: INFO: stdout: ""
Sep 27 20:38:04.083: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-679324930 exec --namespace=services-8102 execpod-affinityrxmf2 -- /bin/sh -x -c nc -zv -t -w 2 10.177.248.117 30253'
Sep 27 20:38:04.546: INFO: stderr: "+ nc -zv -t -w 2 10.177.248.117 30253\nConnection to 10.177.248.117 30253 port [tcp/30253] succeeded!\n"
Sep 27 20:38:04.546: INFO: stdout: ""
Sep 27 20:38:04.546: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-679324930 exec --namespace=services-8102 execpod-affinityrxmf2 -- /bin/sh -x -c nc -zv -t -w 2 169.61.243.2 30253'
Sep 27 20:38:04.911: INFO: stderr: "+ nc -zv -t -w 2 169.61.243.2 30253\nConnection to 169.61.243.2 30253 port [tcp/30253] succeeded!\n"
Sep 27 20:38:04.911: INFO: stdout: ""
Sep 27 20:38:04.912: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-679324930 exec --namespace=services-8102 execpod-affinityrxmf2 -- /bin/sh -x -c nc -zv -t -w 2 169.61.243.3 30253'
Sep 27 20:38:05.439: INFO: stderr: "+ nc -zv -t -w 2 169.61.243.3 30253\nConnection to 169.61.243.3 30253 port [tcp/30253] succeeded!\n"
Sep 27 20:38:05.439: INFO: stdout: ""
Sep 27 20:38:05.439: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-679324930 exec --namespace=services-8102 execpod-affinityrxmf2 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.177.248.114:30253/ ; done'
Sep 27 20:38:05.983: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.177.248.114:30253/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.177.248.114:30253/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.177.248.114:30253/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.177.248.114:30253/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.177.248.114:30253/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.177.248.114:30253/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.177.248.114:30253/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.177.248.114:30253/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.177.248.114:30253/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.177.248.114:30253/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.177.248.114:30253/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.177.248.114:30253/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.177.248.114:30253/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.177.248.114:30253/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.177.248.114:30253/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.177.248.114:30253/\n"
Sep 27 20:38:05.983: INFO: stdout: "\naffinity-nodeport-sdff6\naffinity-nodeport-sdff6\naffinity-nodeport-sdff6\naffinity-nodeport-sdff6\naffinity-nodeport-sdff6\naffinity-nodeport-sdff6\naffinity-nodeport-sdff6\naffinity-nodeport-sdff6\naffinity-nodeport-sdff6\naffinity-nodeport-sdff6\naffinity-nodeport-sdff6\naffinity-nodeport-sdff6\naffinity-nodeport-sdff6\naffinity-nodeport-sdff6\naffinity-nodeport-sdff6\naffinity-nodeport-sdff6"
Sep 27 20:38:05.983: INFO: Received response from host: affinity-nodeport-sdff6
Sep 27 20:38:05.983: INFO: Received response from host: affinity-nodeport-sdff6
Sep 27 20:38:05.983: INFO: Received response from host: affinity-nodeport-sdff6
Sep 27 20:38:05.983: INFO: Received response from host: affinity-nodeport-sdff6
Sep 27 20:38:05.983: INFO: Received response from host: affinity-nodeport-sdff6
Sep 27 20:38:05.983: INFO: Received response from host: affinity-nodeport-sdff6
Sep 27 20:38:05.983: INFO: Received response from host: affinity-nodeport-sdff6
Sep 27 20:38:05.983: INFO: Received response from host: affinity-nodeport-sdff6
Sep 27 20:38:05.983: INFO: Received response from host: affinity-nodeport-sdff6
Sep 27 20:38:05.983: INFO: Received response from host: affinity-nodeport-sdff6
Sep 27 20:38:05.983: INFO: Received response from host: affinity-nodeport-sdff6
Sep 27 20:38:05.983: INFO: Received response from host: affinity-nodeport-sdff6
Sep 27 20:38:05.983: INFO: Received response from host: affinity-nodeport-sdff6
Sep 27 20:38:05.983: INFO: Received response from host: affinity-nodeport-sdff6
Sep 27 20:38:05.983: INFO: Received response from host: affinity-nodeport-sdff6
Sep 27 20:38:05.983: INFO: Received response from host: affinity-nodeport-sdff6
Sep 27 20:38:05.983: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport in namespace services-8102, will wait for the garbage collector to delete the pods
Sep 27 20:38:06.142: INFO: Deleting ReplicationController affinity-nodeport took: 29.171251ms
Sep 27 20:38:06.442: INFO: Terminating ReplicationController affinity-nodeport pods took: 300.247088ms
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 20:38:21.935: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8102" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

• [SLOW TEST:27.586 seconds]
[sig-network] Services
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity work for NodePort service [LinuxOnly] [Conformance]","total":305,"completed":195,"skipped":3069,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 20:38:21.982: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Sep 27 20:38:22.188: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
STEP: Creating configMap with name configmap-test-upd-ea87a327-870c-417e-92ce-a45fde96133b
STEP: Creating the pod
STEP: Updating configmap configmap-test-upd-ea87a327-870c-417e-92ce-a45fde96133b
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 20:38:26.448: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2262" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap updates should be reflected in volume [NodeConformance] [Conformance]","total":305,"completed":196,"skipped":3084,"failed":0}
SSSSS
------------------------------
[sig-network] Services 
  should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 20:38:26.512: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating service in namespace services-1825
Sep 27 20:38:28.790: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-679324930 exec --namespace=services-1825 kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
Sep 27 20:38:29.209: INFO: rc: 7
Sep 27 20:38:29.234: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Sep 27 20:38:29.252: INFO: Pod kube-proxy-mode-detector still exists
Sep 27 20:38:31.252: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Sep 27 20:38:31.266: INFO: Pod kube-proxy-mode-detector still exists
Sep 27 20:38:33.252: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Sep 27 20:38:33.265: INFO: Pod kube-proxy-mode-detector still exists
Sep 27 20:38:35.254: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Sep 27 20:38:35.272: INFO: Pod kube-proxy-mode-detector still exists
Sep 27 20:38:37.252: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Sep 27 20:38:37.266: INFO: Pod kube-proxy-mode-detector still exists
Sep 27 20:38:39.252: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Sep 27 20:38:39.267: INFO: Pod kube-proxy-mode-detector still exists
Sep 27 20:38:41.252: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Sep 27 20:38:41.269: INFO: Pod kube-proxy-mode-detector still exists
Sep 27 20:38:43.252: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Sep 27 20:38:43.267: INFO: Pod kube-proxy-mode-detector no longer exists
Sep 27 20:38:43.267: INFO: Couldn't detect KubeProxy mode - test failure may be expected: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-679324930 exec --namespace=services-1825 kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode:
Command stdout:

stderr:
+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode
command terminated with exit code 7

error:
exit status 7
STEP: creating service affinity-nodeport-timeout in namespace services-1825
STEP: creating replication controller affinity-nodeport-timeout in namespace services-1825
I0927 20:38:43.350384      22 runners.go:190] Created replication controller with name: affinity-nodeport-timeout, namespace: services-1825, replica count: 3
I0927 20:38:46.401052      22 runners.go:190] affinity-nodeport-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Sep 27 20:38:46.463: INFO: Creating new exec pod
Sep 27 20:38:49.568: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-679324930 exec --namespace=services-1825 execpod-affinitycnmqf -- /bin/sh -x -c nc -zv -t -w 2 affinity-nodeport-timeout 80'
Sep 27 20:38:49.980: INFO: stderr: "+ nc -zv -t -w 2 affinity-nodeport-timeout 80\nConnection to affinity-nodeport-timeout 80 port [tcp/http] succeeded!\n"
Sep 27 20:38:49.980: INFO: stdout: ""
Sep 27 20:38:49.981: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-679324930 exec --namespace=services-1825 execpod-affinitycnmqf -- /bin/sh -x -c nc -zv -t -w 2 172.21.243.42 80'
Sep 27 20:38:50.394: INFO: stderr: "+ nc -zv -t -w 2 172.21.243.42 80\nConnection to 172.21.243.42 80 port [tcp/http] succeeded!\n"
Sep 27 20:38:50.394: INFO: stdout: ""
Sep 27 20:38:50.394: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-679324930 exec --namespace=services-1825 execpod-affinitycnmqf -- /bin/sh -x -c nc -zv -t -w 2 10.177.248.126 31154'
Sep 27 20:38:50.821: INFO: stderr: "+ nc -zv -t -w 2 10.177.248.126 31154\nConnection to 10.177.248.126 31154 port [tcp/31154] succeeded!\n"
Sep 27 20:38:50.821: INFO: stdout: ""
Sep 27 20:38:50.821: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-679324930 exec --namespace=services-1825 execpod-affinitycnmqf -- /bin/sh -x -c nc -zv -t -w 2 10.177.248.114 31154'
Sep 27 20:38:51.218: INFO: stderr: "+ nc -zv -t -w 2 10.177.248.114 31154\nConnection to 10.177.248.114 31154 port [tcp/31154] succeeded!\n"
Sep 27 20:38:51.218: INFO: stdout: ""
Sep 27 20:38:51.218: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-679324930 exec --namespace=services-1825 execpod-affinitycnmqf -- /bin/sh -x -c nc -zv -t -w 2 169.61.243.7 31154'
Sep 27 20:38:51.618: INFO: stderr: "+ nc -zv -t -w 2 169.61.243.7 31154\nConnection to 169.61.243.7 31154 port [tcp/31154] succeeded!\n"
Sep 27 20:38:51.618: INFO: stdout: ""
Sep 27 20:38:51.618: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-679324930 exec --namespace=services-1825 execpod-affinitycnmqf -- /bin/sh -x -c nc -zv -t -w 2 169.61.243.2 31154'
Sep 27 20:38:52.067: INFO: stderr: "+ nc -zv -t -w 2 169.61.243.2 31154\nConnection to 169.61.243.2 31154 port [tcp/31154] succeeded!\n"
Sep 27 20:38:52.067: INFO: stdout: ""
Sep 27 20:38:52.067: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-679324930 exec --namespace=services-1825 execpod-affinitycnmqf -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.177.248.114:31154/ ; done'
Sep 27 20:38:52.609: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.177.248.114:31154/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.177.248.114:31154/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.177.248.114:31154/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.177.248.114:31154/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.177.248.114:31154/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.177.248.114:31154/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.177.248.114:31154/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.177.248.114:31154/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.177.248.114:31154/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.177.248.114:31154/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.177.248.114:31154/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.177.248.114:31154/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.177.248.114:31154/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.177.248.114:31154/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.177.248.114:31154/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.177.248.114:31154/\n"
Sep 27 20:38:52.609: INFO: stdout: "\naffinity-nodeport-timeout-989wd\naffinity-nodeport-timeout-989wd\naffinity-nodeport-timeout-989wd\naffinity-nodeport-timeout-989wd\naffinity-nodeport-timeout-989wd\naffinity-nodeport-timeout-989wd\naffinity-nodeport-timeout-989wd\naffinity-nodeport-timeout-989wd\naffinity-nodeport-timeout-989wd\naffinity-nodeport-timeout-989wd\naffinity-nodeport-timeout-989wd\naffinity-nodeport-timeout-989wd\naffinity-nodeport-timeout-989wd\naffinity-nodeport-timeout-989wd\naffinity-nodeport-timeout-989wd\naffinity-nodeport-timeout-989wd"
Sep 27 20:38:52.609: INFO: Received response from host: affinity-nodeport-timeout-989wd
Sep 27 20:38:52.609: INFO: Received response from host: affinity-nodeport-timeout-989wd
Sep 27 20:38:52.609: INFO: Received response from host: affinity-nodeport-timeout-989wd
Sep 27 20:38:52.609: INFO: Received response from host: affinity-nodeport-timeout-989wd
Sep 27 20:38:52.609: INFO: Received response from host: affinity-nodeport-timeout-989wd
Sep 27 20:38:52.609: INFO: Received response from host: affinity-nodeport-timeout-989wd
Sep 27 20:38:52.609: INFO: Received response from host: affinity-nodeport-timeout-989wd
Sep 27 20:38:52.609: INFO: Received response from host: affinity-nodeport-timeout-989wd
Sep 27 20:38:52.609: INFO: Received response from host: affinity-nodeport-timeout-989wd
Sep 27 20:38:52.609: INFO: Received response from host: affinity-nodeport-timeout-989wd
Sep 27 20:38:52.609: INFO: Received response from host: affinity-nodeport-timeout-989wd
Sep 27 20:38:52.609: INFO: Received response from host: affinity-nodeport-timeout-989wd
Sep 27 20:38:52.609: INFO: Received response from host: affinity-nodeport-timeout-989wd
Sep 27 20:38:52.609: INFO: Received response from host: affinity-nodeport-timeout-989wd
Sep 27 20:38:52.609: INFO: Received response from host: affinity-nodeport-timeout-989wd
Sep 27 20:38:52.609: INFO: Received response from host: affinity-nodeport-timeout-989wd
Sep 27 20:38:52.609: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-679324930 exec --namespace=services-1825 execpod-affinitycnmqf -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.177.248.114:31154/'
Sep 27 20:38:53.036: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.177.248.114:31154/\n"
Sep 27 20:38:53.036: INFO: stdout: "affinity-nodeport-timeout-989wd"
Sep 27 20:39:08.037: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-679324930 exec --namespace=services-1825 execpod-affinitycnmqf -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.177.248.114:31154/'
Sep 27 20:39:08.486: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.177.248.114:31154/\n"
Sep 27 20:39:08.487: INFO: stdout: "affinity-nodeport-timeout-xbkq9"
Sep 27 20:39:08.487: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-timeout in namespace services-1825, will wait for the garbage collector to delete the pods
Sep 27 20:39:08.623: INFO: Deleting ReplicationController affinity-nodeport-timeout took: 26.315755ms
Sep 27 20:39:08.824: INFO: Terminating ReplicationController affinity-nodeport-timeout pods took: 200.307099ms
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 20:39:22.214: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1825" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

• [SLOW TEST:55.759 seconds]
[sig-network] Services
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]","total":305,"completed":197,"skipped":3089,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 20:39:22.271: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Sep 27 20:39:22.476: INFO: Waiting up to 5m0s for pod "downwardapi-volume-3c09466f-738f-4f31-b016-99b21cdb60a0" in namespace "projected-5057" to be "Succeeded or Failed"
Sep 27 20:39:22.497: INFO: Pod "downwardapi-volume-3c09466f-738f-4f31-b016-99b21cdb60a0": Phase="Pending", Reason="", readiness=false. Elapsed: 20.92246ms
Sep 27 20:39:24.511: INFO: Pod "downwardapi-volume-3c09466f-738f-4f31-b016-99b21cdb60a0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.035346824s
STEP: Saw pod success
Sep 27 20:39:24.511: INFO: Pod "downwardapi-volume-3c09466f-738f-4f31-b016-99b21cdb60a0" satisfied condition "Succeeded or Failed"
Sep 27 20:39:24.525: INFO: Trying to get logs from node 10.177.248.117 pod downwardapi-volume-3c09466f-738f-4f31-b016-99b21cdb60a0 container client-container: <nil>
STEP: delete the pod
Sep 27 20:39:24.606: INFO: Waiting for pod downwardapi-volume-3c09466f-738f-4f31-b016-99b21cdb60a0 to disappear
Sep 27 20:39:24.624: INFO: Pod downwardapi-volume-3c09466f-738f-4f31-b016-99b21cdb60a0 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 20:39:24.624: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5057" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu limit [NodeConformance] [Conformance]","total":305,"completed":198,"skipped":3112,"failed":0}
S
------------------------------
[k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial] 
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 20:39:24.665: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename taint-multiple-pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/taints.go:345
Sep 27 20:39:24.815: INFO: Waiting up to 1m0s for all nodes to be ready
Sep 27 20:40:24.974: INFO: Waiting for terminating namespaces to be deleted...
[It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Sep 27 20:40:24.997: INFO: Starting informer...
STEP: Starting pods...
Sep 27 20:40:25.324: INFO: Pod1 is running on 10.177.248.117. Tainting Node
Sep 27 20:40:27.629: INFO: Pod2 is running on 10.177.248.117. Tainting Node
STEP: Trying to apply a taint on the Node
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting for Pod1 and Pod2 to be deleted
Sep 27 20:40:41.863: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
Sep 27 20:41:01.802: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
[AfterEach] [k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 20:41:01.874: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-multiple-pods-8712" for this suite.

• [SLOW TEST:97.265 seconds]
[k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]","total":305,"completed":199,"skipped":3113,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 20:41:01.930: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-6858
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a new StatefulSet
Sep 27 20:41:02.136: INFO: Found 0 stateful pods, waiting for 3
Sep 27 20:41:12.151: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Sep 27 20:41:12.151: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Sep 27 20:41:12.151: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from docker.io/library/httpd:2.4.38-alpine to docker.io/library/httpd:2.4.39-alpine
Sep 27 20:41:12.238: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Not applying an update when the partition is greater than the number of replicas
STEP: Performing a canary update
Sep 27 20:41:22.350: INFO: Updating stateful set ss2
Sep 27 20:41:22.381: INFO: Waiting for Pod statefulset-6858/ss2-2 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
STEP: Restoring Pods to the correct revision when they are deleted
Sep 27 20:41:32.541: INFO: Found 2 stateful pods, waiting for 3
Sep 27 20:41:42.557: INFO: Found 2 stateful pods, waiting for 3
Sep 27 20:41:52.558: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Sep 27 20:41:52.558: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Sep 27 20:41:52.558: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Pending - Ready=false
Sep 27 20:42:02.558: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Sep 27 20:42:02.558: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Sep 27 20:42:02.558: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update
Sep 27 20:42:02.639: INFO: Updating stateful set ss2
Sep 27 20:42:02.678: INFO: Waiting for Pod statefulset-6858/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Sep 27 20:42:12.757: INFO: Updating stateful set ss2
Sep 27 20:42:12.787: INFO: Waiting for StatefulSet statefulset-6858/ss2 to complete update
Sep 27 20:42:12.787: INFO: Waiting for Pod statefulset-6858/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Sep 27 20:42:22.819: INFO: Waiting for StatefulSet statefulset-6858/ss2 to complete update
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
Sep 27 20:42:32.817: INFO: Deleting all statefulset in ns statefulset-6858
Sep 27 20:42:32.844: INFO: Scaling statefulset ss2 to 0
Sep 27 20:42:52.914: INFO: Waiting for statefulset status.replicas updated to 0
Sep 27 20:42:52.928: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 20:42:52.997: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-6858" for this suite.

• [SLOW TEST:111.131 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should perform canary updates and phased rolling updates of template modifications [Conformance]","total":305,"completed":200,"skipped":3141,"failed":0}
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD with validation schema [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 20:42:53.062: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD with validation schema [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Sep 27 20:42:53.219: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: client-side validation (kubectl create and apply) allows request with known and required properties
Sep 27 20:43:01.772: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-679324930 --namespace=crd-publish-openapi-2049 create -f -'
Sep 27 20:43:02.749: INFO: stderr: ""
Sep 27 20:43:02.749: INFO: stdout: "e2e-test-crd-publish-openapi-5019-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Sep 27 20:43:02.749: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-679324930 --namespace=crd-publish-openapi-2049 delete e2e-test-crd-publish-openapi-5019-crds test-foo'
Sep 27 20:43:02.909: INFO: stderr: ""
Sep 27 20:43:02.909: INFO: stdout: "e2e-test-crd-publish-openapi-5019-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
Sep 27 20:43:02.909: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-679324930 --namespace=crd-publish-openapi-2049 apply -f -'
Sep 27 20:43:03.669: INFO: stderr: ""
Sep 27 20:43:03.669: INFO: stdout: "e2e-test-crd-publish-openapi-5019-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Sep 27 20:43:03.670: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-679324930 --namespace=crd-publish-openapi-2049 delete e2e-test-crd-publish-openapi-5019-crds test-foo'
Sep 27 20:43:03.913: INFO: stderr: ""
Sep 27 20:43:03.913: INFO: stdout: "e2e-test-crd-publish-openapi-5019-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
STEP: client-side validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema
Sep 27 20:43:03.913: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-679324930 --namespace=crd-publish-openapi-2049 create -f -'
Sep 27 20:43:04.591: INFO: rc: 1
Sep 27 20:43:04.591: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-679324930 --namespace=crd-publish-openapi-2049 apply -f -'
Sep 27 20:43:05.361: INFO: rc: 1
STEP: client-side validation (kubectl create and apply) rejects request without required properties
Sep 27 20:43:05.361: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-679324930 --namespace=crd-publish-openapi-2049 create -f -'
Sep 27 20:43:05.865: INFO: rc: 1
Sep 27 20:43:05.865: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-679324930 --namespace=crd-publish-openapi-2049 apply -f -'
Sep 27 20:43:06.805: INFO: rc: 1
STEP: kubectl explain works to explain CR properties
Sep 27 20:43:06.805: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-679324930 explain e2e-test-crd-publish-openapi-5019-crds'
Sep 27 20:43:07.491: INFO: stderr: ""
Sep 27 20:43:07.491: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-5019-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
STEP: kubectl explain works to explain CR properties recursively
Sep 27 20:43:07.491: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-679324930 explain e2e-test-crd-publish-openapi-5019-crds.metadata'
Sep 27 20:43:08.010: INFO: stderr: ""
Sep 27 20:43:08.010: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-5019-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   clusterName\t<string>\n     The name of the cluster which the object belongs to. This is used to\n     distinguish resources with same name and namespace in different clusters.\n     This field is not set anywhere right now and apiserver is going to ignore\n     it if set in create or update request.\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC.\n\n     Populated by the system. Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested.\n\n     Populated by the system when a graceful deletion is requested. Read-only.\n     More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server.\n\n     If this field is specified and the generated name exists, the server will\n     NOT return a 409 - instead, it will either return 201 Created or 500 with\n     Reason ServerTimeout indicating a unique name could not be found in the\n     time allotted, and the client should retry (optionally after the time\n     indicated in the Retry-After header).\n\n     Applied only if Name is not specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within which each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty.\n\n     Must be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources.\n\n     Populated by the system. Read-only. Value must be treated as opaque by\n     clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     SelfLink is a URL representing this object. Populated by the system.\n     Read-only.\n\n     DEPRECATED Kubernetes will stop propagating this field in 1.20 release and\n     the field is planned to be removed in 1.21 release.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations.\n\n     Populated by the system. Read-only. More info:\n     http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
Sep 27 20:43:08.014: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-679324930 explain e2e-test-crd-publish-openapi-5019-crds.spec'
Sep 27 20:43:08.791: INFO: stderr: ""
Sep 27 20:43:08.791: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-5019-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
Sep 27 20:43:08.791: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-679324930 explain e2e-test-crd-publish-openapi-5019-crds.spec.bars'
Sep 27 20:43:09.304: INFO: stderr: ""
Sep 27 20:43:09.304: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-5019-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
STEP: kubectl explain works to return error when explain is called on property that doesn't exist
Sep 27 20:43:09.304: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-679324930 explain e2e-test-crd-publish-openapi-5019-crds.spec.bars2'
Sep 27 20:43:10.021: INFO: rc: 1
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 20:43:18.392: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-2049" for this suite.

• [SLOW TEST:25.372 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD with validation schema [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD with validation schema [Conformance]","total":305,"completed":201,"skipped":3141,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-network] Ingress API 
  should support creating Ingress API operations [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Ingress API
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 20:43:18.434: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename ingress
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support creating Ingress API operations [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: getting /apis
STEP: getting /apis/networking.k8s.io
STEP: getting /apis/networking.k8s.iov1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Sep 27 20:43:18.656: INFO: starting watch
STEP: cluster-wide listing
STEP: cluster-wide watching
Sep 27 20:43:18.674: INFO: starting watch
STEP: patching
STEP: updating
Sep 27 20:43:18.711: INFO: waiting for watch events with expected annotations
Sep 27 20:43:18.711: INFO: saw patched and updated annotations
STEP: patching /status
STEP: updating /status
STEP: get /status
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-network] Ingress API
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 20:43:18.866: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ingress-2997" for this suite.
•{"msg":"PASSED [sig-network] Ingress API should support creating Ingress API operations [Conformance]","total":305,"completed":202,"skipped":3153,"failed":0}
SSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 20:43:18.919: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Sep 27 20:43:19.100: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ba319533-2b9c-49c2-91a8-c34131b3b32d" in namespace "downward-api-7502" to be "Succeeded or Failed"
Sep 27 20:43:19.114: INFO: Pod "downwardapi-volume-ba319533-2b9c-49c2-91a8-c34131b3b32d": Phase="Pending", Reason="", readiness=false. Elapsed: 13.093627ms
Sep 27 20:43:21.129: INFO: Pod "downwardapi-volume-ba319533-2b9c-49c2-91a8-c34131b3b32d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.028637738s
STEP: Saw pod success
Sep 27 20:43:21.129: INFO: Pod "downwardapi-volume-ba319533-2b9c-49c2-91a8-c34131b3b32d" satisfied condition "Succeeded or Failed"
Sep 27 20:43:21.142: INFO: Trying to get logs from node 10.177.248.117 pod downwardapi-volume-ba319533-2b9c-49c2-91a8-c34131b3b32d container client-container: <nil>
STEP: delete the pod
Sep 27 20:43:21.286: INFO: Waiting for pod downwardapi-volume-ba319533-2b9c-49c2-91a8-c34131b3b32d to disappear
Sep 27 20:43:21.300: INFO: Pod downwardapi-volume-ba319533-2b9c-49c2-91a8-c34131b3b32d no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 20:43:21.300: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7502" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu limit [NodeConformance] [Conformance]","total":305,"completed":203,"skipped":3157,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] 
  validates basic preemption works [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 20:43:21.344: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename sched-preemption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:89
Sep 27 20:43:21.585: INFO: Waiting up to 1m0s for all nodes to be ready
Sep 27 20:44:21.802: INFO: Waiting for terminating namespaces to be deleted...
[It] validates basic preemption works [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Create pods that use 2/3 of node resources.
Sep 27 20:44:21.909: INFO: Created pod: pod0-sched-preemption-low-priority
Sep 27 20:44:21.983: INFO: Created pod: pod1-sched-preemption-medium-priority
Sep 27 20:44:22.076: INFO: Created pod: pod2-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled.
STEP: Run a high priority pod that has same requirements as that of lower priority pod
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 20:44:36.271: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-9733" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:77

• [SLOW TEST:75.159 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates basic preemption works [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] validates basic preemption works [Conformance]","total":305,"completed":204,"skipped":3182,"failed":0}
SSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 20:44:36.503: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Sep 27 20:44:36.690: INFO: Waiting up to 5m0s for pod "downwardapi-volume-2caa6056-788c-46e3-acb4-288c1adae7b8" in namespace "downward-api-1135" to be "Succeeded or Failed"
Sep 27 20:44:36.721: INFO: Pod "downwardapi-volume-2caa6056-788c-46e3-acb4-288c1adae7b8": Phase="Pending", Reason="", readiness=false. Elapsed: 31.53128ms
Sep 27 20:44:38.739: INFO: Pod "downwardapi-volume-2caa6056-788c-46e3-acb4-288c1adae7b8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.049070638s
Sep 27 20:44:40.755: INFO: Pod "downwardapi-volume-2caa6056-788c-46e3-acb4-288c1adae7b8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.064772439s
STEP: Saw pod success
Sep 27 20:44:40.755: INFO: Pod "downwardapi-volume-2caa6056-788c-46e3-acb4-288c1adae7b8" satisfied condition "Succeeded or Failed"
Sep 27 20:44:40.770: INFO: Trying to get logs from node 10.177.248.117 pod downwardapi-volume-2caa6056-788c-46e3-acb4-288c1adae7b8 container client-container: <nil>
STEP: delete the pod
Sep 27 20:44:40.852: INFO: Waiting for pod downwardapi-volume-2caa6056-788c-46e3-acb4-288c1adae7b8 to disappear
Sep 27 20:44:40.870: INFO: Pod downwardapi-volume-2caa6056-788c-46e3-acb4-288c1adae7b8 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 20:44:40.870: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1135" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory limit [NodeConformance] [Conformance]","total":305,"completed":205,"skipped":3187,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 20:44:40.915: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating secret with name secret-test-map-caf13926-08b0-4613-979e-752c9624b530
STEP: Creating a pod to test consume secrets
Sep 27 20:44:41.178: INFO: Waiting up to 5m0s for pod "pod-secrets-c5d659f6-5aa2-48eb-9e0a-f0b9f66d69de" in namespace "secrets-9370" to be "Succeeded or Failed"
Sep 27 20:44:41.193: INFO: Pod "pod-secrets-c5d659f6-5aa2-48eb-9e0a-f0b9f66d69de": Phase="Pending", Reason="", readiness=false. Elapsed: 14.832143ms
Sep 27 20:44:43.207: INFO: Pod "pod-secrets-c5d659f6-5aa2-48eb-9e0a-f0b9f66d69de": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028876477s
Sep 27 20:44:45.224: INFO: Pod "pod-secrets-c5d659f6-5aa2-48eb-9e0a-f0b9f66d69de": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.046023892s
STEP: Saw pod success
Sep 27 20:44:45.224: INFO: Pod "pod-secrets-c5d659f6-5aa2-48eb-9e0a-f0b9f66d69de" satisfied condition "Succeeded or Failed"
Sep 27 20:44:45.236: INFO: Trying to get logs from node 10.177.248.117 pod pod-secrets-c5d659f6-5aa2-48eb-9e0a-f0b9f66d69de container secret-volume-test: <nil>
STEP: delete the pod
Sep 27 20:44:45.309: INFO: Waiting for pod pod-secrets-c5d659f6-5aa2-48eb-9e0a-f0b9f66d69de to disappear
Sep 27 20:44:45.327: INFO: Pod pod-secrets-c5d659f6-5aa2-48eb-9e0a-f0b9f66d69de no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 20:44:45.327: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9370" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":305,"completed":206,"skipped":3198,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 20:44:45.380: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:78
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Sep 27 20:44:45.515: INFO: Creating deployment "test-recreate-deployment"
Sep 27 20:44:45.532: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Sep 27 20:44:45.576: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
Sep 27 20:44:47.602: INFO: Waiting deployment "test-recreate-deployment" to complete
Sep 27 20:44:47.613: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Sep 27 20:44:47.647: INFO: Updating deployment test-recreate-deployment
Sep 27 20:44:47.647: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:72
Sep 27 20:44:47.849: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:{test-recreate-deployment  deployment-5732 /apis/apps/v1/namespaces/deployment-5732/deployments/test-recreate-deployment 9a195b69-cdca-466c-bbcf-3b765fbc1da9 99946 2 2021-09-27 20:44:45 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2021-09-27 20:44:47 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{}}},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-09-27 20:44:47 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}}}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0047a8978 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2021-09-27 20:44:47 +0000 UTC,LastTransitionTime:2021-09-27 20:44:47 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-f79dd4667" is progressing.,LastUpdateTime:2021-09-27 20:44:47 +0000 UTC,LastTransitionTime:2021-09-27 20:44:45 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

Sep 27 20:44:47.879: INFO: New ReplicaSet "test-recreate-deployment-f79dd4667" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:{test-recreate-deployment-f79dd4667  deployment-5732 /apis/apps/v1/namespaces/deployment-5732/replicasets/test-recreate-deployment-f79dd4667 59451d9c-9d4f-45cb-901e-918af53aa5de 99944 1 2021-09-27 20:44:47 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:f79dd4667] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment 9a195b69-cdca-466c-bbcf-3b765fbc1da9 0xc0047a8e90 0xc0047a8e91}] []  [{kube-controller-manager Update apps/v1 2021-09-27 20:44:47 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9a195b69-cdca-466c-bbcf-3b765fbc1da9\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: f79dd4667,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:f79dd4667] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0047a8f08 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Sep 27 20:44:47.879: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Sep 27 20:44:47.879: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-c96cf48f  deployment-5732 /apis/apps/v1/namespaces/deployment-5732/replicasets/test-recreate-deployment-c96cf48f e4f57ca7-e14f-4cb5-9584-7f7ede706aab 99934 2 2021-09-27 20:44:45 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:c96cf48f] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment 9a195b69-cdca-466c-bbcf-3b765fbc1da9 0xc0047a8d9f 0xc0047a8db0}] []  [{kube-controller-manager Update apps/v1 2021-09-27 20:44:47 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9a195b69-cdca-466c-bbcf-3b765fbc1da9\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: c96cf48f,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:c96cf48f] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.20 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0047a8e28 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Sep 27 20:44:47.894: INFO: Pod "test-recreate-deployment-f79dd4667-7w27z" is not available:
&Pod{ObjectMeta:{test-recreate-deployment-f79dd4667-7w27z test-recreate-deployment-f79dd4667- deployment-5732 /api/v1/namespaces/deployment-5732/pods/test-recreate-deployment-f79dd4667-7w27z 11410b3e-b209-4176-9144-1fef7667dd73 99947 0 2021-09-27 20:44:47 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:f79dd4667] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet test-recreate-deployment-f79dd4667 59451d9c-9d4f-45cb-901e-918af53aa5de 0xc0047a9407 0xc0047a9408}] []  [{kube-controller-manager Update v1 2021-09-27 20:44:47 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"59451d9c-9d4f-45cb-901e-918af53aa5de\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-09-27 20:44:47 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-kvhwb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-kvhwb,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-kvhwb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.177.248.117,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c55,c10,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-dwnbc,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-27 20:44:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-27 20:44:47 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-27 20:44:47 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-27 20:44:47 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.177.248.117,PodIP:,StartTime:2021-09-27 20:44:47 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 20:44:47.894: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-5732" for this suite.
•{"msg":"PASSED [sig-apps] Deployment RecreateDeployment should delete old pods and create new ones [Conformance]","total":305,"completed":207,"skipped":3211,"failed":0}
SSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 20:44:47.944: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test override arguments
Sep 27 20:44:48.167: INFO: Waiting up to 5m0s for pod "client-containers-7440950b-ec62-46f7-8cfe-ac4feb040909" in namespace "containers-4672" to be "Succeeded or Failed"
Sep 27 20:44:48.205: INFO: Pod "client-containers-7440950b-ec62-46f7-8cfe-ac4feb040909": Phase="Pending", Reason="", readiness=false. Elapsed: 36.832865ms
Sep 27 20:44:50.218: INFO: Pod "client-containers-7440950b-ec62-46f7-8cfe-ac4feb040909": Phase="Pending", Reason="", readiness=false. Elapsed: 2.05061279s
Sep 27 20:44:52.233: INFO: Pod "client-containers-7440950b-ec62-46f7-8cfe-ac4feb040909": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.064790588s
STEP: Saw pod success
Sep 27 20:44:52.233: INFO: Pod "client-containers-7440950b-ec62-46f7-8cfe-ac4feb040909" satisfied condition "Succeeded or Failed"
Sep 27 20:44:52.246: INFO: Trying to get logs from node 10.177.248.117 pod client-containers-7440950b-ec62-46f7-8cfe-ac4feb040909 container test-container: <nil>
STEP: delete the pod
Sep 27 20:44:52.322: INFO: Waiting for pod client-containers-7440950b-ec62-46f7-8cfe-ac4feb040909 to disappear
Sep 27 20:44:52.334: INFO: Pod client-containers-7440950b-ec62-46f7-8cfe-ac4feb040909 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 20:44:52.334: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-4672" for this suite.
•{"msg":"PASSED [k8s.io] Docker Containers should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]","total":305,"completed":208,"skipped":3220,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 20:44:52.378: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:82
[It] should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 20:44:56.668: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-1842" for this suite.
•{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox command that always fails in a pod should have an terminated reason [NodeConformance] [Conformance]","total":305,"completed":209,"skipped":3246,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of different groups [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 20:44:56.715: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of different groups [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation
Sep 27 20:44:56.900: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
Sep 27 20:45:05.840: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 20:45:41.924: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-8542" for this suite.

• [SLOW TEST:45.257 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of different groups [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of different groups [Conformance]","total":305,"completed":210,"skipped":3270,"failed":0}
SSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should deny crd creation [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 20:45:41.975: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Sep 27 20:45:42.672: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Sep 27 20:45:44.733: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63768372342, loc:(*time.Location)(0x7702840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63768372342, loc:(*time.Location)(0x7702840)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63768372342, loc:(*time.Location)(0x7702840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63768372342, loc:(*time.Location)(0x7702840)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep 27 20:45:47.804: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should deny crd creation [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Registering the crd webhook via the AdmissionRegistration API
STEP: Creating a custom resource definition that should be denied by the webhook
Sep 27 20:45:47.918: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 20:45:48.018: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1161" for this suite.
STEP: Destroying namespace "webhook-1161-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:6.311 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should deny crd creation [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should deny crd creation [Conformance]","total":305,"completed":211,"skipped":3280,"failed":0}
[sig-api-machinery] Garbage collector 
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 20:45:48.287: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs
STEP: Gathering metrics
W0927 20:45:49.669248      22 metrics_grabber.go:98] Can't find kube-scheduler pod. Grabbing metrics from kube-scheduler is disabled.
W0927 20:45:49.669706      22 metrics_grabber.go:102] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
W0927 20:45:49.669899      22 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
Sep 27 20:45:49.670: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 20:45:49.670: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-9600" for this suite.
•{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]","total":305,"completed":212,"skipped":3280,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-instrumentation] Events API 
  should delete a collection of events [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-instrumentation] Events API
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 20:45:49.728: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-instrumentation] Events API
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/instrumentation/events.go:81
[It] should delete a collection of events [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Create set of events
STEP: get a list of Events with a label in the current namespace
STEP: delete a list of events
Sep 27 20:45:50.002: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity
[AfterEach] [sig-instrumentation] Events API
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 20:45:50.091: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-5064" for this suite.
•{"msg":"PASSED [sig-instrumentation] Events API should delete a collection of events [Conformance]","total":305,"completed":213,"skipped":3291,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 20:45:50.152: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-8777
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Initializing watcher for selector baz=blah,foo=bar
STEP: Creating stateful set ss in namespace statefulset-8777
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-8777
Sep 27 20:45:50.414: INFO: Found 0 stateful pods, waiting for 1
Sep 27 20:46:00.429: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod
Sep 27 20:46:00.443: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-679324930 exec --namespace=statefulset-8777 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Sep 27 20:46:00.899: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Sep 27 20:46:00.899: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Sep 27 20:46:00.899: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Sep 27 20:46:00.914: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Sep 27 20:46:10.930: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Sep 27 20:46:10.930: INFO: Waiting for statefulset status.replicas updated to 0
Sep 27 20:46:10.992: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999997266s
Sep 27 20:46:12.009: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.984030287s
Sep 27 20:46:13.080: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.966533007s
Sep 27 20:46:14.095: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.895063761s
Sep 27 20:46:15.110: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.880373097s
Sep 27 20:46:16.135: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.864480704s
Sep 27 20:46:17.152: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.839431611s
Sep 27 20:46:18.168: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.823534373s
Sep 27 20:46:19.184: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.807769561s
Sep 27 20:46:20.200: INFO: Verifying statefulset ss doesn't scale past 1 for another 791.139394ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-8777
Sep 27 20:46:21.216: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-679324930 exec --namespace=statefulset-8777 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 27 20:46:21.605: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Sep 27 20:46:21.605: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Sep 27 20:46:21.605: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Sep 27 20:46:21.619: INFO: Found 1 stateful pods, waiting for 3
Sep 27 20:46:31.698: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Sep 27 20:46:31.698: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Sep 27 20:46:31.698: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order
STEP: Scale down will halt with unhealthy stateful pod
Sep 27 20:46:31.988: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-679324930 exec --namespace=statefulset-8777 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Sep 27 20:46:32.524: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Sep 27 20:46:32.524: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Sep 27 20:46:32.524: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Sep 27 20:46:32.524: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-679324930 exec --namespace=statefulset-8777 ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Sep 27 20:46:33.025: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Sep 27 20:46:33.025: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Sep 27 20:46:33.025: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Sep 27 20:46:33.025: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-679324930 exec --namespace=statefulset-8777 ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Sep 27 20:46:33.511: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Sep 27 20:46:33.511: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Sep 27 20:46:33.511: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Sep 27 20:46:33.511: INFO: Waiting for statefulset status.replicas updated to 0
Sep 27 20:46:33.524: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
Sep 27 20:46:43.557: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Sep 27 20:46:43.557: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Sep 27 20:46:43.557: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Sep 27 20:46:43.610: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999997256s
Sep 27 20:46:44.625: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.983077315s
Sep 27 20:46:45.640: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.968976028s
Sep 27 20:46:46.654: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.953922765s
Sep 27 20:46:47.674: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.939186028s
Sep 27 20:46:48.688: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.920044088s
Sep 27 20:46:49.703: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.904991856s
Sep 27 20:46:50.717: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.890383046s
Sep 27 20:46:51.740: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.876226803s
Sep 27 20:46:52.755: INFO: Verifying statefulset ss doesn't scale past 3 for another 853.356352ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-8777
Sep 27 20:46:53.769: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-679324930 exec --namespace=statefulset-8777 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 27 20:46:54.149: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Sep 27 20:46:54.149: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Sep 27 20:46:54.149: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Sep 27 20:46:54.149: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-679324930 exec --namespace=statefulset-8777 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 27 20:46:54.626: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Sep 27 20:46:54.626: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Sep 27 20:46:54.626: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Sep 27 20:46:54.627: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-679324930 exec --namespace=statefulset-8777 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 27 20:46:55.133: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Sep 27 20:46:55.133: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Sep 27 20:46:55.134: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Sep 27 20:46:55.134: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
Sep 27 20:47:25.202: INFO: Deleting all statefulset in ns statefulset-8777
Sep 27 20:47:25.216: INFO: Scaling statefulset ss to 0
Sep 27 20:47:25.269: INFO: Waiting for statefulset status.replicas updated to 0
Sep 27 20:47:25.285: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 20:47:25.350: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-8777" for this suite.

• [SLOW TEST:95.250 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
    /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]","total":305,"completed":214,"skipped":3366,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 20:47:25.402: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating secret with name secret-test-614ded07-ff4c-49b2-9629-14f6cd96186a
STEP: Creating a pod to test consume secrets
Sep 27 20:47:25.646: INFO: Waiting up to 5m0s for pod "pod-secrets-f56d77ae-5b23-4a08-b897-f6220848cc31" in namespace "secrets-6746" to be "Succeeded or Failed"
Sep 27 20:47:25.660: INFO: Pod "pod-secrets-f56d77ae-5b23-4a08-b897-f6220848cc31": Phase="Pending", Reason="", readiness=false. Elapsed: 13.145058ms
Sep 27 20:47:27.678: INFO: Pod "pod-secrets-f56d77ae-5b23-4a08-b897-f6220848cc31": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031423617s
Sep 27 20:47:29.704: INFO: Pod "pod-secrets-f56d77ae-5b23-4a08-b897-f6220848cc31": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.057272791s
STEP: Saw pod success
Sep 27 20:47:29.704: INFO: Pod "pod-secrets-f56d77ae-5b23-4a08-b897-f6220848cc31" satisfied condition "Succeeded or Failed"
Sep 27 20:47:29.726: INFO: Trying to get logs from node 10.177.248.117 pod pod-secrets-f56d77ae-5b23-4a08-b897-f6220848cc31 container secret-volume-test: <nil>
STEP: delete the pod
Sep 27 20:47:29.875: INFO: Waiting for pod pod-secrets-f56d77ae-5b23-4a08-b897-f6220848cc31 to disappear
Sep 27 20:47:29.900: INFO: Pod pod-secrets-f56d77ae-5b23-4a08-b897-f6220848cc31 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 20:47:29.928: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6746" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume [NodeConformance] [Conformance]","total":305,"completed":215,"skipped":3388,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 20:47:30.020: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-423.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-423.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-423.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-423.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-423.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-423.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Sep 27 20:47:34.818: INFO: DNS probes using dns-423/dns-test-f6b1dbb4-2ea4-4d87-b794-8d61fa97ae4f succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 20:47:34.932: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-423" for this suite.
•{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Hostname [LinuxOnly] [Conformance]","total":305,"completed":216,"skipped":3429,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 20:47:34.975: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name projected-configmap-test-volume-map-a2686dfb-0377-44d1-bdd7-097beedcd2b9
STEP: Creating a pod to test consume configMaps
Sep 27 20:47:35.278: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-831ae721-a9d6-4cf2-9e41-db64e739ea39" in namespace "projected-8030" to be "Succeeded or Failed"
Sep 27 20:47:35.297: INFO: Pod "pod-projected-configmaps-831ae721-a9d6-4cf2-9e41-db64e739ea39": Phase="Pending", Reason="", readiness=false. Elapsed: 19.083208ms
Sep 27 20:47:37.310: INFO: Pod "pod-projected-configmaps-831ae721-a9d6-4cf2-9e41-db64e739ea39": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032777077s
Sep 27 20:47:39.325: INFO: Pod "pod-projected-configmaps-831ae721-a9d6-4cf2-9e41-db64e739ea39": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.047729117s
STEP: Saw pod success
Sep 27 20:47:39.325: INFO: Pod "pod-projected-configmaps-831ae721-a9d6-4cf2-9e41-db64e739ea39" satisfied condition "Succeeded or Failed"
Sep 27 20:47:39.349: INFO: Trying to get logs from node 10.177.248.117 pod pod-projected-configmaps-831ae721-a9d6-4cf2-9e41-db64e739ea39 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Sep 27 20:47:39.430: INFO: Waiting for pod pod-projected-configmaps-831ae721-a9d6-4cf2-9e41-db64e739ea39 to disappear
Sep 27 20:47:39.444: INFO: Pod pod-projected-configmaps-831ae721-a9d6-4cf2-9e41-db64e739ea39 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 20:47:39.445: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8030" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":217,"skipped":3443,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 20:47:39.490: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a watch on configmaps with label A
STEP: creating a watch on configmaps with label B
STEP: creating a watch on configmaps with label A or B
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification
Sep 27 20:47:39.733: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7895 /api/v1/namespaces/watch-7895/configmaps/e2e-watch-test-configmap-a 81f2a4d0-1b85-41db-a946-d0ef283691e9 101947 0 2021-09-27 20:47:39 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-09-27 20:47:39 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Sep 27 20:47:39.739: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7895 /api/v1/namespaces/watch-7895/configmaps/e2e-watch-test-configmap-a 81f2a4d0-1b85-41db-a946-d0ef283691e9 101947 0 2021-09-27 20:47:39 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-09-27 20:47:39 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A and ensuring the correct watchers observe the notification
Sep 27 20:47:49.777: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7895 /api/v1/namespaces/watch-7895/configmaps/e2e-watch-test-configmap-a 81f2a4d0-1b85-41db-a946-d0ef283691e9 102080 0 2021-09-27 20:47:39 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-09-27 20:47:49 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Sep 27 20:47:49.778: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7895 /api/v1/namespaces/watch-7895/configmaps/e2e-watch-test-configmap-a 81f2a4d0-1b85-41db-a946-d0ef283691e9 102080 0 2021-09-27 20:47:39 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-09-27 20:47:49 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification
Sep 27 20:47:59.812: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7895 /api/v1/namespaces/watch-7895/configmaps/e2e-watch-test-configmap-a 81f2a4d0-1b85-41db-a946-d0ef283691e9 102125 0 2021-09-27 20:47:39 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-09-27 20:47:59 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Sep 27 20:47:59.812: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7895 /api/v1/namespaces/watch-7895/configmaps/e2e-watch-test-configmap-a 81f2a4d0-1b85-41db-a946-d0ef283691e9 102125 0 2021-09-27 20:47:39 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-09-27 20:47:59 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap A and ensuring the correct watchers observe the notification
Sep 27 20:48:09.849: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7895 /api/v1/namespaces/watch-7895/configmaps/e2e-watch-test-configmap-a 81f2a4d0-1b85-41db-a946-d0ef283691e9 102198 0 2021-09-27 20:47:39 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-09-27 20:47:59 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Sep 27 20:48:09.849: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7895 /api/v1/namespaces/watch-7895/configmaps/e2e-watch-test-configmap-a 81f2a4d0-1b85-41db-a946-d0ef283691e9 102198 0 2021-09-27 20:47:39 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-09-27 20:47:59 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification
Sep 27 20:48:19.883: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-7895 /api/v1/namespaces/watch-7895/configmaps/e2e-watch-test-configmap-b c6f9067e-cd94-4adc-9470-c5aa757810db 102277 0 2021-09-27 20:48:19 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2021-09-27 20:48:19 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Sep 27 20:48:19.883: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-7895 /api/v1/namespaces/watch-7895/configmaps/e2e-watch-test-configmap-b c6f9067e-cd94-4adc-9470-c5aa757810db 102277 0 2021-09-27 20:48:19 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2021-09-27 20:48:19 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap B and ensuring the correct watchers observe the notification
Sep 27 20:48:29.917: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-7895 /api/v1/namespaces/watch-7895/configmaps/e2e-watch-test-configmap-b c6f9067e-cd94-4adc-9470-c5aa757810db 102326 0 2021-09-27 20:48:19 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2021-09-27 20:48:19 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Sep 27 20:48:29.918: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-7895 /api/v1/namespaces/watch-7895/configmaps/e2e-watch-test-configmap-b c6f9067e-cd94-4adc-9470-c5aa757810db 102326 0 2021-09-27 20:48:19 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2021-09-27 20:48:19 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 20:48:39.918: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-7895" for this suite.

• [SLOW TEST:60.471 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should observe add, update, and delete watch notifications on configmaps [Conformance]","total":305,"completed":218,"skipped":3461,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 20:48:39.963: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Given a Pod with a 'name' label pod-adoption-release is created
STEP: When a replicaset with a matching selector is created
STEP: Then the orphan pod is adopted
STEP: When the matched label of one of its pods change
Sep 27 20:48:43.299: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 20:48:44.381: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-9647" for this suite.
•{"msg":"PASSED [sig-apps] ReplicaSet should adopt matching pods on creation and release no longer matching pods [Conformance]","total":305,"completed":219,"skipped":3616,"failed":0}
SSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 20:48:44.442: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0644 on tmpfs
Sep 27 20:48:44.645: INFO: Waiting up to 5m0s for pod "pod-a3202cc0-a226-4060-8279-f9166936c000" in namespace "emptydir-4501" to be "Succeeded or Failed"
Sep 27 20:48:44.658: INFO: Pod "pod-a3202cc0-a226-4060-8279-f9166936c000": Phase="Pending", Reason="", readiness=false. Elapsed: 12.944594ms
Sep 27 20:48:46.680: INFO: Pod "pod-a3202cc0-a226-4060-8279-f9166936c000": Phase="Pending", Reason="", readiness=false. Elapsed: 2.035753326s
Sep 27 20:48:48.696: INFO: Pod "pod-a3202cc0-a226-4060-8279-f9166936c000": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.05136798s
STEP: Saw pod success
Sep 27 20:48:48.696: INFO: Pod "pod-a3202cc0-a226-4060-8279-f9166936c000" satisfied condition "Succeeded or Failed"
Sep 27 20:48:48.711: INFO: Trying to get logs from node 10.177.248.117 pod pod-a3202cc0-a226-4060-8279-f9166936c000 container test-container: <nil>
STEP: delete the pod
Sep 27 20:48:48.835: INFO: Waiting for pod pod-a3202cc0-a226-4060-8279-f9166936c000 to disappear
Sep 27 20:48:48.849: INFO: Pod pod-a3202cc0-a226-4060-8279-f9166936c000 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 20:48:48.849: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4501" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":220,"skipped":3621,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 20:48:48.897: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation
Sep 27 20:48:49.052: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
Sep 27 20:48:57.587: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 20:49:30.945: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-6726" for this suite.

• [SLOW TEST:42.092 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group and version but different kinds [Conformance]","total":305,"completed":221,"skipped":3639,"failed":0}
SSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 20:49:30.990: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap configmap-5213/configmap-test-599e838d-18d1-4a0e-be57-3cea313c3ba7
STEP: Creating a pod to test consume configMaps
Sep 27 20:49:31.245: INFO: Waiting up to 5m0s for pod "pod-configmaps-ee296207-957b-4788-9bf0-92926ae57c1c" in namespace "configmap-5213" to be "Succeeded or Failed"
Sep 27 20:49:31.260: INFO: Pod "pod-configmaps-ee296207-957b-4788-9bf0-92926ae57c1c": Phase="Pending", Reason="", readiness=false. Elapsed: 14.34303ms
Sep 27 20:49:33.274: INFO: Pod "pod-configmaps-ee296207-957b-4788-9bf0-92926ae57c1c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.028747365s
STEP: Saw pod success
Sep 27 20:49:33.274: INFO: Pod "pod-configmaps-ee296207-957b-4788-9bf0-92926ae57c1c" satisfied condition "Succeeded or Failed"
Sep 27 20:49:33.287: INFO: Trying to get logs from node 10.177.248.117 pod pod-configmaps-ee296207-957b-4788-9bf0-92926ae57c1c container env-test: <nil>
STEP: delete the pod
Sep 27 20:49:33.366: INFO: Waiting for pod pod-configmaps-ee296207-957b-4788-9bf0-92926ae57c1c to disappear
Sep 27 20:49:33.379: INFO: Pod pod-configmaps-ee296207-957b-4788-9bf0-92926ae57c1c no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 20:49:33.379: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5213" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should be consumable via the environment [NodeConformance] [Conformance]","total":305,"completed":222,"skipped":3647,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 20:49:33.418: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod liveness-c6c6a758-032a-44f9-9767-854fbed95df7 in namespace container-probe-3616
Sep 27 20:49:35.661: INFO: Started pod liveness-c6c6a758-032a-44f9-9767-854fbed95df7 in namespace container-probe-3616
STEP: checking the pod's current state and verifying that restartCount is present
Sep 27 20:49:35.675: INFO: Initial restart count of pod liveness-c6c6a758-032a-44f9-9767-854fbed95df7 is 0
Sep 27 20:49:59.861: INFO: Restart count of pod container-probe-3616/liveness-c6c6a758-032a-44f9-9767-854fbed95df7 is now 1 (24.186788452s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 20:49:59.918: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-3616" for this suite.

• [SLOW TEST:26.556 seconds]
[k8s.io] Probing container
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Probing container should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","total":305,"completed":223,"skipped":3662,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 20:49:59.994: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test substitution in container's args
Sep 27 20:50:00.274: INFO: Waiting up to 5m0s for pod "var-expansion-f8835b58-21b5-4656-8f3f-c34de6e13b60" in namespace "var-expansion-25" to be "Succeeded or Failed"
Sep 27 20:50:00.292: INFO: Pod "var-expansion-f8835b58-21b5-4656-8f3f-c34de6e13b60": Phase="Pending", Reason="", readiness=false. Elapsed: 18.480974ms
Sep 27 20:50:02.308: INFO: Pod "var-expansion-f8835b58-21b5-4656-8f3f-c34de6e13b60": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.034928074s
STEP: Saw pod success
Sep 27 20:50:02.309: INFO: Pod "var-expansion-f8835b58-21b5-4656-8f3f-c34de6e13b60" satisfied condition "Succeeded or Failed"
Sep 27 20:50:02.323: INFO: Trying to get logs from node 10.177.248.117 pod var-expansion-f8835b58-21b5-4656-8f3f-c34de6e13b60 container dapi-container: <nil>
STEP: delete the pod
Sep 27 20:50:02.398: INFO: Waiting for pod var-expansion-f8835b58-21b5-4656-8f3f-c34de6e13b60 to disappear
Sep 27 20:50:02.412: INFO: Pod var-expansion-f8835b58-21b5-4656-8f3f-c34de6e13b60 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 20:50:02.412: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-25" for this suite.
•{"msg":"PASSED [k8s.io] Variable Expansion should allow substituting values in a container's args [NodeConformance] [Conformance]","total":305,"completed":224,"skipped":3677,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 20:50:02.454: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Sep 27 20:50:02.778: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"5460119f-3b32-4962-a1eb-9ee2aa25c36d", Controller:(*bool)(0xc003680ae6), BlockOwnerDeletion:(*bool)(0xc003680ae7)}}
Sep 27 20:50:02.815: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"98023adf-04b2-44bc-a850-c9a8c485d0cf", Controller:(*bool)(0xc00b83d432), BlockOwnerDeletion:(*bool)(0xc00b83d433)}}
Sep 27 20:50:02.834: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"d7a83700-00d9-4255-84eb-8d4376820ce3", Controller:(*bool)(0xc00b83d6c2), BlockOwnerDeletion:(*bool)(0xc00b83d6c3)}}
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 20:50:07.884: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-9406" for this suite.

• [SLOW TEST:5.475 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should not be blocked by dependency circle [Conformance]","total":305,"completed":225,"skipped":3704,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 20:50:07.929: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Sep 27 20:50:08.933: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Sep 27 20:50:10.977: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63768372608, loc:(*time.Location)(0x7702840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63768372608, loc:(*time.Location)(0x7702840)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63768372609, loc:(*time.Location)(0x7702840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63768372608, loc:(*time.Location)(0x7702840)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep 27 20:50:14.022: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Sep 27 20:50:14.037: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Registering the custom resource webhook via the AdmissionRegistration API
STEP: Creating a custom resource that should be denied by the webhook
STEP: Creating a custom resource whose deletion would be denied by the webhook
STEP: Updating the custom resource with disallowed data should be denied
STEP: Deleting the custom resource should be denied
STEP: Remove the offending key and value from the custom resource data
STEP: Deleting the updated custom resource should be successful
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 20:50:15.606: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3624" for this suite.
STEP: Destroying namespace "webhook-3624-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:8.060 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny custom resource creation, update and deletion [Conformance]","total":305,"completed":226,"skipped":3725,"failed":0}
SSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 20:50:15.989: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Performing setup for networking test in namespace pod-network-test-3198
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Sep 27 20:50:16.552: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Sep 27 20:50:16.911: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Sep 27 20:50:18.941: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Sep 27 20:50:20.925: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep 27 20:50:22.927: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep 27 20:50:24.925: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep 27 20:50:26.925: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep 27 20:50:28.926: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep 27 20:50:30.924: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep 27 20:50:32.928: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep 27 20:50:34.925: INFO: The status of Pod netserver-0 is Running (Ready = true)
Sep 27 20:50:34.951: INFO: The status of Pod netserver-1 is Running (Ready = false)
Sep 27 20:50:36.972: INFO: The status of Pod netserver-1 is Running (Ready = false)
Sep 27 20:50:38.967: INFO: The status of Pod netserver-1 is Running (Ready = true)
Sep 27 20:50:38.995: INFO: The status of Pod netserver-2 is Running (Ready = true)
STEP: Creating test pods
Sep 27 20:50:43.292: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.30.137.113:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-3198 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep 27 20:50:43.292: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
Sep 27 20:50:43.642: INFO: Found all expected endpoints: [netserver-0]
Sep 27 20:50:43.705: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.30.75.52:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-3198 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep 27 20:50:43.705: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
Sep 27 20:50:44.108: INFO: Found all expected endpoints: [netserver-1]
Sep 27 20:50:44.128: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.30.85.177:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-3198 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep 27 20:50:44.128: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
Sep 27 20:50:44.692: INFO: Found all expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 20:50:44.692: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-3198" for this suite.

• [SLOW TEST:28.794 seconds]
[sig-network] Networking
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:26
  Granular Checks: Pods
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:29
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":227,"skipped":3729,"failed":0}
SSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 20:50:44.802: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Sep 27 20:50:45.135: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 20:50:45.731: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-2184" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition getting/updating/patching custom resource definition status sub-resource works  [Conformance]","total":305,"completed":228,"skipped":3733,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 20:50:45.967: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward api env vars
Sep 27 20:50:46.272: INFO: Waiting up to 5m0s for pod "downward-api-a85268c7-7949-45e0-884e-061fdfb86a5e" in namespace "downward-api-7087" to be "Succeeded or Failed"
Sep 27 20:50:46.311: INFO: Pod "downward-api-a85268c7-7949-45e0-884e-061fdfb86a5e": Phase="Pending", Reason="", readiness=false. Elapsed: 19.715715ms
Sep 27 20:50:48.330: INFO: Pod "downward-api-a85268c7-7949-45e0-884e-061fdfb86a5e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.038756831s
Sep 27 20:50:50.350: INFO: Pod "downward-api-a85268c7-7949-45e0-884e-061fdfb86a5e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.058134503s
STEP: Saw pod success
Sep 27 20:50:50.350: INFO: Pod "downward-api-a85268c7-7949-45e0-884e-061fdfb86a5e" satisfied condition "Succeeded or Failed"
Sep 27 20:50:50.363: INFO: Trying to get logs from node 10.177.248.117 pod downward-api-a85268c7-7949-45e0-884e-061fdfb86a5e container dapi-container: <nil>
STEP: delete the pod
Sep 27 20:50:50.453: INFO: Waiting for pod downward-api-a85268c7-7949-45e0-884e-061fdfb86a5e to disappear
Sep 27 20:50:50.471: INFO: Pod downward-api-a85268c7-7949-45e0-884e-061fdfb86a5e no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 20:50:50.471: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7087" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]","total":305,"completed":229,"skipped":3760,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with different stored version [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 20:50:50.527: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Sep 27 20:50:51.418: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Sep 27 20:50:53.458: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63768372651, loc:(*time.Location)(0x7702840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63768372651, loc:(*time.Location)(0x7702840)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63768372651, loc:(*time.Location)(0x7702840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63768372651, loc:(*time.Location)(0x7702840)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep 27 20:50:56.508: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with different stored version [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Sep 27 20:50:56.523: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-3475-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource while v1 is storage version
STEP: Patching Custom Resource Definition to set v2 as storage
STEP: Patching the custom resource while v2 is storage version
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 20:50:57.960: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4027" for this suite.
STEP: Destroying namespace "webhook-4027-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:7.681 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource with different stored version [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with different stored version [Conformance]","total":305,"completed":230,"skipped":3775,"failed":0}
SSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 20:50:58.210: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Sep 27 20:50:58.464: INFO: Waiting up to 5m0s for pod "downwardapi-volume-bc24447a-4d8f-439a-b82a-0a79842370b4" in namespace "projected-8758" to be "Succeeded or Failed"
Sep 27 20:50:58.494: INFO: Pod "downwardapi-volume-bc24447a-4d8f-439a-b82a-0a79842370b4": Phase="Pending", Reason="", readiness=false. Elapsed: 29.966064ms
Sep 27 20:51:00.510: INFO: Pod "downwardapi-volume-bc24447a-4d8f-439a-b82a-0a79842370b4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.046126425s
STEP: Saw pod success
Sep 27 20:51:00.510: INFO: Pod "downwardapi-volume-bc24447a-4d8f-439a-b82a-0a79842370b4" satisfied condition "Succeeded or Failed"
Sep 27 20:51:00.524: INFO: Trying to get logs from node 10.177.248.117 pod downwardapi-volume-bc24447a-4d8f-439a-b82a-0a79842370b4 container client-container: <nil>
STEP: delete the pod
Sep 27 20:51:00.599: INFO: Waiting for pod downwardapi-volume-bc24447a-4d8f-439a-b82a-0a79842370b4 to disappear
Sep 27 20:51:00.615: INFO: Pod downwardapi-volume-bc24447a-4d8f-439a-b82a-0a79842370b4 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 20:51:00.615: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8758" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu request [NodeConformance] [Conformance]","total":305,"completed":231,"skipped":3779,"failed":0}
SS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should include webhook resources in discovery documents [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 20:51:00.657: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Sep 27 20:51:02.225: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Sep 27 20:51:04.286: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63768372662, loc:(*time.Location)(0x7702840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63768372662, loc:(*time.Location)(0x7702840)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63768372662, loc:(*time.Location)(0x7702840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63768372662, loc:(*time.Location)(0x7702840)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep 27 20:51:07.350: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should include webhook resources in discovery documents [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: fetching the /apis discovery document
STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/admissionregistration.k8s.io discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document
STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document
STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 20:51:07.383: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3956" for this suite.
STEP: Destroying namespace "webhook-3956-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:6.963 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should include webhook resources in discovery documents [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should include webhook resources in discovery documents [Conformance]","total":305,"completed":232,"skipped":3781,"failed":0}
SSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support rollover [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 20:51:07.620: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:78
[It] deployment should support rollover [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Sep 27 20:51:07.816: INFO: Pod name rollover-pod: Found 0 pods out of 1
Sep 27 20:51:12.833: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Sep 27 20:51:12.833: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Sep 27 20:51:14.848: INFO: Creating deployment "test-rollover-deployment"
Sep 27 20:51:14.881: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Sep 27 20:51:16.906: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Sep 27 20:51:16.929: INFO: Ensure that both replica sets have 1 created replica
Sep 27 20:51:16.955: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Sep 27 20:51:16.985: INFO: Updating deployment test-rollover-deployment
Sep 27 20:51:16.985: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Sep 27 20:51:19.010: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Sep 27 20:51:19.036: INFO: Make sure deployment "test-rollover-deployment" is complete
Sep 27 20:51:19.066: INFO: all replica sets need to contain the pod-template-hash label
Sep 27 20:51:19.066: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63768372674, loc:(*time.Location)(0x7702840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63768372674, loc:(*time.Location)(0x7702840)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63768372677, loc:(*time.Location)(0x7702840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63768372674, loc:(*time.Location)(0x7702840)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-5797c7764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 27 20:51:21.096: INFO: all replica sets need to contain the pod-template-hash label
Sep 27 20:51:21.096: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63768372674, loc:(*time.Location)(0x7702840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63768372674, loc:(*time.Location)(0x7702840)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63768372679, loc:(*time.Location)(0x7702840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63768372674, loc:(*time.Location)(0x7702840)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-5797c7764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 27 20:51:23.097: INFO: all replica sets need to contain the pod-template-hash label
Sep 27 20:51:23.097: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63768372674, loc:(*time.Location)(0x7702840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63768372674, loc:(*time.Location)(0x7702840)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63768372679, loc:(*time.Location)(0x7702840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63768372674, loc:(*time.Location)(0x7702840)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-5797c7764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 27 20:51:25.104: INFO: all replica sets need to contain the pod-template-hash label
Sep 27 20:51:25.104: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63768372674, loc:(*time.Location)(0x7702840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63768372674, loc:(*time.Location)(0x7702840)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63768372679, loc:(*time.Location)(0x7702840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63768372674, loc:(*time.Location)(0x7702840)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-5797c7764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 27 20:51:27.098: INFO: all replica sets need to contain the pod-template-hash label
Sep 27 20:51:27.098: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63768372674, loc:(*time.Location)(0x7702840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63768372674, loc:(*time.Location)(0x7702840)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63768372679, loc:(*time.Location)(0x7702840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63768372674, loc:(*time.Location)(0x7702840)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-5797c7764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 27 20:51:29.095: INFO: all replica sets need to contain the pod-template-hash label
Sep 27 20:51:29.095: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63768372674, loc:(*time.Location)(0x7702840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63768372674, loc:(*time.Location)(0x7702840)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63768372679, loc:(*time.Location)(0x7702840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63768372674, loc:(*time.Location)(0x7702840)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-5797c7764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 27 20:51:31.096: INFO: 
Sep 27 20:51:31.096: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:72
Sep 27 20:51:31.139: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:{test-rollover-deployment  deployment-5925 /apis/apps/v1/namespaces/deployment-5925/deployments/test-rollover-deployment 41666a52-359f-4d01-898f-9faf980921aa 104826 2 2021-09-27 20:51:14 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2021-09-27 20:51:16 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{}}},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-09-27 20:51:29 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}}}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.20 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003680438 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2021-09-27 20:51:14 +0000 UTC,LastTransitionTime:2021-09-27 20:51:14 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-5797c7764" has successfully progressed.,LastUpdateTime:2021-09-27 20:51:29 +0000 UTC,LastTransitionTime:2021-09-27 20:51:14 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Sep 27 20:51:31.158: INFO: New ReplicaSet "test-rollover-deployment-5797c7764" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:{test-rollover-deployment-5797c7764  deployment-5925 /apis/apps/v1/namespaces/deployment-5925/replicasets/test-rollover-deployment-5797c7764 6df636d8-37e1-4328-9dec-034e237ea3e6 104815 2 2021-09-27 20:51:16 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:5797c7764] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment 41666a52-359f-4d01-898f-9faf980921aa 0xc003680960 0xc003680961}] []  [{kube-controller-manager Update apps/v1 2021-09-27 20:51:29 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"41666a52-359f-4d01-898f-9faf980921aa\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 5797c7764,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:5797c7764] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.20 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0036809d8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Sep 27 20:51:31.158: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Sep 27 20:51:31.158: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-5925 /apis/apps/v1/namespaces/deployment-5925/replicasets/test-rollover-controller db3c34d2-31d2-4b3c-8f53-2aa29dcce4c9 104824 2 2021-09-27 20:51:07 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment 41666a52-359f-4d01-898f-9faf980921aa 0xc003680857 0xc003680858}] []  [{e2e.test Update apps/v1 2021-09-27 20:51:07 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{"f:matchLabels":{".":{},"f:name":{},"f:pod":{}}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-09-27 20:51:29 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"41666a52-359f-4d01-898f-9faf980921aa\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{}},"f:status":{"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0036808f8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Sep 27 20:51:31.158: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-78bc8b888c  deployment-5925 /apis/apps/v1/namespaces/deployment-5925/replicasets/test-rollover-deployment-78bc8b888c a2cdafee-6c75-4452-8156-44e9d04dae74 104739 2 2021-09-27 20:51:14 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:78bc8b888c] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment 41666a52-359f-4d01-898f-9faf980921aa 0xc003680a47 0xc003680a48}] []  [{kube-controller-manager Update apps/v1 2021-09-27 20:51:17 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"41666a52-359f-4d01-898f-9faf980921aa\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 78bc8b888c,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:78bc8b888c] map[] [] []  []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003680ad8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Sep 27 20:51:31.176: INFO: Pod "test-rollover-deployment-5797c7764-lmv88" is available:
&Pod{ObjectMeta:{test-rollover-deployment-5797c7764-lmv88 test-rollover-deployment-5797c7764- deployment-5925 /api/v1/namespaces/deployment-5925/pods/test-rollover-deployment-5797c7764-lmv88 4ce40d9e-7d95-44f4-bb1c-95836358b526 104765 0 2021-09-27 20:51:17 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:5797c7764] map[cni.projectcalico.org/podIP:172.30.75.57/32 cni.projectcalico.org/podIPs:172.30.75.57/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "",
    "ips": [
        "172.30.75.57"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "",
    "ips": [
        "172.30.75.57"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet test-rollover-deployment-5797c7764 6df636d8-37e1-4328-9dec-034e237ea3e6 0xc0036810b7 0xc0036810b8}] []  [{kube-controller-manager Update v1 2021-09-27 20:51:17 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6df636d8-37e1-4328-9dec-034e237ea3e6\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-09-27 20:51:18 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {multus Update v1 2021-09-27 20:51:18 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}}} {kubelet Update v1 2021-09-27 20:51:19 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.75.57\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-6gvbj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-6gvbj,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:k8s.gcr.io/e2e-test-images/agnhost:2.20,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-6gvbj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.177.248.117,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c57,c49,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-tbtww,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-27 20:51:17 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-27 20:51:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-27 20:51:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-27 20:51:17 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.177.248.117,PodIP:172.30.75.57,StartTime:2021-09-27 20:51:17 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-09-27 20:51:18 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.20,ImageID:k8s.gcr.io/e2e-test-images/agnhost@sha256:17e61a0b9e498b6c73ed97670906be3d5a3ae394739c1bd5b619e1a004885cf0,ContainerID:cri-o://f42226d5da97fd0cd69322c568cf208fc5790c5d11e40e3c19870cb53feddf7e,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.75.57,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 20:51:31.176: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-5925" for this suite.

• [SLOW TEST:23.604 seconds]
[sig-apps] Deployment
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support rollover [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should support rollover [Conformance]","total":305,"completed":233,"skipped":3789,"failed":0}
SSSSS
------------------------------
[sig-instrumentation] Events API 
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-instrumentation] Events API
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 20:51:31.225: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-instrumentation] Events API
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/instrumentation/events.go:81
[It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a test event
STEP: listing events in all namespaces
STEP: listing events in test namespace
STEP: listing events with field selection filtering on source
STEP: listing events with field selection filtering on reportingController
STEP: getting the test event
STEP: patching the test event
STEP: getting the test event
STEP: updating the test event
STEP: getting the test event
STEP: deleting the test event
STEP: listing events in all namespaces
STEP: listing events in test namespace
[AfterEach] [sig-instrumentation] Events API
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 20:51:31.852: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-8212" for this suite.
•{"msg":"PASSED [sig-instrumentation] Events API should ensure that an event can be fetched, patched, deleted, and listed [Conformance]","total":305,"completed":234,"skipped":3794,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Aggregator 
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 20:51:31.904: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename aggregator
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:76
Sep 27 20:51:32.058: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
[It] Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Registering the sample API server.
Sep 27 20:51:32.667: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
Sep 27 20:51:34.871: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63768372692, loc:(*time.Location)(0x7702840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63768372692, loc:(*time.Location)(0x7702840)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63768372692, loc:(*time.Location)(0x7702840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63768372692, loc:(*time.Location)(0x7702840)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67c46cd746\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 27 20:51:36.884: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63768372692, loc:(*time.Location)(0x7702840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63768372692, loc:(*time.Location)(0x7702840)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63768372692, loc:(*time.Location)(0x7702840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63768372692, loc:(*time.Location)(0x7702840)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67c46cd746\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 27 20:51:38.884: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63768372692, loc:(*time.Location)(0x7702840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63768372692, loc:(*time.Location)(0x7702840)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63768372692, loc:(*time.Location)(0x7702840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63768372692, loc:(*time.Location)(0x7702840)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67c46cd746\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 27 20:51:40.883: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63768372692, loc:(*time.Location)(0x7702840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63768372692, loc:(*time.Location)(0x7702840)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63768372692, loc:(*time.Location)(0x7702840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63768372692, loc:(*time.Location)(0x7702840)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67c46cd746\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 27 20:51:42.888: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63768372692, loc:(*time.Location)(0x7702840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63768372692, loc:(*time.Location)(0x7702840)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63768372692, loc:(*time.Location)(0x7702840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63768372692, loc:(*time.Location)(0x7702840)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67c46cd746\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 27 20:51:44.885: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63768372692, loc:(*time.Location)(0x7702840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63768372692, loc:(*time.Location)(0x7702840)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63768372692, loc:(*time.Location)(0x7702840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63768372692, loc:(*time.Location)(0x7702840)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67c46cd746\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 27 20:51:49.744: INFO: Waited 2.828787006s for the sample-apiserver to be ready to handle requests.
I0927 20:51:50.903299      22 request.go:645] Throttling request took 1.017647165s, request: GET:https://172.21.0.1:443/apis/operator.openshift.io/v1?timeout=32s
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:67
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 20:51:52.215: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "aggregator-5847" for this suite.

• [SLOW TEST:20.421 seconds]
[sig-api-machinery] Aggregator
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Aggregator Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]","total":305,"completed":235,"skipped":3808,"failed":0}
SSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 20:51:52.328: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name projected-configmap-test-volume-map-53893b4a-6bf4-46d4-8669-604a5ba4809c
STEP: Creating a pod to test consume configMaps
Sep 27 20:51:52.594: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-4ebe2dc2-b1ed-45f4-a0bb-c25acf94949b" in namespace "projected-4468" to be "Succeeded or Failed"
Sep 27 20:51:52.609: INFO: Pod "pod-projected-configmaps-4ebe2dc2-b1ed-45f4-a0bb-c25acf94949b": Phase="Pending", Reason="", readiness=false. Elapsed: 14.769323ms
Sep 27 20:51:54.622: INFO: Pod "pod-projected-configmaps-4ebe2dc2-b1ed-45f4-a0bb-c25acf94949b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027556162s
Sep 27 20:51:56.636: INFO: Pod "pod-projected-configmaps-4ebe2dc2-b1ed-45f4-a0bb-c25acf94949b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.041995247s
STEP: Saw pod success
Sep 27 20:51:56.636: INFO: Pod "pod-projected-configmaps-4ebe2dc2-b1ed-45f4-a0bb-c25acf94949b" satisfied condition "Succeeded or Failed"
Sep 27 20:51:56.650: INFO: Trying to get logs from node 10.177.248.117 pod pod-projected-configmaps-4ebe2dc2-b1ed-45f4-a0bb-c25acf94949b container projected-configmap-volume-test: <nil>
STEP: delete the pod
Sep 27 20:51:56.744: INFO: Waiting for pod pod-projected-configmaps-4ebe2dc2-b1ed-45f4-a0bb-c25acf94949b to disappear
Sep 27 20:51:56.758: INFO: Pod pod-projected-configmaps-4ebe2dc2-b1ed-45f4-a0bb-c25acf94949b no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 20:51:56.758: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4468" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","total":305,"completed":236,"skipped":3815,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 20:51:56.808: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Sep 27 20:51:59.039: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 20:51:59.096: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-1824" for this suite.
•{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":305,"completed":237,"skipped":3838,"failed":0}
SS
------------------------------
[sig-network] Services 
  should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 20:51:59.152: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a service nodeport-service with the type=NodePort in namespace services-2880
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-2880
STEP: creating replication controller externalsvc in namespace services-2880
I0927 20:51:59.411769      22 runners.go:190] Created replication controller with name: externalsvc, namespace: services-2880, replica count: 2
I0927 20:52:02.467086      22 runners.go:190] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the NodePort service to type=ExternalName
Sep 27 20:52:02.563: INFO: Creating new exec pod
Sep 27 20:52:06.632: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-679324930 exec --namespace=services-2880 execpod2flft -- /bin/sh -x -c nslookup nodeport-service.services-2880.svc.cluster.local'
Sep 27 20:52:07.067: INFO: stderr: "+ nslookup nodeport-service.services-2880.svc.cluster.local\n"
Sep 27 20:52:07.067: INFO: stdout: "Server:\t\t172.21.0.10\nAddress:\t172.21.0.10#53\n\nnodeport-service.services-2880.svc.cluster.local\tcanonical name = externalsvc.services-2880.svc.cluster.local.\nName:\texternalsvc.services-2880.svc.cluster.local\nAddress: 172.21.175.19\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-2880, will wait for the garbage collector to delete the pods
Sep 27 20:52:07.158: INFO: Deleting ReplicationController externalsvc took: 28.133109ms
Sep 27 20:52:07.259: INFO: Terminating ReplicationController externalsvc pods took: 101.631945ms
Sep 27 20:52:21.940: INFO: Cleaning up the NodePort to ExternalName test service
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 20:52:22.003: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-2880" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

• [SLOW TEST:22.897 seconds]
[sig-network] Services
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from NodePort to ExternalName [Conformance]","total":305,"completed":238,"skipped":3840,"failed":0}
SSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] 
  custom resource defaulting for requests and from storage works  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 20:52:22.049: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] custom resource defaulting for requests and from storage works  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Sep 27 20:52:22.208: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 20:52:23.534: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-6876" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] custom resource defaulting for requests and from storage works  [Conformance]","total":305,"completed":239,"skipped":3844,"failed":0}
SSS
------------------------------
[sig-node] ConfigMap 
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 20:52:23.578: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap that has name configmap-test-emptyKey-83050c81-333e-4ef5-af2f-fcf762787795
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 20:52:23.762: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9689" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should fail to create ConfigMap with empty key [Conformance]","total":305,"completed":240,"skipped":3847,"failed":0}
S
------------------------------
[sig-network] Services 
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 20:52:23.817: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating service endpoint-test2 in namespace services-5443
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-5443 to expose endpoints map[]
Sep 27 20:52:24.044: INFO: Failed go get Endpoints object: endpoints "endpoint-test2" not found
Sep 27 20:52:25.092: INFO: successfully validated that service endpoint-test2 in namespace services-5443 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-5443
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-5443 to expose endpoints map[pod1:[80]]
Sep 27 20:52:27.244: INFO: successfully validated that service endpoint-test2 in namespace services-5443 exposes endpoints map[pod1:[80]]
STEP: Creating pod pod2 in namespace services-5443
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-5443 to expose endpoints map[pod1:[80] pod2:[80]]
Sep 27 20:52:30.395: INFO: successfully validated that service endpoint-test2 in namespace services-5443 exposes endpoints map[pod1:[80] pod2:[80]]
STEP: Deleting pod pod1 in namespace services-5443
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-5443 to expose endpoints map[pod2:[80]]
Sep 27 20:52:30.479: INFO: successfully validated that service endpoint-test2 in namespace services-5443 exposes endpoints map[pod2:[80]]
STEP: Deleting pod pod2 in namespace services-5443
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-5443 to expose endpoints map[]
Sep 27 20:52:30.561: INFO: successfully validated that service endpoint-test2 in namespace services-5443 exposes endpoints map[]
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 20:52:30.650: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-5443" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

• [SLOW TEST:6.909 seconds]
[sig-network] Services
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should serve a basic endpoint from pods  [Conformance]","total":305,"completed":241,"skipped":3848,"failed":0}
S
------------------------------
[k8s.io] Security Context when creating containers with AllowPrivilegeEscalation 
  should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 20:52:30.728: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:41
[It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Sep 27 20:52:30.958: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-28e717ac-09c3-42eb-8de4-523faa1f13f6" in namespace "security-context-test-177" to be "Succeeded or Failed"
Sep 27 20:52:30.985: INFO: Pod "alpine-nnp-false-28e717ac-09c3-42eb-8de4-523faa1f13f6": Phase="Pending", Reason="", readiness=false. Elapsed: 26.809548ms
Sep 27 20:52:33.001: INFO: Pod "alpine-nnp-false-28e717ac-09c3-42eb-8de4-523faa1f13f6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.043640855s
Sep 27 20:52:35.015: INFO: Pod "alpine-nnp-false-28e717ac-09c3-42eb-8de4-523faa1f13f6": Phase="Pending", Reason="", readiness=false. Elapsed: 4.057792676s
Sep 27 20:52:37.031: INFO: Pod "alpine-nnp-false-28e717ac-09c3-42eb-8de4-523faa1f13f6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.073762342s
Sep 27 20:52:37.032: INFO: Pod "alpine-nnp-false-28e717ac-09c3-42eb-8de4-523faa1f13f6" satisfied condition "Succeeded or Failed"
[AfterEach] [k8s.io] Security Context
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 20:52:37.064: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-177" for this suite.

• [SLOW TEST:6.384 seconds]
[k8s.io] Security Context
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  when creating containers with AllowPrivilegeEscalation
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:291
    should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Security Context when creating containers with AllowPrivilegeEscalation should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":242,"skipped":3849,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Service endpoints latency 
  should not be very high  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 20:52:37.112: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename svc-latency
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be very high  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Sep 27 20:52:37.255: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: creating replication controller svc-latency-rc in namespace svc-latency-7707
I0927 20:52:37.284735      22 runners.go:190] Created replication controller with name: svc-latency-rc, namespace: svc-latency-7707, replica count: 1
I0927 20:52:38.335384      22 runners.go:190] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0927 20:52:39.335721      22 runners.go:190] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Sep 27 20:52:39.475: INFO: Created: latency-svc-gqrm6
Sep 27 20:52:39.503: INFO: Got endpoints: latency-svc-gqrm6 [67.094902ms]
Sep 27 20:52:39.544: INFO: Created: latency-svc-pfd5f
Sep 27 20:52:39.579: INFO: Got endpoints: latency-svc-pfd5f [75.950998ms]
Sep 27 20:52:39.581: INFO: Created: latency-svc-zvwgf
Sep 27 20:52:39.602: INFO: Created: latency-svc-9k4rl
Sep 27 20:52:39.606: INFO: Got endpoints: latency-svc-zvwgf [101.801718ms]
Sep 27 20:52:39.624: INFO: Created: latency-svc-p2tzx
Sep 27 20:52:39.625: INFO: Got endpoints: latency-svc-9k4rl [120.665026ms]
Sep 27 20:52:39.638: INFO: Created: latency-svc-89q6n
Sep 27 20:52:39.647: INFO: Got endpoints: latency-svc-p2tzx [143.303551ms]
Sep 27 20:52:39.652: INFO: Created: latency-svc-24lsq
Sep 27 20:52:39.655: INFO: Got endpoints: latency-svc-89q6n [150.632914ms]
Sep 27 20:52:39.667: INFO: Got endpoints: latency-svc-24lsq [163.691542ms]
Sep 27 20:52:39.674: INFO: Created: latency-svc-6xm2z
Sep 27 20:52:39.693: INFO: Created: latency-svc-s6982
Sep 27 20:52:39.697: INFO: Got endpoints: latency-svc-6xm2z [192.499869ms]
Sep 27 20:52:39.707: INFO: Created: latency-svc-2gjrf
Sep 27 20:52:39.712: INFO: Got endpoints: latency-svc-s6982 [207.547051ms]
Sep 27 20:52:39.724: INFO: Got endpoints: latency-svc-2gjrf [220.164815ms]
Sep 27 20:52:39.725: INFO: Created: latency-svc-42kqz
Sep 27 20:52:39.738: INFO: Created: latency-svc-dh6wk
Sep 27 20:52:39.750: INFO: Got endpoints: latency-svc-42kqz [246.994939ms]
Sep 27 20:52:39.758: INFO: Got endpoints: latency-svc-dh6wk [254.43344ms]
Sep 27 20:52:39.759: INFO: Created: latency-svc-xtzgg
Sep 27 20:52:39.775: INFO: Got endpoints: latency-svc-xtzgg [271.65643ms]
Sep 27 20:52:39.780: INFO: Created: latency-svc-hh9bq
Sep 27 20:52:39.792: INFO: Created: latency-svc-xxfxn
Sep 27 20:52:39.803: INFO: Got endpoints: latency-svc-hh9bq [299.158901ms]
Sep 27 20:52:39.810: INFO: Got endpoints: latency-svc-xxfxn [305.581523ms]
Sep 27 20:52:39.814: INFO: Created: latency-svc-hscp2
Sep 27 20:52:39.833: INFO: Created: latency-svc-vpth8
Sep 27 20:52:39.835: INFO: Got endpoints: latency-svc-hscp2 [331.66545ms]
Sep 27 20:52:39.846: INFO: Created: latency-svc-hblk5
Sep 27 20:52:39.854: INFO: Got endpoints: latency-svc-vpth8 [274.334082ms]
Sep 27 20:52:39.861: INFO: Created: latency-svc-76bgb
Sep 27 20:52:39.864: INFO: Got endpoints: latency-svc-hblk5 [258.492001ms]
Sep 27 20:52:39.876: INFO: Created: latency-svc-xdfm4
Sep 27 20:52:39.880: INFO: Got endpoints: latency-svc-76bgb [255.047484ms]
Sep 27 20:52:39.894: INFO: Got endpoints: latency-svc-xdfm4 [246.96637ms]
Sep 27 20:52:39.895: INFO: Created: latency-svc-rk58c
Sep 27 20:52:39.913: INFO: Created: latency-svc-m7xq8
Sep 27 20:52:39.917: INFO: Got endpoints: latency-svc-rk58c [261.999753ms]
Sep 27 20:52:39.929: INFO: Created: latency-svc-m6rjr
Sep 27 20:52:39.930: INFO: Got endpoints: latency-svc-m7xq8 [262.532559ms]
Sep 27 20:52:39.942: INFO: Created: latency-svc-fc9hz
Sep 27 20:52:39.948: INFO: Got endpoints: latency-svc-m6rjr [251.620362ms]
Sep 27 20:52:39.970: INFO: Got endpoints: latency-svc-fc9hz [257.753278ms]
Sep 27 20:52:39.972: INFO: Created: latency-svc-swmjh
Sep 27 20:52:39.986: INFO: Created: latency-svc-vdqlx
Sep 27 20:52:39.995: INFO: Got endpoints: latency-svc-swmjh [270.657515ms]
Sep 27 20:52:40.002: INFO: Created: latency-svc-fxxdn
Sep 27 20:52:40.022: INFO: Got endpoints: latency-svc-vdqlx [271.794887ms]
Sep 27 20:52:40.024: INFO: Got endpoints: latency-svc-fxxdn [265.452162ms]
Sep 27 20:52:40.036: INFO: Created: latency-svc-l28mh
Sep 27 20:52:40.053: INFO: Created: latency-svc-lccht
Sep 27 20:52:40.056: INFO: Got endpoints: latency-svc-l28mh [280.21747ms]
Sep 27 20:52:40.067: INFO: Created: latency-svc-lhdjc
Sep 27 20:52:40.071: INFO: Got endpoints: latency-svc-lccht [267.405877ms]
Sep 27 20:52:40.092: INFO: Created: latency-svc-84f9s
Sep 27 20:52:40.093: INFO: Got endpoints: latency-svc-lhdjc [283.299739ms]
Sep 27 20:52:40.104: INFO: Created: latency-svc-dtx48
Sep 27 20:52:40.112: INFO: Got endpoints: latency-svc-84f9s [276.975845ms]
Sep 27 20:52:40.124: INFO: Created: latency-svc-b25l9
Sep 27 20:52:40.132: INFO: Got endpoints: latency-svc-dtx48 [278.298345ms]
Sep 27 20:52:40.136: INFO: Got endpoints: latency-svc-b25l9 [271.190922ms]
Sep 27 20:52:40.145: INFO: Created: latency-svc-6frpk
Sep 27 20:52:40.156: INFO: Created: latency-svc-wl5pz
Sep 27 20:52:40.163: INFO: Got endpoints: latency-svc-6frpk [282.951979ms]
Sep 27 20:52:40.173: INFO: Created: latency-svc-rhpsr
Sep 27 20:52:40.176: INFO: Got endpoints: latency-svc-wl5pz [280.712353ms]
Sep 27 20:52:40.185: INFO: Created: latency-svc-j94mm
Sep 27 20:52:40.192: INFO: Got endpoints: latency-svc-rhpsr [56.030585ms]
Sep 27 20:52:40.202: INFO: Got endpoints: latency-svc-j94mm [285.568205ms]
Sep 27 20:52:40.208: INFO: Created: latency-svc-qtx8b
Sep 27 20:52:40.229: INFO: Created: latency-svc-jwksx
Sep 27 20:52:40.229: INFO: Got endpoints: latency-svc-qtx8b [298.971204ms]
Sep 27 20:52:40.241: INFO: Created: latency-svc-mtfr2
Sep 27 20:52:40.243: INFO: Got endpoints: latency-svc-jwksx [294.245873ms]
Sep 27 20:52:40.252: INFO: Created: latency-svc-rd9m5
Sep 27 20:52:40.258: INFO: Got endpoints: latency-svc-mtfr2 [288.501216ms]
Sep 27 20:52:40.272: INFO: Created: latency-svc-vgdlr
Sep 27 20:52:40.272: INFO: Got endpoints: latency-svc-rd9m5 [276.916526ms]
Sep 27 20:52:40.288: INFO: Created: latency-svc-vpgvw
Sep 27 20:52:40.294: INFO: Got endpoints: latency-svc-vgdlr [271.236418ms]
Sep 27 20:52:40.297: INFO: Created: latency-svc-rwvwm
Sep 27 20:52:40.310: INFO: Got endpoints: latency-svc-vpgvw [286.085712ms]
Sep 27 20:52:40.317: INFO: Got endpoints: latency-svc-rwvwm [261.468036ms]
Sep 27 20:52:40.318: INFO: Created: latency-svc-7hlkw
Sep 27 20:52:40.339: INFO: Created: latency-svc-qmjtr
Sep 27 20:52:40.349: INFO: Got endpoints: latency-svc-7hlkw [278.534303ms]
Sep 27 20:52:40.355: INFO: Created: latency-svc-r8wdn
Sep 27 20:52:40.363: INFO: Got endpoints: latency-svc-qmjtr [270.191519ms]
Sep 27 20:52:40.372: INFO: Created: latency-svc-ntlww
Sep 27 20:52:40.378: INFO: Got endpoints: latency-svc-r8wdn [265.232596ms]
Sep 27 20:52:40.392: INFO: Created: latency-svc-lcx4l
Sep 27 20:52:40.392: INFO: Got endpoints: latency-svc-ntlww [260.016128ms]
Sep 27 20:52:40.401: INFO: Created: latency-svc-8pwcw
Sep 27 20:52:40.402: INFO: Got endpoints: latency-svc-lcx4l [238.940752ms]
Sep 27 20:52:40.424: INFO: Created: latency-svc-4kmr2
Sep 27 20:52:40.430: INFO: Got endpoints: latency-svc-8pwcw [254.387049ms]
Sep 27 20:52:40.451: INFO: Created: latency-svc-tzcmd
Sep 27 20:52:40.451: INFO: Got endpoints: latency-svc-4kmr2 [258.644693ms]
Sep 27 20:52:40.467: INFO: Got endpoints: latency-svc-tzcmd [264.748027ms]
Sep 27 20:52:40.468: INFO: Created: latency-svc-9xlq9
Sep 27 20:52:40.479: INFO: Got endpoints: latency-svc-9xlq9 [249.778891ms]
Sep 27 20:52:40.485: INFO: Created: latency-svc-qrwq7
Sep 27 20:52:40.504: INFO: Created: latency-svc-4wh9r
Sep 27 20:52:40.514: INFO: Got endpoints: latency-svc-qrwq7 [271.477057ms]
Sep 27 20:52:40.538: INFO: Created: latency-svc-f8tp9
Sep 27 20:52:40.539: INFO: Got endpoints: latency-svc-4wh9r [280.802879ms]
Sep 27 20:52:40.556: INFO: Got endpoints: latency-svc-f8tp9 [279.209483ms]
Sep 27 20:52:40.557: INFO: Created: latency-svc-wkk8q
Sep 27 20:52:40.574: INFO: Got endpoints: latency-svc-wkk8q [276.168209ms]
Sep 27 20:52:40.587: INFO: Created: latency-svc-qn5dq
Sep 27 20:52:40.595: INFO: Created: latency-svc-t5zhc
Sep 27 20:52:40.603: INFO: Got endpoints: latency-svc-qn5dq [292.541788ms]
Sep 27 20:52:40.625: INFO: Got endpoints: latency-svc-t5zhc [306.368955ms]
Sep 27 20:52:40.633: INFO: Created: latency-svc-d7wnw
Sep 27 20:52:40.642: INFO: Got endpoints: latency-svc-d7wnw [292.470696ms]
Sep 27 20:52:40.644: INFO: Created: latency-svc-6mf29
Sep 27 20:52:40.661: INFO: Created: latency-svc-xchsm
Sep 27 20:52:40.666: INFO: Got endpoints: latency-svc-6mf29 [302.693711ms]
Sep 27 20:52:40.673: INFO: Got endpoints: latency-svc-xchsm [294.889231ms]
Sep 27 20:52:40.689: INFO: Created: latency-svc-2xktr
Sep 27 20:52:40.699: INFO: Created: latency-svc-ntzkg
Sep 27 20:52:40.708: INFO: Got endpoints: latency-svc-2xktr [315.392123ms]
Sep 27 20:52:40.716: INFO: Created: latency-svc-ld57f
Sep 27 20:52:40.725: INFO: Got endpoints: latency-svc-ntzkg [322.602965ms]
Sep 27 20:52:40.738: INFO: Got endpoints: latency-svc-ld57f [297.216028ms]
Sep 27 20:52:40.738: INFO: Created: latency-svc-cfczt
Sep 27 20:52:40.751: INFO: Created: latency-svc-6x5k7
Sep 27 20:52:40.754: INFO: Got endpoints: latency-svc-cfczt [302.617256ms]
Sep 27 20:52:40.767: INFO: Created: latency-svc-2bxgw
Sep 27 20:52:40.773: INFO: Got endpoints: latency-svc-6x5k7 [305.453842ms]
Sep 27 20:52:40.789: INFO: Created: latency-svc-6xphh
Sep 27 20:52:40.790: INFO: Got endpoints: latency-svc-2bxgw [310.192842ms]
Sep 27 20:52:40.801: INFO: Created: latency-svc-d86xf
Sep 27 20:52:40.804: INFO: Got endpoints: latency-svc-6xphh [284.385921ms]
Sep 27 20:52:40.814: INFO: Created: latency-svc-4767q
Sep 27 20:52:40.825: INFO: Got endpoints: latency-svc-d86xf [285.955516ms]
Sep 27 20:52:40.832: INFO: Created: latency-svc-vsk8q
Sep 27 20:52:40.837: INFO: Got endpoints: latency-svc-4767q [280.520322ms]
Sep 27 20:52:40.850: INFO: Created: latency-svc-5lm59
Sep 27 20:52:40.852: INFO: Got endpoints: latency-svc-vsk8q [277.234735ms]
Sep 27 20:52:40.868: INFO: Created: latency-svc-nvxpf
Sep 27 20:52:40.869: INFO: Got endpoints: latency-svc-5lm59 [266.238789ms]
Sep 27 20:52:40.882: INFO: Created: latency-svc-5bb28
Sep 27 20:52:40.887: INFO: Got endpoints: latency-svc-nvxpf [261.95992ms]
Sep 27 20:52:40.899: INFO: Created: latency-svc-w5l5f
Sep 27 20:52:40.901: INFO: Got endpoints: latency-svc-5bb28 [258.249175ms]
Sep 27 20:52:40.917: INFO: Created: latency-svc-f5qq4
Sep 27 20:52:40.923: INFO: Got endpoints: latency-svc-w5l5f [257.013309ms]
Sep 27 20:52:40.931: INFO: Created: latency-svc-g227j
Sep 27 20:52:40.944: INFO: Got endpoints: latency-svc-f5qq4 [270.720481ms]
Sep 27 20:52:40.956: INFO: Got endpoints: latency-svc-g227j [248.554751ms]
Sep 27 20:52:40.972: INFO: Created: latency-svc-9z452
Sep 27 20:52:40.980: INFO: Created: latency-svc-t4tjn
Sep 27 20:52:41.005: INFO: Got endpoints: latency-svc-t4tjn [267.265451ms]
Sep 27 20:52:41.005: INFO: Created: latency-svc-q79z8
Sep 27 20:52:41.005: INFO: Got endpoints: latency-svc-9z452 [280.492965ms]
Sep 27 20:52:41.018: INFO: Got endpoints: latency-svc-q79z8 [264.017701ms]
Sep 27 20:52:41.037: INFO: Created: latency-svc-kh6fq
Sep 27 20:52:41.049: INFO: Created: latency-svc-5dwm6
Sep 27 20:52:41.060: INFO: Got endpoints: latency-svc-kh6fq [286.717198ms]
Sep 27 20:52:41.076: INFO: Got endpoints: latency-svc-5dwm6 [286.484232ms]
Sep 27 20:52:41.086: INFO: Created: latency-svc-nrwx9
Sep 27 20:52:41.087: INFO: Created: latency-svc-trf6q
Sep 27 20:52:41.107: INFO: Created: latency-svc-h2v24
Sep 27 20:52:41.108: INFO: Got endpoints: latency-svc-trf6q [282.753898ms]
Sep 27 20:52:41.109: INFO: Got endpoints: latency-svc-nrwx9 [305.125689ms]
Sep 27 20:52:41.122: INFO: Created: latency-svc-7tg7h
Sep 27 20:52:41.124: INFO: Got endpoints: latency-svc-h2v24 [286.230083ms]
Sep 27 20:52:41.143: INFO: Created: latency-svc-b9sr2
Sep 27 20:52:41.143: INFO: Got endpoints: latency-svc-7tg7h [291.17169ms]
Sep 27 20:52:41.159: INFO: Created: latency-svc-sg4hl
Sep 27 20:52:41.160: INFO: Got endpoints: latency-svc-b9sr2 [290.901134ms]
Sep 27 20:52:41.185: INFO: Got endpoints: latency-svc-sg4hl [297.808723ms]
Sep 27 20:52:41.190: INFO: Created: latency-svc-fdcrf
Sep 27 20:52:41.208: INFO: Created: latency-svc-6tlfj
Sep 27 20:52:41.222: INFO: Got endpoints: latency-svc-fdcrf [321.337368ms]
Sep 27 20:52:41.222: INFO: Created: latency-svc-wdwgc
Sep 27 20:52:41.239: INFO: Got endpoints: latency-svc-6tlfj [315.661216ms]
Sep 27 20:52:41.240: INFO: Created: latency-svc-q67h8
Sep 27 20:52:41.246: INFO: Got endpoints: latency-svc-wdwgc [302.745874ms]
Sep 27 20:52:41.253: INFO: Created: latency-svc-jbtrg
Sep 27 20:52:41.258: INFO: Got endpoints: latency-svc-q67h8 [301.921987ms]
Sep 27 20:52:41.275: INFO: Got endpoints: latency-svc-jbtrg [269.992213ms]
Sep 27 20:52:41.276: INFO: Created: latency-svc-ndvw6
Sep 27 20:52:41.294: INFO: Got endpoints: latency-svc-ndvw6 [288.72158ms]
Sep 27 20:52:41.295: INFO: Created: latency-svc-p7vww
Sep 27 20:52:41.328: INFO: Created: latency-svc-xzmbh
Sep 27 20:52:41.328: INFO: Created: latency-svc-c4rhl
Sep 27 20:52:41.328: INFO: Got endpoints: latency-svc-c4rhl [268.435969ms]
Sep 27 20:52:41.329: INFO: Got endpoints: latency-svc-p7vww [310.389853ms]
Sep 27 20:52:41.339: INFO: Created: latency-svc-r5x5p
Sep 27 20:52:41.341: INFO: Got endpoints: latency-svc-xzmbh [264.27469ms]
Sep 27 20:52:41.363: INFO: Got endpoints: latency-svc-r5x5p [254.466286ms]
Sep 27 20:52:41.363: INFO: Created: latency-svc-wvps7
Sep 27 20:52:41.371: INFO: Created: latency-svc-k2l7x
Sep 27 20:52:41.375: INFO: Got endpoints: latency-svc-wvps7 [264.757067ms]
Sep 27 20:52:41.382: INFO: Created: latency-svc-98b7n
Sep 27 20:52:41.388: INFO: Got endpoints: latency-svc-k2l7x [264.438904ms]
Sep 27 20:52:41.402: INFO: Got endpoints: latency-svc-98b7n [259.055022ms]
Sep 27 20:52:41.406: INFO: Created: latency-svc-db6rx
Sep 27 20:52:41.417: INFO: Created: latency-svc-gg429
Sep 27 20:52:41.424: INFO: Got endpoints: latency-svc-db6rx [262.84215ms]
Sep 27 20:52:41.430: INFO: Got endpoints: latency-svc-gg429 [244.569179ms]
Sep 27 20:52:41.432: INFO: Created: latency-svc-nj487
Sep 27 20:52:41.451: INFO: Created: latency-svc-9r7vl
Sep 27 20:52:41.451: INFO: Got endpoints: latency-svc-nj487 [229.156069ms]
Sep 27 20:52:41.471: INFO: Got endpoints: latency-svc-9r7vl [232.322324ms]
Sep 27 20:52:41.472: INFO: Created: latency-svc-2zdkp
Sep 27 20:52:41.477: INFO: Created: latency-svc-dcx5q
Sep 27 20:52:41.483: INFO: Got endpoints: latency-svc-2zdkp [236.177222ms]
Sep 27 20:52:41.497: INFO: Created: latency-svc-hj87h
Sep 27 20:52:41.498: INFO: Got endpoints: latency-svc-dcx5q [239.077996ms]
Sep 27 20:52:41.513: INFO: Created: latency-svc-qj86p
Sep 27 20:52:41.517: INFO: Got endpoints: latency-svc-hj87h [241.236211ms]
Sep 27 20:52:41.534: INFO: Got endpoints: latency-svc-qj86p [240.275966ms]
Sep 27 20:52:41.535: INFO: Created: latency-svc-48wj2
Sep 27 20:52:41.554: INFO: Got endpoints: latency-svc-48wj2 [225.525991ms]
Sep 27 20:52:41.554: INFO: Created: latency-svc-tn9gg
Sep 27 20:52:41.570: INFO: Created: latency-svc-xd8mp
Sep 27 20:52:41.571: INFO: Got endpoints: latency-svc-tn9gg [241.935193ms]
Sep 27 20:52:41.586: INFO: Created: latency-svc-77wg4
Sep 27 20:52:41.590: INFO: Got endpoints: latency-svc-xd8mp [248.61526ms]
Sep 27 20:52:41.617: INFO: Got endpoints: latency-svc-77wg4 [253.590167ms]
Sep 27 20:52:41.617: INFO: Created: latency-svc-w4xtr
Sep 27 20:52:41.618: INFO: Created: latency-svc-8h8t6
Sep 27 20:52:41.618: INFO: Got endpoints: latency-svc-w4xtr [242.96632ms]
Sep 27 20:52:41.632: INFO: Created: latency-svc-gr99h
Sep 27 20:52:41.632: INFO: Got endpoints: latency-svc-8h8t6 [243.852165ms]
Sep 27 20:52:41.643: INFO: Created: latency-svc-vw7xs
Sep 27 20:52:41.665: INFO: Got endpoints: latency-svc-gr99h [261.853635ms]
Sep 27 20:52:41.665: INFO: Got endpoints: latency-svc-vw7xs [240.80869ms]
Sep 27 20:52:41.665: INFO: Created: latency-svc-5bc6z
Sep 27 20:52:41.672: INFO: Created: latency-svc-hw4pg
Sep 27 20:52:41.692: INFO: Created: latency-svc-ztpgx
Sep 27 20:52:41.692: INFO: Got endpoints: latency-svc-5bc6z [262.671138ms]
Sep 27 20:52:41.697: INFO: Got endpoints: latency-svc-hw4pg [245.584695ms]
Sep 27 20:52:41.709: INFO: Created: latency-svc-2tw7x
Sep 27 20:52:41.724: INFO: Created: latency-svc-tf4pl
Sep 27 20:52:41.725: INFO: Got endpoints: latency-svc-ztpgx [253.234966ms]
Sep 27 20:52:41.732: INFO: Got endpoints: latency-svc-2tw7x [249.152497ms]
Sep 27 20:52:41.742: INFO: Got endpoints: latency-svc-tf4pl [244.790217ms]
Sep 27 20:52:41.743: INFO: Created: latency-svc-cl9t4
Sep 27 20:52:41.764: INFO: Created: latency-svc-7cjfg
Sep 27 20:52:41.764: INFO: Got endpoints: latency-svc-cl9t4 [247.609378ms]
Sep 27 20:52:41.791: INFO: Got endpoints: latency-svc-7cjfg [256.847147ms]
Sep 27 20:52:41.796: INFO: Created: latency-svc-dt7jn
Sep 27 20:52:41.796: INFO: Created: latency-svc-qtwpl
Sep 27 20:52:41.796: INFO: Got endpoints: latency-svc-qtwpl [239.482232ms]
Sep 27 20:52:41.809: INFO: Created: latency-svc-5rnz9
Sep 27 20:52:41.821: INFO: Created: latency-svc-72q7v
Sep 27 20:52:41.823: INFO: Got endpoints: latency-svc-dt7jn [251.674101ms]
Sep 27 20:52:41.838: INFO: Got endpoints: latency-svc-5rnz9 [248.62409ms]
Sep 27 20:52:41.842: INFO: Got endpoints: latency-svc-72q7v [224.294804ms]
Sep 27 20:52:41.847: INFO: Created: latency-svc-78sk8
Sep 27 20:52:41.865: INFO: Got endpoints: latency-svc-78sk8 [247.903941ms]
Sep 27 20:52:41.866: INFO: Created: latency-svc-xfxt8
Sep 27 20:52:41.884: INFO: Created: latency-svc-dp6rt
Sep 27 20:52:41.885: INFO: Got endpoints: latency-svc-xfxt8 [252.054068ms]
Sep 27 20:52:41.907: INFO: Got endpoints: latency-svc-dp6rt [241.149842ms]
Sep 27 20:52:41.907: INFO: Created: latency-svc-c8ts4
Sep 27 20:52:41.917: INFO: Created: latency-svc-7972g
Sep 27 20:52:41.930: INFO: Got endpoints: latency-svc-c8ts4 [264.047999ms]
Sep 27 20:52:41.937: INFO: Created: latency-svc-7jbf4
Sep 27 20:52:41.936: INFO: Got endpoints: latency-svc-7972g [241.636289ms]
Sep 27 20:52:41.949: INFO: Created: latency-svc-srkht
Sep 27 20:52:41.952: INFO: Got endpoints: latency-svc-7jbf4 [253.171803ms]
Sep 27 20:52:41.967: INFO: Created: latency-svc-ngw26
Sep 27 20:52:41.969: INFO: Got endpoints: latency-svc-srkht [243.615046ms]
Sep 27 20:52:41.981: INFO: Created: latency-svc-5gvtk
Sep 27 20:52:41.989: INFO: Got endpoints: latency-svc-ngw26 [252.816182ms]
Sep 27 20:52:42.001: INFO: Created: latency-svc-sclhk
Sep 27 20:52:42.007: INFO: Got endpoints: latency-svc-5gvtk [264.631504ms]
Sep 27 20:52:42.008: INFO: Created: latency-svc-9m5rp
Sep 27 20:52:42.022: INFO: Got endpoints: latency-svc-sclhk [257.120672ms]
Sep 27 20:52:42.027: INFO: Got endpoints: latency-svc-9m5rp [235.645247ms]
Sep 27 20:52:42.029: INFO: Created: latency-svc-xtbz6
Sep 27 20:52:42.041: INFO: Created: latency-svc-dkfc6
Sep 27 20:52:42.048: INFO: Got endpoints: latency-svc-xtbz6 [251.646691ms]
Sep 27 20:52:42.059: INFO: Created: latency-svc-tpl9t
Sep 27 20:52:42.064: INFO: Got endpoints: latency-svc-dkfc6 [240.899171ms]
Sep 27 20:52:42.073: INFO: Created: latency-svc-qbpbt
Sep 27 20:52:42.082: INFO: Got endpoints: latency-svc-tpl9t [243.005605ms]
Sep 27 20:52:42.090: INFO: Created: latency-svc-pwk2v
Sep 27 20:52:42.098: INFO: Got endpoints: latency-svc-qbpbt [254.771106ms]
Sep 27 20:52:42.106: INFO: Created: latency-svc-bftw4
Sep 27 20:52:42.110: INFO: Got endpoints: latency-svc-pwk2v [245.220549ms]
Sep 27 20:52:42.123: INFO: Created: latency-svc-2wqwc
Sep 27 20:52:42.131: INFO: Got endpoints: latency-svc-bftw4 [245.897349ms]
Sep 27 20:52:42.132: INFO: Created: latency-svc-5j7hs
Sep 27 20:52:42.144: INFO: Got endpoints: latency-svc-2wqwc [236.955935ms]
Sep 27 20:52:42.147: INFO: Created: latency-svc-6x4kd
Sep 27 20:52:42.148: INFO: Got endpoints: latency-svc-5j7hs [217.34402ms]
Sep 27 20:52:42.166: INFO: Created: latency-svc-wr8s4
Sep 27 20:52:42.181: INFO: Created: latency-svc-47djd
Sep 27 20:52:42.186: INFO: Got endpoints: latency-svc-6x4kd [246.32326ms]
Sep 27 20:52:42.189: INFO: Got endpoints: latency-svc-wr8s4 [236.794725ms]
Sep 27 20:52:42.195: INFO: Created: latency-svc-8xblf
Sep 27 20:52:42.197: INFO: Got endpoints: latency-svc-47djd [224.464382ms]
Sep 27 20:52:42.205: INFO: Created: latency-svc-684lb
Sep 27 20:52:42.211: INFO: Got endpoints: latency-svc-8xblf [222.313538ms]
Sep 27 20:52:42.221: INFO: Created: latency-svc-pk5kh
Sep 27 20:52:42.222: INFO: Got endpoints: latency-svc-684lb [214.727781ms]
Sep 27 20:52:42.234: INFO: Created: latency-svc-jrcz8
Sep 27 20:52:42.239: INFO: Got endpoints: latency-svc-pk5kh [216.982562ms]
Sep 27 20:52:42.248: INFO: Created: latency-svc-4xjhg
Sep 27 20:52:42.251: INFO: Got endpoints: latency-svc-jrcz8 [223.426142ms]
Sep 27 20:52:42.268: INFO: Got endpoints: latency-svc-4xjhg [219.451614ms]
Sep 27 20:52:42.269: INFO: Created: latency-svc-mgnb4
Sep 27 20:52:42.275: INFO: Created: latency-svc-nb54r
Sep 27 20:52:42.286: INFO: Got endpoints: latency-svc-mgnb4 [221.845138ms]
Sep 27 20:52:42.291: INFO: Got endpoints: latency-svc-nb54r [208.889293ms]
Sep 27 20:52:42.303: INFO: Created: latency-svc-qnphk
Sep 27 20:52:42.307: INFO: Created: latency-svc-jjxbg
Sep 27 20:52:42.320: INFO: Got endpoints: latency-svc-qnphk [222.437247ms]
Sep 27 20:52:42.327: INFO: Got endpoints: latency-svc-jjxbg [215.221321ms]
Sep 27 20:52:42.333: INFO: Created: latency-svc-7fdsq
Sep 27 20:52:42.345: INFO: Created: latency-svc-zjdfw
Sep 27 20:52:42.354: INFO: Got endpoints: latency-svc-7fdsq [223.137055ms]
Sep 27 20:52:42.367: INFO: Got endpoints: latency-svc-zjdfw [222.55589ms]
Sep 27 20:52:42.368: INFO: Created: latency-svc-j89zr
Sep 27 20:52:42.381: INFO: Created: latency-svc-x49wn
Sep 27 20:52:42.386: INFO: Got endpoints: latency-svc-j89zr [237.758427ms]
Sep 27 20:52:42.399: INFO: Created: latency-svc-6xjhc
Sep 27 20:52:42.400: INFO: Got endpoints: latency-svc-x49wn [211.540489ms]
Sep 27 20:52:42.411: INFO: Created: latency-svc-8bkhr
Sep 27 20:52:42.424: INFO: Got endpoints: latency-svc-6xjhc [235.369482ms]
Sep 27 20:52:42.440: INFO: Got endpoints: latency-svc-8bkhr [242.479573ms]
Sep 27 20:52:42.442: INFO: Created: latency-svc-ls9vg
Sep 27 20:52:42.470: INFO: Got endpoints: latency-svc-ls9vg [258.312831ms]
Sep 27 20:52:42.471: INFO: Created: latency-svc-pdzfp
Sep 27 20:52:42.482: INFO: Got endpoints: latency-svc-pdzfp [259.331847ms]
Sep 27 20:52:42.486: INFO: Created: latency-svc-9n7nl
Sep 27 20:52:42.504: INFO: Got endpoints: latency-svc-9n7nl [264.900522ms]
Sep 27 20:52:42.505: INFO: Created: latency-svc-bghd6
Sep 27 20:52:42.517: INFO: Created: latency-svc-dr9s4
Sep 27 20:52:42.519: INFO: Got endpoints: latency-svc-bghd6 [267.747393ms]
Sep 27 20:52:42.533: INFO: Created: latency-svc-vnkzg
Sep 27 20:52:42.538: INFO: Got endpoints: latency-svc-dr9s4 [269.602279ms]
Sep 27 20:52:42.544: INFO: Created: latency-svc-pgpb7
Sep 27 20:52:42.551: INFO: Got endpoints: latency-svc-vnkzg [265.172384ms]
Sep 27 20:52:42.560: INFO: Created: latency-svc-f2fqg
Sep 27 20:52:42.563: INFO: Got endpoints: latency-svc-pgpb7 [272.361411ms]
Sep 27 20:52:42.579: INFO: Created: latency-svc-fvstd
Sep 27 20:52:42.583: INFO: Got endpoints: latency-svc-f2fqg [262.221695ms]
Sep 27 20:52:42.592: INFO: Created: latency-svc-xq5zv
Sep 27 20:52:42.595: INFO: Got endpoints: latency-svc-fvstd [267.821409ms]
Sep 27 20:52:42.609: INFO: Created: latency-svc-kb9lj
Sep 27 20:52:42.612: INFO: Got endpoints: latency-svc-xq5zv [257.522984ms]
Sep 27 20:52:42.623: INFO: Got endpoints: latency-svc-kb9lj [255.461445ms]
Sep 27 20:52:42.623: INFO: Created: latency-svc-v9qp5
Sep 27 20:52:42.632: INFO: Created: latency-svc-7fs7z
Sep 27 20:52:42.638: INFO: Got endpoints: latency-svc-v9qp5 [252.323706ms]
Sep 27 20:52:42.647: INFO: Created: latency-svc-gtzc2
Sep 27 20:52:42.650: INFO: Got endpoints: latency-svc-7fs7z [250.417712ms]
Sep 27 20:52:42.668: INFO: Created: latency-svc-hj24z
Sep 27 20:52:42.668: INFO: Got endpoints: latency-svc-gtzc2 [244.09133ms]
Sep 27 20:52:42.677: INFO: Created: latency-svc-s5fnm
Sep 27 20:52:42.689: INFO: Got endpoints: latency-svc-hj24z [249.281909ms]
Sep 27 20:52:42.694: INFO: Got endpoints: latency-svc-s5fnm [224.373765ms]
Sep 27 20:52:42.698: INFO: Created: latency-svc-2lhbz
Sep 27 20:52:42.720: INFO: Got endpoints: latency-svc-2lhbz [238.022834ms]
Sep 27 20:52:42.721: INFO: Created: latency-svc-ncjb4
Sep 27 20:52:42.729: INFO: Created: latency-svc-9ccz5
Sep 27 20:52:42.744: INFO: Created: latency-svc-lwvk4
Sep 27 20:52:42.744: INFO: Got endpoints: latency-svc-ncjb4 [240.17334ms]
Sep 27 20:52:42.755: INFO: Got endpoints: latency-svc-9ccz5 [235.462058ms]
Sep 27 20:52:42.758: INFO: Got endpoints: latency-svc-lwvk4 [219.732911ms]
Sep 27 20:52:42.760: INFO: Created: latency-svc-j9nfg
Sep 27 20:52:42.779: INFO: Got endpoints: latency-svc-j9nfg [227.062244ms]
Sep 27 20:52:42.782: INFO: Created: latency-svc-zsd6z
Sep 27 20:52:42.798: INFO: Created: latency-svc-vzlrw
Sep 27 20:52:42.799: INFO: Got endpoints: latency-svc-zsd6z [236.331388ms]
Sep 27 20:52:42.822: INFO: Got endpoints: latency-svc-vzlrw [238.906639ms]
Sep 27 20:52:42.822: INFO: Created: latency-svc-shfl6
Sep 27 20:52:42.828: INFO: Created: latency-svc-94q49
Sep 27 20:52:42.833: INFO: Got endpoints: latency-svc-shfl6 [237.580596ms]
Sep 27 20:52:42.847: INFO: Created: latency-svc-fm8ht
Sep 27 20:52:42.847: INFO: Got endpoints: latency-svc-94q49 [234.834165ms]
Sep 27 20:52:42.859: INFO: Created: latency-svc-jpfx8
Sep 27 20:52:42.864: INFO: Got endpoints: latency-svc-fm8ht [241.249931ms]
Sep 27 20:52:42.877: INFO: Created: latency-svc-sg6wb
Sep 27 20:52:42.879: INFO: Got endpoints: latency-svc-jpfx8 [239.986314ms]
Sep 27 20:52:42.887: INFO: Created: latency-svc-xdrn7
Sep 27 20:52:42.892: INFO: Got endpoints: latency-svc-sg6wb [241.748732ms]
Sep 27 20:52:42.902: INFO: Created: latency-svc-c7lld
Sep 27 20:52:42.903: INFO: Got endpoints: latency-svc-xdrn7 [233.380035ms]
Sep 27 20:52:42.917: INFO: Created: latency-svc-6hktq
Sep 27 20:52:42.921: INFO: Got endpoints: latency-svc-c7lld [231.135637ms]
Sep 27 20:52:42.936: INFO: Got endpoints: latency-svc-6hktq [241.741617ms]
Sep 27 20:52:42.937: INFO: Created: latency-svc-ngwbp
Sep 27 20:52:42.948: INFO: Created: latency-svc-kgjzk
Sep 27 20:52:42.953: INFO: Got endpoints: latency-svc-ngwbp [232.994051ms]
Sep 27 20:52:42.958: INFO: Created: latency-svc-s55nx
Sep 27 20:52:42.962: INFO: Got endpoints: latency-svc-kgjzk [217.426187ms]
Sep 27 20:52:42.973: INFO: Got endpoints: latency-svc-s55nx [217.802267ms]
Sep 27 20:52:42.973: INFO: Created: latency-svc-zb7zd
Sep 27 20:52:42.990: INFO: Got endpoints: latency-svc-zb7zd [231.688843ms]
Sep 27 20:52:42.990: INFO: Latencies: [56.030585ms 75.950998ms 101.801718ms 120.665026ms 143.303551ms 150.632914ms 163.691542ms 192.499869ms 207.547051ms 208.889293ms 211.540489ms 214.727781ms 215.221321ms 216.982562ms 217.34402ms 217.426187ms 217.802267ms 219.451614ms 219.732911ms 220.164815ms 221.845138ms 222.313538ms 222.437247ms 222.55589ms 223.137055ms 223.426142ms 224.294804ms 224.373765ms 224.464382ms 225.525991ms 227.062244ms 229.156069ms 231.135637ms 231.688843ms 232.322324ms 232.994051ms 233.380035ms 234.834165ms 235.369482ms 235.462058ms 235.645247ms 236.177222ms 236.331388ms 236.794725ms 236.955935ms 237.580596ms 237.758427ms 238.022834ms 238.906639ms 238.940752ms 239.077996ms 239.482232ms 239.986314ms 240.17334ms 240.275966ms 240.80869ms 240.899171ms 241.149842ms 241.236211ms 241.249931ms 241.636289ms 241.741617ms 241.748732ms 241.935193ms 242.479573ms 242.96632ms 243.005605ms 243.615046ms 243.852165ms 244.09133ms 244.569179ms 244.790217ms 245.220549ms 245.584695ms 245.897349ms 246.32326ms 246.96637ms 246.994939ms 247.609378ms 247.903941ms 248.554751ms 248.61526ms 248.62409ms 249.152497ms 249.281909ms 249.778891ms 250.417712ms 251.620362ms 251.646691ms 251.674101ms 252.054068ms 252.323706ms 252.816182ms 253.171803ms 253.234966ms 253.590167ms 254.387049ms 254.43344ms 254.466286ms 254.771106ms 255.047484ms 255.461445ms 256.847147ms 257.013309ms 257.120672ms 257.522984ms 257.753278ms 258.249175ms 258.312831ms 258.492001ms 258.644693ms 259.055022ms 259.331847ms 260.016128ms 261.468036ms 261.853635ms 261.95992ms 261.999753ms 262.221695ms 262.532559ms 262.671138ms 262.84215ms 264.017701ms 264.047999ms 264.27469ms 264.438904ms 264.631504ms 264.748027ms 264.757067ms 264.900522ms 265.172384ms 265.232596ms 265.452162ms 266.238789ms 267.265451ms 267.405877ms 267.747393ms 267.821409ms 268.435969ms 269.602279ms 269.992213ms 270.191519ms 270.657515ms 270.720481ms 271.190922ms 271.236418ms 271.477057ms 271.65643ms 271.794887ms 272.361411ms 274.334082ms 276.168209ms 276.916526ms 276.975845ms 277.234735ms 278.298345ms 278.534303ms 279.209483ms 280.21747ms 280.492965ms 280.520322ms 280.712353ms 280.802879ms 282.753898ms 282.951979ms 283.299739ms 284.385921ms 285.568205ms 285.955516ms 286.085712ms 286.230083ms 286.484232ms 286.717198ms 288.501216ms 288.72158ms 290.901134ms 291.17169ms 292.470696ms 292.541788ms 294.245873ms 294.889231ms 297.216028ms 297.808723ms 298.971204ms 299.158901ms 301.921987ms 302.617256ms 302.693711ms 302.745874ms 305.125689ms 305.453842ms 305.581523ms 306.368955ms 310.192842ms 310.389853ms 315.392123ms 315.661216ms 321.337368ms 322.602965ms 331.66545ms]
Sep 27 20:52:42.990: INFO: 50 %ile: 255.047484ms
Sep 27 20:52:42.991: INFO: 90 %ile: 294.889231ms
Sep 27 20:52:42.991: INFO: 99 %ile: 322.602965ms
Sep 27 20:52:42.991: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 20:52:42.991: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svc-latency-7707" for this suite.

• [SLOW TEST:5.951 seconds]
[sig-network] Service endpoints latency
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should not be very high  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Service endpoints latency should not be very high  [Conformance]","total":305,"completed":243,"skipped":3892,"failed":0}
SS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 20:52:43.084: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Sep 27 20:52:43.342: INFO: Waiting up to 5m0s for pod "downwardapi-volume-192356ed-4e72-40c5-baf0-2f0ecb52f585" in namespace "projected-7161" to be "Succeeded or Failed"
Sep 27 20:52:43.356: INFO: Pod "downwardapi-volume-192356ed-4e72-40c5-baf0-2f0ecb52f585": Phase="Pending", Reason="", readiness=false. Elapsed: 14.619126ms
Sep 27 20:52:45.383: INFO: Pod "downwardapi-volume-192356ed-4e72-40c5-baf0-2f0ecb52f585": Phase="Running", Reason="", readiness=true. Elapsed: 2.041090995s
Sep 27 20:52:47.398: INFO: Pod "downwardapi-volume-192356ed-4e72-40c5-baf0-2f0ecb52f585": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.056091875s
STEP: Saw pod success
Sep 27 20:52:47.398: INFO: Pod "downwardapi-volume-192356ed-4e72-40c5-baf0-2f0ecb52f585" satisfied condition "Succeeded or Failed"
Sep 27 20:52:47.413: INFO: Trying to get logs from node 10.177.248.117 pod downwardapi-volume-192356ed-4e72-40c5-baf0-2f0ecb52f585 container client-container: <nil>
STEP: delete the pod
Sep 27 20:52:47.495: INFO: Waiting for pod downwardapi-volume-192356ed-4e72-40c5-baf0-2f0ecb52f585 to disappear
Sep 27 20:52:47.512: INFO: Pod downwardapi-volume-192356ed-4e72-40c5-baf0-2f0ecb52f585 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 20:52:47.512: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7161" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory request [NodeConformance] [Conformance]","total":305,"completed":244,"skipped":3894,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 20:52:47.557: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0777 on node default medium
Sep 27 20:52:47.779: INFO: Waiting up to 5m0s for pod "pod-9a252637-f0f4-490a-ba6c-d41a522b7562" in namespace "emptydir-5358" to be "Succeeded or Failed"
Sep 27 20:52:47.794: INFO: Pod "pod-9a252637-f0f4-490a-ba6c-d41a522b7562": Phase="Pending", Reason="", readiness=false. Elapsed: 14.572767ms
Sep 27 20:52:49.811: INFO: Pod "pod-9a252637-f0f4-490a-ba6c-d41a522b7562": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031818115s
Sep 27 20:52:51.825: INFO: Pod "pod-9a252637-f0f4-490a-ba6c-d41a522b7562": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.045304684s
STEP: Saw pod success
Sep 27 20:52:51.825: INFO: Pod "pod-9a252637-f0f4-490a-ba6c-d41a522b7562" satisfied condition "Succeeded or Failed"
Sep 27 20:52:51.838: INFO: Trying to get logs from node 10.177.248.117 pod pod-9a252637-f0f4-490a-ba6c-d41a522b7562 container test-container: <nil>
STEP: delete the pod
Sep 27 20:52:51.949: INFO: Waiting for pod pod-9a252637-f0f4-490a-ba6c-d41a522b7562 to disappear
Sep 27 20:52:51.964: INFO: Pod pod-9a252637-f0f4-490a-ba6c-d41a522b7562 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 20:52:51.964: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5358" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":245,"skipped":3902,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 20:52:52.014: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 20:52:59.218: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-7028" for this suite.

• [SLOW TEST:7.246 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]","total":305,"completed":246,"skipped":3914,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates 
  should run the lifecycle of PodTemplates [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-node] PodTemplates
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 20:52:59.261: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename podtemplate
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run the lifecycle of PodTemplates [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [sig-node] PodTemplates
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 20:52:59.550: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-2619" for this suite.
•{"msg":"PASSED [sig-node] PodTemplates should run the lifecycle of PodTemplates [Conformance]","total":305,"completed":247,"skipped":3961,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 20:52:59.597: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward api env vars
Sep 27 20:52:59.780: INFO: Waiting up to 5m0s for pod "downward-api-d14f7d6c-6411-4f02-9283-2fcd979b78fa" in namespace "downward-api-5875" to be "Succeeded or Failed"
Sep 27 20:52:59.804: INFO: Pod "downward-api-d14f7d6c-6411-4f02-9283-2fcd979b78fa": Phase="Pending", Reason="", readiness=false. Elapsed: 23.856172ms
Sep 27 20:53:01.826: INFO: Pod "downward-api-d14f7d6c-6411-4f02-9283-2fcd979b78fa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.046186415s
Sep 27 20:53:03.846: INFO: Pod "downward-api-d14f7d6c-6411-4f02-9283-2fcd979b78fa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.06632281s
STEP: Saw pod success
Sep 27 20:53:03.846: INFO: Pod "downward-api-d14f7d6c-6411-4f02-9283-2fcd979b78fa" satisfied condition "Succeeded or Failed"
Sep 27 20:53:03.864: INFO: Trying to get logs from node 10.177.248.117 pod downward-api-d14f7d6c-6411-4f02-9283-2fcd979b78fa container dapi-container: <nil>
STEP: delete the pod
Sep 27 20:53:03.942: INFO: Waiting for pod downward-api-d14f7d6c-6411-4f02-9283-2fcd979b78fa to disappear
Sep 27 20:53:03.965: INFO: Pod downward-api-d14f7d6c-6411-4f02-9283-2fcd979b78fa no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 20:53:03.965: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5875" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]","total":305,"completed":248,"skipped":3985,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny attaching pod [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 20:53:04.016: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Sep 27 20:53:04.654: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Sep 27 20:53:06.700: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63768372784, loc:(*time.Location)(0x7702840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63768372784, loc:(*time.Location)(0x7702840)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63768372784, loc:(*time.Location)(0x7702840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63768372784, loc:(*time.Location)(0x7702840)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep 27 20:53:09.748: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny attaching pod [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod
STEP: 'kubectl attach' the pod, should be denied by the webhook
Sep 27 20:53:11.905: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-679324930 attach --namespace=webhook-7692 to-be-attached-pod -i -c=container1'
Sep 27 20:53:12.375: INFO: rc: 1
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 20:53:12.407: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7692" for this suite.
STEP: Destroying namespace "webhook-7692-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:8.626 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny attaching pod [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny attaching pod [Conformance]","total":305,"completed":249,"skipped":3990,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 20:53:12.642: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name configmap-test-volume-map-608af1e6-35ea-4a63-9db4-60bd0bbd8078
STEP: Creating a pod to test consume configMaps
Sep 27 20:53:12.861: INFO: Waiting up to 5m0s for pod "pod-configmaps-3138ce49-8e6c-4901-9bd0-abe0a6d0a3e0" in namespace "configmap-9944" to be "Succeeded or Failed"
Sep 27 20:53:12.883: INFO: Pod "pod-configmaps-3138ce49-8e6c-4901-9bd0-abe0a6d0a3e0": Phase="Pending", Reason="", readiness=false. Elapsed: 21.445317ms
Sep 27 20:53:14.897: INFO: Pod "pod-configmaps-3138ce49-8e6c-4901-9bd0-abe0a6d0a3e0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.03562756s
STEP: Saw pod success
Sep 27 20:53:14.897: INFO: Pod "pod-configmaps-3138ce49-8e6c-4901-9bd0-abe0a6d0a3e0" satisfied condition "Succeeded or Failed"
Sep 27 20:53:14.911: INFO: Trying to get logs from node 10.177.248.117 pod pod-configmaps-3138ce49-8e6c-4901-9bd0-abe0a6d0a3e0 container configmap-volume-test: <nil>
STEP: delete the pod
Sep 27 20:53:14.980: INFO: Waiting for pod pod-configmaps-3138ce49-8e6c-4901-9bd0-abe0a6d0a3e0 to disappear
Sep 27 20:53:14.993: INFO: Pod pod-configmaps-3138ce49-8e6c-4901-9bd0-abe0a6d0a3e0 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 20:53:14.994: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9944" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":305,"completed":250,"skipped":4030,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 20:53:15.045: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating secret with name secret-test-e1e58986-a774-466d-b17a-806083b22e65
STEP: Creating a pod to test consume secrets
Sep 27 20:53:15.379: INFO: Waiting up to 5m0s for pod "pod-secrets-16bfe4cb-519e-4b1b-b1e9-725d6f11094b" in namespace "secrets-9251" to be "Succeeded or Failed"
Sep 27 20:53:15.394: INFO: Pod "pod-secrets-16bfe4cb-519e-4b1b-b1e9-725d6f11094b": Phase="Pending", Reason="", readiness=false. Elapsed: 14.606001ms
Sep 27 20:53:17.409: INFO: Pod "pod-secrets-16bfe4cb-519e-4b1b-b1e9-725d6f11094b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029134091s
Sep 27 20:53:19.421: INFO: Pod "pod-secrets-16bfe4cb-519e-4b1b-b1e9-725d6f11094b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.041937787s
STEP: Saw pod success
Sep 27 20:53:19.422: INFO: Pod "pod-secrets-16bfe4cb-519e-4b1b-b1e9-725d6f11094b" satisfied condition "Succeeded or Failed"
Sep 27 20:53:19.437: INFO: Trying to get logs from node 10.177.248.117 pod pod-secrets-16bfe4cb-519e-4b1b-b1e9-725d6f11094b container secret-volume-test: <nil>
STEP: delete the pod
Sep 27 20:53:19.519: INFO: Waiting for pod pod-secrets-16bfe4cb-519e-4b1b-b1e9-725d6f11094b to disappear
Sep 27 20:53:19.534: INFO: Pod pod-secrets-16bfe4cb-519e-4b1b-b1e9-725d6f11094b no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 20:53:19.535: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9251" for this suite.
STEP: Destroying namespace "secret-namespace-4671" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]","total":305,"completed":251,"skipped":4083,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath 
  runs ReplicaSets to verify preemption running path [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 20:53:19.598: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename sched-preemption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:89
Sep 27 20:53:19.800: INFO: Waiting up to 1m0s for all nodes to be ready
Sep 27 20:54:19.997: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PreemptionExecutionPath
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 20:54:20.018: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename sched-preemption-path
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] PreemptionExecutionPath
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:487
STEP: Finding an available node
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
Sep 27 20:54:22.300: INFO: found a healthy node: 10.177.248.117
[It] runs ReplicaSets to verify preemption running path [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Sep 27 20:54:34.565: INFO: pods created so far: [1 1 1]
Sep 27 20:54:34.565: INFO: length of pods created so far: 3
Sep 27 20:54:48.615: INFO: pods created so far: [2 2 1]
[AfterEach] PreemptionExecutionPath
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 20:54:55.616: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-path-2623" for this suite.
[AfterEach] PreemptionExecutionPath
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:461
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 20:54:55.821: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-6390" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:77

• [SLOW TEST:96.484 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  PreemptionExecutionPath
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:450
    runs ReplicaSets to verify preemption running path [Conformance]
    /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath runs ReplicaSets to verify preemption running path [Conformance]","total":305,"completed":252,"skipped":4101,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 20:54:56.082: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod pod-subpath-test-configmap-2hqm
STEP: Creating a pod to test atomic-volume-subpath
Sep 27 20:54:56.315: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-2hqm" in namespace "subpath-2859" to be "Succeeded or Failed"
Sep 27 20:54:56.332: INFO: Pod "pod-subpath-test-configmap-2hqm": Phase="Pending", Reason="", readiness=false. Elapsed: 17.12575ms
Sep 27 20:54:58.347: INFO: Pod "pod-subpath-test-configmap-2hqm": Phase="Running", Reason="", readiness=true. Elapsed: 2.031765167s
Sep 27 20:55:00.372: INFO: Pod "pod-subpath-test-configmap-2hqm": Phase="Running", Reason="", readiness=true. Elapsed: 4.057579481s
Sep 27 20:55:02.386: INFO: Pod "pod-subpath-test-configmap-2hqm": Phase="Running", Reason="", readiness=true. Elapsed: 6.071172896s
Sep 27 20:55:04.403: INFO: Pod "pod-subpath-test-configmap-2hqm": Phase="Running", Reason="", readiness=true. Elapsed: 8.088444769s
Sep 27 20:55:06.418: INFO: Pod "pod-subpath-test-configmap-2hqm": Phase="Running", Reason="", readiness=true. Elapsed: 10.102885884s
Sep 27 20:55:08.431: INFO: Pod "pod-subpath-test-configmap-2hqm": Phase="Running", Reason="", readiness=true. Elapsed: 12.116203491s
Sep 27 20:55:10.445: INFO: Pod "pod-subpath-test-configmap-2hqm": Phase="Running", Reason="", readiness=true. Elapsed: 14.130650375s
Sep 27 20:55:12.460: INFO: Pod "pod-subpath-test-configmap-2hqm": Phase="Running", Reason="", readiness=true. Elapsed: 16.145667884s
Sep 27 20:55:14.473: INFO: Pod "pod-subpath-test-configmap-2hqm": Phase="Running", Reason="", readiness=true. Elapsed: 18.158602701s
Sep 27 20:55:16.525: INFO: Pod "pod-subpath-test-configmap-2hqm": Phase="Running", Reason="", readiness=true. Elapsed: 20.210189822s
Sep 27 20:55:18.541: INFO: Pod "pod-subpath-test-configmap-2hqm": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.225823623s
STEP: Saw pod success
Sep 27 20:55:18.541: INFO: Pod "pod-subpath-test-configmap-2hqm" satisfied condition "Succeeded or Failed"
Sep 27 20:55:18.559: INFO: Trying to get logs from node 10.177.248.117 pod pod-subpath-test-configmap-2hqm container test-container-subpath-configmap-2hqm: <nil>
STEP: delete the pod
Sep 27 20:55:18.722: INFO: Waiting for pod pod-subpath-test-configmap-2hqm to disappear
Sep 27 20:55:18.735: INFO: Pod pod-subpath-test-configmap-2hqm no longer exists
STEP: Deleting pod pod-subpath-test-configmap-2hqm
Sep 27 20:55:18.735: INFO: Deleting pod "pod-subpath-test-configmap-2hqm" in namespace "subpath-2859"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 20:55:18.755: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-2859" for this suite.

• [SLOW TEST:22.719 seconds]
[sig-storage] Subpath
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
    /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]","total":305,"completed":253,"skipped":4110,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Events 
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Events
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 20:55:18.803: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a test event
STEP: listing all events in all namespaces
STEP: patching the test event
STEP: fetching the test event
STEP: deleting the test event
STEP: listing all events in all namespaces
[AfterEach] [sig-api-machinery] Events
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 20:55:19.211: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-7616" for this suite.
•{"msg":"PASSED [sig-api-machinery] Events should ensure that an event can be fetched, patched, deleted, and listed [Conformance]","total":305,"completed":254,"skipped":4156,"failed":0}

------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 20:55:19.263: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name configmap-test-volume-70ff5fc0-b0a8-4a4e-936f-26a146550436
STEP: Creating a pod to test consume configMaps
Sep 27 20:55:19.500: INFO: Waiting up to 5m0s for pod "pod-configmaps-dc36d286-3082-4c16-90f8-5c2e8f0dbe8c" in namespace "configmap-8404" to be "Succeeded or Failed"
Sep 27 20:55:19.515: INFO: Pod "pod-configmaps-dc36d286-3082-4c16-90f8-5c2e8f0dbe8c": Phase="Pending", Reason="", readiness=false. Elapsed: 14.931191ms
Sep 27 20:55:21.530: INFO: Pod "pod-configmaps-dc36d286-3082-4c16-90f8-5c2e8f0dbe8c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.029991836s
STEP: Saw pod success
Sep 27 20:55:21.531: INFO: Pod "pod-configmaps-dc36d286-3082-4c16-90f8-5c2e8f0dbe8c" satisfied condition "Succeeded or Failed"
Sep 27 20:55:21.545: INFO: Trying to get logs from node 10.177.248.117 pod pod-configmaps-dc36d286-3082-4c16-90f8-5c2e8f0dbe8c container configmap-volume-test: <nil>
STEP: delete the pod
Sep 27 20:55:21.626: INFO: Waiting for pod pod-configmaps-dc36d286-3082-4c16-90f8-5c2e8f0dbe8c to disappear
Sep 27 20:55:21.642: INFO: Pod pod-configmaps-dc36d286-3082-4c16-90f8-5c2e8f0dbe8c no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 20:55:21.643: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8404" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","total":305,"completed":255,"skipped":4156,"failed":0}
SSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 20:55:21.691: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-7115
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a new StatefulSet
Sep 27 20:55:21.940: INFO: Found 0 stateful pods, waiting for 3
Sep 27 20:55:31.957: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Sep 27 20:55:31.957: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Sep 27 20:55:31.957: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Sep 27 20:55:32.011: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-679324930 exec --namespace=statefulset-7115 ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Sep 27 20:55:32.501: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Sep 27 20:55:32.501: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Sep 27 20:55:32.501: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from docker.io/library/httpd:2.4.38-alpine to docker.io/library/httpd:2.4.39-alpine
Sep 27 20:55:42.607: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Updating Pods in reverse ordinal order
Sep 27 20:55:52.699: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-679324930 exec --namespace=statefulset-7115 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 27 20:55:53.196: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Sep 27 20:55:53.196: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Sep 27 20:55:53.196: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Sep 27 20:56:03.276: INFO: Waiting for StatefulSet statefulset-7115/ss2 to complete update
Sep 27 20:56:03.276: INFO: Waiting for Pod statefulset-7115/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Sep 27 20:56:03.276: INFO: Waiting for Pod statefulset-7115/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Sep 27 20:56:13.309: INFO: Waiting for StatefulSet statefulset-7115/ss2 to complete update
Sep 27 20:56:13.309: INFO: Waiting for Pod statefulset-7115/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Sep 27 20:56:13.309: INFO: Waiting for Pod statefulset-7115/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Sep 27 20:56:23.306: INFO: Waiting for StatefulSet statefulset-7115/ss2 to complete update
Sep 27 20:56:23.306: INFO: Waiting for Pod statefulset-7115/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
STEP: Rolling back to a previous revision
Sep 27 20:56:33.307: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-679324930 exec --namespace=statefulset-7115 ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Sep 27 20:56:33.748: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Sep 27 20:56:33.748: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Sep 27 20:56:33.748: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Sep 27 20:56:43.850: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order
Sep 27 20:56:53.941: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-679324930 exec --namespace=statefulset-7115 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 27 20:56:54.422: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Sep 27 20:56:54.422: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Sep 27 20:56:54.422: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Sep 27 20:57:04.509: INFO: Waiting for StatefulSet statefulset-7115/ss2 to complete update
Sep 27 20:57:04.509: INFO: Waiting for Pod statefulset-7115/ss2-0 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Sep 27 20:57:04.509: INFO: Waiting for Pod statefulset-7115/ss2-1 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Sep 27 20:57:14.537: INFO: Waiting for StatefulSet statefulset-7115/ss2 to complete update
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
Sep 27 20:57:24.536: INFO: Deleting all statefulset in ns statefulset-7115
Sep 27 20:57:24.549: INFO: Scaling statefulset ss2 to 0
Sep 27 20:57:54.627: INFO: Waiting for statefulset status.replicas updated to 0
Sep 27 20:57:54.640: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 20:57:54.702: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-7115" for this suite.

• [SLOW TEST:153.059 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
    should perform rolling updates and roll backs of template modifications [Conformance]
    /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should perform rolling updates and roll backs of template modifications [Conformance]","total":305,"completed":256,"skipped":4159,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 20:57:54.751: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name projected-configmap-test-volume-28abc56d-7d61-4281-a278-0b3cd1b9f190
STEP: Creating a pod to test consume configMaps
Sep 27 20:57:54.964: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-9e18d109-1a61-4281-b8aa-8e474d88749b" in namespace "projected-3543" to be "Succeeded or Failed"
Sep 27 20:57:54.977: INFO: Pod "pod-projected-configmaps-9e18d109-1a61-4281-b8aa-8e474d88749b": Phase="Pending", Reason="", readiness=false. Elapsed: 12.213566ms
Sep 27 20:57:56.991: INFO: Pod "pod-projected-configmaps-9e18d109-1a61-4281-b8aa-8e474d88749b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026152664s
Sep 27 20:57:59.011: INFO: Pod "pod-projected-configmaps-9e18d109-1a61-4281-b8aa-8e474d88749b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.046514023s
STEP: Saw pod success
Sep 27 20:57:59.011: INFO: Pod "pod-projected-configmaps-9e18d109-1a61-4281-b8aa-8e474d88749b" satisfied condition "Succeeded or Failed"
Sep 27 20:57:59.024: INFO: Trying to get logs from node 10.177.248.117 pod pod-projected-configmaps-9e18d109-1a61-4281-b8aa-8e474d88749b container projected-configmap-volume-test: <nil>
STEP: delete the pod
Sep 27 20:57:59.124: INFO: Waiting for pod pod-projected-configmaps-9e18d109-1a61-4281-b8aa-8e474d88749b to disappear
Sep 27 20:57:59.137: INFO: Pod pod-projected-configmaps-9e18d109-1a61-4281-b8aa-8e474d88749b no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 20:57:59.137: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3543" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","total":305,"completed":257,"skipped":4169,"failed":0}
SSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 20:57:59.182: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:181
[It] should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod
STEP: setting up watch
STEP: submitting the pod to kubernetes
Sep 27 20:57:59.331: INFO: observed the pod list
STEP: verifying the pod is in kubernetes
STEP: verifying pod creation was observed
STEP: deleting the pod gracefully
STEP: verifying pod deletion was observed
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 20:58:11.821: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-9997" for this suite.

• [SLOW TEST:12.687 seconds]
[k8s.io] Pods
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Pods should be submitted and removed [NodeConformance] [Conformance]","total":305,"completed":258,"skipped":4182,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 20:58:11.879: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0644 on node default medium
Sep 27 20:58:12.116: INFO: Waiting up to 5m0s for pod "pod-f6ad14b6-c1c8-4b27-8936-61171e283629" in namespace "emptydir-5248" to be "Succeeded or Failed"
Sep 27 20:58:12.132: INFO: Pod "pod-f6ad14b6-c1c8-4b27-8936-61171e283629": Phase="Pending", Reason="", readiness=false. Elapsed: 16.220322ms
Sep 27 20:58:14.145: INFO: Pod "pod-f6ad14b6-c1c8-4b27-8936-61171e283629": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.029434776s
STEP: Saw pod success
Sep 27 20:58:14.145: INFO: Pod "pod-f6ad14b6-c1c8-4b27-8936-61171e283629" satisfied condition "Succeeded or Failed"
Sep 27 20:58:14.159: INFO: Trying to get logs from node 10.177.248.117 pod pod-f6ad14b6-c1c8-4b27-8936-61171e283629 container test-container: <nil>
STEP: delete the pod
Sep 27 20:58:14.235: INFO: Waiting for pod pod-f6ad14b6-c1c8-4b27-8936-61171e283629 to disappear
Sep 27 20:58:14.249: INFO: Pod pod-f6ad14b6-c1c8-4b27-8936-61171e283629 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 20:58:14.249: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5248" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":259,"skipped":4267,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should be able to update and delete ResourceQuota. [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 20:58:14.305: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to update and delete ResourceQuota. [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a ResourceQuota
STEP: Getting a ResourceQuota
STEP: Updating a ResourceQuota
STEP: Verifying a ResourceQuota was modified
STEP: Deleting a ResourceQuota
STEP: Verifying the deleted ResourceQuota
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 20:58:14.598: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-3871" for this suite.
•{"msg":"PASSED [sig-api-machinery] ResourceQuota should be able to update and delete ResourceQuota. [Conformance]","total":305,"completed":260,"skipped":4303,"failed":0}
SSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 20:58:14.659: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the container
STEP: wait for the container to reach Failed
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Sep 27 20:58:16.940: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 20:58:16.991: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-2372" for this suite.
•{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":305,"completed":261,"skipped":4307,"failed":0}
SSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 20:58:17.047: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test substitution in container's command
Sep 27 20:58:17.299: INFO: Waiting up to 5m0s for pod "var-expansion-71c954ae-7bef-4c5f-9bc1-5292a7fd9488" in namespace "var-expansion-4793" to be "Succeeded or Failed"
Sep 27 20:58:17.326: INFO: Pod "var-expansion-71c954ae-7bef-4c5f-9bc1-5292a7fd9488": Phase="Pending", Reason="", readiness=false. Elapsed: 27.199413ms
Sep 27 20:58:19.352: INFO: Pod "var-expansion-71c954ae-7bef-4c5f-9bc1-5292a7fd9488": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.052489235s
STEP: Saw pod success
Sep 27 20:58:19.352: INFO: Pod "var-expansion-71c954ae-7bef-4c5f-9bc1-5292a7fd9488" satisfied condition "Succeeded or Failed"
Sep 27 20:58:19.366: INFO: Trying to get logs from node 10.177.248.117 pod var-expansion-71c954ae-7bef-4c5f-9bc1-5292a7fd9488 container dapi-container: <nil>
STEP: delete the pod
Sep 27 20:58:19.473: INFO: Waiting for pod var-expansion-71c954ae-7bef-4c5f-9bc1-5292a7fd9488 to disappear
Sep 27 20:58:19.488: INFO: Pod var-expansion-71c954ae-7bef-4c5f-9bc1-5292a7fd9488 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 20:58:19.488: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-4793" for this suite.
•{"msg":"PASSED [k8s.io] Variable Expansion should allow substituting values in a container's command [NodeConformance] [Conformance]","total":305,"completed":262,"skipped":4310,"failed":0}
SSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should have a working scale subresource [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 20:58:19.537: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-8596
[It] should have a working scale subresource [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating statefulset ss in namespace statefulset-8596
Sep 27 20:58:19.756: INFO: Found 0 stateful pods, waiting for 1
Sep 27 20:58:29.771: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: getting scale subresource
STEP: updating a scale subresource
STEP: verifying the statefulset Spec.Replicas was modified
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
Sep 27 20:58:29.848: INFO: Deleting all statefulset in ns statefulset-8596
Sep 27 20:58:29.860: INFO: Scaling statefulset ss to 0
Sep 27 20:58:59.952: INFO: Waiting for statefulset status.replicas updated to 0
Sep 27 20:58:59.966: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 20:59:00.040: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-8596" for this suite.

• [SLOW TEST:40.557 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
    should have a working scale subresource [Conformance]
    /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should have a working scale subresource [Conformance]","total":305,"completed":263,"skipped":4316,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 20:59:00.096: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating replication controller my-hostname-basic-d423016c-1a21-4176-a75a-e2f8e8507a4c
Sep 27 20:59:00.300: INFO: Pod name my-hostname-basic-d423016c-1a21-4176-a75a-e2f8e8507a4c: Found 0 pods out of 1
Sep 27 20:59:05.314: INFO: Pod name my-hostname-basic-d423016c-1a21-4176-a75a-e2f8e8507a4c: Found 1 pods out of 1
Sep 27 20:59:05.314: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-d423016c-1a21-4176-a75a-e2f8e8507a4c" are running
Sep 27 20:59:05.329: INFO: Pod "my-hostname-basic-d423016c-1a21-4176-a75a-e2f8e8507a4c-zhmqh" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-09-27 20:59:00 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-09-27 20:59:01 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-09-27 20:59:01 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-09-27 20:59:00 +0000 UTC Reason: Message:}])
Sep 27 20:59:05.330: INFO: Trying to dial the pod
Sep 27 20:59:10.393: INFO: Controller my-hostname-basic-d423016c-1a21-4176-a75a-e2f8e8507a4c: Got expected result from replica 1 [my-hostname-basic-d423016c-1a21-4176-a75a-e2f8e8507a4c-zhmqh]: "my-hostname-basic-d423016c-1a21-4176-a75a-e2f8e8507a4c-zhmqh", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 20:59:10.393: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-2585" for this suite.

• [SLOW TEST:10.345 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should serve a basic image on each replica with a public image  [Conformance]","total":305,"completed":264,"skipped":4333,"failed":0}
[sig-storage] Downward API volume 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 20:59:10.442: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Sep 27 20:59:10.629: INFO: Waiting up to 5m0s for pod "downwardapi-volume-50ce4810-87f7-4d06-9707-af81fb857a3b" in namespace "downward-api-7866" to be "Succeeded or Failed"
Sep 27 20:59:10.643: INFO: Pod "downwardapi-volume-50ce4810-87f7-4d06-9707-af81fb857a3b": Phase="Pending", Reason="", readiness=false. Elapsed: 12.673366ms
Sep 27 20:59:12.658: INFO: Pod "downwardapi-volume-50ce4810-87f7-4d06-9707-af81fb857a3b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027389106s
Sep 27 20:59:14.673: INFO: Pod "downwardapi-volume-50ce4810-87f7-4d06-9707-af81fb857a3b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.042629145s
STEP: Saw pod success
Sep 27 20:59:14.673: INFO: Pod "downwardapi-volume-50ce4810-87f7-4d06-9707-af81fb857a3b" satisfied condition "Succeeded or Failed"
Sep 27 20:59:14.687: INFO: Trying to get logs from node 10.177.248.117 pod downwardapi-volume-50ce4810-87f7-4d06-9707-af81fb857a3b container client-container: <nil>
STEP: delete the pod
Sep 27 20:59:14.769: INFO: Waiting for pod downwardapi-volume-50ce4810-87f7-4d06-9707-af81fb857a3b to disappear
Sep 27 20:59:14.782: INFO: Pod downwardapi-volume-50ce4810-87f7-4d06-9707-af81fb857a3b no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 20:59:14.782: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7866" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory request [NodeConformance] [Conformance]","total":305,"completed":265,"skipped":4333,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 20:59:14.832: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Sep 27 20:59:15.606: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Sep 27 20:59:17.649: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63768373155, loc:(*time.Location)(0x7702840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63768373155, loc:(*time.Location)(0x7702840)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63768373155, loc:(*time.Location)(0x7702840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63768373155, loc:(*time.Location)(0x7702840)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep 27 20:59:20.698: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API
STEP: create a namespace for the webhook
STEP: create a configmap should be unconditionally rejected by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 20:59:20.981: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1602" for this suite.
STEP: Destroying namespace "webhook-1602-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:6.419 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should unconditionally reject operations on fail closed webhook [Conformance]","total":305,"completed":266,"skipped":4346,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 20:59:21.255: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name projected-configmap-test-volume-42005cc4-cc63-4b61-9b97-6ca61298cf43
STEP: Creating a pod to test consume configMaps
Sep 27 20:59:21.497: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-8ad65b94-1bee-4b28-81f6-aef3890ee8bf" in namespace "projected-8434" to be "Succeeded or Failed"
Sep 27 20:59:21.511: INFO: Pod "pod-projected-configmaps-8ad65b94-1bee-4b28-81f6-aef3890ee8bf": Phase="Pending", Reason="", readiness=false. Elapsed: 13.219018ms
Sep 27 20:59:23.524: INFO: Pod "pod-projected-configmaps-8ad65b94-1bee-4b28-81f6-aef3890ee8bf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0266728s
Sep 27 20:59:25.539: INFO: Pod "pod-projected-configmaps-8ad65b94-1bee-4b28-81f6-aef3890ee8bf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.04175123s
STEP: Saw pod success
Sep 27 20:59:25.539: INFO: Pod "pod-projected-configmaps-8ad65b94-1bee-4b28-81f6-aef3890ee8bf" satisfied condition "Succeeded or Failed"
Sep 27 20:59:25.553: INFO: Trying to get logs from node 10.177.248.117 pod pod-projected-configmaps-8ad65b94-1bee-4b28-81f6-aef3890ee8bf container projected-configmap-volume-test: <nil>
STEP: delete the pod
Sep 27 20:59:25.650: INFO: Waiting for pod pod-projected-configmaps-8ad65b94-1bee-4b28-81f6-aef3890ee8bf to disappear
Sep 27 20:59:25.662: INFO: Pod pod-projected-configmaps-8ad65b94-1bee-4b28-81f6-aef3890ee8bf no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 20:59:25.662: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8434" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":267,"skipped":4365,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 20:59:25.709: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:78
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Sep 27 20:59:25.871: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Sep 27 20:59:25.930: INFO: Pod name sample-pod: Found 0 pods out of 1
Sep 27 20:59:30.956: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Sep 27 20:59:30.956: INFO: Creating deployment "test-rolling-update-deployment"
Sep 27 20:59:30.973: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Sep 27 20:59:31.001: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
Sep 27 20:59:33.033: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Sep 27 20:59:33.046: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63768373171, loc:(*time.Location)(0x7702840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63768373171, loc:(*time.Location)(0x7702840)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63768373171, loc:(*time.Location)(0x7702840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63768373171, loc:(*time.Location)(0x7702840)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-c4cb8d6d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 27 20:59:35.062: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:72
Sep 27 20:59:35.098: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-7713 /apis/apps/v1/namespaces/deployment-7713/deployments/test-rolling-update-deployment e88a9a38-b0f3-422b-9b74-4f6c44c0fa13 113733 1 2021-09-27 20:59:30 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] []  [{e2e.test Update apps/v1 2021-09-27 20:59:30 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{}}},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-09-27 20:59:34 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}}}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.20 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002c77d38 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2021-09-27 20:59:31 +0000 UTC,LastTransitionTime:2021-09-27 20:59:31 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-c4cb8d6d9" has successfully progressed.,LastUpdateTime:2021-09-27 20:59:34 +0000 UTC,LastTransitionTime:2021-09-27 20:59:31 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Sep 27 20:59:35.112: INFO: New ReplicaSet "test-rolling-update-deployment-c4cb8d6d9" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:{test-rolling-update-deployment-c4cb8d6d9  deployment-7713 /apis/apps/v1/namespaces/deployment-7713/replicasets/test-rolling-update-deployment-c4cb8d6d9 b1b3754a-dc74-4e88-8106-201423701c2d 113720 1 2021-09-27 20:59:31 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:c4cb8d6d9] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment e88a9a38-b0f3-422b-9b74-4f6c44c0fa13 0xc003681eb0 0xc003681eb1}] []  [{kube-controller-manager Update apps/v1 2021-09-27 20:59:34 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e88a9a38-b0f3-422b-9b74-4f6c44c0fa13\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: c4cb8d6d9,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:c4cb8d6d9] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.20 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003681f28 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Sep 27 20:59:35.112: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Sep 27 20:59:35.112: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-7713 /apis/apps/v1/namespaces/deployment-7713/replicasets/test-rolling-update-controller c458b6a5-8bc5-4413-bc23-045f77d2dab6 113732 2 2021-09-27 20:59:25 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment e88a9a38-b0f3-422b-9b74-4f6c44c0fa13 0xc003681da7 0xc003681da8}] []  [{e2e.test Update apps/v1 2021-09-27 20:59:25 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{"f:matchLabels":{".":{},"f:name":{},"f:pod":{}}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-09-27 20:59:34 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e88a9a38-b0f3-422b-9b74-4f6c44c0fa13\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{}},"f:status":{"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc003681e48 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Sep 27 20:59:35.129: INFO: Pod "test-rolling-update-deployment-c4cb8d6d9-jd58f" is available:
&Pod{ObjectMeta:{test-rolling-update-deployment-c4cb8d6d9-jd58f test-rolling-update-deployment-c4cb8d6d9- deployment-7713 /api/v1/namespaces/deployment-7713/pods/test-rolling-update-deployment-c4cb8d6d9-jd58f 9ff7ccc2-389e-4a12-9781-560f4a10bcc7 113719 0 2021-09-27 20:59:31 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:c4cb8d6d9] map[cni.projectcalico.org/podIP:172.30.75.12/32 cni.projectcalico.org/podIPs:172.30.75.12/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "",
    "ips": [
        "172.30.75.12"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "",
    "ips": [
        "172.30.75.12"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet test-rolling-update-deployment-c4cb8d6d9 b1b3754a-dc74-4e88-8106-201423701c2d 0xc0032e24c7 0xc0032e24c8}] []  [{kube-controller-manager Update v1 2021-09-27 20:59:31 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b1b3754a-dc74-4e88-8106-201423701c2d\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-09-27 20:59:32 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {multus Update v1 2021-09-27 20:59:33 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}}} {kubelet Update v1 2021-09-27 20:59:34 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.75.12\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-d78nv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-d78nv,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:k8s.gcr.io/e2e-test-images/agnhost:2.20,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-d78nv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.177.248.117,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c61,c15,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-g27c7,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-27 20:59:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-27 20:59:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-27 20:59:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-27 20:59:31 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.177.248.117,PodIP:172.30.75.12,StartTime:2021-09-27 20:59:31 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-09-27 20:59:33 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.20,ImageID:k8s.gcr.io/e2e-test-images/agnhost@sha256:17e61a0b9e498b6c73ed97670906be3d5a3ae394739c1bd5b619e1a004885cf0,ContainerID:cri-o://7a5c8c6eab972c2004ca609d79bfee4f561231da8c03310af931693f3816f512,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.75.12,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 20:59:35.129: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-7713" for this suite.

• [SLOW TEST:9.462 seconds]
[sig-apps] Deployment
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Deployment RollingUpdateDeployment should delete old pods and create new ones [Conformance]","total":305,"completed":268,"skipped":4398,"failed":0}
SSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 20:59:35.172: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0666 on tmpfs
Sep 27 20:59:35.356: INFO: Waiting up to 5m0s for pod "pod-0c1547e0-d4b1-441f-97fa-5bff3ca47d20" in namespace "emptydir-6445" to be "Succeeded or Failed"
Sep 27 20:59:35.369: INFO: Pod "pod-0c1547e0-d4b1-441f-97fa-5bff3ca47d20": Phase="Pending", Reason="", readiness=false. Elapsed: 12.992694ms
Sep 27 20:59:37.389: INFO: Pod "pod-0c1547e0-d4b1-441f-97fa-5bff3ca47d20": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.032668987s
STEP: Saw pod success
Sep 27 20:59:37.389: INFO: Pod "pod-0c1547e0-d4b1-441f-97fa-5bff3ca47d20" satisfied condition "Succeeded or Failed"
Sep 27 20:59:37.405: INFO: Trying to get logs from node 10.177.248.117 pod pod-0c1547e0-d4b1-441f-97fa-5bff3ca47d20 container test-container: <nil>
STEP: delete the pod
Sep 27 20:59:37.481: INFO: Waiting for pod pod-0c1547e0-d4b1-441f-97fa-5bff3ca47d20 to disappear
Sep 27 20:59:37.494: INFO: Pod pod-0c1547e0-d4b1-441f-97fa-5bff3ca47d20 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 20:59:37.494: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6445" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":269,"skipped":4401,"failed":0}

------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 20:59:37.547: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod pod-subpath-test-configmap-whxc
STEP: Creating a pod to test atomic-volume-subpath
Sep 27 20:59:37.795: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-whxc" in namespace "subpath-3372" to be "Succeeded or Failed"
Sep 27 20:59:37.809: INFO: Pod "pod-subpath-test-configmap-whxc": Phase="Pending", Reason="", readiness=false. Elapsed: 13.871453ms
Sep 27 20:59:39.823: INFO: Pod "pod-subpath-test-configmap-whxc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02797202s
Sep 27 20:59:41.838: INFO: Pod "pod-subpath-test-configmap-whxc": Phase="Running", Reason="", readiness=true. Elapsed: 4.042855329s
Sep 27 20:59:43.853: INFO: Pod "pod-subpath-test-configmap-whxc": Phase="Running", Reason="", readiness=true. Elapsed: 6.057846007s
Sep 27 20:59:45.868: INFO: Pod "pod-subpath-test-configmap-whxc": Phase="Running", Reason="", readiness=true. Elapsed: 8.073507172s
Sep 27 20:59:47.882: INFO: Pod "pod-subpath-test-configmap-whxc": Phase="Running", Reason="", readiness=true. Elapsed: 10.08671731s
Sep 27 20:59:49.897: INFO: Pod "pod-subpath-test-configmap-whxc": Phase="Running", Reason="", readiness=true. Elapsed: 12.102342957s
Sep 27 20:59:51.914: INFO: Pod "pod-subpath-test-configmap-whxc": Phase="Running", Reason="", readiness=true. Elapsed: 14.118787536s
Sep 27 20:59:53.928: INFO: Pod "pod-subpath-test-configmap-whxc": Phase="Running", Reason="", readiness=true. Elapsed: 16.133133971s
Sep 27 20:59:55.944: INFO: Pod "pod-subpath-test-configmap-whxc": Phase="Running", Reason="", readiness=true. Elapsed: 18.149261377s
Sep 27 20:59:57.960: INFO: Pod "pod-subpath-test-configmap-whxc": Phase="Running", Reason="", readiness=true. Elapsed: 20.164769474s
Sep 27 20:59:59.975: INFO: Pod "pod-subpath-test-configmap-whxc": Phase="Running", Reason="", readiness=true. Elapsed: 22.179863913s
Sep 27 21:00:01.995: INFO: Pod "pod-subpath-test-configmap-whxc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.200041952s
STEP: Saw pod success
Sep 27 21:00:01.995: INFO: Pod "pod-subpath-test-configmap-whxc" satisfied condition "Succeeded or Failed"
Sep 27 21:00:02.011: INFO: Trying to get logs from node 10.177.248.117 pod pod-subpath-test-configmap-whxc container test-container-subpath-configmap-whxc: <nil>
STEP: delete the pod
Sep 27 21:00:02.106: INFO: Waiting for pod pod-subpath-test-configmap-whxc to disappear
Sep 27 21:00:02.135: INFO: Pod pod-subpath-test-configmap-whxc no longer exists
STEP: Deleting pod pod-subpath-test-configmap-whxc
Sep 27 21:00:02.135: INFO: Deleting pod "pod-subpath-test-configmap-whxc" in namespace "subpath-3372"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 21:00:02.161: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-3372" for this suite.

• [SLOW TEST:24.668 seconds]
[sig-storage] Subpath
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod [LinuxOnly] [Conformance]","total":305,"completed":270,"skipped":4401,"failed":0}
SSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 21:00:02.216: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Sep 27 21:00:02.394: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Sep 27 21:00:11.863: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-679324930 --namespace=crd-publish-openapi-6699 create -f -'
Sep 27 21:00:12.846: INFO: stderr: ""
Sep 27 21:00:12.846: INFO: stdout: "e2e-test-crd-publish-openapi-9043-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Sep 27 21:00:12.847: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-679324930 --namespace=crd-publish-openapi-6699 delete e2e-test-crd-publish-openapi-9043-crds test-cr'
Sep 27 21:00:13.131: INFO: stderr: ""
Sep 27 21:00:13.131: INFO: stdout: "e2e-test-crd-publish-openapi-9043-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
Sep 27 21:00:13.131: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-679324930 --namespace=crd-publish-openapi-6699 apply -f -'
Sep 27 21:00:13.966: INFO: stderr: ""
Sep 27 21:00:13.967: INFO: stdout: "e2e-test-crd-publish-openapi-9043-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Sep 27 21:00:13.967: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-679324930 --namespace=crd-publish-openapi-6699 delete e2e-test-crd-publish-openapi-9043-crds test-cr'
Sep 27 21:00:14.180: INFO: stderr: ""
Sep 27 21:00:14.180: INFO: stdout: "e2e-test-crd-publish-openapi-9043-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
Sep 27 21:00:14.180: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-679324930 explain e2e-test-crd-publish-openapi-9043-crds'
Sep 27 21:00:14.676: INFO: stderr: ""
Sep 27 21:00:14.676: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-9043-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 21:00:24.840: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-6699" for this suite.

• [SLOW TEST:22.694 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields at the schema root [Conformance]","total":305,"completed":271,"skipped":4404,"failed":0}
SSSSSSSSS
------------------------------
[k8s.io] Security Context When creating a pod with readOnlyRootFilesystem 
  should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 21:00:24.911: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:41
[It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Sep 27 21:00:25.139: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-dd9c6886-b07f-4890-833f-410634a3103b" in namespace "security-context-test-1912" to be "Succeeded or Failed"
Sep 27 21:00:25.152: INFO: Pod "busybox-readonly-false-dd9c6886-b07f-4890-833f-410634a3103b": Phase="Pending", Reason="", readiness=false. Elapsed: 12.974743ms
Sep 27 21:00:27.167: INFO: Pod "busybox-readonly-false-dd9c6886-b07f-4890-833f-410634a3103b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028032657s
Sep 27 21:00:29.182: INFO: Pod "busybox-readonly-false-dd9c6886-b07f-4890-833f-410634a3103b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.042811135s
Sep 27 21:00:29.182: INFO: Pod "busybox-readonly-false-dd9c6886-b07f-4890-833f-410634a3103b" satisfied condition "Succeeded or Failed"
[AfterEach] [k8s.io] Security Context
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 21:00:29.182: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-1912" for this suite.
•{"msg":"PASSED [k8s.io] Security Context When creating a pod with readOnlyRootFilesystem should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]","total":305,"completed":272,"skipped":4413,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl api-versions 
  should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 21:00:29.235: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[It] should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: validating api versions
Sep 27 21:00:29.454: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-679324930 api-versions'
Sep 27 21:00:29.643: INFO: stderr: ""
Sep 27 21:00:29.643: INFO: stdout: "admissionregistration.k8s.io/v1\nadmissionregistration.k8s.io/v1beta1\napiextensions.k8s.io/v1\napiextensions.k8s.io/v1beta1\napiregistration.k8s.io/v1\napiregistration.k8s.io/v1beta1\napps.openshift.io/v1\napps/v1\nauthentication.k8s.io/v1\nauthentication.k8s.io/v1beta1\nauthorization.k8s.io/v1\nauthorization.k8s.io/v1beta1\nauthorization.openshift.io/v1\nautoscaling/v1\nautoscaling/v2beta1\nautoscaling/v2beta2\nbatch/v1\nbatch/v1beta1\nbuild.openshift.io/v1\ncertificates.k8s.io/v1\ncertificates.k8s.io/v1beta1\ncloudcredential.openshift.io/v1\nconfig.openshift.io/v1\nconsole.openshift.io/v1\ncoordination.k8s.io/v1\ncoordination.k8s.io/v1beta1\ncrd.projectcalico.org/v1\ndiscovery.k8s.io/v1beta1\nevents.k8s.io/v1\nevents.k8s.io/v1beta1\nextensions/v1beta1\nflowcontrol.apiserver.k8s.io/v1alpha1\nhelm.openshift.io/v1beta1\nibm.com/v1alpha1\nimage.openshift.io/v1\nimageregistry.operator.openshift.io/v1\ningress.operator.openshift.io/v1\nk8s.cni.cncf.io/v1\nmachineconfiguration.openshift.io/v1\nmetal3.io/v1alpha1\nmetrics.k8s.io/v1beta1\nmigration.k8s.io/v1alpha1\nmonitoring.coreos.com/v1\nnetwork.operator.openshift.io/v1\nnetworking.k8s.io/v1\nnetworking.k8s.io/v1beta1\nnode.k8s.io/v1beta1\noauth.openshift.io/v1\noperator.openshift.io/v1\noperator.openshift.io/v1alpha1\noperator.tigera.io/v1\noperators.coreos.com/v1\noperators.coreos.com/v1alpha1\noperators.coreos.com/v1alpha2\npackages.operators.coreos.com/v1\npolicy/v1beta1\nproject.openshift.io/v1\nquota.openshift.io/v1\nrbac.authorization.k8s.io/v1\nrbac.authorization.k8s.io/v1beta1\nroute.openshift.io/v1\nsamples.operator.openshift.io/v1\nscheduling.k8s.io/v1\nscheduling.k8s.io/v1beta1\nsecurity.internal.openshift.io/v1\nsecurity.openshift.io/v1\nsnapshot.storage.k8s.io/v1beta1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\ntemplate.openshift.io/v1\ntuned.openshift.io/v1\nuser.openshift.io/v1\nv1\nwhereabouts.cni.cncf.io/v1alpha1\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 21:00:29.644: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5526" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl api-versions should check if v1 is in available api versions  [Conformance]","total":305,"completed":273,"skipped":4429,"failed":0}
SSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch 
  watch on custom resource definition objects [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 21:00:29.697: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename crd-watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] watch on custom resource definition objects [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Sep 27 21:00:29.851: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Creating first CR 
Sep 27 21:00:30.543: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-09-27T21:00:30Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-09-27T21:00:30Z]] name:name1 resourceVersion:114345 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:4b1cc530-c5c2-4f55-ba48-c81fabd29b8c] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Creating second CR
Sep 27 21:00:40.567: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-09-27T21:00:40Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-09-27T21:00:40Z]] name:name2 resourceVersion:114449 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:780bb950-2f24-44a2-aa61-0c7ecdd3ca80] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying first CR
Sep 27 21:00:50.586: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-09-27T21:00:30Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-09-27T21:00:50Z]] name:name1 resourceVersion:114491 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:4b1cc530-c5c2-4f55-ba48-c81fabd29b8c] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying second CR
Sep 27 21:01:00.607: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-09-27T21:00:40Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-09-27T21:01:00Z]] name:name2 resourceVersion:114538 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:780bb950-2f24-44a2-aa61-0c7ecdd3ca80] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting first CR
Sep 27 21:01:10.639: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-09-27T21:00:30Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-09-27T21:00:50Z]] name:name1 resourceVersion:114584 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:4b1cc530-c5c2-4f55-ba48-c81fabd29b8c] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting second CR
Sep 27 21:01:20.666: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-09-27T21:00:40Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-09-27T21:01:00Z]] name:name2 resourceVersion:114629 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:780bb950-2f24-44a2-aa61-0c7ecdd3ca80] num:map[num1:9223372036854775807 num2:1000000]]}
[AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 21:01:31.211: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-watch-8894" for this suite.

• [SLOW TEST:61.558 seconds]
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  CustomResourceDefinition Watch
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_watch.go:42
    watch on custom resource definition objects [Conformance]
    /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch watch on custom resource definition objects [Conformance]","total":305,"completed":274,"skipped":4439,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job 
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Job
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 21:01:31.260: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a job
STEP: Ensuring job reaches completions
[AfterEach] [sig-apps] Job
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 21:01:37.447: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-9106" for this suite.

• [SLOW TEST:6.233 seconds]
[sig-apps] Job
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Job should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]","total":305,"completed":275,"skipped":4464,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 21:01:37.493: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir volume type on tmpfs
Sep 27 21:01:37.731: INFO: Waiting up to 5m0s for pod "pod-b45269d0-9098-42ac-8876-1df6b83e879c" in namespace "emptydir-2884" to be "Succeeded or Failed"
Sep 27 21:01:37.747: INFO: Pod "pod-b45269d0-9098-42ac-8876-1df6b83e879c": Phase="Pending", Reason="", readiness=false. Elapsed: 15.102285ms
Sep 27 21:01:39.767: INFO: Pod "pod-b45269d0-9098-42ac-8876-1df6b83e879c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.035085112s
STEP: Saw pod success
Sep 27 21:01:39.767: INFO: Pod "pod-b45269d0-9098-42ac-8876-1df6b83e879c" satisfied condition "Succeeded or Failed"
Sep 27 21:01:39.780: INFO: Trying to get logs from node 10.177.248.117 pod pod-b45269d0-9098-42ac-8876-1df6b83e879c container test-container: <nil>
STEP: delete the pod
Sep 27 21:01:39.962: INFO: Waiting for pod pod-b45269d0-9098-42ac-8876-1df6b83e879c to disappear
Sep 27 21:01:39.978: INFO: Pod pod-b45269d0-9098-42ac-8876-1df6b83e879c no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 21:01:39.979: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2884" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":276,"skipped":4499,"failed":0}
SSSSSSSS
------------------------------
[sig-network] Services 
  should provide secure master service  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 21:01:40.026: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should provide secure master service  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 21:01:40.210: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-2479" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786
•{"msg":"PASSED [sig-network] Services should provide secure master service  [Conformance]","total":305,"completed":277,"skipped":4507,"failed":0}
SSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 21:01:40.255: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 21:02:40.511: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-9945" for this suite.

• [SLOW TEST:60.300 seconds]
[k8s.io] Probing container
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Probing container with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]","total":305,"completed":278,"skipped":4513,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl version 
  should check is all data is printed  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 21:02:40.556: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[It] should check is all data is printed  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Sep 27 21:02:40.702: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-679324930 version'
Sep 27 21:02:40.826: INFO: stderr: ""
Sep 27 21:02:40.826: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"19\", GitVersion:\"v1.19.0\", GitCommit:\"e19964183377d0ec2052d1f1fa930c4d7575bd50\", GitTreeState:\"clean\", BuildDate:\"2020-08-26T14:30:33Z\", GoVersion:\"go1.15\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"19\", GitVersion:\"v1.19.0+4c3480d\", GitCommit:\"4c3480dcd4299c3b3e9a75e28d643177600e7d72\", GitTreeState:\"clean\", BuildDate:\"2021-07-09T00:02:08Z\", GoVersion:\"go1.15.14\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 21:02:40.826: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3060" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl version should check is all data is printed  [Conformance]","total":305,"completed":279,"skipped":4531,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 21:02:40.869: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Performing setup for networking test in namespace pod-network-test-9833
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Sep 27 21:02:40.999: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Sep 27 21:02:41.176: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Sep 27 21:02:43.190: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep 27 21:02:45.195: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep 27 21:02:47.203: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep 27 21:02:49.193: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep 27 21:02:51.190: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep 27 21:02:53.198: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep 27 21:02:55.190: INFO: The status of Pod netserver-0 is Running (Ready = true)
Sep 27 21:02:55.215: INFO: The status of Pod netserver-1 is Running (Ready = false)
Sep 27 21:02:57.230: INFO: The status of Pod netserver-1 is Running (Ready = true)
Sep 27 21:02:57.256: INFO: The status of Pod netserver-2 is Running (Ready = true)
STEP: Creating test pods
Sep 27 21:03:01.432: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.30.137.94 8081 | grep -v '^\s*$'] Namespace:pod-network-test-9833 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep 27 21:03:01.432: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
Sep 27 21:03:02.701: INFO: Found all expected endpoints: [netserver-0]
Sep 27 21:03:02.714: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.30.75.45 8081 | grep -v '^\s*$'] Namespace:pod-network-test-9833 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep 27 21:03:02.714: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
Sep 27 21:03:03.942: INFO: Found all expected endpoints: [netserver-1]
Sep 27 21:03:03.960: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.30.85.180 8081 | grep -v '^\s*$'] Namespace:pod-network-test-9833 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep 27 21:03:03.960: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
Sep 27 21:03:05.225: INFO: Found all expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 21:03:05.225: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-9833" for this suite.

• [SLOW TEST:24.407 seconds]
[sig-network] Networking
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:26
  Granular Checks: Pods
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:29
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":280,"skipped":4542,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 21:03:05.276: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0777 on node default medium
Sep 27 21:03:05.467: INFO: Waiting up to 5m0s for pod "pod-232f8468-6941-434b-a421-91b59b21a56a" in namespace "emptydir-143" to be "Succeeded or Failed"
Sep 27 21:03:05.483: INFO: Pod "pod-232f8468-6941-434b-a421-91b59b21a56a": Phase="Pending", Reason="", readiness=false. Elapsed: 16.014189ms
Sep 27 21:03:07.496: INFO: Pod "pod-232f8468-6941-434b-a421-91b59b21a56a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029210804s
Sep 27 21:03:09.511: INFO: Pod "pod-232f8468-6941-434b-a421-91b59b21a56a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.044322423s
STEP: Saw pod success
Sep 27 21:03:09.511: INFO: Pod "pod-232f8468-6941-434b-a421-91b59b21a56a" satisfied condition "Succeeded or Failed"
Sep 27 21:03:09.524: INFO: Trying to get logs from node 10.177.248.117 pod pod-232f8468-6941-434b-a421-91b59b21a56a container test-container: <nil>
STEP: delete the pod
Sep 27 21:03:09.646: INFO: Waiting for pod pod-232f8468-6941-434b-a421-91b59b21a56a to disappear
Sep 27 21:03:09.662: INFO: Pod pod-232f8468-6941-434b-a421-91b59b21a56a no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 21:03:09.662: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-143" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":281,"skipped":4558,"failed":0}
SS
------------------------------
[sig-api-machinery] Servers with support for Table transformation 
  should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 21:03:09.709: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename tables
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/table_conversion.go:47
[It] should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 21:03:09.847: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "tables-1927" for this suite.
•{"msg":"PASSED [sig-api-machinery] Servers with support for Table transformation should return a 406 for a backend which does not implement metadata [Conformance]","total":305,"completed":282,"skipped":4560,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 21:03:09.896: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0666 on node default medium
Sep 27 21:03:10.091: INFO: Waiting up to 5m0s for pod "pod-487119f1-4311-4a0c-8a51-2f44768b9565" in namespace "emptydir-7484" to be "Succeeded or Failed"
Sep 27 21:03:10.105: INFO: Pod "pod-487119f1-4311-4a0c-8a51-2f44768b9565": Phase="Pending", Reason="", readiness=false. Elapsed: 13.539732ms
Sep 27 21:03:12.132: INFO: Pod "pod-487119f1-4311-4a0c-8a51-2f44768b9565": Phase="Pending", Reason="", readiness=false. Elapsed: 2.04058598s
Sep 27 21:03:14.149: INFO: Pod "pod-487119f1-4311-4a0c-8a51-2f44768b9565": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.057770297s
STEP: Saw pod success
Sep 27 21:03:14.149: INFO: Pod "pod-487119f1-4311-4a0c-8a51-2f44768b9565" satisfied condition "Succeeded or Failed"
Sep 27 21:03:14.161: INFO: Trying to get logs from node 10.177.248.117 pod pod-487119f1-4311-4a0c-8a51-2f44768b9565 container test-container: <nil>
STEP: delete the pod
Sep 27 21:03:14.250: INFO: Waiting for pod pod-487119f1-4311-4a0c-8a51-2f44768b9565 to disappear
Sep 27 21:03:14.263: INFO: Pod pod-487119f1-4311-4a0c-8a51-2f44768b9565 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 21:03:14.263: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7484" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":283,"skipped":4568,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 21:03:14.315: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a pod in the namespace
STEP: Waiting for the pod to have running status
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there are no pods in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 21:03:49.945: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-2492" for this suite.
STEP: Destroying namespace "nsdeletetest-7220" for this suite.
Sep 27 21:03:50.004: INFO: Namespace nsdeletetest-7220 was already deleted
STEP: Destroying namespace "nsdeletetest-5260" for this suite.

• [SLOW TEST:35.708 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all pods are removed when a namespace is deleted [Conformance]","total":305,"completed":284,"skipped":4694,"failed":0}
S
------------------------------
[sig-api-machinery] Garbage collector 
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 21:03:50.025: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the rc
STEP: delete the rc
STEP: wait for all pods to be garbage collected
STEP: Gathering metrics
Sep 27 21:04:00.344: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 21:04:00.345: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
W0927 21:04:00.344832      22 metrics_grabber.go:98] Can't find kube-scheduler pod. Grabbing metrics from kube-scheduler is disabled.
W0927 21:04:00.344862      22 metrics_grabber.go:102] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
W0927 21:04:00.344871      22 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
STEP: Destroying namespace "gc-3365" for this suite.

• [SLOW TEST:10.376 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should delete pods created by rc when not orphaning [Conformance]","total":305,"completed":285,"skipped":4695,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a validating webhook should work [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 21:04:00.404: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Sep 27 21:04:01.247: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Sep 27 21:04:03.303: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63768373441, loc:(*time.Location)(0x7702840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63768373441, loc:(*time.Location)(0x7702840)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63768373441, loc:(*time.Location)(0x7702840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63768373441, loc:(*time.Location)(0x7702840)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep 27 21:04:06.364: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a validating webhook should work [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a validating webhook configuration
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Updating a validating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Patching a validating webhook configuration's rules to include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 21:04:06.661: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2387" for this suite.
STEP: Destroying namespace "webhook-2387-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:6.488 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  patching/updating a validating webhook should work [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a validating webhook should work [Conformance]","total":305,"completed":286,"skipped":4706,"failed":0}
[k8s.io] Docker Containers 
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 21:04:06.894: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test override all
Sep 27 21:04:07.105: INFO: Waiting up to 5m0s for pod "client-containers-ab2b2a66-f384-46f3-a42f-6e5355fed00d" in namespace "containers-2625" to be "Succeeded or Failed"
Sep 27 21:04:07.117: INFO: Pod "client-containers-ab2b2a66-f384-46f3-a42f-6e5355fed00d": Phase="Pending", Reason="", readiness=false. Elapsed: 12.416813ms
Sep 27 21:04:09.131: INFO: Pod "client-containers-ab2b2a66-f384-46f3-a42f-6e5355fed00d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026558325s
Sep 27 21:04:11.145: INFO: Pod "client-containers-ab2b2a66-f384-46f3-a42f-6e5355fed00d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.040119102s
STEP: Saw pod success
Sep 27 21:04:11.145: INFO: Pod "client-containers-ab2b2a66-f384-46f3-a42f-6e5355fed00d" satisfied condition "Succeeded or Failed"
Sep 27 21:04:11.158: INFO: Trying to get logs from node 10.177.248.117 pod client-containers-ab2b2a66-f384-46f3-a42f-6e5355fed00d container test-container: <nil>
STEP: delete the pod
Sep 27 21:04:11.244: INFO: Waiting for pod client-containers-ab2b2a66-f384-46f3-a42f-6e5355fed00d to disappear
Sep 27 21:04:11.257: INFO: Pod client-containers-ab2b2a66-f384-46f3-a42f-6e5355fed00d no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 21:04:11.258: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-2625" for this suite.
•{"msg":"PASSED [k8s.io] Docker Containers should be able to override the image's default command and arguments [NodeConformance] [Conformance]","total":305,"completed":287,"skipped":4706,"failed":0}
SSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 21:04:11.304: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-4738
[It] Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Looking for a node to schedule stateful set and pod
STEP: Creating pod with conflicting port in namespace statefulset-4738
STEP: Creating statefulset with conflicting port in namespace statefulset-4738
STEP: Waiting until pod test-pod will start running in namespace statefulset-4738
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-4738
Sep 27 21:04:13.718: INFO: Observed stateful pod in namespace: statefulset-4738, name: ss-0, uid: 55686ec4-bea2-4277-9bfa-694b6c34ef80, status phase: Failed. Waiting for statefulset controller to delete.
Sep 27 21:04:13.724: INFO: Observed stateful pod in namespace: statefulset-4738, name: ss-0, uid: 55686ec4-bea2-4277-9bfa-694b6c34ef80, status phase: Failed. Waiting for statefulset controller to delete.
Sep 27 21:04:13.738: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-4738
STEP: Removing pod with conflicting port in namespace statefulset-4738
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-4738 and will be in running state
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
Sep 27 21:04:15.820: INFO: Deleting all statefulset in ns statefulset-4738
Sep 27 21:04:15.833: INFO: Scaling statefulset ss to 0
Sep 27 21:04:25.905: INFO: Waiting for statefulset status.replicas updated to 0
Sep 27 21:04:25.919: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 21:04:25.980: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-4738" for this suite.

• [SLOW TEST:14.728 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
    Should recreate evicted statefulset [Conformance]
    /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] Should recreate evicted statefulset [Conformance]","total":305,"completed":288,"skipped":4710,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 21:04:26.036: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-4065 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-4065;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-4065 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-4065;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-4065.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-4065.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-4065.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-4065.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-4065.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-4065.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-4065.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-4065.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-4065.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-4065.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-4065.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-4065.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-4065.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 11.16.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.16.11_udp@PTR;check="$$(dig +tcp +noall +answer +search 11.16.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.16.11_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-4065 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-4065;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-4065 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-4065;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-4065.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-4065.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-4065.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-4065.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-4065.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-4065.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-4065.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-4065.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-4065.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-4065.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-4065.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-4065.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-4065.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 11.16.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.16.11_udp@PTR;check="$$(dig +tcp +noall +answer +search 11.16.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.16.11_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Sep 27 21:04:30.435: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-4065/dns-test-e34429e2-7e6a-4ffc-b4a3-de3856c95107: the server could not find the requested resource (get pods dns-test-e34429e2-7e6a-4ffc-b4a3-de3856c95107)
Sep 27 21:04:30.458: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-4065/dns-test-e34429e2-7e6a-4ffc-b4a3-de3856c95107: the server could not find the requested resource (get pods dns-test-e34429e2-7e6a-4ffc-b4a3-de3856c95107)
Sep 27 21:04:30.481: INFO: Unable to read wheezy_udp@dns-test-service.dns-4065 from pod dns-4065/dns-test-e34429e2-7e6a-4ffc-b4a3-de3856c95107: the server could not find the requested resource (get pods dns-test-e34429e2-7e6a-4ffc-b4a3-de3856c95107)
Sep 27 21:04:30.507: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4065 from pod dns-4065/dns-test-e34429e2-7e6a-4ffc-b4a3-de3856c95107: the server could not find the requested resource (get pods dns-test-e34429e2-7e6a-4ffc-b4a3-de3856c95107)
Sep 27 21:04:30.531: INFO: Unable to read wheezy_udp@dns-test-service.dns-4065.svc from pod dns-4065/dns-test-e34429e2-7e6a-4ffc-b4a3-de3856c95107: the server could not find the requested resource (get pods dns-test-e34429e2-7e6a-4ffc-b4a3-de3856c95107)
Sep 27 21:04:30.558: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4065.svc from pod dns-4065/dns-test-e34429e2-7e6a-4ffc-b4a3-de3856c95107: the server could not find the requested resource (get pods dns-test-e34429e2-7e6a-4ffc-b4a3-de3856c95107)
Sep 27 21:04:30.585: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-4065.svc from pod dns-4065/dns-test-e34429e2-7e6a-4ffc-b4a3-de3856c95107: the server could not find the requested resource (get pods dns-test-e34429e2-7e6a-4ffc-b4a3-de3856c95107)
Sep 27 21:04:30.609: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-4065.svc from pod dns-4065/dns-test-e34429e2-7e6a-4ffc-b4a3-de3856c95107: the server could not find the requested resource (get pods dns-test-e34429e2-7e6a-4ffc-b4a3-de3856c95107)
Sep 27 21:04:30.801: INFO: Unable to read jessie_udp@dns-test-service from pod dns-4065/dns-test-e34429e2-7e6a-4ffc-b4a3-de3856c95107: the server could not find the requested resource (get pods dns-test-e34429e2-7e6a-4ffc-b4a3-de3856c95107)
Sep 27 21:04:30.823: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-4065/dns-test-e34429e2-7e6a-4ffc-b4a3-de3856c95107: the server could not find the requested resource (get pods dns-test-e34429e2-7e6a-4ffc-b4a3-de3856c95107)
Sep 27 21:04:30.846: INFO: Unable to read jessie_udp@dns-test-service.dns-4065 from pod dns-4065/dns-test-e34429e2-7e6a-4ffc-b4a3-de3856c95107: the server could not find the requested resource (get pods dns-test-e34429e2-7e6a-4ffc-b4a3-de3856c95107)
Sep 27 21:04:30.868: INFO: Unable to read jessie_tcp@dns-test-service.dns-4065 from pod dns-4065/dns-test-e34429e2-7e6a-4ffc-b4a3-de3856c95107: the server could not find the requested resource (get pods dns-test-e34429e2-7e6a-4ffc-b4a3-de3856c95107)
Sep 27 21:04:30.892: INFO: Unable to read jessie_udp@dns-test-service.dns-4065.svc from pod dns-4065/dns-test-e34429e2-7e6a-4ffc-b4a3-de3856c95107: the server could not find the requested resource (get pods dns-test-e34429e2-7e6a-4ffc-b4a3-de3856c95107)
Sep 27 21:04:30.913: INFO: Unable to read jessie_tcp@dns-test-service.dns-4065.svc from pod dns-4065/dns-test-e34429e2-7e6a-4ffc-b4a3-de3856c95107: the server could not find the requested resource (get pods dns-test-e34429e2-7e6a-4ffc-b4a3-de3856c95107)
Sep 27 21:04:30.935: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-4065.svc from pod dns-4065/dns-test-e34429e2-7e6a-4ffc-b4a3-de3856c95107: the server could not find the requested resource (get pods dns-test-e34429e2-7e6a-4ffc-b4a3-de3856c95107)
Sep 27 21:04:30.955: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-4065.svc from pod dns-4065/dns-test-e34429e2-7e6a-4ffc-b4a3-de3856c95107: the server could not find the requested resource (get pods dns-test-e34429e2-7e6a-4ffc-b4a3-de3856c95107)
Sep 27 21:04:31.089: INFO: Lookups using dns-4065/dns-test-e34429e2-7e6a-4ffc-b4a3-de3856c95107 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-4065 wheezy_tcp@dns-test-service.dns-4065 wheezy_udp@dns-test-service.dns-4065.svc wheezy_tcp@dns-test-service.dns-4065.svc wheezy_udp@_http._tcp.dns-test-service.dns-4065.svc wheezy_tcp@_http._tcp.dns-test-service.dns-4065.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-4065 jessie_tcp@dns-test-service.dns-4065 jessie_udp@dns-test-service.dns-4065.svc jessie_tcp@dns-test-service.dns-4065.svc jessie_udp@_http._tcp.dns-test-service.dns-4065.svc jessie_tcp@_http._tcp.dns-test-service.dns-4065.svc]

Sep 27 21:04:36.760: INFO: DNS probes using dns-4065/dns-test-e34429e2-7e6a-4ffc-b4a3-de3856c95107 succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 21:04:36.926: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-4065" for this suite.

• [SLOW TEST:10.952 seconds]
[sig-network] DNS
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] DNS should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]","total":305,"completed":289,"skipped":4722,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 21:04:36.989: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the rc1
STEP: create the rc2
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well
STEP: delete the rc simpletest-rc-to-be-deleted
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W0927 21:04:47.441981      22 metrics_grabber.go:98] Can't find kube-scheduler pod. Grabbing metrics from kube-scheduler is disabled.
W0927 21:04:47.442224      22 metrics_grabber.go:102] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
W0927 21:04:47.442322      22 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
Sep 27 21:04:47.442: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

Sep 27 21:04:47.442: INFO: Deleting pod "simpletest-rc-to-be-deleted-2wlkc" in namespace "gc-4714"
Sep 27 21:04:47.485: INFO: Deleting pod "simpletest-rc-to-be-deleted-54bd4" in namespace "gc-4714"
Sep 27 21:04:47.527: INFO: Deleting pod "simpletest-rc-to-be-deleted-9sx5x" in namespace "gc-4714"
Sep 27 21:04:47.574: INFO: Deleting pod "simpletest-rc-to-be-deleted-bw22r" in namespace "gc-4714"
Sep 27 21:04:47.627: INFO: Deleting pod "simpletest-rc-to-be-deleted-mlpb6" in namespace "gc-4714"
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 21:04:47.673: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-4714" for this suite.

• [SLOW TEST:10.723 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]","total":305,"completed":290,"skipped":4739,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 21:04:47.717: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Sep 27 21:04:47.963: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c00ed049-7937-412e-9ce9-21fd43de89d6" in namespace "projected-3555" to be "Succeeded or Failed"
Sep 27 21:04:47.979: INFO: Pod "downwardapi-volume-c00ed049-7937-412e-9ce9-21fd43de89d6": Phase="Pending", Reason="", readiness=false. Elapsed: 16.355191ms
Sep 27 21:04:50.002: INFO: Pod "downwardapi-volume-c00ed049-7937-412e-9ce9-21fd43de89d6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.039012895s
Sep 27 21:04:52.015: INFO: Pod "downwardapi-volume-c00ed049-7937-412e-9ce9-21fd43de89d6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.052224087s
STEP: Saw pod success
Sep 27 21:04:52.015: INFO: Pod "downwardapi-volume-c00ed049-7937-412e-9ce9-21fd43de89d6" satisfied condition "Succeeded or Failed"
Sep 27 21:04:52.040: INFO: Trying to get logs from node 10.177.248.117 pod downwardapi-volume-c00ed049-7937-412e-9ce9-21fd43de89d6 container client-container: <nil>
STEP: delete the pod
Sep 27 21:04:52.137: INFO: Waiting for pod downwardapi-volume-c00ed049-7937-412e-9ce9-21fd43de89d6 to disappear
Sep 27 21:04:52.150: INFO: Pod downwardapi-volume-c00ed049-7937-412e-9ce9-21fd43de89d6 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 21:04:52.151: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3555" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":291,"skipped":4771,"failed":0}
S
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 21:04:52.195: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name projected-configmap-test-volume-3882a556-a930-4d1f-9814-56e22ddfad6a
STEP: Creating a pod to test consume configMaps
Sep 27 21:04:52.412: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-d42096ee-d83a-4283-8831-25d26d8dd08f" in namespace "projected-7124" to be "Succeeded or Failed"
Sep 27 21:04:52.433: INFO: Pod "pod-projected-configmaps-d42096ee-d83a-4283-8831-25d26d8dd08f": Phase="Pending", Reason="", readiness=false. Elapsed: 21.088828ms
Sep 27 21:04:54.453: INFO: Pod "pod-projected-configmaps-d42096ee-d83a-4283-8831-25d26d8dd08f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.040895583s
STEP: Saw pod success
Sep 27 21:04:54.453: INFO: Pod "pod-projected-configmaps-d42096ee-d83a-4283-8831-25d26d8dd08f" satisfied condition "Succeeded or Failed"
Sep 27 21:04:54.473: INFO: Trying to get logs from node 10.177.248.117 pod pod-projected-configmaps-d42096ee-d83a-4283-8831-25d26d8dd08f container projected-configmap-volume-test: <nil>
STEP: delete the pod
Sep 27 21:04:54.565: INFO: Waiting for pod pod-projected-configmaps-d42096ee-d83a-4283-8831-25d26d8dd08f to disappear
Sep 27 21:04:54.596: INFO: Pod pod-projected-configmaps-d42096ee-d83a-4283-8831-25d26d8dd08f no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 21:04:54.596: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7124" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","total":305,"completed":292,"skipped":4772,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 21:04:54.644: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:134
[It] should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Sep 27 21:04:55.002: INFO: Number of nodes with available pods: 0
Sep 27 21:04:55.002: INFO: Node 10.177.248.114 is running more than one daemon pod
Sep 27 21:04:56.050: INFO: Number of nodes with available pods: 0
Sep 27 21:04:56.050: INFO: Node 10.177.248.114 is running more than one daemon pod
Sep 27 21:04:57.042: INFO: Number of nodes with available pods: 1
Sep 27 21:04:57.042: INFO: Node 10.177.248.117 is running more than one daemon pod
Sep 27 21:04:58.039: INFO: Number of nodes with available pods: 3
Sep 27 21:04:58.039: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived.
Sep 27 21:04:58.135: INFO: Number of nodes with available pods: 2
Sep 27 21:04:58.135: INFO: Node 10.177.248.114 is running more than one daemon pod
Sep 27 21:04:59.179: INFO: Number of nodes with available pods: 2
Sep 27 21:04:59.179: INFO: Node 10.177.248.114 is running more than one daemon pod
Sep 27 21:05:00.177: INFO: Number of nodes with available pods: 3
Sep 27 21:05:00.177: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Wait for the failed daemon pod to be completely deleted.
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:100
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-6999, will wait for the garbage collector to delete the pods
Sep 27 21:05:00.311: INFO: Deleting DaemonSet.extensions daemon-set took: 40.970886ms
Sep 27 21:05:00.411: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.256514ms
Sep 27 21:05:12.124: INFO: Number of nodes with available pods: 0
Sep 27 21:05:12.125: INFO: Number of running nodes: 0, number of available pods: 0
Sep 27 21:05:12.137: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-6999/daemonsets","resourceVersion":"118305"},"items":null}

Sep 27 21:05:12.150: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-6999/pods","resourceVersion":"118305"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 21:05:12.251: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-6999" for this suite.

• [SLOW TEST:17.652 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should retry creating failed daemon pods [Conformance]","total":305,"completed":293,"skipped":4787,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Events 
  should delete a collection of events [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Events
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 21:05:12.303: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a collection of events [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Create set of events
Sep 27 21:05:12.467: INFO: created test-event-1
Sep 27 21:05:12.483: INFO: created test-event-2
Sep 27 21:05:12.500: INFO: created test-event-3
STEP: get a list of Events with a label in the current namespace
STEP: delete collection of events
Sep 27 21:05:12.515: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity
Sep 27 21:05:12.590: INFO: requesting list of events to confirm quantity
[AfterEach] [sig-api-machinery] Events
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 21:05:12.602: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-9504" for this suite.
•{"msg":"PASSED [sig-api-machinery] Events should delete a collection of events [Conformance]","total":305,"completed":294,"skipped":4827,"failed":0}
SSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command in a pod 
  should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 21:05:12.647: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[It] should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 21:05:17.109: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-7560" for this suite.
•{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox command in a pod should print the output to logs [NodeConformance] [Conformance]","total":305,"completed":295,"skipped":4833,"failed":0}
SSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 21:05:17.164: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:181
[It] should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Sep 27 21:05:19.616: INFO: Waiting up to 5m0s for pod "client-envvars-4f1773b1-5c76-420c-b5e0-bd29bdc81d04" in namespace "pods-6893" to be "Succeeded or Failed"
Sep 27 21:05:19.629: INFO: Pod "client-envvars-4f1773b1-5c76-420c-b5e0-bd29bdc81d04": Phase="Pending", Reason="", readiness=false. Elapsed: 13.515685ms
Sep 27 21:05:21.645: INFO: Pod "client-envvars-4f1773b1-5c76-420c-b5e0-bd29bdc81d04": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.028781266s
STEP: Saw pod success
Sep 27 21:05:21.645: INFO: Pod "client-envvars-4f1773b1-5c76-420c-b5e0-bd29bdc81d04" satisfied condition "Succeeded or Failed"
Sep 27 21:05:21.661: INFO: Trying to get logs from node 10.177.248.117 pod client-envvars-4f1773b1-5c76-420c-b5e0-bd29bdc81d04 container env3cont: <nil>
STEP: delete the pod
Sep 27 21:05:21.733: INFO: Waiting for pod client-envvars-4f1773b1-5c76-420c-b5e0-bd29bdc81d04 to disappear
Sep 27 21:05:21.748: INFO: Pod client-envvars-4f1773b1-5c76-420c-b5e0-bd29bdc81d04 no longer exists
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 21:05:21.748: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-6893" for this suite.
•{"msg":"PASSED [k8s.io] Pods should contain environment variables for services [NodeConformance] [Conformance]","total":305,"completed":296,"skipped":4843,"failed":0}
S
------------------------------
[sig-network] DNS 
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 21:05:21.791: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-6329.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-6329.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Sep 27 21:05:26.252: INFO: DNS probes using dns-6329/dns-test-19a35ebf-3ac6-4349-b158-a9cda1461244 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 21:05:26.293: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-6329" for this suite.
•{"msg":"PASSED [sig-network] DNS should provide DNS for the cluster  [Conformance]","total":305,"completed":297,"skipped":4844,"failed":0}
SSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 21:05:26.360: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating projection with secret that has name projected-secret-test-78a1b3ac-8dd9-4fbc-8f5c-b0d3497ef686
STEP: Creating a pod to test consume secrets
Sep 27 21:05:26.597: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-3cc72faa-5976-4ea4-a16d-0b90039c52c1" in namespace "projected-2257" to be "Succeeded or Failed"
Sep 27 21:05:26.615: INFO: Pod "pod-projected-secrets-3cc72faa-5976-4ea4-a16d-0b90039c52c1": Phase="Pending", Reason="", readiness=false. Elapsed: 18.116367ms
Sep 27 21:05:28.630: INFO: Pod "pod-projected-secrets-3cc72faa-5976-4ea4-a16d-0b90039c52c1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033682142s
Sep 27 21:05:30.647: INFO: Pod "pod-projected-secrets-3cc72faa-5976-4ea4-a16d-0b90039c52c1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.049760075s
STEP: Saw pod success
Sep 27 21:05:30.647: INFO: Pod "pod-projected-secrets-3cc72faa-5976-4ea4-a16d-0b90039c52c1" satisfied condition "Succeeded or Failed"
Sep 27 21:05:30.659: INFO: Trying to get logs from node 10.177.248.117 pod pod-projected-secrets-3cc72faa-5976-4ea4-a16d-0b90039c52c1 container projected-secret-volume-test: <nil>
STEP: delete the pod
Sep 27 21:05:30.748: INFO: Waiting for pod pod-projected-secrets-3cc72faa-5976-4ea4-a16d-0b90039c52c1 to disappear
Sep 27 21:05:30.761: INFO: Pod pod-projected-secrets-3cc72faa-5976-4ea4-a16d-0b90039c52c1 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 21:05:30.761: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2257" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume [NodeConformance] [Conformance]","total":305,"completed":298,"skipped":4851,"failed":0}
SSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 21:05:30.819: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name configmap-test-volume-51f02e34-d353-4fd3-81ee-5d06b7f6f24d
STEP: Creating a pod to test consume configMaps
Sep 27 21:05:31.035: INFO: Waiting up to 5m0s for pod "pod-configmaps-5ded7a85-48b7-451f-ac58-b66062d84254" in namespace "configmap-4367" to be "Succeeded or Failed"
Sep 27 21:05:31.055: INFO: Pod "pod-configmaps-5ded7a85-48b7-451f-ac58-b66062d84254": Phase="Pending", Reason="", readiness=false. Elapsed: 19.043081ms
Sep 27 21:05:33.074: INFO: Pod "pod-configmaps-5ded7a85-48b7-451f-ac58-b66062d84254": Phase="Pending", Reason="", readiness=false. Elapsed: 2.037864055s
Sep 27 21:05:35.089: INFO: Pod "pod-configmaps-5ded7a85-48b7-451f-ac58-b66062d84254": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.053320627s
STEP: Saw pod success
Sep 27 21:05:35.089: INFO: Pod "pod-configmaps-5ded7a85-48b7-451f-ac58-b66062d84254" satisfied condition "Succeeded or Failed"
Sep 27 21:05:35.107: INFO: Trying to get logs from node 10.177.248.117 pod pod-configmaps-5ded7a85-48b7-451f-ac58-b66062d84254 container configmap-volume-test: <nil>
STEP: delete the pod
Sep 27 21:05:35.186: INFO: Waiting for pod pod-configmaps-5ded7a85-48b7-451f-ac58-b66062d84254 to disappear
Sep 27 21:05:35.201: INFO: Pod pod-configmaps-5ded7a85-48b7-451f-ac58-b66062d84254 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 21:05:35.202: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4367" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume [NodeConformance] [Conformance]","total":305,"completed":299,"skipped":4855,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] 
  should include custom resource definition resources in discovery documents [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 21:05:35.257: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] should include custom resource definition resources in discovery documents [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: fetching the /apis discovery document
STEP: finding the apiextensions.k8s.io API group in the /apis discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/apiextensions.k8s.io discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document
STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document
STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 21:05:35.440: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-9394" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] should include custom resource definition resources in discovery documents [Conformance]","total":305,"completed":300,"skipped":4868,"failed":0}
SSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing mutating webhooks should work [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 21:05:35.491: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Sep 27 21:05:36.492: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Sep 27 21:05:38.537: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63768373536, loc:(*time.Location)(0x7702840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63768373536, loc:(*time.Location)(0x7702840)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63768373536, loc:(*time.Location)(0x7702840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63768373536, loc:(*time.Location)(0x7702840)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep 27 21:05:41.581: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing mutating webhooks should work [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that should be mutated
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that should not be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 21:05:42.531: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6290" for this suite.
STEP: Destroying namespace "webhook-6290-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:7.280 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  listing mutating webhooks should work [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing mutating webhooks should work [Conformance]","total":305,"completed":301,"skipped":4872,"failed":0}
SS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with pruning [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 21:05:42.771: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Sep 27 21:05:44.945: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Sep 27 21:05:46.988: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63768373544, loc:(*time.Location)(0x7702840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63768373544, loc:(*time.Location)(0x7702840)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63768373544, loc:(*time.Location)(0x7702840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63768373544, loc:(*time.Location)(0x7702840)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep 27 21:05:50.054: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with pruning [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Sep 27 21:05:50.077: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-9722-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 21:05:51.453: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3617" for this suite.
STEP: Destroying namespace "webhook-3617-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:8.969 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource with pruning [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with pruning [Conformance]","total":305,"completed":302,"skipped":4874,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 21:05:51.740: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Sep 27 21:05:51.991: INFO: Waiting up to 5m0s for pod "downwardapi-volume-213a9737-7a2c-49cc-bdeb-4fb11fc74343" in namespace "projected-7707" to be "Succeeded or Failed"
Sep 27 21:05:52.011: INFO: Pod "downwardapi-volume-213a9737-7a2c-49cc-bdeb-4fb11fc74343": Phase="Pending", Reason="", readiness=false. Elapsed: 19.525971ms
Sep 27 21:05:54.026: INFO: Pod "downwardapi-volume-213a9737-7a2c-49cc-bdeb-4fb11fc74343": Phase="Pending", Reason="", readiness=false. Elapsed: 2.034777815s
Sep 27 21:05:56.042: INFO: Pod "downwardapi-volume-213a9737-7a2c-49cc-bdeb-4fb11fc74343": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.050558586s
STEP: Saw pod success
Sep 27 21:05:56.042: INFO: Pod "downwardapi-volume-213a9737-7a2c-49cc-bdeb-4fb11fc74343" satisfied condition "Succeeded or Failed"
Sep 27 21:05:56.055: INFO: Trying to get logs from node 10.177.248.117 pod downwardapi-volume-213a9737-7a2c-49cc-bdeb-4fb11fc74343 container client-container: <nil>
STEP: delete the pod
Sep 27 21:05:56.136: INFO: Waiting for pod downwardapi-volume-213a9737-7a2c-49cc-bdeb-4fb11fc74343 to disappear
Sep 27 21:05:56.158: INFO: Pod downwardapi-volume-213a9737-7a2c-49cc-bdeb-4fb11fc74343 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 21:05:56.159: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7707" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","total":305,"completed":303,"skipped":4882,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 21:05:56.213: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:90
Sep 27 21:05:56.379: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Sep 27 21:05:56.432: INFO: Waiting for terminating namespaces to be deleted...
Sep 27 21:05:56.488: INFO: 
Logging pods the apiserver thinks is on node 10.177.248.114 before test
Sep 27 21:05:56.540: INFO: calico-node-kxvx5 from calico-system started at 2021-09-27 17:50:17 +0000 UTC (1 container statuses recorded)
Sep 27 21:05:56.540: INFO: 	Container calico-node ready: true, restart count 0
Sep 27 21:05:56.540: INFO: calico-typha-7d789bfc7c-8ftcw from calico-system started at 2021-09-27 17:52:15 +0000 UTC (1 container statuses recorded)
Sep 27 21:05:56.540: INFO: 	Container calico-typha ready: true, restart count 0
Sep 27 21:05:56.540: INFO: test-k8s-e2e-pvg-master-verification from default started at 2021-09-27 17:52:52 +0000 UTC (1 container statuses recorded)
Sep 27 21:05:56.540: INFO: 	Container test-k8s-e2e-pvg-master-verification ready: true, restart count 0
Sep 27 21:05:56.541: INFO: ibm-cloud-provider-ip-169-61-239-242-59b4964694-ngqgl from ibm-system started at 2021-09-27 18:03:32 +0000 UTC (1 container statuses recorded)
Sep 27 21:05:56.541: INFO: 	Container ibm-cloud-provider-ip-169-61-239-242 ready: true, restart count 0
Sep 27 21:05:56.541: INFO: ibm-keepalived-watcher-8lj4c from kube-system started at 2021-09-27 17:50:13 +0000 UTC (1 container statuses recorded)
Sep 27 21:05:56.541: INFO: 	Container keepalived-watcher ready: true, restart count 0
Sep 27 21:05:56.541: INFO: ibm-master-proxy-static-10.177.248.114 from kube-system started at 2021-09-27 17:49:33 +0000 UTC (2 container statuses recorded)
Sep 27 21:05:56.541: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Sep 27 21:05:56.541: INFO: 	Container pause ready: true, restart count 0
Sep 27 21:05:56.541: INFO: ibmcloud-block-storage-driver-mxb6q from kube-system started at 2021-09-27 17:50:13 +0000 UTC (1 container statuses recorded)
Sep 27 21:05:56.541: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Sep 27 21:05:56.541: INFO: vpn-bc979587-kglsg from kube-system started at 2021-09-27 17:53:53 +0000 UTC (1 container statuses recorded)
Sep 27 21:05:56.541: INFO: 	Container vpn ready: true, restart count 0
Sep 27 21:05:56.541: INFO: tuned-dsf6r from openshift-cluster-node-tuning-operator started at 2021-09-27 17:51:40 +0000 UTC (1 container statuses recorded)
Sep 27 21:05:56.541: INFO: 	Container tuned ready: true, restart count 0
Sep 27 21:05:56.541: INFO: console-5b57dff4ff-2hr57 from openshift-console started at 2021-09-27 17:54:58 +0000 UTC (1 container statuses recorded)
Sep 27 21:05:56.541: INFO: 	Container console ready: true, restart count 0
Sep 27 21:05:56.541: INFO: dns-default-ktd8w from openshift-dns started at 2021-09-27 17:52:55 +0000 UTC (3 container statuses recorded)
Sep 27 21:05:56.541: INFO: 	Container dns ready: true, restart count 0
Sep 27 21:05:56.541: INFO: 	Container dns-node-resolver ready: true, restart count 0
Sep 27 21:05:56.541: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep 27 21:05:56.541: INFO: node-ca-bv4wj from openshift-image-registry started at 2021-09-27 17:53:17 +0000 UTC (1 container statuses recorded)
Sep 27 21:05:56.541: INFO: 	Container node-ca ready: true, restart count 0
Sep 27 21:05:56.542: INFO: router-default-768f4875db-n8g9c from openshift-ingress started at 2021-09-27 17:53:15 +0000 UTC (1 container statuses recorded)
Sep 27 21:05:56.542: INFO: 	Container router ready: true, restart count 0
Sep 27 21:05:56.542: INFO: openshift-kube-proxy-lxdz4 from openshift-kube-proxy started at 2021-09-27 17:50:13 +0000 UTC (1 container statuses recorded)
Sep 27 21:05:56.542: INFO: 	Container kube-proxy ready: true, restart count 0
Sep 27 21:05:56.542: INFO: migrator-6656c87b46-jpz8f from openshift-kube-storage-version-migrator started at 2021-09-27 20:40:27 +0000 UTC (1 container statuses recorded)
Sep 27 21:05:56.542: INFO: 	Container migrator ready: true, restart count 0
Sep 27 21:05:56.542: INFO: certified-operators-qvnmv from openshift-marketplace started at 2021-09-27 17:54:49 +0000 UTC (1 container statuses recorded)
Sep 27 21:05:56.542: INFO: 	Container registry-server ready: true, restart count 0
Sep 27 21:05:56.542: INFO: community-operators-mc4rj from openshift-marketplace started at 2021-09-27 17:54:50 +0000 UTC (1 container statuses recorded)
Sep 27 21:05:56.542: INFO: 	Container registry-server ready: true, restart count 0
Sep 27 21:05:56.542: INFO: redhat-marketplace-tf75q from openshift-marketplace started at 2021-09-27 17:54:48 +0000 UTC (1 container statuses recorded)
Sep 27 21:05:56.542: INFO: 	Container registry-server ready: true, restart count 0
Sep 27 21:05:56.542: INFO: redhat-operators-2sd6b from openshift-marketplace started at 2021-09-27 17:54:49 +0000 UTC (1 container statuses recorded)
Sep 27 21:05:56.542: INFO: 	Container registry-server ready: true, restart count 0
Sep 27 21:05:56.542: INFO: alertmanager-main-0 from openshift-monitoring started at 2021-09-27 17:53:55 +0000 UTC (5 container statuses recorded)
Sep 27 21:05:56.542: INFO: 	Container alertmanager ready: true, restart count 0
Sep 27 21:05:56.542: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Sep 27 21:05:56.542: INFO: 	Container config-reloader ready: true, restart count 0
Sep 27 21:05:56.542: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep 27 21:05:56.542: INFO: 	Container prom-label-proxy ready: true, restart count 0
Sep 27 21:05:56.542: INFO: grafana-6f7d589b46-r88hj from openshift-monitoring started at 2021-09-27 17:53:53 +0000 UTC (2 container statuses recorded)
Sep 27 21:05:56.543: INFO: 	Container grafana ready: true, restart count 0
Sep 27 21:05:56.543: INFO: 	Container grafana-proxy ready: true, restart count 0
Sep 27 21:05:56.543: INFO: kube-state-metrics-5d4985f6b7-jx87q from openshift-monitoring started at 2021-09-27 17:51:49 +0000 UTC (3 container statuses recorded)
Sep 27 21:05:56.543: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Sep 27 21:05:56.543: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Sep 27 21:05:56.543: INFO: 	Container kube-state-metrics ready: true, restart count 0
Sep 27 21:05:56.543: INFO: node-exporter-r84r5 from openshift-monitoring started at 2021-09-27 17:51:50 +0000 UTC (2 container statuses recorded)
Sep 27 21:05:56.543: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep 27 21:05:56.543: INFO: 	Container node-exporter ready: true, restart count 0
Sep 27 21:05:56.543: INFO: openshift-state-metrics-58bfb7bff-qxd59 from openshift-monitoring started at 2021-09-27 17:51:50 +0000 UTC (3 container statuses recorded)
Sep 27 21:05:56.543: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Sep 27 21:05:56.543: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Sep 27 21:05:56.543: INFO: 	Container openshift-state-metrics ready: true, restart count 0
Sep 27 21:05:56.543: INFO: prometheus-adapter-66469d976-gl6wl from openshift-monitoring started at 2021-09-27 17:54:45 +0000 UTC (1 container statuses recorded)
Sep 27 21:05:56.543: INFO: 	Container prometheus-adapter ready: true, restart count 0
Sep 27 21:05:56.543: INFO: prometheus-k8s-0 from openshift-monitoring started at 2021-09-27 17:56:49 +0000 UTC (6 container statuses recorded)
Sep 27 21:05:56.543: INFO: 	Container config-reloader ready: true, restart count 0
Sep 27 21:05:56.543: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep 27 21:05:56.543: INFO: 	Container prom-label-proxy ready: true, restart count 0
Sep 27 21:05:56.543: INFO: 	Container prometheus ready: true, restart count 1
Sep 27 21:05:56.543: INFO: 	Container prometheus-proxy ready: true, restart count 0
Sep 27 21:05:56.544: INFO: 	Container thanos-sidecar ready: true, restart count 0
Sep 27 21:05:56.544: INFO: prometheus-operator-856d6cddd4-blncm from openshift-monitoring started at 2021-09-27 17:54:31 +0000 UTC (2 container statuses recorded)
Sep 27 21:05:56.544: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep 27 21:05:56.544: INFO: 	Container prometheus-operator ready: true, restart count 0
Sep 27 21:05:56.544: INFO: telemeter-client-68cfc9967c-sr2hj from openshift-monitoring started at 2021-09-27 17:51:59 +0000 UTC (3 container statuses recorded)
Sep 27 21:05:56.544: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep 27 21:05:56.544: INFO: 	Container reload ready: true, restart count 0
Sep 27 21:05:56.544: INFO: 	Container telemeter-client ready: true, restart count 0
Sep 27 21:05:56.544: INFO: thanos-querier-6b5789bbbd-qwc9q from openshift-monitoring started at 2021-09-27 17:53:50 +0000 UTC (5 container statuses recorded)
Sep 27 21:05:56.544: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep 27 21:05:56.544: INFO: 	Container kube-rbac-proxy-rules ready: true, restart count 0
Sep 27 21:05:56.544: INFO: 	Container oauth-proxy ready: true, restart count 0
Sep 27 21:05:56.544: INFO: 	Container prom-label-proxy ready: true, restart count 0
Sep 27 21:05:56.544: INFO: 	Container thanos-query ready: true, restart count 0
Sep 27 21:05:56.544: INFO: multus-admission-controller-pp4c5 from openshift-multus started at 2021-09-27 17:51:20 +0000 UTC (2 container statuses recorded)
Sep 27 21:05:56.544: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep 27 21:05:56.544: INFO: 	Container multus-admission-controller ready: true, restart count 0
Sep 27 21:05:56.544: INFO: multus-qwrqb from openshift-multus started at 2021-09-27 17:50:13 +0000 UTC (1 container statuses recorded)
Sep 27 21:05:56.544: INFO: 	Container kube-multus ready: true, restart count 0
Sep 27 21:05:56.545: INFO: network-metrics-daemon-zdwnk from openshift-multus started at 2021-09-27 17:50:13 +0000 UTC (2 container statuses recorded)
Sep 27 21:05:56.545: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep 27 21:05:56.545: INFO: 	Container network-metrics-daemon ready: true, restart count 0
Sep 27 21:05:56.545: INFO: network-operator-dbdd595f7-rxqfl from openshift-network-operator started at 2021-09-27 20:40:27 +0000 UTC (1 container statuses recorded)
Sep 27 21:05:56.545: INFO: 	Container network-operator ready: true, restart count 0
Sep 27 21:05:56.545: INFO: packageserver-67b4ccc984-2lht2 from openshift-operator-lifecycle-manager started at 2021-09-27 17:53:13 +0000 UTC (1 container statuses recorded)
Sep 27 21:05:56.545: INFO: 	Container packageserver ready: true, restart count 0
Sep 27 21:05:56.545: INFO: service-ca-5655fcb96b-ntfqm from openshift-service-ca started at 2021-09-27 17:51:48 +0000 UTC (1 container statuses recorded)
Sep 27 21:05:56.545: INFO: 	Container service-ca-controller ready: true, restart count 0
Sep 27 21:05:56.545: INFO: sonobuoy from sonobuoy started at 2021-09-27 19:21:37 +0000 UTC (1 container statuses recorded)
Sep 27 21:05:56.545: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Sep 27 21:05:56.545: INFO: sonobuoy-e2e-job-f7e56101f16c459b from sonobuoy started at 2021-09-27 19:21:44 +0000 UTC (2 container statuses recorded)
Sep 27 21:05:56.545: INFO: 	Container e2e ready: true, restart count 0
Sep 27 21:05:56.545: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep 27 21:05:56.545: INFO: sonobuoy-systemd-logs-daemon-set-a625d2c9d0384434-sl5dj from sonobuoy started at 2021-09-27 19:21:44 +0000 UTC (2 container statuses recorded)
Sep 27 21:05:56.545: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep 27 21:05:56.545: INFO: 	Container systemd-logs ready: true, restart count 0
Sep 27 21:05:56.545: INFO: 
Logging pods the apiserver thinks is on node 10.177.248.117 before test
Sep 27 21:05:56.602: INFO: calico-node-d4rns from calico-system started at 2021-09-27 17:50:17 +0000 UTC (1 container statuses recorded)
Sep 27 21:05:56.602: INFO: 	Container calico-node ready: true, restart count 0
Sep 27 21:05:56.603: INFO: calico-typha-7d789bfc7c-79bn2 from calico-system started at 2021-09-27 17:52:15 +0000 UTC (1 container statuses recorded)
Sep 27 21:05:56.603: INFO: 	Container calico-typha ready: true, restart count 0
Sep 27 21:05:56.603: INFO: ibm-keepalived-watcher-9fxhn from kube-system started at 2021-09-27 17:48:31 +0000 UTC (1 container statuses recorded)
Sep 27 21:05:56.603: INFO: 	Container keepalived-watcher ready: true, restart count 0
Sep 27 21:05:56.603: INFO: ibm-master-proxy-static-10.177.248.117 from kube-system started at 2021-09-27 17:47:53 +0000 UTC (2 container statuses recorded)
Sep 27 21:05:56.603: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Sep 27 21:05:56.603: INFO: 	Container pause ready: true, restart count 0
Sep 27 21:05:56.603: INFO: ibmcloud-block-storage-driver-4ldjm from kube-system started at 2021-09-27 17:48:31 +0000 UTC (1 container statuses recorded)
Sep 27 21:05:56.603: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Sep 27 21:05:56.603: INFO: busybox-scheduling-90644803-d0bc-4b06-ada5-187b20f1319a from kubelet-test-7560 started at 2021-09-27 21:05:12 +0000 UTC (1 container statuses recorded)
Sep 27 21:05:56.603: INFO: 	Container busybox-scheduling-90644803-d0bc-4b06-ada5-187b20f1319a ready: false, restart count 0
Sep 27 21:05:56.603: INFO: tuned-k9qlw from openshift-cluster-node-tuning-operator started at 2021-09-27 17:51:40 +0000 UTC (1 container statuses recorded)
Sep 27 21:05:56.603: INFO: 	Container tuned ready: true, restart count 0
Sep 27 21:05:56.603: INFO: dns-default-cb2zk from openshift-dns started at 2021-09-27 17:52:55 +0000 UTC (3 container statuses recorded)
Sep 27 21:05:56.603: INFO: 	Container dns ready: true, restart count 0
Sep 27 21:05:56.603: INFO: 	Container dns-node-resolver ready: true, restart count 0
Sep 27 21:05:56.603: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep 27 21:05:56.603: INFO: node-ca-82tbs from openshift-image-registry started at 2021-09-27 17:53:17 +0000 UTC (1 container statuses recorded)
Sep 27 21:05:56.603: INFO: 	Container node-ca ready: true, restart count 0
Sep 27 21:05:56.603: INFO: registry-pvc-permissions-vncj6 from openshift-image-registry started at 2021-09-27 17:55:54 +0000 UTC (1 container statuses recorded)
Sep 27 21:05:56.603: INFO: 	Container pvc-permissions ready: false, restart count 0
Sep 27 21:05:56.603: INFO: openshift-kube-proxy-2dc5b from openshift-kube-proxy started at 2021-09-27 17:49:34 +0000 UTC (1 container statuses recorded)
Sep 27 21:05:56.603: INFO: 	Container kube-proxy ready: true, restart count 0
Sep 27 21:05:56.604: INFO: node-exporter-r6mdq from openshift-monitoring started at 2021-09-27 17:51:50 +0000 UTC (2 container statuses recorded)
Sep 27 21:05:56.604: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep 27 21:05:56.604: INFO: 	Container node-exporter ready: true, restart count 0
Sep 27 21:05:56.604: INFO: multus-admission-controller-2rld9 from openshift-multus started at 2021-09-27 20:41:11 +0000 UTC (2 container statuses recorded)
Sep 27 21:05:56.604: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep 27 21:05:56.604: INFO: 	Container multus-admission-controller ready: true, restart count 0
Sep 27 21:05:56.604: INFO: multus-vjt2q from openshift-multus started at 2021-09-27 17:49:24 +0000 UTC (1 container statuses recorded)
Sep 27 21:05:56.604: INFO: 	Container kube-multus ready: true, restart count 0
Sep 27 21:05:56.604: INFO: network-metrics-daemon-4t2kd from openshift-multus started at 2021-09-27 17:49:26 +0000 UTC (2 container statuses recorded)
Sep 27 21:05:56.604: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep 27 21:05:56.604: INFO: 	Container network-metrics-daemon ready: true, restart count 0
Sep 27 21:05:56.604: INFO: sonobuoy-systemd-logs-daemon-set-a625d2c9d0384434-z8xz7 from sonobuoy started at 2021-09-27 19:21:44 +0000 UTC (2 container statuses recorded)
Sep 27 21:05:56.604: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep 27 21:05:56.604: INFO: 	Container systemd-logs ready: true, restart count 0
Sep 27 21:05:56.604: INFO: 
Logging pods the apiserver thinks is on node 10.177.248.126 before test
Sep 27 21:05:56.653: INFO: calico-kube-controllers-5465c95dd-7hlj7 from calico-system started at 2021-09-27 17:50:29 +0000 UTC (1 container statuses recorded)
Sep 27 21:05:56.653: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Sep 27 21:05:56.653: INFO: calico-node-dhz9l from calico-system started at 2021-09-27 17:50:17 +0000 UTC (1 container statuses recorded)
Sep 27 21:05:56.653: INFO: 	Container calico-node ready: true, restart count 0
Sep 27 21:05:56.653: INFO: calico-typha-7d789bfc7c-7669x from calico-system started at 2021-09-27 17:50:17 +0000 UTC (1 container statuses recorded)
Sep 27 21:05:56.654: INFO: 	Container calico-typha ready: true, restart count 0
Sep 27 21:05:56.654: INFO: ibm-cloud-provider-ip-169-61-239-242-59b4964694-mc2b8 from ibm-system started at 2021-09-27 19:52:27 +0000 UTC (1 container statuses recorded)
Sep 27 21:05:56.654: INFO: 	Container ibm-cloud-provider-ip-169-61-239-242 ready: true, restart count 0
Sep 27 21:05:56.654: INFO: ibm-file-plugin-fcdf5f569-x8hfq from kube-system started at 2021-09-27 17:50:29 +0000 UTC (1 container statuses recorded)
Sep 27 21:05:56.654: INFO: 	Container ibm-file-plugin-container ready: true, restart count 0
Sep 27 21:05:56.654: INFO: ibm-keepalived-watcher-rgwfm from kube-system started at 2021-09-27 17:49:04 +0000 UTC (1 container statuses recorded)
Sep 27 21:05:56.654: INFO: 	Container keepalived-watcher ready: true, restart count 0
Sep 27 21:05:56.654: INFO: ibm-master-proxy-static-10.177.248.126 from kube-system started at 2021-09-27 17:48:20 +0000 UTC (2 container statuses recorded)
Sep 27 21:05:56.654: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Sep 27 21:05:56.654: INFO: 	Container pause ready: true, restart count 0
Sep 27 21:05:56.654: INFO: ibm-storage-watcher-7cd75f8d4f-pwtm7 from kube-system started at 2021-09-27 20:40:27 +0000 UTC (1 container statuses recorded)
Sep 27 21:05:56.654: INFO: 	Container ibm-storage-watcher-container ready: true, restart count 0
Sep 27 21:05:56.654: INFO: ibmcloud-block-storage-driver-kbd9g from kube-system started at 2021-09-27 17:49:04 +0000 UTC (1 container statuses recorded)
Sep 27 21:05:56.654: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Sep 27 21:05:56.654: INFO: ibmcloud-block-storage-plugin-74d6877898-zqtnb from kube-system started at 2021-09-27 17:50:29 +0000 UTC (1 container statuses recorded)
Sep 27 21:05:56.655: INFO: 	Container ibmcloud-block-storage-plugin-container ready: true, restart count 0
Sep 27 21:05:56.655: INFO: cluster-node-tuning-operator-67b4b4fbf5-zslfs from openshift-cluster-node-tuning-operator started at 2021-09-27 17:50:29 +0000 UTC (1 container statuses recorded)
Sep 27 21:05:56.655: INFO: 	Container cluster-node-tuning-operator ready: true, restart count 0
Sep 27 21:05:56.655: INFO: tuned-qc5dt from openshift-cluster-node-tuning-operator started at 2021-09-27 17:51:41 +0000 UTC (1 container statuses recorded)
Sep 27 21:05:56.655: INFO: 	Container tuned ready: true, restart count 0
Sep 27 21:05:56.655: INFO: cluster-samples-operator-97cc95ff8-nmc7q from openshift-cluster-samples-operator started at 2021-09-27 17:50:29 +0000 UTC (2 container statuses recorded)
Sep 27 21:05:56.655: INFO: 	Container cluster-samples-operator ready: true, restart count 0
Sep 27 21:05:56.655: INFO: 	Container cluster-samples-operator-watch ready: true, restart count 0
Sep 27 21:05:56.655: INFO: cluster-storage-operator-59698ddbdf-w86gw from openshift-cluster-storage-operator started at 2021-09-27 17:50:29 +0000 UTC (1 container statuses recorded)
Sep 27 21:05:56.655: INFO: 	Container cluster-storage-operator ready: true, restart count 0
Sep 27 21:05:56.655: INFO: console-operator-d6cf9dd7d-4zmd6 from openshift-console-operator started at 2021-09-27 20:40:27 +0000 UTC (1 container statuses recorded)
Sep 27 21:05:56.655: INFO: 	Container console-operator ready: true, restart count 0
Sep 27 21:05:56.655: INFO: console-5b57dff4ff-7hk2q from openshift-console started at 2021-09-27 20:40:27 +0000 UTC (1 container statuses recorded)
Sep 27 21:05:56.655: INFO: 	Container console ready: true, restart count 0
Sep 27 21:05:56.656: INFO: downloads-c785794b6-q92tc from openshift-console started at 2021-09-27 17:50:29 +0000 UTC (1 container statuses recorded)
Sep 27 21:05:56.656: INFO: 	Container download-server ready: true, restart count 0
Sep 27 21:05:56.656: INFO: downloads-c785794b6-w28kb from openshift-console started at 2021-09-27 17:50:29 +0000 UTC (1 container statuses recorded)
Sep 27 21:05:56.656: INFO: 	Container download-server ready: true, restart count 0
Sep 27 21:05:56.656: INFO: dns-operator-7489bbc67f-9ztr4 from openshift-dns-operator started at 2021-09-27 17:50:29 +0000 UTC (2 container statuses recorded)
Sep 27 21:05:56.656: INFO: 	Container dns-operator ready: true, restart count 0
Sep 27 21:05:56.656: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep 27 21:05:56.656: INFO: dns-default-msjgz from openshift-dns started at 2021-09-27 17:52:55 +0000 UTC (3 container statuses recorded)
Sep 27 21:05:56.656: INFO: 	Container dns ready: true, restart count 0
Sep 27 21:05:56.656: INFO: 	Container dns-node-resolver ready: true, restart count 0
Sep 27 21:05:56.656: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep 27 21:05:56.656: INFO: cluster-image-registry-operator-675674456f-k2fc9 from openshift-image-registry started at 2021-09-27 17:50:29 +0000 UTC (1 container statuses recorded)
Sep 27 21:05:56.656: INFO: 	Container cluster-image-registry-operator ready: true, restart count 0
Sep 27 21:05:56.656: INFO: image-registry-ffbbf6ddb-xl9b9 from openshift-image-registry started at 2021-09-27 17:55:54 +0000 UTC (1 container statuses recorded)
Sep 27 21:05:56.656: INFO: 	Container registry ready: true, restart count 0
Sep 27 21:05:56.656: INFO: node-ca-2rrgl from openshift-image-registry started at 2021-09-27 17:53:17 +0000 UTC (1 container statuses recorded)
Sep 27 21:05:56.657: INFO: 	Container node-ca ready: true, restart count 0
Sep 27 21:05:56.657: INFO: ingress-operator-8469759c95-4mm9m from openshift-ingress-operator started at 2021-09-27 17:50:29 +0000 UTC (2 container statuses recorded)
Sep 27 21:05:56.657: INFO: 	Container ingress-operator ready: true, restart count 0
Sep 27 21:05:56.657: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep 27 21:05:56.657: INFO: router-default-768f4875db-8qx4w from openshift-ingress started at 2021-09-27 17:53:15 +0000 UTC (1 container statuses recorded)
Sep 27 21:05:56.657: INFO: 	Container router ready: true, restart count 0
Sep 27 21:05:56.657: INFO: openshift-kube-proxy-7gx28 from openshift-kube-proxy started at 2021-09-27 17:49:34 +0000 UTC (1 container statuses recorded)
Sep 27 21:05:56.657: INFO: 	Container kube-proxy ready: true, restart count 0
Sep 27 21:05:56.657: INFO: kube-storage-version-migrator-operator-55c7c8f996-xr7ds from openshift-kube-storage-version-migrator-operator started at 2021-09-27 17:50:29 +0000 UTC (1 container statuses recorded)
Sep 27 21:05:56.657: INFO: 	Container kube-storage-version-migrator-operator ready: true, restart count 1
Sep 27 21:05:56.657: INFO: marketplace-operator-7cd49bbf56-nggpg from openshift-marketplace started at 2021-09-27 17:50:29 +0000 UTC (1 container statuses recorded)
Sep 27 21:05:56.657: INFO: 	Container marketplace-operator ready: true, restart count 0
Sep 27 21:05:56.657: INFO: alertmanager-main-1 from openshift-monitoring started at 2021-09-27 17:53:55 +0000 UTC (5 container statuses recorded)
Sep 27 21:05:56.657: INFO: 	Container alertmanager ready: true, restart count 0
Sep 27 21:05:56.657: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Sep 27 21:05:56.658: INFO: 	Container config-reloader ready: true, restart count 0
Sep 27 21:05:56.658: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep 27 21:05:56.658: INFO: 	Container prom-label-proxy ready: true, restart count 0
Sep 27 21:05:56.658: INFO: alertmanager-main-2 from openshift-monitoring started at 2021-09-27 20:40:42 +0000 UTC (5 container statuses recorded)
Sep 27 21:05:56.658: INFO: 	Container alertmanager ready: true, restart count 0
Sep 27 21:05:56.658: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Sep 27 21:05:56.658: INFO: 	Container config-reloader ready: true, restart count 0
Sep 27 21:05:56.658: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep 27 21:05:56.658: INFO: 	Container prom-label-proxy ready: true, restart count 0
Sep 27 21:05:56.658: INFO: cluster-monitoring-operator-7656b489c8-z2wmx from openshift-monitoring started at 2021-09-27 17:50:29 +0000 UTC (2 container statuses recorded)
Sep 27 21:05:56.658: INFO: 	Container cluster-monitoring-operator ready: true, restart count 0
Sep 27 21:05:56.658: INFO: 	Container kube-rbac-proxy ready: true, restart count 3
Sep 27 21:05:56.658: INFO: node-exporter-42nr4 from openshift-monitoring started at 2021-09-27 17:51:50 +0000 UTC (2 container statuses recorded)
Sep 27 21:05:56.658: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep 27 21:05:56.658: INFO: 	Container node-exporter ready: true, restart count 0
Sep 27 21:05:56.658: INFO: prometheus-adapter-66469d976-t6rf2 from openshift-monitoring started at 2021-09-27 17:54:46 +0000 UTC (1 container statuses recorded)
Sep 27 21:05:56.659: INFO: 	Container prometheus-adapter ready: true, restart count 0
Sep 27 21:05:56.659: INFO: prometheus-k8s-1 from openshift-monitoring started at 2021-09-27 20:40:42 +0000 UTC (6 container statuses recorded)
Sep 27 21:05:56.659: INFO: 	Container config-reloader ready: true, restart count 0
Sep 27 21:05:56.659: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep 27 21:05:56.659: INFO: 	Container prom-label-proxy ready: true, restart count 0
Sep 27 21:05:56.659: INFO: 	Container prometheus ready: true, restart count 1
Sep 27 21:05:56.659: INFO: 	Container prometheus-proxy ready: true, restart count 0
Sep 27 21:05:56.659: INFO: 	Container thanos-sidecar ready: true, restart count 0
Sep 27 21:05:56.659: INFO: thanos-querier-6b5789bbbd-l8dmh from openshift-monitoring started at 2021-09-27 17:53:51 +0000 UTC (5 container statuses recorded)
Sep 27 21:05:56.659: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep 27 21:05:56.659: INFO: 	Container kube-rbac-proxy-rules ready: true, restart count 0
Sep 27 21:05:56.660: INFO: 	Container oauth-proxy ready: true, restart count 0
Sep 27 21:05:56.660: INFO: 	Container prom-label-proxy ready: true, restart count 0
Sep 27 21:05:56.660: INFO: 	Container thanos-query ready: true, restart count 0
Sep 27 21:05:56.660: INFO: multus-admission-controller-fgtsq from openshift-multus started at 2021-09-27 17:50:29 +0000 UTC (2 container statuses recorded)
Sep 27 21:05:56.660: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep 27 21:05:56.660: INFO: 	Container multus-admission-controller ready: true, restart count 0
Sep 27 21:05:56.660: INFO: multus-fzpdl from openshift-multus started at 2021-09-27 17:49:24 +0000 UTC (1 container statuses recorded)
Sep 27 21:05:56.660: INFO: 	Container kube-multus ready: true, restart count 0
Sep 27 21:05:56.660: INFO: network-metrics-daemon-5wdz5 from openshift-multus started at 2021-09-27 17:49:26 +0000 UTC (2 container statuses recorded)
Sep 27 21:05:56.660: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep 27 21:05:56.660: INFO: 	Container network-metrics-daemon ready: true, restart count 0
Sep 27 21:05:56.661: INFO: catalog-operator-69c4599997-btc8l from openshift-operator-lifecycle-manager started at 2021-09-27 17:50:29 +0000 UTC (1 container statuses recorded)
Sep 27 21:05:56.661: INFO: 	Container catalog-operator ready: true, restart count 0
Sep 27 21:05:56.661: INFO: olm-operator-db7cd6974-2gxjx from openshift-operator-lifecycle-manager started at 2021-09-27 17:50:29 +0000 UTC (1 container statuses recorded)
Sep 27 21:05:56.661: INFO: 	Container olm-operator ready: true, restart count 0
Sep 27 21:05:56.661: INFO: packageserver-67b4ccc984-w6sdn from openshift-operator-lifecycle-manager started at 2021-09-27 17:53:13 +0000 UTC (1 container statuses recorded)
Sep 27 21:05:56.661: INFO: 	Container packageserver ready: true, restart count 0
Sep 27 21:05:56.661: INFO: metrics-666cbf9545-t95w7 from openshift-roks-metrics started at 2021-09-27 17:50:29 +0000 UTC (1 container statuses recorded)
Sep 27 21:05:56.661: INFO: 	Container metrics ready: true, restart count 3
Sep 27 21:05:56.661: INFO: push-gateway-6f6b5cb7f7-k6lpz from openshift-roks-metrics started at 2021-09-27 19:52:28 +0000 UTC (1 container statuses recorded)
Sep 27 21:05:56.661: INFO: 	Container push-gateway ready: false, restart count 0
Sep 27 21:05:56.661: INFO: service-ca-operator-6ccc7cff9-l9wfj from openshift-service-ca-operator started at 2021-09-27 17:50:29 +0000 UTC (1 container statuses recorded)
Sep 27 21:05:56.661: INFO: 	Container service-ca-operator ready: true, restart count 1
Sep 27 21:05:56.661: INFO: sonobuoy-systemd-logs-daemon-set-a625d2c9d0384434-6w8vw from sonobuoy started at 2021-09-27 19:21:44 +0000 UTC (2 container statuses recorded)
Sep 27 21:05:56.661: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep 27 21:05:56.662: INFO: 	Container systemd-logs ready: true, restart count 0
Sep 27 21:05:56.662: INFO: tigera-operator-db8ddcc79-mf5vc from tigera-operator started at 2021-09-27 19:52:27 +0000 UTC (1 container statuses recorded)
Sep 27 21:05:56.662: INFO: 	Container tigera-operator ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-21f481f9-7955-49c6-953b-01a4651c8dc1 42
STEP: Trying to relaunch the pod, now with labels.
STEP: removing the label kubernetes.io/e2e-21f481f9-7955-49c6-953b-01a4651c8dc1 off the node 10.177.248.117
STEP: verifying the node doesn't have the label kubernetes.io/e2e-21f481f9-7955-49c6-953b-01a4651c8dc1
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 21:06:02.997: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-8055" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81

• [SLOW TEST:6.828 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if matching  [Conformance]","total":305,"completed":304,"skipped":4909,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 27 21:06:03.043: INFO: >>> kubeConfig: /tmp/kubeconfig-679324930
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: starting a background goroutine to produce watch events
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 27 21:06:07.562: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-101" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should receive events on concurrent watches in same order [Conformance]","total":305,"completed":305,"skipped":4923,"failed":0}
SSSSSep 27 21:06:07.663: INFO: Running AfterSuite actions on all nodes
Sep 27 21:06:07.663: INFO: Running AfterSuite actions on node 1
Sep 27 21:06:07.663: INFO: Skipping dumping logs from cluster

JUnit report was created: /tmp/results/junit_01.xml
{"msg":"Test Suite completed","total":305,"completed":305,"skipped":4927,"failed":0}

Ran 305 of 5232 Specs in 6240.802 seconds
SUCCESS! -- 305 Passed | 0 Failed | 0 Pending | 4927 Skipped
PASS

Ginkgo ran 1 suite in 1h44m3.048199817s
Test Suite Passed
