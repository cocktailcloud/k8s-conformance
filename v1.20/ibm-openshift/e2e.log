I0823 21:37:41.232032      25 test_context.go:436] Using a temporary kubeconfig file from in-cluster config : /tmp/kubeconfig-585719795
I0823 21:37:41.232333      25 test_context.go:457] Tolerating taints "node-role.kubernetes.io/master" when considering if nodes are ready
I0823 21:37:41.232627      25 e2e.go:129] Starting e2e run "6c3cf079-64c9-4ed1-a2c3-c86e319b7e92" on Ginkgo node 1
{"msg":"Test Suite starting","total":311,"completed":0,"skipped":0,"failed":0}
Running Suite: Kubernetes e2e suite
===================================
Random Seed: 1629754659 - Will randomize all specs
Will run 311 of 5667 specs

Aug 23 21:37:41.250: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
E0823 21:37:41.253474      25 progress.go:119] Failed to post progress update to http://localhost:8099/progress: Post "http://localhost:8099/progress": dial tcp [::1]:8099: connect: connection refused
Aug 23 21:37:41.255: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
Aug 23 21:37:41.980: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Aug 23 21:37:42.355: INFO: 13 / 13 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Aug 23 21:37:42.355: INFO: expected 4 pod replicas in namespace 'kube-system', 4 are Running and Ready.
Aug 23 21:37:42.355: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Aug 23 21:37:42.479: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'ibm-keepalived-watcher' (0 seconds elapsed)
Aug 23 21:37:42.479: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'ibmcloud-block-storage-driver' (0 seconds elapsed)
Aug 23 21:37:42.479: INFO: e2e test version: v1.20.6
Aug 23 21:37:42.589: INFO: kube-apiserver version: v1.20.0+558d959
Aug 23 21:37:42.589: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
Aug 23 21:37:42.715: INFO: Cluster IP family: ipv4
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 21:37:42.715: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename container-probe
Aug 23 21:37:43.220: INFO: No PodSecurityPolicies found; assuming PodSecurityPolicy is disabled.
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:53
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating pod test-webserver-718e9fb9-641a-478f-ac67-ceb084195317 in namespace container-probe-3009
Aug 23 21:37:53.865: INFO: Started pod test-webserver-718e9fb9-641a-478f-ac67-ceb084195317 in namespace container-probe-3009
STEP: checking the pod's current state and verifying that restartCount is present
Aug 23 21:37:53.905: INFO: Initial restart count of pod test-webserver-718e9fb9-641a-478f-ac67-ceb084195317 is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 21:41:55.530: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-3009" for this suite.

• [SLOW TEST:254.133 seconds]
[k8s.io] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Probing container should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","total":311,"completed":1,"skipped":26,"failed":0}
SSS
------------------------------
[k8s.io] Pods 
  should be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 21:41:56.848: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:187
[It] should be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Aug 23 21:42:05.606: INFO: Successfully updated pod "pod-update-0eaf224b-4833-4b4a-b2f3-1cd8b9b888e6"
STEP: verifying the updated pod is in kubernetes
Aug 23 21:42:05.915: INFO: Pod update OK
[AfterEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 21:42:05.915: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-3181" for this suite.

• [SLOW TEST:9.865 seconds]
[k8s.io] Pods
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Pods should be updated [NodeConformance] [Conformance]","total":311,"completed":2,"skipped":29,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 21:42:06.714: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name configmap-test-volume-e20755f6-ce6e-4f62-b560-e1dd0ee79029
STEP: Creating a pod to test consume configMaps
Aug 23 21:42:08.119: INFO: Waiting up to 5m0s for pod "pod-configmaps-f6ea57d3-45bf-4b37-9482-a77e237d2669" in namespace "configmap-1972" to be "Succeeded or Failed"
Aug 23 21:42:08.480: INFO: Pod "pod-configmaps-f6ea57d3-45bf-4b37-9482-a77e237d2669": Phase="Pending", Reason="", readiness=false. Elapsed: 360.231793ms
Aug 23 21:42:10.689: INFO: Pod "pod-configmaps-f6ea57d3-45bf-4b37-9482-a77e237d2669": Phase="Pending", Reason="", readiness=false. Elapsed: 2.569890982s
Aug 23 21:42:12.866: INFO: Pod "pod-configmaps-f6ea57d3-45bf-4b37-9482-a77e237d2669": Phase="Pending", Reason="", readiness=false. Elapsed: 4.746596934s
Aug 23 21:42:15.060: INFO: Pod "pod-configmaps-f6ea57d3-45bf-4b37-9482-a77e237d2669": Phase="Pending", Reason="", readiness=false. Elapsed: 6.940531769s
Aug 23 21:42:17.250: INFO: Pod "pod-configmaps-f6ea57d3-45bf-4b37-9482-a77e237d2669": Phase="Pending", Reason="", readiness=false. Elapsed: 9.130253619s
Aug 23 21:42:19.365: INFO: Pod "pod-configmaps-f6ea57d3-45bf-4b37-9482-a77e237d2669": Phase="Succeeded", Reason="", readiness=false. Elapsed: 11.245315151s
STEP: Saw pod success
Aug 23 21:42:19.365: INFO: Pod "pod-configmaps-f6ea57d3-45bf-4b37-9482-a77e237d2669" satisfied condition "Succeeded or Failed"
Aug 23 21:42:19.486: INFO: Trying to get logs from node 10.149.248.9 pod pod-configmaps-f6ea57d3-45bf-4b37-9482-a77e237d2669 container agnhost-container: <nil>
STEP: delete the pod
Aug 23 21:42:20.403: INFO: Waiting for pod pod-configmaps-f6ea57d3-45bf-4b37-9482-a77e237d2669 to disappear
Aug 23 21:42:20.567: INFO: Pod pod-configmaps-f6ea57d3-45bf-4b37-9482-a77e237d2669 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 21:42:20.567: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1972" for this suite.

• [SLOW TEST:14.567 seconds]
[sig-storage] ConfigMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:36
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":3,"skipped":53,"failed":0}
[k8s.io] Security Context When creating a container with runAsUser 
  should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 21:42:21.299: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:41
[It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Aug 23 21:42:21.996: INFO: Waiting up to 5m0s for pod "busybox-user-65534-dfd3daa6-6ee8-4870-b425-3ad557bc112c" in namespace "security-context-test-3229" to be "Succeeded or Failed"
Aug 23 21:42:22.130: INFO: Pod "busybox-user-65534-dfd3daa6-6ee8-4870-b425-3ad557bc112c": Phase="Pending", Reason="", readiness=false. Elapsed: 134.061315ms
Aug 23 21:42:24.211: INFO: Pod "busybox-user-65534-dfd3daa6-6ee8-4870-b425-3ad557bc112c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.215147822s
Aug 23 21:42:26.276: INFO: Pod "busybox-user-65534-dfd3daa6-6ee8-4870-b425-3ad557bc112c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.27986373s
Aug 23 21:42:28.317: INFO: Pod "busybox-user-65534-dfd3daa6-6ee8-4870-b425-3ad557bc112c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.321410022s
Aug 23 21:42:28.317: INFO: Pod "busybox-user-65534-dfd3daa6-6ee8-4870-b425-3ad557bc112c" satisfied condition "Succeeded or Failed"
[AfterEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 21:42:28.317: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-3229" for this suite.

• [SLOW TEST:7.239 seconds]
[k8s.io] Security Context
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  When creating a container with runAsUser
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:45
    should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Security Context When creating a container with runAsUser should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":4,"skipped":53,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to start watching from a specific resource version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 21:42:28.521: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to start watching from a specific resource version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: modifying the configmap a second time
STEP: deleting the configmap
STEP: creating a watch on configmaps from the resource version returned by the first update
STEP: Expecting to observe notifications for all changes to the configmap after the first update
Aug 23 21:42:29.276: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-567 /api/v1/namespaces/watch-567/configmaps/e2e-watch-test-resource-version 8bd7e35c-c2bd-4697-8b2f-223c12f711d7 53672 0 2021-08-23 21:42:28 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  [{e2e.test Update v1 2021-08-23 21:42:28 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Aug 23 21:42:29.276: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-567 /api/v1/namespaces/watch-567/configmaps/e2e-watch-test-resource-version 8bd7e35c-c2bd-4697-8b2f-223c12f711d7 53674 0 2021-08-23 21:42:28 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  [{e2e.test Update v1 2021-08-23 21:42:28 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 21:42:29.276: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-567" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should be able to start watching from a specific resource version [Conformance]","total":311,"completed":5,"skipped":61,"failed":0}
SSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 21:42:29.534: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir 0666 on node default medium
Aug 23 21:42:30.092: INFO: Waiting up to 5m0s for pod "pod-02c16f54-43cf-46bb-a834-7e4b384e0155" in namespace "emptydir-8459" to be "Succeeded or Failed"
Aug 23 21:42:30.257: INFO: Pod "pod-02c16f54-43cf-46bb-a834-7e4b384e0155": Phase="Pending", Reason="", readiness=false. Elapsed: 164.200339ms
Aug 23 21:42:32.431: INFO: Pod "pod-02c16f54-43cf-46bb-a834-7e4b384e0155": Phase="Pending", Reason="", readiness=false. Elapsed: 2.338260132s
Aug 23 21:42:34.583: INFO: Pod "pod-02c16f54-43cf-46bb-a834-7e4b384e0155": Phase="Pending", Reason="", readiness=false. Elapsed: 4.490700861s
Aug 23 21:42:36.652: INFO: Pod "pod-02c16f54-43cf-46bb-a834-7e4b384e0155": Phase="Pending", Reason="", readiness=false. Elapsed: 6.560002408s
Aug 23 21:42:38.724: INFO: Pod "pod-02c16f54-43cf-46bb-a834-7e4b384e0155": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.631210657s
STEP: Saw pod success
Aug 23 21:42:38.724: INFO: Pod "pod-02c16f54-43cf-46bb-a834-7e4b384e0155" satisfied condition "Succeeded or Failed"
Aug 23 21:42:38.813: INFO: Trying to get logs from node 10.149.248.24 pod pod-02c16f54-43cf-46bb-a834-7e4b384e0155 container test-container: <nil>
STEP: delete the pod
Aug 23 21:42:39.269: INFO: Waiting for pod pod-02c16f54-43cf-46bb-a834-7e4b384e0155 to disappear
Aug 23 21:42:39.311: INFO: Pod pod-02c16f54-43cf-46bb-a834-7e4b384e0155 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 21:42:39.311: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8459" for this suite.

• [SLOW TEST:9.915 seconds]
[sig-storage] EmptyDir volumes
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:45
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":6,"skipped":65,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Events 
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 21:42:39.449: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a test event
STEP: listing all events in all namespaces
STEP: patching the test event
STEP: fetching the test event
STEP: deleting the test event
STEP: listing all events in all namespaces
[AfterEach] [sig-api-machinery] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 21:42:40.090: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-771" for this suite.
•{"msg":"PASSED [sig-api-machinery] Events should ensure that an event can be fetched, patched, deleted, and listed [Conformance]","total":311,"completed":7,"skipped":82,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] 
  validates basic preemption works [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 21:42:40.350: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename sched-preemption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:90
Aug 23 21:42:41.193: INFO: Waiting up to 1m0s for all nodes to be ready
Aug 23 21:43:43.192: INFO: Waiting for terminating namespaces to be deleted...
[It] validates basic preemption works [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Create pods that use 2/3 of node resources.
Aug 23 21:43:43.382: INFO: Created pod: pod0-sched-preemption-low-priority
Aug 23 21:43:43.498: INFO: Created pod: pod1-sched-preemption-medium-priority
Aug 23 21:43:43.604: INFO: Created pod: pod2-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled.
STEP: Run a high priority pod that has same requirements as that of lower priority pod
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 21:44:19.550: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-7208" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:78

• [SLOW TEST:100.964 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates basic preemption works [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] validates basic preemption works [Conformance]","total":311,"completed":8,"skipped":104,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 21:44:21.318: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Aug 23 21:44:23.489: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63765351862, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63765351862, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63765351863, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63765351862, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 23 21:44:25.562: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63765351862, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63765351862, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63765351863, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63765351862, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 23 21:44:27.606: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63765351862, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63765351862, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63765351863, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63765351862, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Aug 23 21:44:30.732: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Aug 23 21:44:30.898: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-621-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 21:44:32.294: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3407" for this suite.
STEP: Destroying namespace "webhook-3407-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

• [SLOW TEST:12.479 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource [Conformance]","total":311,"completed":9,"skipped":153,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 21:44:33.797: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Performing setup for networking test in namespace pod-network-test-1786
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Aug 23 21:44:34.428: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Aug 23 21:44:35.587: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Aug 23 21:44:37.665: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Aug 23 21:44:39.663: INFO: The status of Pod netserver-0 is Running (Ready = false)
Aug 23 21:44:41.629: INFO: The status of Pod netserver-0 is Running (Ready = false)
Aug 23 21:44:43.603: INFO: The status of Pod netserver-0 is Running (Ready = false)
Aug 23 21:44:45.629: INFO: The status of Pod netserver-0 is Running (Ready = false)
Aug 23 21:44:47.608: INFO: The status of Pod netserver-0 is Running (Ready = false)
Aug 23 21:44:49.641: INFO: The status of Pod netserver-0 is Running (Ready = false)
Aug 23 21:44:51.677: INFO: The status of Pod netserver-0 is Running (Ready = false)
Aug 23 21:44:53.668: INFO: The status of Pod netserver-0 is Running (Ready = false)
Aug 23 21:44:55.604: INFO: The status of Pod netserver-0 is Running (Ready = false)
Aug 23 21:44:57.610: INFO: The status of Pod netserver-0 is Running (Ready = true)
Aug 23 21:44:57.640: INFO: The status of Pod netserver-1 is Running (Ready = true)
Aug 23 21:44:57.678: INFO: The status of Pod netserver-2 is Running (Ready = true)
STEP: Creating test pods
Aug 23 21:45:06.508: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
Aug 23 21:45:06.508: INFO: Going to poll 172.30.87.99 on port 8080 at least 0 times, with a maximum of 39 tries before failing
Aug 23 21:45:06.642: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.30.87.99:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-1786 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 23 21:45:06.642: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
Aug 23 21:45:07.829: INFO: Found all 1 expected endpoints: [netserver-0]
Aug 23 21:45:07.829: INFO: Going to poll 172.30.240.106 on port 8080 at least 0 times, with a maximum of 39 tries before failing
Aug 23 21:45:07.966: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.30.240.106:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-1786 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 23 21:45:07.966: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
Aug 23 21:45:09.736: INFO: Found all 1 expected endpoints: [netserver-1]
Aug 23 21:45:09.736: INFO: Going to poll 172.30.210.167 on port 8080 at least 0 times, with a maximum of 39 tries before failing
Aug 23 21:45:09.827: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.30.210.167:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-1786 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 23 21:45:09.827: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
Aug 23 21:45:10.002: INFO: Found all 1 expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 21:45:10.003: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-1786" for this suite.

• [SLOW TEST:36.989 seconds]
[sig-network] Networking
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:27
  Granular Checks: Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:30
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":10,"skipped":170,"failed":0}
S
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD without validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 21:45:10.787: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD without validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Aug 23 21:45:11.479: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Aug 23 21:45:26.655: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585719795 --namespace=crd-publish-openapi-2862 --namespace=crd-publish-openapi-2862 create -f -'
Aug 23 21:45:30.283: INFO: stderr: ""
Aug 23 21:45:30.283: INFO: stdout: "e2e-test-crd-publish-openapi-7461-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Aug 23 21:45:30.283: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585719795 --namespace=crd-publish-openapi-2862 --namespace=crd-publish-openapi-2862 delete e2e-test-crd-publish-openapi-7461-crds test-cr'
Aug 23 21:45:31.806: INFO: stderr: ""
Aug 23 21:45:31.806: INFO: stdout: "e2e-test-crd-publish-openapi-7461-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
Aug 23 21:45:31.806: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585719795 --namespace=crd-publish-openapi-2862 --namespace=crd-publish-openapi-2862 apply -f -'
Aug 23 21:45:32.477: INFO: stderr: ""
Aug 23 21:45:32.477: INFO: stdout: "e2e-test-crd-publish-openapi-7461-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Aug 23 21:45:32.477: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585719795 --namespace=crd-publish-openapi-2862 --namespace=crd-publish-openapi-2862 delete e2e-test-crd-publish-openapi-7461-crds test-cr'
Aug 23 21:45:33.567: INFO: stderr: ""
Aug 23 21:45:33.567: INFO: stdout: "e2e-test-crd-publish-openapi-7461-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR without validation schema
Aug 23 21:45:33.567: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585719795 --namespace=crd-publish-openapi-2862 explain e2e-test-crd-publish-openapi-7461-crds'
Aug 23 21:45:34.359: INFO: stderr: ""
Aug 23 21:45:34.359: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-7461-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 21:45:47.468: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-2862" for this suite.

• [SLOW TEST:37.200 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD without validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD without validation schema [Conformance]","total":311,"completed":11,"skipped":171,"failed":0}
SSSSSS
------------------------------
[sig-network] Services 
  should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 21:45:47.987: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating service in namespace services-1194
Aug 23 21:45:50.883: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585719795 --namespace=services-1194 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
Aug 23 21:45:52.092: INFO: rc: 7
Aug 23 21:45:52.175: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Aug 23 21:45:52.233: INFO: Pod kube-proxy-mode-detector no longer exists
Aug 23 21:45:52.233: INFO: Couldn't detect KubeProxy mode - test failure may be expected: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585719795 --namespace=services-1194 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode:
Command stdout:

stderr:
+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode
command terminated with exit code 7

error:
exit status 7
STEP: creating service affinity-clusterip-timeout in namespace services-1194
STEP: creating replication controller affinity-clusterip-timeout in namespace services-1194
I0823 21:45:52.416535      25 runners.go:190] Created replication controller with name: affinity-clusterip-timeout, namespace: services-1194, replica count: 3
I0823 21:45:55.517152      25 runners.go:190] affinity-clusterip-timeout Pods: 3 out of 3 created, 1 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0823 21:45:58.517352      25 runners.go:190] affinity-clusterip-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Aug 23 21:45:58.550: INFO: Creating new exec pod
Aug 23 21:46:03.658: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585719795 --namespace=services-1194 exec execpod-affinitysvvhf -- /bin/sh -x -c nc -zv -t -w 2 affinity-clusterip-timeout 80'
Aug 23 21:46:04.843: INFO: stderr: "+ nc -zv -t -w 2 affinity-clusterip-timeout 80\nConnection to affinity-clusterip-timeout 80 port [tcp/http] succeeded!\n"
Aug 23 21:46:04.843: INFO: stdout: ""
Aug 23 21:46:04.844: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585719795 --namespace=services-1194 exec execpod-affinitysvvhf -- /bin/sh -x -c nc -zv -t -w 2 172.21.5.169 80'
Aug 23 21:46:05.596: INFO: stderr: "+ nc -zv -t -w 2 172.21.5.169 80\nConnection to 172.21.5.169 80 port [tcp/http] succeeded!\n"
Aug 23 21:46:05.596: INFO: stdout: ""
Aug 23 21:46:05.596: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585719795 --namespace=services-1194 exec execpod-affinitysvvhf -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.21.5.169:80/ ; done'
Aug 23 21:46:06.735: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.5.169:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.5.169:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.5.169:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.5.169:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.5.169:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.5.169:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.5.169:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.5.169:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.5.169:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.5.169:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.5.169:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.5.169:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.5.169:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.5.169:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.5.169:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.5.169:80/\n"
Aug 23 21:46:06.735: INFO: stdout: "\naffinity-clusterip-timeout-rbp9d\naffinity-clusterip-timeout-rbp9d\naffinity-clusterip-timeout-rbp9d\naffinity-clusterip-timeout-rbp9d\naffinity-clusterip-timeout-rbp9d\naffinity-clusterip-timeout-rbp9d\naffinity-clusterip-timeout-rbp9d\naffinity-clusterip-timeout-rbp9d\naffinity-clusterip-timeout-rbp9d\naffinity-clusterip-timeout-rbp9d\naffinity-clusterip-timeout-rbp9d\naffinity-clusterip-timeout-rbp9d\naffinity-clusterip-timeout-rbp9d\naffinity-clusterip-timeout-rbp9d\naffinity-clusterip-timeout-rbp9d\naffinity-clusterip-timeout-rbp9d"
Aug 23 21:46:06.735: INFO: Received response from host: affinity-clusterip-timeout-rbp9d
Aug 23 21:46:06.735: INFO: Received response from host: affinity-clusterip-timeout-rbp9d
Aug 23 21:46:06.735: INFO: Received response from host: affinity-clusterip-timeout-rbp9d
Aug 23 21:46:06.735: INFO: Received response from host: affinity-clusterip-timeout-rbp9d
Aug 23 21:46:06.735: INFO: Received response from host: affinity-clusterip-timeout-rbp9d
Aug 23 21:46:06.735: INFO: Received response from host: affinity-clusterip-timeout-rbp9d
Aug 23 21:46:06.735: INFO: Received response from host: affinity-clusterip-timeout-rbp9d
Aug 23 21:46:06.735: INFO: Received response from host: affinity-clusterip-timeout-rbp9d
Aug 23 21:46:06.735: INFO: Received response from host: affinity-clusterip-timeout-rbp9d
Aug 23 21:46:06.735: INFO: Received response from host: affinity-clusterip-timeout-rbp9d
Aug 23 21:46:06.735: INFO: Received response from host: affinity-clusterip-timeout-rbp9d
Aug 23 21:46:06.735: INFO: Received response from host: affinity-clusterip-timeout-rbp9d
Aug 23 21:46:06.735: INFO: Received response from host: affinity-clusterip-timeout-rbp9d
Aug 23 21:46:06.735: INFO: Received response from host: affinity-clusterip-timeout-rbp9d
Aug 23 21:46:06.735: INFO: Received response from host: affinity-clusterip-timeout-rbp9d
Aug 23 21:46:06.735: INFO: Received response from host: affinity-clusterip-timeout-rbp9d
Aug 23 21:46:06.736: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585719795 --namespace=services-1194 exec execpod-affinitysvvhf -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://172.21.5.169:80/'
Aug 23 21:46:07.735: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://172.21.5.169:80/\n"
Aug 23 21:46:07.735: INFO: stdout: "affinity-clusterip-timeout-rbp9d"
Aug 23 21:46:27.735: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585719795 --namespace=services-1194 exec execpod-affinitysvvhf -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://172.21.5.169:80/'
Aug 23 21:46:31.270: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://172.21.5.169:80/\n"
Aug 23 21:46:31.270: INFO: stdout: "affinity-clusterip-timeout-hzd5z"
Aug 23 21:46:31.270: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-timeout in namespace services-1194, will wait for the garbage collector to delete the pods
Aug 23 21:46:32.263: INFO: Deleting ReplicationController affinity-clusterip-timeout took: 355.040198ms
Aug 23 21:46:32.464: INFO: Terminating ReplicationController affinity-clusterip-timeout pods took: 200.200139ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 21:46:44.247: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1194" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749

• [SLOW TEST:57.497 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]","total":311,"completed":12,"skipped":177,"failed":0}
SSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 21:46:45.485: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:41
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Aug 23 21:46:47.079: INFO: Waiting up to 5m0s for pod "downwardapi-volume-dadf3dcd-c862-4803-a0c9-1e793e9c2f6d" in namespace "downward-api-7532" to be "Succeeded or Failed"
Aug 23 21:46:47.309: INFO: Pod "downwardapi-volume-dadf3dcd-c862-4803-a0c9-1e793e9c2f6d": Phase="Pending", Reason="", readiness=false. Elapsed: 229.7502ms
Aug 23 21:46:49.624: INFO: Pod "downwardapi-volume-dadf3dcd-c862-4803-a0c9-1e793e9c2f6d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.545117022s
Aug 23 21:46:51.905: INFO: Pod "downwardapi-volume-dadf3dcd-c862-4803-a0c9-1e793e9c2f6d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.826671323s
Aug 23 21:46:54.089: INFO: Pod "downwardapi-volume-dadf3dcd-c862-4803-a0c9-1e793e9c2f6d": Phase="Pending", Reason="", readiness=false. Elapsed: 7.009808811s
Aug 23 21:46:56.195: INFO: Pod "downwardapi-volume-dadf3dcd-c862-4803-a0c9-1e793e9c2f6d": Phase="Pending", Reason="", readiness=false. Elapsed: 9.116481667s
Aug 23 21:46:58.288: INFO: Pod "downwardapi-volume-dadf3dcd-c862-4803-a0c9-1e793e9c2f6d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 11.20916374s
STEP: Saw pod success
Aug 23 21:46:58.288: INFO: Pod "downwardapi-volume-dadf3dcd-c862-4803-a0c9-1e793e9c2f6d" satisfied condition "Succeeded or Failed"
Aug 23 21:46:58.362: INFO: Trying to get logs from node 10.149.248.25 pod downwardapi-volume-dadf3dcd-c862-4803-a0c9-1e793e9c2f6d container client-container: <nil>
STEP: delete the pod
Aug 23 21:46:58.914: INFO: Waiting for pod downwardapi-volume-dadf3dcd-c862-4803-a0c9-1e793e9c2f6d to disappear
Aug 23 21:46:59.002: INFO: Pod downwardapi-volume-dadf3dcd-c862-4803-a0c9-1e793e9c2f6d no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 21:46:59.002: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7532" for this suite.

• [SLOW TEST:13.958 seconds]
[sig-storage] Downward API volume
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:36
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":13,"skipped":184,"failed":0}
SSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 21:46:59.443: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir volume type on tmpfs
Aug 23 21:46:59.968: INFO: Waiting up to 5m0s for pod "pod-cb39043f-4a51-4951-931c-f18d81c44b30" in namespace "emptydir-493" to be "Succeeded or Failed"
Aug 23 21:47:00.041: INFO: Pod "pod-cb39043f-4a51-4951-931c-f18d81c44b30": Phase="Pending", Reason="", readiness=false. Elapsed: 73.680965ms
Aug 23 21:47:02.179: INFO: Pod "pod-cb39043f-4a51-4951-931c-f18d81c44b30": Phase="Pending", Reason="", readiness=false. Elapsed: 2.211338376s
Aug 23 21:47:04.312: INFO: Pod "pod-cb39043f-4a51-4951-931c-f18d81c44b30": Phase="Pending", Reason="", readiness=false. Elapsed: 4.343975422s
Aug 23 21:47:06.325: INFO: Pod "pod-cb39043f-4a51-4951-931c-f18d81c44b30": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.357508401s
STEP: Saw pod success
Aug 23 21:47:06.325: INFO: Pod "pod-cb39043f-4a51-4951-931c-f18d81c44b30" satisfied condition "Succeeded or Failed"
Aug 23 21:47:06.356: INFO: Trying to get logs from node 10.149.248.24 pod pod-cb39043f-4a51-4951-931c-f18d81c44b30 container test-container: <nil>
STEP: delete the pod
Aug 23 21:47:06.445: INFO: Waiting for pod pod-cb39043f-4a51-4951-931c-f18d81c44b30 to disappear
Aug 23 21:47:06.463: INFO: Pod pod-cb39043f-4a51-4951-931c-f18d81c44b30 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 21:47:06.463: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-493" for this suite.

• [SLOW TEST:7.136 seconds]
[sig-storage] EmptyDir volumes
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:45
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":14,"skipped":189,"failed":0}
SSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 21:47:06.579: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating secret with name secret-test-aed01bf2-778f-4e78-8f49-515c303c6e57
STEP: Creating a pod to test consume secrets
Aug 23 21:47:06.896: INFO: Waiting up to 5m0s for pod "pod-secrets-790e3435-425b-4185-81ee-36081e711cba" in namespace "secrets-7189" to be "Succeeded or Failed"
Aug 23 21:47:06.921: INFO: Pod "pod-secrets-790e3435-425b-4185-81ee-36081e711cba": Phase="Pending", Reason="", readiness=false. Elapsed: 24.682685ms
Aug 23 21:47:08.980: INFO: Pod "pod-secrets-790e3435-425b-4185-81ee-36081e711cba": Phase="Pending", Reason="", readiness=false. Elapsed: 2.08332268s
Aug 23 21:47:11.001: INFO: Pod "pod-secrets-790e3435-425b-4185-81ee-36081e711cba": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.104909947s
STEP: Saw pod success
Aug 23 21:47:11.001: INFO: Pod "pod-secrets-790e3435-425b-4185-81ee-36081e711cba" satisfied condition "Succeeded or Failed"
Aug 23 21:47:11.022: INFO: Trying to get logs from node 10.149.248.24 pod pod-secrets-790e3435-425b-4185-81ee-36081e711cba container secret-volume-test: <nil>
STEP: delete the pod
Aug 23 21:47:11.118: INFO: Waiting for pod pod-secrets-790e3435-425b-4185-81ee-36081e711cba to disappear
Aug 23 21:47:11.149: INFO: Pod pod-secrets-790e3435-425b-4185-81ee-36081e711cba no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 21:47:11.149: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-7189" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume [NodeConformance] [Conformance]","total":311,"completed":15,"skipped":195,"failed":0}

------------------------------
[sig-network] DNS 
  should provide DNS for services  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 21:47:11.225: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for services  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-7748.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-7748.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-7748.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-7748.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-7748.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-7748.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-7748.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-7748.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-7748.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-7748.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-7748.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-7748.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-7748.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 34.182.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.182.34_udp@PTR;check="$$(dig +tcp +noall +answer +search 34.182.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.182.34_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-7748.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-7748.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-7748.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-7748.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-7748.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-7748.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-7748.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-7748.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-7748.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-7748.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-7748.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-7748.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-7748.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 34.182.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.182.34_udp@PTR;check="$$(dig +tcp +noall +answer +search 34.182.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.182.34_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Aug 23 21:47:26.018: INFO: Unable to read wheezy_udp@dns-test-service.dns-7748.svc.cluster.local from pod dns-7748/dns-test-61569fa4-4d95-49a0-9316-feea59a4016e: the server could not find the requested resource (get pods dns-test-61569fa4-4d95-49a0-9316-feea59a4016e)
Aug 23 21:47:26.155: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7748.svc.cluster.local from pod dns-7748/dns-test-61569fa4-4d95-49a0-9316-feea59a4016e: the server could not find the requested resource (get pods dns-test-61569fa4-4d95-49a0-9316-feea59a4016e)
Aug 23 21:47:26.282: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-7748.svc.cluster.local from pod dns-7748/dns-test-61569fa4-4d95-49a0-9316-feea59a4016e: the server could not find the requested resource (get pods dns-test-61569fa4-4d95-49a0-9316-feea59a4016e)
Aug 23 21:47:26.387: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-7748.svc.cluster.local from pod dns-7748/dns-test-61569fa4-4d95-49a0-9316-feea59a4016e: the server could not find the requested resource (get pods dns-test-61569fa4-4d95-49a0-9316-feea59a4016e)
Aug 23 21:47:26.890: INFO: Unable to read jessie_udp@dns-test-service.dns-7748.svc.cluster.local from pod dns-7748/dns-test-61569fa4-4d95-49a0-9316-feea59a4016e: the server could not find the requested resource (get pods dns-test-61569fa4-4d95-49a0-9316-feea59a4016e)
Aug 23 21:47:26.986: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-7748.svc.cluster.local from pod dns-7748/dns-test-61569fa4-4d95-49a0-9316-feea59a4016e: the server could not find the requested resource (get pods dns-test-61569fa4-4d95-49a0-9316-feea59a4016e)
Aug 23 21:47:27.032: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-7748.svc.cluster.local from pod dns-7748/dns-test-61569fa4-4d95-49a0-9316-feea59a4016e: the server could not find the requested resource (get pods dns-test-61569fa4-4d95-49a0-9316-feea59a4016e)
Aug 23 21:47:27.313: INFO: Lookups using dns-7748/dns-test-61569fa4-4d95-49a0-9316-feea59a4016e failed for: [wheezy_udp@dns-test-service.dns-7748.svc.cluster.local wheezy_tcp@dns-test-service.dns-7748.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-7748.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-7748.svc.cluster.local jessie_udp@dns-test-service.dns-7748.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-7748.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-7748.svc.cluster.local]

Aug 23 21:47:35.343: INFO: DNS probes using dns-7748/dns-test-61569fa4-4d95-49a0-9316-feea59a4016e succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 21:47:35.914: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-7748" for this suite.

• [SLOW TEST:25.316 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for services  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for services  [Conformance]","total":311,"completed":16,"skipped":195,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 21:47:36.542: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test substitution in container's args
Aug 23 21:47:37.383: INFO: Waiting up to 5m0s for pod "var-expansion-520aae56-df97-4c8b-a871-6d7ae3e14a7f" in namespace "var-expansion-975" to be "Succeeded or Failed"
Aug 23 21:47:37.516: INFO: Pod "var-expansion-520aae56-df97-4c8b-a871-6d7ae3e14a7f": Phase="Pending", Reason="", readiness=false. Elapsed: 132.875793ms
Aug 23 21:47:39.627: INFO: Pod "var-expansion-520aae56-df97-4c8b-a871-6d7ae3e14a7f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.243755715s
Aug 23 21:47:41.732: INFO: Pod "var-expansion-520aae56-df97-4c8b-a871-6d7ae3e14a7f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.349060847s
STEP: Saw pod success
Aug 23 21:47:41.732: INFO: Pod "var-expansion-520aae56-df97-4c8b-a871-6d7ae3e14a7f" satisfied condition "Succeeded or Failed"
Aug 23 21:47:41.854: INFO: Trying to get logs from node 10.149.248.24 pod var-expansion-520aae56-df97-4c8b-a871-6d7ae3e14a7f container dapi-container: <nil>
STEP: delete the pod
Aug 23 21:47:42.182: INFO: Waiting for pod var-expansion-520aae56-df97-4c8b-a871-6d7ae3e14a7f to disappear
Aug 23 21:47:42.298: INFO: Pod var-expansion-520aae56-df97-4c8b-a871-6d7ae3e14a7f no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 21:47:42.298: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-975" for this suite.

• [SLOW TEST:6.330 seconds]
[k8s.io] Variable Expansion
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Variable Expansion should allow substituting values in a container's args [NodeConformance] [Conformance]","total":311,"completed":17,"skipped":214,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] server version 
  should find the server version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] server version
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 21:47:42.871: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename server-version
STEP: Waiting for a default service account to be provisioned in namespace
[It] should find the server version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Request ServerVersion
STEP: Confirm major version
Aug 23 21:47:43.547: INFO: Major version: 1
STEP: Confirm minor version
Aug 23 21:47:43.547: INFO: cleanMinorVersion: 20
Aug 23 21:47:43.547: INFO: Minor version: 20
[AfterEach] [sig-api-machinery] server version
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 21:47:43.547: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "server-version-9145" for this suite.
•{"msg":"PASSED [sig-api-machinery] server version should find the server version [Conformance]","total":311,"completed":18,"skipped":219,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 21:47:44.183: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: set up a multi version CRD
Aug 23 21:47:44.708: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: mark a version not serverd
STEP: check the unserved version gets removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 21:48:36.507: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-4472" for this suite.

• [SLOW TEST:52.595 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] removes definition from spec when one version gets changed to not be served [Conformance]","total":311,"completed":19,"skipped":263,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 21:48:36.778: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:41
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Aug 23 21:48:37.568: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e6a1ae53-704e-4493-98b9-b4284c6e6422" in namespace "downward-api-4651" to be "Succeeded or Failed"
Aug 23 21:48:37.651: INFO: Pod "downwardapi-volume-e6a1ae53-704e-4493-98b9-b4284c6e6422": Phase="Pending", Reason="", readiness=false. Elapsed: 82.944596ms
Aug 23 21:48:39.747: INFO: Pod "downwardapi-volume-e6a1ae53-704e-4493-98b9-b4284c6e6422": Phase="Pending", Reason="", readiness=false. Elapsed: 2.178687818s
Aug 23 21:48:41.815: INFO: Pod "downwardapi-volume-e6a1ae53-704e-4493-98b9-b4284c6e6422": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.246963819s
STEP: Saw pod success
Aug 23 21:48:41.815: INFO: Pod "downwardapi-volume-e6a1ae53-704e-4493-98b9-b4284c6e6422" satisfied condition "Succeeded or Failed"
Aug 23 21:48:41.869: INFO: Trying to get logs from node 10.149.248.25 pod downwardapi-volume-e6a1ae53-704e-4493-98b9-b4284c6e6422 container client-container: <nil>
STEP: delete the pod
Aug 23 21:48:42.056: INFO: Waiting for pod downwardapi-volume-e6a1ae53-704e-4493-98b9-b4284c6e6422 to disappear
Aug 23 21:48:42.164: INFO: Pod downwardapi-volume-e6a1ae53-704e-4493-98b9-b4284c6e6422 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 21:48:42.164: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4651" for this suite.

• [SLOW TEST:5.755 seconds]
[sig-storage] Downward API volume
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:36
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu request [NodeConformance] [Conformance]","total":311,"completed":20,"skipped":273,"failed":0}
SS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 21:48:42.534: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:92
Aug 23 21:48:43.264: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Aug 23 21:48:43.628: INFO: Waiting for terminating namespaces to be deleted...
Aug 23 21:48:43.966: INFO: 
Logging pods the apiserver thinks is on node 10.149.248.24 before test
Aug 23 21:48:44.125: INFO: calico-node-9qqn4 from calico-system started at 2021-08-23 20:02:35 +0000 UTC (1 container statuses recorded)
Aug 23 21:48:44.125: INFO: 	Container calico-node ready: true, restart count 0
Aug 23 21:48:44.125: INFO: calico-typha-84b7fc6fc8-9fjkp from calico-system started at 2021-08-23 20:02:39 +0000 UTC (1 container statuses recorded)
Aug 23 21:48:44.125: INFO: 	Container calico-typha ready: true, restart count 0
Aug 23 21:48:44.125: INFO: test-k8s-e2e-pvg-master-verification from default started at 2021-08-23 20:04:37 +0000 UTC (1 container statuses recorded)
Aug 23 21:48:44.125: INFO: 	Container test-k8s-e2e-pvg-master-verification ready: true, restart count 0
Aug 23 21:48:44.125: INFO: ibm-keepalived-watcher-vrlwd from kube-system started at 2021-08-23 20:00:36 +0000 UTC (1 container statuses recorded)
Aug 23 21:48:44.125: INFO: 	Container keepalived-watcher ready: true, restart count 0
Aug 23 21:48:44.125: INFO: ibm-master-proxy-static-10.149.248.24 from kube-system started at 2021-08-23 20:00:33 +0000 UTC (2 container statuses recorded)
Aug 23 21:48:44.125: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Aug 23 21:48:44.125: INFO: 	Container pause ready: true, restart count 0
Aug 23 21:48:44.125: INFO: ibm-storage-watcher-5c4d7b6cd6-wbrm7 from kube-system started at 2021-08-23 20:03:01 +0000 UTC (1 container statuses recorded)
Aug 23 21:48:44.125: INFO: 	Container ibm-storage-watcher-container ready: true, restart count 0
Aug 23 21:48:44.125: INFO: ibmcloud-block-storage-driver-jgx4r from kube-system started at 2021-08-23 20:00:42 +0000 UTC (1 container statuses recorded)
Aug 23 21:48:44.125: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Aug 23 21:48:44.125: INFO: cluster-node-tuning-operator-84b8576b47-xq5x6 from openshift-cluster-node-tuning-operator started at 2021-08-23 20:03:01 +0000 UTC (1 container statuses recorded)
Aug 23 21:48:44.126: INFO: 	Container cluster-node-tuning-operator ready: true, restart count 0
Aug 23 21:48:44.126: INFO: tuned-7sb7t from openshift-cluster-node-tuning-operator started at 2021-08-23 20:05:27 +0000 UTC (1 container statuses recorded)
Aug 23 21:48:44.126: INFO: 	Container tuned ready: true, restart count 0
Aug 23 21:48:44.126: INFO: cluster-samples-operator-866dcfc6c4-ztmg7 from openshift-cluster-samples-operator started at 2021-08-23 20:03:01 +0000 UTC (2 container statuses recorded)
Aug 23 21:48:44.126: INFO: 	Container cluster-samples-operator ready: true, restart count 0
Aug 23 21:48:44.126: INFO: 	Container cluster-samples-operator-watch ready: true, restart count 0
Aug 23 21:48:44.126: INFO: csi-snapshot-controller-566544547f-wflt9 from openshift-cluster-storage-operator started at 2021-08-23 20:03:18 +0000 UTC (1 container statuses recorded)
Aug 23 21:48:44.126: INFO: 	Container snapshot-controller ready: true, restart count 0
Aug 23 21:48:44.126: INFO: csi-snapshot-webhook-585d4946dc-j69rh from openshift-cluster-storage-operator started at 2021-08-23 20:03:17 +0000 UTC (1 container statuses recorded)
Aug 23 21:48:44.126: INFO: 	Container webhook ready: true, restart count 0
Aug 23 21:48:44.126: INFO: console-operator-c7f9f8687-ln5mh from openshift-console-operator started at 2021-08-23 20:03:01 +0000 UTC (1 container statuses recorded)
Aug 23 21:48:44.126: INFO: 	Container console-operator ready: true, restart count 2
Aug 23 21:48:44.126: INFO: console-dc8c686bb-z8vlk from openshift-console started at 2021-08-23 20:06:49 +0000 UTC (1 container statuses recorded)
Aug 23 21:48:44.126: INFO: 	Container console ready: true, restart count 0
Aug 23 21:48:44.126: INFO: downloads-6c96776f98-xmtbv from openshift-console started at 2021-08-23 20:03:01 +0000 UTC (1 container statuses recorded)
Aug 23 21:48:44.126: INFO: 	Container download-server ready: true, restart count 0
Aug 23 21:48:44.126: INFO: dns-default-mgbp7 from openshift-dns started at 2021-08-23 20:05:27 +0000 UTC (3 container statuses recorded)
Aug 23 21:48:44.126: INFO: 	Container dns ready: true, restart count 0
Aug 23 21:48:44.126: INFO: 	Container dns-node-resolver ready: true, restart count 0
Aug 23 21:48:44.126: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 23 21:48:44.126: INFO: image-registry-555cc7f64c-dwlvt from openshift-image-registry started at 2021-08-23 20:08:40 +0000 UTC (1 container statuses recorded)
Aug 23 21:48:44.126: INFO: 	Container registry ready: true, restart count 0
Aug 23 21:48:44.126: INFO: node-ca-dl95t from openshift-image-registry started at 2021-08-23 20:04:29 +0000 UTC (1 container statuses recorded)
Aug 23 21:48:44.126: INFO: 	Container node-ca ready: true, restart count 0
Aug 23 21:48:44.126: INFO: ingress-canary-jjqc9 from openshift-ingress-canary started at 2021-08-23 20:05:35 +0000 UTC (1 container statuses recorded)
Aug 23 21:48:44.126: INFO: 	Container hello-openshift-canary ready: true, restart count 0
Aug 23 21:48:44.126: INFO: ingress-operator-5d97777898-47pvl from openshift-ingress-operator started at 2021-08-23 20:03:01 +0000 UTC (2 container statuses recorded)
Aug 23 21:48:44.126: INFO: 	Container ingress-operator ready: true, restart count 0
Aug 23 21:48:44.126: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 23 21:48:44.126: INFO: openshift-kube-proxy-wn8f2 from openshift-kube-proxy started at 2021-08-23 20:01:25 +0000 UTC (2 container statuses recorded)
Aug 23 21:48:44.126: INFO: 	Container kube-proxy ready: true, restart count 0
Aug 23 21:48:44.126: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 23 21:48:44.126: INFO: kube-storage-version-migrator-operator-6d55bddccb-pthmv from openshift-kube-storage-version-migrator-operator started at 2021-08-23 20:03:01 +0000 UTC (1 container statuses recorded)
Aug 23 21:48:44.126: INFO: 	Container kube-storage-version-migrator-operator ready: true, restart count 1
Aug 23 21:48:44.126: INFO: migrator-f58676cd4-nll9r from openshift-kube-storage-version-migrator started at 2021-08-23 20:03:23 +0000 UTC (1 container statuses recorded)
Aug 23 21:48:44.126: INFO: 	Container migrator ready: true, restart count 0
Aug 23 21:48:44.126: INFO: marketplace-operator-7fd8fdcc5b-tp797 from openshift-marketplace started at 2021-08-23 20:03:01 +0000 UTC (1 container statuses recorded)
Aug 23 21:48:44.126: INFO: 	Container marketplace-operator ready: true, restart count 0
Aug 23 21:48:44.126: INFO: alertmanager-main-0 from openshift-monitoring started at 2021-08-23 20:06:03 +0000 UTC (5 container statuses recorded)
Aug 23 21:48:44.126: INFO: 	Container alertmanager ready: true, restart count 0
Aug 23 21:48:44.126: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Aug 23 21:48:44.126: INFO: 	Container config-reloader ready: true, restart count 0
Aug 23 21:48:44.126: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 23 21:48:44.126: INFO: 	Container prom-label-proxy ready: true, restart count 0
Aug 23 21:48:44.126: INFO: grafana-85d4bdb748-9dngw from openshift-monitoring started at 2021-08-23 20:05:57 +0000 UTC (2 container statuses recorded)
Aug 23 21:48:44.126: INFO: 	Container grafana ready: true, restart count 0
Aug 23 21:48:44.126: INFO: 	Container grafana-proxy ready: true, restart count 0
Aug 23 21:48:44.126: INFO: kube-state-metrics-d956df775-skqgm from openshift-monitoring started at 2021-08-23 20:03:19 +0000 UTC (3 container statuses recorded)
Aug 23 21:48:44.126: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Aug 23 21:48:44.126: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Aug 23 21:48:44.126: INFO: 	Container kube-state-metrics ready: true, restart count 0
Aug 23 21:48:44.126: INFO: node-exporter-b5rwn from openshift-monitoring started at 2021-08-23 20:03:19 +0000 UTC (2 container statuses recorded)
Aug 23 21:48:44.126: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 23 21:48:44.126: INFO: 	Container node-exporter ready: true, restart count 0
Aug 23 21:48:44.126: INFO: openshift-state-metrics-74b58f578c-2bphx from openshift-monitoring started at 2021-08-23 20:03:19 +0000 UTC (3 container statuses recorded)
Aug 23 21:48:44.126: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Aug 23 21:48:44.126: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Aug 23 21:48:44.126: INFO: 	Container openshift-state-metrics ready: true, restart count 0
Aug 23 21:48:44.126: INFO: prometheus-adapter-6685ccb7f-gp2zd from openshift-monitoring started at 2021-08-23 20:06:45 +0000 UTC (1 container statuses recorded)
Aug 23 21:48:44.126: INFO: 	Container prometheus-adapter ready: true, restart count 0
Aug 23 21:48:44.126: INFO: telemeter-client-9d6d6f95f-gjnsk from openshift-monitoring started at 2021-08-23 20:03:30 +0000 UTC (3 container statuses recorded)
Aug 23 21:48:44.126: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 23 21:48:44.126: INFO: 	Container reload ready: true, restart count 0
Aug 23 21:48:44.126: INFO: 	Container telemeter-client ready: true, restart count 0
Aug 23 21:48:44.126: INFO: thanos-querier-97cd894c4-hxlql from openshift-monitoring started at 2021-08-23 20:06:10 +0000 UTC (5 container statuses recorded)
Aug 23 21:48:44.126: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 23 21:48:44.126: INFO: 	Container kube-rbac-proxy-rules ready: true, restart count 0
Aug 23 21:48:44.126: INFO: 	Container oauth-proxy ready: true, restart count 0
Aug 23 21:48:44.126: INFO: 	Container prom-label-proxy ready: true, restart count 0
Aug 23 21:48:44.126: INFO: 	Container thanos-query ready: true, restart count 0
Aug 23 21:48:44.126: INFO: multus-48kkt from openshift-multus started at 2021-08-23 20:01:12 +0000 UTC (1 container statuses recorded)
Aug 23 21:48:44.126: INFO: 	Container kube-multus ready: true, restart count 0
Aug 23 21:48:44.126: INFO: multus-admission-controller-7pgs9 from openshift-multus started at 2021-08-23 20:02:57 +0000 UTC (2 container statuses recorded)
Aug 23 21:48:44.126: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 23 21:48:44.126: INFO: 	Container multus-admission-controller ready: true, restart count 0
Aug 23 21:48:44.126: INFO: network-metrics-daemon-cqr85 from openshift-multus started at 2021-08-23 20:01:15 +0000 UTC (2 container statuses recorded)
Aug 23 21:48:44.126: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 23 21:48:44.126: INFO: 	Container network-metrics-daemon ready: true, restart count 0
Aug 23 21:48:44.126: INFO: network-check-target-447wc from openshift-network-diagnostics started at 2021-08-23 20:01:33 +0000 UTC (1 container statuses recorded)
Aug 23 21:48:44.126: INFO: 	Container network-check-target-container ready: true, restart count 0
Aug 23 21:48:44.126: INFO: catalog-operator-6f8ff86686-bmj5l from openshift-operator-lifecycle-manager started at 2021-08-23 20:03:01 +0000 UTC (1 container statuses recorded)
Aug 23 21:48:44.126: INFO: 	Container catalog-operator ready: true, restart count 0
Aug 23 21:48:44.126: INFO: packageserver-6d5f99f754-xpr2g from openshift-operator-lifecycle-manager started at 2021-08-23 20:04:35 +0000 UTC (1 container statuses recorded)
Aug 23 21:48:44.126: INFO: 	Container packageserver ready: true, restart count 0
Aug 23 21:48:44.126: INFO: push-gateway-59f6bc56d4-grxnm from openshift-roks-metrics started at 2021-08-23 20:03:01 +0000 UTC (1 container statuses recorded)
Aug 23 21:48:44.126: INFO: 	Container push-gateway ready: true, restart count 0
Aug 23 21:48:44.126: INFO: service-ca-operator-64b7cc7c85-zc49g from openshift-service-ca-operator started at 2021-08-23 20:03:01 +0000 UTC (1 container statuses recorded)
Aug 23 21:48:44.126: INFO: 	Container service-ca-operator ready: true, restart count 1
Aug 23 21:48:44.126: INFO: service-ca-54b7675c-9czch from openshift-service-ca started at 2021-08-23 20:03:44 +0000 UTC (1 container statuses recorded)
Aug 23 21:48:44.126: INFO: 	Container service-ca-controller ready: true, restart count 0
Aug 23 21:48:44.126: INFO: sonobuoy-systemd-logs-daemon-set-9034e69b695e4ee2-s28vf from sonobuoy started at 2021-08-23 21:37:23 +0000 UTC (2 container statuses recorded)
Aug 23 21:48:44.126: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 23 21:48:44.126: INFO: 	Container systemd-logs ready: true, restart count 0
Aug 23 21:48:44.126: INFO: 
Logging pods the apiserver thinks is on node 10.149.248.25 before test
Aug 23 21:48:44.380: INFO: calico-node-4m7h2 from calico-system started at 2021-08-23 20:02:35 +0000 UTC (1 container statuses recorded)
Aug 23 21:48:44.380: INFO: 	Container calico-node ready: true, restart count 0
Aug 23 21:48:44.380: INFO: calico-typha-84b7fc6fc8-tf4pd from calico-system started at 2021-08-23 20:02:33 +0000 UTC (1 container statuses recorded)
Aug 23 21:48:44.380: INFO: 	Container calico-typha ready: true, restart count 0
Aug 23 21:48:44.380: INFO: ibm-cloud-provider-ip-169-55-101-126-6bb454748-g6kdf from ibm-system started at 2021-08-23 20:10:53 +0000 UTC (1 container statuses recorded)
Aug 23 21:48:44.380: INFO: 	Container ibm-cloud-provider-ip-169-55-101-126 ready: true, restart count 0
Aug 23 21:48:44.380: INFO: ibm-file-plugin-68fbcccc88-bgbn9 from kube-system started at 2021-08-23 20:03:01 +0000 UTC (1 container statuses recorded)
Aug 23 21:48:44.380: INFO: 	Container ibm-file-plugin-container ready: true, restart count 0
Aug 23 21:48:44.380: INFO: ibm-keepalived-watcher-c94bg from kube-system started at 2021-08-23 20:00:20 +0000 UTC (1 container statuses recorded)
Aug 23 21:48:44.380: INFO: 	Container keepalived-watcher ready: true, restart count 0
Aug 23 21:48:44.380: INFO: ibm-master-proxy-static-10.149.248.25 from kube-system started at 2021-08-23 20:00:16 +0000 UTC (2 container statuses recorded)
Aug 23 21:48:44.380: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Aug 23 21:48:44.380: INFO: 	Container pause ready: true, restart count 0
Aug 23 21:48:44.380: INFO: ibmcloud-block-storage-driver-zdvf4 from kube-system started at 2021-08-23 20:00:24 +0000 UTC (1 container statuses recorded)
Aug 23 21:48:44.380: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Aug 23 21:48:44.380: INFO: ibmcloud-block-storage-plugin-7d6d9649b-zdvgj from kube-system started at 2021-08-23 20:03:01 +0000 UTC (1 container statuses recorded)
Aug 23 21:48:44.380: INFO: 	Container ibmcloud-block-storage-plugin-container ready: true, restart count 0
Aug 23 21:48:44.380: INFO: vpn-db594b4f9-9mxgx from kube-system started at 2021-08-23 20:08:59 +0000 UTC (1 container statuses recorded)
Aug 23 21:48:44.380: INFO: 	Container vpn ready: true, restart count 0
Aug 23 21:48:44.380: INFO: tuned-bfbjn from openshift-cluster-node-tuning-operator started at 2021-08-23 20:05:28 +0000 UTC (1 container statuses recorded)
Aug 23 21:48:44.380: INFO: 	Container tuned ready: true, restart count 0
Aug 23 21:48:44.380: INFO: cluster-storage-operator-6c5c749558-97wht from openshift-cluster-storage-operator started at 2021-08-23 20:03:01 +0000 UTC (1 container statuses recorded)
Aug 23 21:48:44.380: INFO: 	Container cluster-storage-operator ready: true, restart count 1
Aug 23 21:48:44.380: INFO: csi-snapshot-controller-operator-645f4897d-2vvbp from openshift-cluster-storage-operator started at 2021-08-23 20:03:01 +0000 UTC (1 container statuses recorded)
Aug 23 21:48:44.380: INFO: 	Container csi-snapshot-controller-operator ready: true, restart count 0
Aug 23 21:48:44.380: INFO: console-dc8c686bb-8xpf8 from openshift-console started at 2021-08-23 20:07:03 +0000 UTC (1 container statuses recorded)
Aug 23 21:48:44.380: INFO: 	Container console ready: true, restart count 0
Aug 23 21:48:44.380: INFO: downloads-6c96776f98-5w9bj from openshift-console started at 2021-08-23 20:03:01 +0000 UTC (1 container statuses recorded)
Aug 23 21:48:44.380: INFO: 	Container download-server ready: true, restart count 0
Aug 23 21:48:44.380: INFO: dns-operator-6dbb54f776-8k9pq from openshift-dns-operator started at 2021-08-23 20:03:01 +0000 UTC (2 container statuses recorded)
Aug 23 21:48:44.380: INFO: 	Container dns-operator ready: true, restart count 0
Aug 23 21:48:44.381: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 23 21:48:44.381: INFO: dns-default-mxjt9 from openshift-dns started at 2021-08-23 20:05:27 +0000 UTC (3 container statuses recorded)
Aug 23 21:48:44.381: INFO: 	Container dns ready: true, restart count 0
Aug 23 21:48:44.381: INFO: 	Container dns-node-resolver ready: true, restart count 0
Aug 23 21:48:44.381: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 23 21:48:44.381: INFO: cluster-image-registry-operator-85978c675-rdng6 from openshift-image-registry started at 2021-08-23 20:03:01 +0000 UTC (1 container statuses recorded)
Aug 23 21:48:44.381: INFO: 	Container cluster-image-registry-operator ready: true, restart count 0
Aug 23 21:48:44.381: INFO: node-ca-czldz from openshift-image-registry started at 2021-08-23 20:04:29 +0000 UTC (1 container statuses recorded)
Aug 23 21:48:44.381: INFO: 	Container node-ca ready: true, restart count 0
Aug 23 21:48:44.381: INFO: ingress-canary-9skj4 from openshift-ingress-canary started at 2021-08-23 20:05:35 +0000 UTC (1 container statuses recorded)
Aug 23 21:48:44.381: INFO: 	Container hello-openshift-canary ready: true, restart count 0
Aug 23 21:48:44.381: INFO: router-default-678bfcd875-g2q6m from openshift-ingress started at 2021-08-23 20:05:36 +0000 UTC (1 container statuses recorded)
Aug 23 21:48:44.381: INFO: 	Container router ready: true, restart count 0
Aug 23 21:48:44.381: INFO: openshift-kube-proxy-sbzjn from openshift-kube-proxy started at 2021-08-23 20:01:25 +0000 UTC (2 container statuses recorded)
Aug 23 21:48:44.381: INFO: 	Container kube-proxy ready: true, restart count 0
Aug 23 21:48:44.381: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 23 21:48:44.381: INFO: redhat-marketplace-hvcn5 from openshift-marketplace started at 2021-08-23 20:04:56 +0000 UTC (1 container statuses recorded)
Aug 23 21:48:44.381: INFO: 	Container registry-server ready: true, restart count 0
Aug 23 21:48:44.381: INFO: alertmanager-main-1 from openshift-monitoring started at 2021-08-23 20:06:03 +0000 UTC (5 container statuses recorded)
Aug 23 21:48:44.381: INFO: 	Container alertmanager ready: true, restart count 0
Aug 23 21:48:44.381: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Aug 23 21:48:44.381: INFO: 	Container config-reloader ready: true, restart count 0
Aug 23 21:48:44.381: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 23 21:48:44.381: INFO: 	Container prom-label-proxy ready: true, restart count 0
Aug 23 21:48:44.381: INFO: cluster-monitoring-operator-c69d85486-m52jl from openshift-monitoring started at 2021-08-23 20:03:01 +0000 UTC (2 container statuses recorded)
Aug 23 21:48:44.381: INFO: 	Container cluster-monitoring-operator ready: true, restart count 0
Aug 23 21:48:44.381: INFO: 	Container kube-rbac-proxy ready: true, restart count 4
Aug 23 21:48:44.381: INFO: node-exporter-fsg2g from openshift-monitoring started at 2021-08-23 20:03:19 +0000 UTC (2 container statuses recorded)
Aug 23 21:48:44.381: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 23 21:48:44.381: INFO: 	Container node-exporter ready: true, restart count 0
Aug 23 21:48:44.381: INFO: prometheus-adapter-6685ccb7f-gxd72 from openshift-monitoring started at 2021-08-23 20:06:45 +0000 UTC (1 container statuses recorded)
Aug 23 21:48:44.381: INFO: 	Container prometheus-adapter ready: true, restart count 0
Aug 23 21:48:44.381: INFO: prometheus-k8s-1 from openshift-monitoring started at 2021-08-23 20:06:14 +0000 UTC (7 container statuses recorded)
Aug 23 21:48:44.381: INFO: 	Container config-reloader ready: true, restart count 0
Aug 23 21:48:44.381: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 23 21:48:44.381: INFO: 	Container kube-rbac-proxy-thanos ready: true, restart count 0
Aug 23 21:48:44.381: INFO: 	Container prom-label-proxy ready: true, restart count 0
Aug 23 21:48:44.381: INFO: 	Container prometheus ready: true, restart count 1
Aug 23 21:48:44.381: INFO: 	Container prometheus-proxy ready: true, restart count 0
Aug 23 21:48:44.381: INFO: 	Container thanos-sidecar ready: true, restart count 0
Aug 23 21:48:44.381: INFO: thanos-querier-97cd894c4-z4jqj from openshift-monitoring started at 2021-08-23 20:06:10 +0000 UTC (5 container statuses recorded)
Aug 23 21:48:44.381: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 23 21:48:44.381: INFO: 	Container kube-rbac-proxy-rules ready: true, restart count 0
Aug 23 21:48:44.381: INFO: 	Container oauth-proxy ready: true, restart count 0
Aug 23 21:48:44.381: INFO: 	Container prom-label-proxy ready: true, restart count 0
Aug 23 21:48:44.381: INFO: 	Container thanos-query ready: true, restart count 0
Aug 23 21:48:44.381: INFO: multus-2p958 from openshift-multus started at 2021-08-23 20:01:12 +0000 UTC (1 container statuses recorded)
Aug 23 21:48:44.381: INFO: 	Container kube-multus ready: true, restart count 0
Aug 23 21:48:44.381: INFO: multus-admission-controller-ww7jt from openshift-multus started at 2021-08-23 20:02:56 +0000 UTC (2 container statuses recorded)
Aug 23 21:48:44.381: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 23 21:48:44.381: INFO: 	Container multus-admission-controller ready: true, restart count 0
Aug 23 21:48:44.381: INFO: network-metrics-daemon-6bmnm from openshift-multus started at 2021-08-23 20:01:15 +0000 UTC (2 container statuses recorded)
Aug 23 21:48:44.381: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 23 21:48:44.381: INFO: 	Container network-metrics-daemon ready: true, restart count 0
Aug 23 21:48:44.381: INFO: network-check-target-cjqqr from openshift-network-diagnostics started at 2021-08-23 20:01:33 +0000 UTC (1 container statuses recorded)
Aug 23 21:48:44.381: INFO: 	Container network-check-target-container ready: true, restart count 0
Aug 23 21:48:44.381: INFO: olm-operator-d5dc9548d-mwfkk from openshift-operator-lifecycle-manager started at 2021-08-23 20:03:01 +0000 UTC (1 container statuses recorded)
Aug 23 21:48:44.381: INFO: 	Container olm-operator ready: true, restart count 0
Aug 23 21:48:44.381: INFO: metrics-8fc5c5f56-lq4k6 from openshift-roks-metrics started at 2021-08-23 20:03:01 +0000 UTC (1 container statuses recorded)
Aug 23 21:48:44.381: INFO: 	Container metrics ready: true, restart count 3
Aug 23 21:48:44.381: INFO: sonobuoy-e2e-job-90ed16fed1444409 from sonobuoy started at 2021-08-23 21:37:22 +0000 UTC (2 container statuses recorded)
Aug 23 21:48:44.381: INFO: 	Container e2e ready: true, restart count 0
Aug 23 21:48:44.381: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 23 21:48:44.381: INFO: sonobuoy-systemd-logs-daemon-set-9034e69b695e4ee2-q8h6l from sonobuoy started at 2021-08-23 21:37:23 +0000 UTC (2 container statuses recorded)
Aug 23 21:48:44.381: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 23 21:48:44.381: INFO: 	Container systemd-logs ready: true, restart count 0
Aug 23 21:48:44.381: INFO: 
Logging pods the apiserver thinks is on node 10.149.248.9 before test
Aug 23 21:48:44.607: INFO: calico-kube-controllers-7dcbcc7c66-x65ng from calico-system started at 2021-08-23 20:02:55 +0000 UTC (1 container statuses recorded)
Aug 23 21:48:44.607: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Aug 23 21:48:44.607: INFO: calico-node-cgrcs from calico-system started at 2021-08-23 20:02:35 +0000 UTC (1 container statuses recorded)
Aug 23 21:48:44.607: INFO: 	Container calico-node ready: true, restart count 0
Aug 23 21:48:44.607: INFO: calico-typha-84b7fc6fc8-h6lnw from calico-system started at 2021-08-23 20:02:39 +0000 UTC (1 container statuses recorded)
Aug 23 21:48:44.607: INFO: 	Container calico-typha ready: true, restart count 0
Aug 23 21:48:44.607: INFO: ibm-cloud-provider-ip-169-55-101-126-6bb454748-t7nvk from ibm-system started at 2021-08-23 20:10:53 +0000 UTC (1 container statuses recorded)
Aug 23 21:48:44.607: INFO: 	Container ibm-cloud-provider-ip-169-55-101-126 ready: true, restart count 0
Aug 23 21:48:44.607: INFO: ibm-keepalived-watcher-rsh2g from kube-system started at 2021-08-23 20:00:08 +0000 UTC (1 container statuses recorded)
Aug 23 21:48:44.607: INFO: 	Container keepalived-watcher ready: true, restart count 0
Aug 23 21:48:44.607: INFO: ibm-master-proxy-static-10.149.248.9 from kube-system started at 2021-08-23 20:00:06 +0000 UTC (2 container statuses recorded)
Aug 23 21:48:44.607: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Aug 23 21:48:44.607: INFO: 	Container pause ready: true, restart count 0
Aug 23 21:48:44.607: INFO: ibmcloud-block-storage-driver-5fllx from kube-system started at 2021-08-23 20:00:13 +0000 UTC (1 container statuses recorded)
Aug 23 21:48:44.607: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Aug 23 21:48:44.607: INFO: tuned-m585x from openshift-cluster-node-tuning-operator started at 2021-08-23 20:05:28 +0000 UTC (1 container statuses recorded)
Aug 23 21:48:44.607: INFO: 	Container tuned ready: true, restart count 0
Aug 23 21:48:44.607: INFO: dns-default-4hmcv from openshift-dns started at 2021-08-23 20:05:27 +0000 UTC (3 container statuses recorded)
Aug 23 21:48:44.607: INFO: 	Container dns ready: true, restart count 0
Aug 23 21:48:44.607: INFO: 	Container dns-node-resolver ready: true, restart count 0
Aug 23 21:48:44.607: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 23 21:48:44.607: INFO: node-ca-g989x from openshift-image-registry started at 2021-08-23 20:04:29 +0000 UTC (1 container statuses recorded)
Aug 23 21:48:44.607: INFO: 	Container node-ca ready: true, restart count 0
Aug 23 21:48:44.608: INFO: registry-pvc-permissions-dbktz from openshift-image-registry started at 2021-08-23 20:08:40 +0000 UTC (1 container statuses recorded)
Aug 23 21:48:44.608: INFO: 	Container pvc-permissions ready: false, restart count 0
Aug 23 21:48:44.608: INFO: ingress-canary-l5l42 from openshift-ingress-canary started at 2021-08-23 20:05:35 +0000 UTC (1 container statuses recorded)
Aug 23 21:48:44.608: INFO: 	Container hello-openshift-canary ready: true, restart count 0
Aug 23 21:48:44.608: INFO: router-default-678bfcd875-2f5hb from openshift-ingress started at 2021-08-23 20:05:36 +0000 UTC (1 container statuses recorded)
Aug 23 21:48:44.608: INFO: 	Container router ready: true, restart count 0
Aug 23 21:48:44.608: INFO: openshift-kube-proxy-wk9m8 from openshift-kube-proxy started at 2021-08-23 20:01:25 +0000 UTC (2 container statuses recorded)
Aug 23 21:48:44.608: INFO: 	Container kube-proxy ready: true, restart count 0
Aug 23 21:48:44.608: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 23 21:48:44.608: INFO: certified-operators-rlpm2 from openshift-marketplace started at 2021-08-23 20:04:55 +0000 UTC (1 container statuses recorded)
Aug 23 21:48:44.608: INFO: 	Container registry-server ready: true, restart count 0
Aug 23 21:48:44.608: INFO: community-operators-ztwk2 from openshift-marketplace started at 2021-08-23 20:04:56 +0000 UTC (1 container statuses recorded)
Aug 23 21:48:44.608: INFO: 	Container registry-server ready: true, restart count 0
Aug 23 21:48:44.608: INFO: redhat-operators-cpqsn from openshift-marketplace started at 2021-08-23 20:04:56 +0000 UTC (1 container statuses recorded)
Aug 23 21:48:44.608: INFO: 	Container registry-server ready: true, restart count 0
Aug 23 21:48:44.608: INFO: alertmanager-main-2 from openshift-monitoring started at 2021-08-23 20:06:04 +0000 UTC (5 container statuses recorded)
Aug 23 21:48:44.608: INFO: 	Container alertmanager ready: true, restart count 0
Aug 23 21:48:44.608: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Aug 23 21:48:44.608: INFO: 	Container config-reloader ready: true, restart count 0
Aug 23 21:48:44.608: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 23 21:48:44.608: INFO: 	Container prom-label-proxy ready: true, restart count 0
Aug 23 21:48:44.608: INFO: node-exporter-kcjzv from openshift-monitoring started at 2021-08-23 20:03:19 +0000 UTC (2 container statuses recorded)
Aug 23 21:48:44.608: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 23 21:48:44.608: INFO: 	Container node-exporter ready: true, restart count 0
Aug 23 21:48:44.608: INFO: prometheus-k8s-0 from openshift-monitoring started at 2021-08-23 20:06:14 +0000 UTC (7 container statuses recorded)
Aug 23 21:48:44.608: INFO: 	Container config-reloader ready: true, restart count 0
Aug 23 21:48:44.608: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 23 21:48:44.608: INFO: 	Container kube-rbac-proxy-thanos ready: true, restart count 0
Aug 23 21:48:44.608: INFO: 	Container prom-label-proxy ready: true, restart count 0
Aug 23 21:48:44.608: INFO: 	Container prometheus ready: true, restart count 1
Aug 23 21:48:44.608: INFO: 	Container prometheus-proxy ready: true, restart count 0
Aug 23 21:48:44.608: INFO: 	Container thanos-sidecar ready: true, restart count 0
Aug 23 21:48:44.608: INFO: prometheus-operator-5699bb49dc-8q47w from openshift-monitoring started at 2021-08-23 20:06:44 +0000 UTC (2 container statuses recorded)
Aug 23 21:48:44.608: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 23 21:48:44.608: INFO: 	Container prometheus-operator ready: true, restart count 0
Aug 23 21:48:44.608: INFO: multus-admission-controller-hcwf4 from openshift-multus started at 2021-08-23 20:02:50 +0000 UTC (2 container statuses recorded)
Aug 23 21:48:44.608: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 23 21:48:44.608: INFO: 	Container multus-admission-controller ready: true, restart count 0
Aug 23 21:48:44.608: INFO: multus-z4wqr from openshift-multus started at 2021-08-23 20:01:12 +0000 UTC (1 container statuses recorded)
Aug 23 21:48:44.608: INFO: 	Container kube-multus ready: true, restart count 0
Aug 23 21:48:44.608: INFO: network-metrics-daemon-mbjhv from openshift-multus started at 2021-08-23 20:01:15 +0000 UTC (2 container statuses recorded)
Aug 23 21:48:44.608: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 23 21:48:44.608: INFO: 	Container network-metrics-daemon ready: true, restart count 0
Aug 23 21:48:44.608: INFO: network-check-source-6cd65cf589-6rbmz from openshift-network-diagnostics started at 2021-08-23 20:03:01 +0000 UTC (1 container statuses recorded)
Aug 23 21:48:44.608: INFO: 	Container check-endpoints ready: true, restart count 0
Aug 23 21:48:44.608: INFO: network-check-target-d6zj2 from openshift-network-diagnostics started at 2021-08-23 20:01:33 +0000 UTC (1 container statuses recorded)
Aug 23 21:48:44.608: INFO: 	Container network-check-target-container ready: true, restart count 0
Aug 23 21:48:44.608: INFO: network-operator-86dcc4df56-crx6q from openshift-network-operator started at 2021-08-23 20:00:18 +0000 UTC (1 container statuses recorded)
Aug 23 21:48:44.608: INFO: 	Container network-operator ready: true, restart count 0
Aug 23 21:48:44.608: INFO: packageserver-6d5f99f754-9mfv7 from openshift-operator-lifecycle-manager started at 2021-08-23 20:04:35 +0000 UTC (1 container statuses recorded)
Aug 23 21:48:44.608: INFO: 	Container packageserver ready: true, restart count 0
Aug 23 21:48:44.608: INFO: sonobuoy from sonobuoy started at 2021-08-23 21:37:12 +0000 UTC (1 container statuses recorded)
Aug 23 21:48:44.608: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Aug 23 21:48:44.608: INFO: sonobuoy-systemd-logs-daemon-set-9034e69b695e4ee2-zn2c4 from sonobuoy started at 2021-08-23 21:37:23 +0000 UTC (2 container statuses recorded)
Aug 23 21:48:44.608: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 23 21:48:44.608: INFO: 	Container systemd-logs ready: true, restart count 0
Aug 23 21:48:44.608: INFO: tigera-operator-667cd558f7-d78zp from tigera-operator started at 2021-08-23 20:00:19 +0000 UTC (1 container statuses recorded)
Aug 23 21:48:44.608: INFO: 	Container tigera-operator ready: true, restart count 4
[It] validates that NodeSelector is respected if matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-1f5c1ec1-4f52-467b-ad21-f519b574ca84 42
STEP: Trying to relaunch the pod, now with labels.
STEP: removing the label kubernetes.io/e2e-1f5c1ec1-4f52-467b-ad21-f519b574ca84 off the node 10.149.248.24
STEP: verifying the node doesn't have the label kubernetes.io/e2e-1f5c1ec1-4f52-467b-ad21-f519b574ca84
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 21:49:03.256: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-6630" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:83

• [SLOW TEST:21.187 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if matching  [Conformance]","total":311,"completed":21,"skipped":275,"failed":0}
SSSS
------------------------------
[sig-instrumentation] Events API 
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 21:49:03.723: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/instrumentation/events.go:81
[It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a test event
STEP: listing events in all namespaces
STEP: listing events in test namespace
STEP: listing events with field selection filtering on source
STEP: listing events with field selection filtering on reportingController
STEP: getting the test event
STEP: patching the test event
STEP: getting the test event
STEP: updating the test event
STEP: getting the test event
STEP: deleting the test event
STEP: listing events in all namespaces
STEP: listing events in test namespace
[AfterEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 21:49:06.925: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-2636" for this suite.
•{"msg":"PASSED [sig-instrumentation] Events API should ensure that an event can be fetched, patched, deleted, and listed [Conformance]","total":311,"completed":22,"skipped":279,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 21:49:07.496: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating secret with name secret-test-c02c465c-12f7-442e-a81a-648e39443010
STEP: Creating a pod to test consume secrets
Aug 23 21:49:08.789: INFO: Waiting up to 5m0s for pod "pod-secrets-bdac3a91-865c-48a9-8ae4-820d34f4ec45" in namespace "secrets-3776" to be "Succeeded or Failed"
Aug 23 21:49:08.930: INFO: Pod "pod-secrets-bdac3a91-865c-48a9-8ae4-820d34f4ec45": Phase="Pending", Reason="", readiness=false. Elapsed: 140.505ms
Aug 23 21:49:11.016: INFO: Pod "pod-secrets-bdac3a91-865c-48a9-8ae4-820d34f4ec45": Phase="Pending", Reason="", readiness=false. Elapsed: 2.226970207s
Aug 23 21:49:13.177: INFO: Pod "pod-secrets-bdac3a91-865c-48a9-8ae4-820d34f4ec45": Phase="Pending", Reason="", readiness=false. Elapsed: 4.387690753s
Aug 23 21:49:15.279: INFO: Pod "pod-secrets-bdac3a91-865c-48a9-8ae4-820d34f4ec45": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.490243186s
STEP: Saw pod success
Aug 23 21:49:15.280: INFO: Pod "pod-secrets-bdac3a91-865c-48a9-8ae4-820d34f4ec45" satisfied condition "Succeeded or Failed"
Aug 23 21:49:15.390: INFO: Trying to get logs from node 10.149.248.9 pod pod-secrets-bdac3a91-865c-48a9-8ae4-820d34f4ec45 container secret-env-test: <nil>
STEP: delete the pod
Aug 23 21:49:16.130: INFO: Waiting for pod pod-secrets-bdac3a91-865c-48a9-8ae4-820d34f4ec45 to disappear
Aug 23 21:49:16.281: INFO: Pod pod-secrets-bdac3a91-865c-48a9-8ae4-820d34f4ec45 no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 21:49:16.281: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3776" for this suite.

• [SLOW TEST:9.224 seconds]
[sig-api-machinery] Secrets
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:36
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] Secrets should be consumable from pods in env vars [NodeConformance] [Conformance]","total":311,"completed":23,"skipped":298,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test when starting a container that exits 
  should run with the expected status [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 21:49:16.720: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run with the expected status [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpa': should get the expected 'State'
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpof': should get the expected 'State'
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpn': should get the expected 'State'
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance]
[AfterEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 21:49:52.620: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-3368" for this suite.

• [SLOW TEST:36.259 seconds]
[k8s.io] Container Runtime
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  blackbox test
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:41
    when starting a container that exits
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:42
      should run with the expected status [NodeConformance] [Conformance]
      /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Container Runtime blackbox test when starting a container that exits should run with the expected status [NodeConformance] [Conformance]","total":311,"completed":24,"skipped":326,"failed":0}
SSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 21:49:52.980: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Aug 23 21:49:59.669: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 21:49:59.921: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-9470" for this suite.

• [SLOW TEST:7.314 seconds]
[k8s.io] Container Runtime
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  blackbox test
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:41
    on terminated container
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:134
      should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
      /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]","total":311,"completed":25,"skipped":330,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support rollover [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 21:50:00.294: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:85
[It] deployment should support rollover [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Aug 23 21:50:01.070: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Aug 23 21:50:13.334: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Aug 23 21:50:15.455: INFO: Creating deployment "test-rollover-deployment"
Aug 23 21:50:15.702: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Aug 23 21:50:15.831: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Aug 23 21:50:16.730: INFO: Ensure that both replica sets have 1 created replica
Aug 23 21:50:17.090: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Aug 23 21:50:17.504: INFO: Updating deployment test-rollover-deployment
Aug 23 21:50:17.504: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Aug 23 21:50:17.721: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Aug 23 21:50:18.156: INFO: Make sure deployment "test-rollover-deployment" is complete
Aug 23 21:50:18.506: INFO: all replica sets need to contain the pod-template-hash label
Aug 23 21:50:18.506: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63765352215, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63765352215, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63765352217, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63765352215, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-668db69979\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 23 21:50:21.046: INFO: all replica sets need to contain the pod-template-hash label
Aug 23 21:50:21.046: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63765352215, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63765352215, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63765352217, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63765352215, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-668db69979\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 23 21:50:23.090: INFO: all replica sets need to contain the pod-template-hash label
Aug 23 21:50:23.090: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63765352215, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63765352215, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63765352217, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63765352215, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-668db69979\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 23 21:50:24.964: INFO: all replica sets need to contain the pod-template-hash label
Aug 23 21:50:24.965: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63765352215, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63765352215, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63765352217, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63765352215, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-668db69979\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 23 21:50:26.908: INFO: all replica sets need to contain the pod-template-hash label
Aug 23 21:50:26.908: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63765352215, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63765352215, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63765352225, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63765352215, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-668db69979\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 23 21:50:28.991: INFO: all replica sets need to contain the pod-template-hash label
Aug 23 21:50:28.991: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63765352215, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63765352215, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63765352225, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63765352215, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-668db69979\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 23 21:50:31.081: INFO: all replica sets need to contain the pod-template-hash label
Aug 23 21:50:31.081: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63765352215, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63765352215, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63765352225, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63765352215, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-668db69979\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 23 21:50:32.994: INFO: all replica sets need to contain the pod-template-hash label
Aug 23 21:50:32.995: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63765352215, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63765352215, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63765352225, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63765352215, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-668db69979\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 23 21:50:34.958: INFO: all replica sets need to contain the pod-template-hash label
Aug 23 21:50:34.958: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63765352215, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63765352215, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63765352225, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63765352215, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-668db69979\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 23 21:50:36.971: INFO: 
Aug 23 21:50:36.971: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:79
Aug 23 21:50:37.617: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:{test-rollover-deployment  deployment-9716 /apis/apps/v1/namespaces/deployment-9716/deployments/test-rollover-deployment e047204d-0beb-4827-beec-72ec73676d17 58545 2 2021-08-23 21:50:15 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2021-08-23 21:50:17 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-08-23 21:50:35 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}}}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.21 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00b224478 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2021-08-23 21:50:15 +0000 UTC,LastTransitionTime:2021-08-23 21:50:15 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-668db69979" has successfully progressed.,LastUpdateTime:2021-08-23 21:50:35 +0000 UTC,LastTransitionTime:2021-08-23 21:50:15 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Aug 23 21:50:37.844: INFO: New ReplicaSet "test-rollover-deployment-668db69979" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:{test-rollover-deployment-668db69979  deployment-9716 /apis/apps/v1/namespaces/deployment-9716/replicasets/test-rollover-deployment-668db69979 fb9df24e-6d9d-45f5-acff-367bd4c979bf 58535 2 2021-08-23 21:50:17 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:668db69979] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment e047204d-0beb-4827-beec-72ec73676d17 0xc00b2248e7 0xc00b2248e8}] []  [{kube-controller-manager Update apps/v1 2021-08-23 21:50:35 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e047204d-0beb-4827-beec-72ec73676d17\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 668db69979,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:668db69979] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.21 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00b224978 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Aug 23 21:50:37.844: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Aug 23 21:50:37.844: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-9716 /apis/apps/v1/namespaces/deployment-9716/replicasets/test-rollover-controller 37ebc9fb-d6a7-4396-8ff7-b9585c5218f5 58543 2 2021-08-23 21:50:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment e047204d-0beb-4827-beec-72ec73676d17 0xc00b2247d7 0xc00b2247d8}] []  [{e2e.test Update apps/v1 2021-08-23 21:50:00 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-08-23 21:50:35 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e047204d-0beb-4827-beec-72ec73676d17\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{}},"f:status":{"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc00b224878 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Aug 23 21:50:37.844: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-78bc8b888c  deployment-9716 /apis/apps/v1/namespaces/deployment-9716/replicasets/test-rollover-deployment-78bc8b888c 4ff12c96-480d-4feb-a96e-26af298b344e 58428 2 2021-08-23 21:50:15 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:78bc8b888c] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment e047204d-0beb-4827-beec-72ec73676d17 0xc00b2249e7 0xc00b2249e8}] []  [{kube-controller-manager Update apps/v1 2021-08-23 21:50:17 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e047204d-0beb-4827-beec-72ec73676d17\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 78bc8b888c,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:78bc8b888c] map[] [] []  []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00b224a78 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Aug 23 21:50:38.095: INFO: Pod "test-rollover-deployment-668db69979-zpj7g" is available:
&Pod{ObjectMeta:{test-rollover-deployment-668db69979-zpj7g test-rollover-deployment-668db69979- deployment-9716 /api/v1/namespaces/deployment-9716/pods/test-rollover-deployment-668db69979-zpj7g 69a884f4-21ef-44fb-957e-7461a1628250 58484 0 2021-08-23 21:50:17 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:668db69979] map[cni.projectcalico.org/podIP:172.30.210.172/32 cni.projectcalico.org/podIPs:172.30.210.172/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "",
    "ips": [
        "172.30.210.172"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "",
    "ips": [
        "172.30.210.172"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet test-rollover-deployment-668db69979 fb9df24e-6d9d-45f5-acff-367bd4c979bf 0xc00b24c5d7 0xc00b24c5d8}] []  [{kube-controller-manager Update v1 2021-08-23 21:50:17 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fb9df24e-6d9d-45f5-acff-367bd4c979bf\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{".":{},"f:capabilities":{"f:drop":{}}},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{".":{},"f:seLinuxOptions":{"f:level":{}}},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-08-23 21:50:24 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2021-08-23 21:50:24 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.210.172\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}} {multus Update v1 2021-08-23 21:50:24 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-z8bfv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-z8bfv,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:k8s.gcr.io/e2e-test-images/agnhost:2.21,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-z8bfv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.149.248.9,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c33,c17,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-22jjc,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-08-23 21:50:17 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-08-23 21:50:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-08-23 21:50:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-08-23 21:50:17 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.149.248.9,PodIP:172.30.210.172,StartTime:2021-08-23 21:50:17 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-08-23 21:50:24 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.21,ImageID:k8s.gcr.io/e2e-test-images/agnhost@sha256:ab055cd3d45f50b90732c14593a5bf50f210871bb4f91994c756fc22db6d922a,ContainerID:cri-o://796bf578427ff266cdaa4a7bbad5d4727ce27b2d850da8d2652635c9cd6a765c,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.210.172,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 21:50:38.095: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-9716" for this suite.

• [SLOW TEST:38.476 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support rollover [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should support rollover [Conformance]","total":311,"completed":26,"skipped":354,"failed":0}
[sig-apps] Daemon set [Serial] 
  should retry creating failed daemon pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 21:50:38.771: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:129
[It] should retry creating failed daemon pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Aug 23 21:50:42.861: INFO: Number of nodes with available pods: 0
Aug 23 21:50:42.861: INFO: Node 10.149.248.24 is running more than one daemon pod
Aug 23 21:50:44.367: INFO: Number of nodes with available pods: 0
Aug 23 21:50:44.367: INFO: Node 10.149.248.24 is running more than one daemon pod
Aug 23 21:50:45.270: INFO: Number of nodes with available pods: 0
Aug 23 21:50:45.270: INFO: Node 10.149.248.24 is running more than one daemon pod
Aug 23 21:50:46.133: INFO: Number of nodes with available pods: 0
Aug 23 21:50:46.133: INFO: Node 10.149.248.24 is running more than one daemon pod
Aug 23 21:50:47.115: INFO: Number of nodes with available pods: 1
Aug 23 21:50:47.115: INFO: Node 10.149.248.25 is running more than one daemon pod
Aug 23 21:50:48.059: INFO: Number of nodes with available pods: 1
Aug 23 21:50:48.059: INFO: Node 10.149.248.25 is running more than one daemon pod
Aug 23 21:50:49.064: INFO: Number of nodes with available pods: 1
Aug 23 21:50:49.064: INFO: Node 10.149.248.25 is running more than one daemon pod
Aug 23 21:50:50.237: INFO: Number of nodes with available pods: 2
Aug 23 21:50:50.237: INFO: Node 10.149.248.9 is running more than one daemon pod
Aug 23 21:50:51.136: INFO: Number of nodes with available pods: 2
Aug 23 21:50:51.136: INFO: Node 10.149.248.9 is running more than one daemon pod
Aug 23 21:50:52.204: INFO: Number of nodes with available pods: 2
Aug 23 21:50:52.204: INFO: Node 10.149.248.9 is running more than one daemon pod
Aug 23 21:50:53.236: INFO: Number of nodes with available pods: 3
Aug 23 21:50:53.236: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived.
Aug 23 21:50:54.180: INFO: Number of nodes with available pods: 2
Aug 23 21:50:54.180: INFO: Node 10.149.248.24 is running more than one daemon pod
Aug 23 21:50:55.417: INFO: Number of nodes with available pods: 2
Aug 23 21:50:55.417: INFO: Node 10.149.248.24 is running more than one daemon pod
Aug 23 21:50:56.395: INFO: Number of nodes with available pods: 2
Aug 23 21:50:56.395: INFO: Node 10.149.248.24 is running more than one daemon pod
Aug 23 21:50:57.302: INFO: Number of nodes with available pods: 2
Aug 23 21:50:57.302: INFO: Node 10.149.248.24 is running more than one daemon pod
Aug 23 21:50:58.257: INFO: Number of nodes with available pods: 2
Aug 23 21:50:58.257: INFO: Node 10.149.248.24 is running more than one daemon pod
Aug 23 21:50:59.228: INFO: Number of nodes with available pods: 2
Aug 23 21:50:59.228: INFO: Node 10.149.248.24 is running more than one daemon pod
Aug 23 21:51:00.280: INFO: Number of nodes with available pods: 2
Aug 23 21:51:00.280: INFO: Node 10.149.248.24 is running more than one daemon pod
Aug 23 21:51:01.317: INFO: Number of nodes with available pods: 2
Aug 23 21:51:01.317: INFO: Node 10.149.248.24 is running more than one daemon pod
Aug 23 21:51:02.501: INFO: Number of nodes with available pods: 3
Aug 23 21:51:02.501: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Wait for the failed daemon pod to be completely deleted.
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:95
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-5221, will wait for the garbage collector to delete the pods
Aug 23 21:51:02.944: INFO: Deleting DaemonSet.extensions daemon-set took: 92.287789ms
Aug 23 21:51:03.145: INFO: Terminating DaemonSet.extensions daemon-set pods took: 200.270622ms
Aug 23 21:51:13.759: INFO: Number of nodes with available pods: 0
Aug 23 21:51:13.759: INFO: Number of running nodes: 0, number of available pods: 0
Aug 23 21:51:13.931: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-5221/daemonsets","resourceVersion":"58984"},"items":null}

Aug 23 21:51:14.445: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-5221/pods","resourceVersion":"58985"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 21:51:15.854: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-5221" for this suite.

• [SLOW TEST:37.730 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should retry creating failed daemon pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should retry creating failed daemon pods [Conformance]","total":311,"completed":27,"skipped":354,"failed":0}
S
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should include webhook resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 21:51:16.502: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Aug 23 21:51:20.167: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63765352279, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63765352279, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63765352279, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63765352279, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 23 21:51:22.342: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63765352279, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63765352279, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63765352279, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63765352279, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 23 21:51:24.421: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63765352279, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63765352279, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63765352279, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63765352279, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 23 21:51:26.330: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63765352279, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63765352279, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63765352279, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63765352279, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Aug 23 21:51:29.499: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should include webhook resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: fetching the /apis discovery document
STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/admissionregistration.k8s.io discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document
STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document
STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 21:51:29.991: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7883" for this suite.
STEP: Destroying namespace "webhook-7883-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

• [SLOW TEST:14.786 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should include webhook resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should include webhook resources in discovery documents [Conformance]","total":311,"completed":28,"skipped":355,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 21:51:31.289: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Aug 23 21:51:33.819: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63765352293, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63765352293, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63765352293, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63765352293, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 23 21:51:35.943: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63765352293, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63765352293, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63765352293, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63765352293, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 23 21:51:37.931: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63765352293, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63765352293, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63765352293, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63765352293, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Aug 23 21:51:41.049: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API
STEP: create a namespace for the webhook
STEP: create a configmap should be unconditionally rejected by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 21:51:42.100: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-273" for this suite.
STEP: Destroying namespace "webhook-273-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

• [SLOW TEST:12.073 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should unconditionally reject operations on fail closed webhook [Conformance]","total":311,"completed":29,"skipped":366,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support proportional scaling [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 21:51:43.363: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:85
[It] deployment should support proportional scaling [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Aug 23 21:51:43.970: INFO: Creating deployment "webserver-deployment"
Aug 23 21:51:44.158: INFO: Waiting for observed generation 1
Aug 23 21:51:44.300: INFO: Waiting for all required pods to come up
Aug 23 21:51:44.548: INFO: Pod name httpd: Found 10 pods out of 10
STEP: ensuring each pod is running
Aug 23 21:51:54.937: INFO: Waiting for deployment "webserver-deployment" to complete
Aug 23 21:51:55.234: INFO: Updating deployment "webserver-deployment" with a non-existent image
Aug 23 21:51:55.568: INFO: Updating deployment webserver-deployment
Aug 23 21:51:55.568: INFO: Waiting for observed generation 2
Aug 23 21:51:57.742: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Aug 23 21:51:57.762: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Aug 23 21:51:57.793: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Aug 23 21:51:57.887: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Aug 23 21:51:57.887: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Aug 23 21:51:57.922: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Aug 23 21:51:57.998: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
Aug 23 21:51:57.998: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
Aug 23 21:51:58.073: INFO: Updating deployment webserver-deployment
Aug 23 21:51:58.073: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
Aug 23 21:51:58.125: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Aug 23 21:52:00.228: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:79
Aug 23 21:52:00.317: INFO: Deployment "webserver-deployment":
&Deployment{ObjectMeta:{webserver-deployment  deployment-9669 /apis/apps/v1/namespaces/deployment-9669/deployments/webserver-deployment a38857d9-cf68-4a16-accc-5232095b5de4 60079 3 2021-08-23 21:51:43 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2021-08-23 21:51:43 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-08-23 21:51:55 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}}}]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00b2249f8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:33,UpdatedReplicas:13,AvailableReplicas:8,UnavailableReplicas:25,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2021-08-23 21:51:58 +0000 UTC,LastTransitionTime:2021-08-23 21:51:58 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-795d758f88" is progressing.,LastUpdateTime:2021-08-23 21:51:58 +0000 UTC,LastTransitionTime:2021-08-23 21:51:44 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

Aug 23 21:52:00.361: INFO: New ReplicaSet "webserver-deployment-795d758f88" of Deployment "webserver-deployment":
&ReplicaSet{ObjectMeta:{webserver-deployment-795d758f88  deployment-9669 /apis/apps/v1/namespaces/deployment-9669/replicasets/webserver-deployment-795d758f88 04776711-c57f-4619-9a54-f15492b927e7 60076 3 2021-08-23 21:51:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment a38857d9-cf68-4a16-accc-5232095b5de4 0xc008989047 0xc008989048}] []  [{kube-controller-manager Update apps/v1 2021-08-23 21:51:55 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a38857d9-cf68-4a16-accc-5232095b5de4\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 795d758f88,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0089890c8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:13,FullyLabeledReplicas:13,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Aug 23 21:52:00.361: INFO: All old ReplicaSets of Deployment "webserver-deployment":
Aug 23 21:52:00.361: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-dd94f59b7  deployment-9669 /apis/apps/v1/namespaces/deployment-9669/replicasets/webserver-deployment-dd94f59b7 b9385c37-f291-464c-8a9c-f62597365c25 60026 3 2021-08-23 21:51:44 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment a38857d9-cf68-4a16-accc-5232095b5de4 0xc008989127 0xc008989128}] []  [{kube-controller-manager Update apps/v1 2021-08-23 21:51:47 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a38857d9-cf68-4a16-accc-5232095b5de4\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: dd94f59b7,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc008989198 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:20,FullyLabeledReplicas:20,ObservedGeneration:3,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
Aug 23 21:52:00.547: INFO: Pod "webserver-deployment-795d758f88-2lp5r" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-2lp5r webserver-deployment-795d758f88- deployment-9669 /api/v1/namespaces/deployment-9669/pods/webserver-deployment-795d758f88-2lp5r d848d43c-0aeb-40d9-aac7-fa08895c2c9d 59922 0 2021-08-23 21:51:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 04776711-c57f-4619-9a54-f15492b927e7 0xc00b224dc7 0xc00b224dc8}] []  [{kube-controller-manager Update v1 2021-08-23 21:51:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"04776711-c57f-4619-9a54-f15492b927e7\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{".":{},"f:capabilities":{"f:drop":{}}},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{".":{},"f:seLinuxOptions":{"f:level":{}}},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-08-23 21:51:55 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-84wmt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-84wmt,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-84wmt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.149.248.24,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c34,c19,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-wbkg9,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-08-23 21:51:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-08-23 21:51:55 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-08-23 21:51:55 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-08-23 21:51:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.149.248.24,PodIP:,StartTime:2021-08-23 21:51:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 23 21:52:00.547: INFO: Pod "webserver-deployment-795d758f88-4nzr5" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-4nzr5 webserver-deployment-795d758f88- deployment-9669 /api/v1/namespaces/deployment-9669/pods/webserver-deployment-795d758f88-4nzr5 f34a5428-ed32-4802-8dbe-c0d5608dc959 59929 0 2021-08-23 21:51:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 04776711-c57f-4619-9a54-f15492b927e7 0xc00b224fa7 0xc00b224fa8}] []  [{kube-controller-manager Update v1 2021-08-23 21:51:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"04776711-c57f-4619-9a54-f15492b927e7\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{".":{},"f:capabilities":{"f:drop":{}}},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{".":{},"f:seLinuxOptions":{"f:level":{}}},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-08-23 21:51:55 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-84wmt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-84wmt,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-84wmt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.149.248.25,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c34,c19,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-wbkg9,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-08-23 21:51:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-08-23 21:51:55 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-08-23 21:51:55 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-08-23 21:51:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.149.248.25,PodIP:,StartTime:2021-08-23 21:51:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 23 21:52:00.547: INFO: Pod "webserver-deployment-795d758f88-56drw" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-56drw webserver-deployment-795d758f88- deployment-9669 /api/v1/namespaces/deployment-9669/pods/webserver-deployment-795d758f88-56drw f0707827-2fef-4b33-bef7-ed700abff0f2 60098 0 2021-08-23 21:51:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[cni.projectcalico.org/podIP:172.30.210.181/32 cni.projectcalico.org/podIPs:172.30.210.181/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "",
    "ips": [
        "172.30.210.181"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "",
    "ips": [
        "172.30.210.181"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 04776711-c57f-4619-9a54-f15492b927e7 0xc00b225177 0xc00b225178}] []  [{kube-controller-manager Update v1 2021-08-23 21:51:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"04776711-c57f-4619-9a54-f15492b927e7\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{".":{},"f:capabilities":{"f:drop":{}}},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{".":{},"f:seLinuxOptions":{"f:level":{}}},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-08-23 21:51:55 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}} {calico Update v1 2021-08-23 21:51:59 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {multus Update v1 2021-08-23 21:51:59 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-84wmt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-84wmt,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-84wmt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.149.248.9,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c34,c19,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-wbkg9,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-08-23 21:51:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-08-23 21:51:55 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-08-23 21:51:55 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-08-23 21:51:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.149.248.9,PodIP:,StartTime:2021-08-23 21:51:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 23 21:52:00.548: INFO: Pod "webserver-deployment-795d758f88-6q4hg" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-6q4hg webserver-deployment-795d758f88- deployment-9669 /api/v1/namespaces/deployment-9669/pods/webserver-deployment-795d758f88-6q4hg c6805b8c-8a64-422c-9d3d-c5724d38cd71 60056 0 2021-08-23 21:51:58 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 04776711-c57f-4619-9a54-f15492b927e7 0xc00b225377 0xc00b225378}] []  [{kube-controller-manager Update v1 2021-08-23 21:51:58 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"04776711-c57f-4619-9a54-f15492b927e7\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{".":{},"f:capabilities":{"f:drop":{}}},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{".":{},"f:seLinuxOptions":{"f:level":{}}},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-08-23 21:51:58 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-84wmt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-84wmt,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-84wmt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.149.248.25,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c34,c19,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-wbkg9,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-08-23 21:51:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-08-23 21:51:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-08-23 21:51:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-08-23 21:51:58 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.149.248.25,PodIP:,StartTime:2021-08-23 21:51:58 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 23 21:52:00.549: INFO: Pod "webserver-deployment-795d758f88-7cjkq" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-7cjkq webserver-deployment-795d758f88- deployment-9669 /api/v1/namespaces/deployment-9669/pods/webserver-deployment-795d758f88-7cjkq 37741678-c994-4cd0-8d26-09dfbbfb156a 60084 0 2021-08-23 21:51:58 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 04776711-c57f-4619-9a54-f15492b927e7 0xc00b225547 0xc00b225548}] []  [{kube-controller-manager Update v1 2021-08-23 21:51:58 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"04776711-c57f-4619-9a54-f15492b927e7\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{".":{},"f:capabilities":{"f:drop":{}}},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{".":{},"f:seLinuxOptions":{"f:level":{}}},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-08-23 21:51:58 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-84wmt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-84wmt,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-84wmt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.149.248.24,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c34,c19,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-wbkg9,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-08-23 21:51:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-08-23 21:51:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-08-23 21:51:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-08-23 21:51:58 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.149.248.24,PodIP:,StartTime:2021-08-23 21:51:58 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 23 21:52:00.549: INFO: Pod "webserver-deployment-795d758f88-c85d6" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-c85d6 webserver-deployment-795d758f88- deployment-9669 /api/v1/namespaces/deployment-9669/pods/webserver-deployment-795d758f88-c85d6 645bc022-ac94-4c00-b789-d3d89875bc71 60092 0 2021-08-23 21:51:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[cni.projectcalico.org/podIP:172.30.87.117/32 cni.projectcalico.org/podIPs:172.30.87.117/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "",
    "ips": [
        "172.30.87.117"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "",
    "ips": [
        "172.30.87.117"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 04776711-c57f-4619-9a54-f15492b927e7 0xc00b225737 0xc00b225738}] []  [{kube-controller-manager Update v1 2021-08-23 21:51:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"04776711-c57f-4619-9a54-f15492b927e7\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{".":{},"f:capabilities":{"f:drop":{}}},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{".":{},"f:seLinuxOptions":{"f:level":{}}},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-08-23 21:51:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {multus Update v1 2021-08-23 21:51:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}}} {kubelet Update v1 2021-08-23 21:51:59 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.87.117\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-84wmt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-84wmt,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-84wmt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.149.248.24,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c34,c19,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-wbkg9,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-08-23 21:51:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-08-23 21:51:55 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-08-23 21:51:55 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-08-23 21:51:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.149.248.24,PodIP:172.30.87.117,StartTime:2021-08-23 21:51:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ImagePullBackOff,Message:Back-off pulling image "webserver:404",},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.87.117,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 23 21:52:00.550: INFO: Pod "webserver-deployment-795d758f88-hbwdr" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-hbwdr webserver-deployment-795d758f88- deployment-9669 /api/v1/namespaces/deployment-9669/pods/webserver-deployment-795d758f88-hbwdr 77b22005-4376-4ef5-9b1a-ed9b6d4fdad7 60008 0 2021-08-23 21:51:58 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 04776711-c57f-4619-9a54-f15492b927e7 0xc00b225967 0xc00b225968}] []  [{kube-controller-manager Update v1 2021-08-23 21:51:58 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"04776711-c57f-4619-9a54-f15492b927e7\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{".":{},"f:capabilities":{"f:drop":{}}},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{".":{},"f:seLinuxOptions":{"f:level":{}}},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-08-23 21:51:58 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-84wmt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-84wmt,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-84wmt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.149.248.9,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c34,c19,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-wbkg9,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-08-23 21:51:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-08-23 21:51:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-08-23 21:51:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-08-23 21:51:58 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.149.248.9,PodIP:,StartTime:2021-08-23 21:51:58 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 23 21:52:00.550: INFO: Pod "webserver-deployment-795d758f88-klhrh" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-klhrh webserver-deployment-795d758f88- deployment-9669 /api/v1/namespaces/deployment-9669/pods/webserver-deployment-795d758f88-klhrh 82a367ad-4979-46d1-a594-69b63af74973 60035 0 2021-08-23 21:51:58 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 04776711-c57f-4619-9a54-f15492b927e7 0xc00b225b37 0xc00b225b38}] []  [{kube-controller-manager Update v1 2021-08-23 21:51:58 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"04776711-c57f-4619-9a54-f15492b927e7\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{".":{},"f:capabilities":{"f:drop":{}}},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{".":{},"f:seLinuxOptions":{"f:level":{}}},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-08-23 21:51:58 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-84wmt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-84wmt,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-84wmt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.149.248.9,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c34,c19,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-wbkg9,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-08-23 21:51:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-08-23 21:51:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-08-23 21:51:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-08-23 21:51:58 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.149.248.9,PodIP:,StartTime:2021-08-23 21:51:58 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 23 21:52:00.550: INFO: Pod "webserver-deployment-795d758f88-mjztk" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-mjztk webserver-deployment-795d758f88- deployment-9669 /api/v1/namespaces/deployment-9669/pods/webserver-deployment-795d758f88-mjztk 9cfbcb08-b497-4c52-8ef6-785ebc860d34 60077 0 2021-08-23 21:51:58 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 04776711-c57f-4619-9a54-f15492b927e7 0xc00b225d07 0xc00b225d08}] []  [{kube-controller-manager Update v1 2021-08-23 21:51:58 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"04776711-c57f-4619-9a54-f15492b927e7\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{".":{},"f:capabilities":{"f:drop":{}}},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{".":{},"f:seLinuxOptions":{"f:level":{}}},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-08-23 21:51:58 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-84wmt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-84wmt,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-84wmt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.149.248.9,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c34,c19,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-wbkg9,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-08-23 21:51:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-08-23 21:51:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-08-23 21:51:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-08-23 21:51:58 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.149.248.9,PodIP:,StartTime:2021-08-23 21:51:58 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 23 21:52:00.550: INFO: Pod "webserver-deployment-795d758f88-qrpsd" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-qrpsd webserver-deployment-795d758f88- deployment-9669 /api/v1/namespaces/deployment-9669/pods/webserver-deployment-795d758f88-qrpsd 1ef824bc-2960-4210-9b67-0ddfbc5faf68 60060 0 2021-08-23 21:51:58 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 04776711-c57f-4619-9a54-f15492b927e7 0xc00b225ed7 0xc00b225ed8}] []  [{kube-controller-manager Update v1 2021-08-23 21:51:58 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"04776711-c57f-4619-9a54-f15492b927e7\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{".":{},"f:capabilities":{"f:drop":{}}},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{".":{},"f:seLinuxOptions":{"f:level":{}}},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-08-23 21:51:58 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-84wmt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-84wmt,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-84wmt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.149.248.9,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c34,c19,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-wbkg9,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-08-23 21:51:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-08-23 21:51:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-08-23 21:51:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-08-23 21:51:58 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.149.248.9,PodIP:,StartTime:2021-08-23 21:51:58 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 23 21:52:00.550: INFO: Pod "webserver-deployment-795d758f88-r2g64" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-r2g64 webserver-deployment-795d758f88- deployment-9669 /api/v1/namespaces/deployment-9669/pods/webserver-deployment-795d758f88-r2g64 34fdf03b-92e3-43bf-bc85-b0545b887143 60067 0 2021-08-23 21:51:58 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 04776711-c57f-4619-9a54-f15492b927e7 0xc0014060a7 0xc0014060a8}] []  [{kube-controller-manager Update v1 2021-08-23 21:51:58 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"04776711-c57f-4619-9a54-f15492b927e7\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{".":{},"f:capabilities":{"f:drop":{}}},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{".":{},"f:seLinuxOptions":{"f:level":{}}},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-08-23 21:51:58 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-84wmt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-84wmt,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-84wmt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.149.248.25,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c34,c19,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-wbkg9,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-08-23 21:51:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-08-23 21:51:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-08-23 21:51:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-08-23 21:51:58 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.149.248.25,PodIP:,StartTime:2021-08-23 21:51:58 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 23 21:52:00.551: INFO: Pod "webserver-deployment-795d758f88-tjcvk" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-tjcvk webserver-deployment-795d758f88- deployment-9669 /api/v1/namespaces/deployment-9669/pods/webserver-deployment-795d758f88-tjcvk 03abd458-b44a-4b0f-b08b-fc1911dfecbe 59946 0 2021-08-23 21:51:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 04776711-c57f-4619-9a54-f15492b927e7 0xc001406277 0xc001406278}] []  [{kube-controller-manager Update v1 2021-08-23 21:51:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"04776711-c57f-4619-9a54-f15492b927e7\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{".":{},"f:capabilities":{"f:drop":{}}},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{".":{},"f:seLinuxOptions":{"f:level":{}}},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-08-23 21:51:55 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-84wmt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-84wmt,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-84wmt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.149.248.25,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c34,c19,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-wbkg9,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-08-23 21:51:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-08-23 21:51:55 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-08-23 21:51:55 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-08-23 21:51:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.149.248.25,PodIP:,StartTime:2021-08-23 21:51:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 23 21:52:00.551: INFO: Pod "webserver-deployment-795d758f88-wtfbc" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-wtfbc webserver-deployment-795d758f88- deployment-9669 /api/v1/namespaces/deployment-9669/pods/webserver-deployment-795d758f88-wtfbc 15df23bf-5662-4171-8adf-08fd1490a088 60082 0 2021-08-23 21:51:58 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 04776711-c57f-4619-9a54-f15492b927e7 0xc001406447 0xc001406448}] []  [{kube-controller-manager Update v1 2021-08-23 21:51:58 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"04776711-c57f-4619-9a54-f15492b927e7\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{".":{},"f:capabilities":{"f:drop":{}}},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{".":{},"f:seLinuxOptions":{"f:level":{}}},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-08-23 21:51:58 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-84wmt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-84wmt,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-84wmt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.149.248.24,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c34,c19,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-wbkg9,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-08-23 21:51:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-08-23 21:51:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-08-23 21:51:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-08-23 21:51:58 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.149.248.24,PodIP:,StartTime:2021-08-23 21:51:58 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 23 21:52:00.551: INFO: Pod "webserver-deployment-dd94f59b7-2xdpj" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-2xdpj webserver-deployment-dd94f59b7- deployment-9669 /api/v1/namespaces/deployment-9669/pods/webserver-deployment-dd94f59b7-2xdpj 51f31723-c98c-40d2-9d83-81644df3f8ca 60106 0 2021-08-23 21:51:58 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[cni.projectcalico.org/podIP:172.30.210.180/32 cni.projectcalico.org/podIPs:172.30.210.180/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "",
    "ips": [
        "172.30.210.180"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "",
    "ips": [
        "172.30.210.180"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 b9385c37-f291-464c-8a9c-f62597365c25 0xc001406617 0xc001406618}] []  [{kube-controller-manager Update v1 2021-08-23 21:51:58 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b9385c37-f291-464c-8a9c-f62597365c25\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{".":{},"f:capabilities":{"f:drop":{}}},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{".":{},"f:seLinuxOptions":{"f:level":{}}},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-08-23 21:51:58 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}} {calico Update v1 2021-08-23 21:52:00 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {multus Update v1 2021-08-23 21:52:00 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-84wmt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-84wmt,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-84wmt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.149.248.9,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c34,c19,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-wbkg9,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-08-23 21:51:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-08-23 21:51:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-08-23 21:51:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-08-23 21:51:58 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.149.248.9,PodIP:,StartTime:2021-08-23 21:51:58 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 23 21:52:00.551: INFO: Pod "webserver-deployment-dd94f59b7-5bfjm" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-5bfjm webserver-deployment-dd94f59b7- deployment-9669 /api/v1/namespaces/deployment-9669/pods/webserver-deployment-dd94f59b7-5bfjm 2a6db9c9-b059-4eee-9142-512147c98451 59803 0 2021-08-23 21:51:44 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[cni.projectcalico.org/podIP:172.30.240.114/32 cni.projectcalico.org/podIPs:172.30.240.114/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "",
    "ips": [
        "172.30.240.114"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "",
    "ips": [
        "172.30.240.114"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 b9385c37-f291-464c-8a9c-f62597365c25 0xc0014067f7 0xc0014067f8}] []  [{kube-controller-manager Update v1 2021-08-23 21:51:44 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b9385c37-f291-464c-8a9c-f62597365c25\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{".":{},"f:capabilities":{"f:drop":{}}},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{".":{},"f:seLinuxOptions":{"f:level":{}}},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-08-23 21:51:49 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2021-08-23 21:51:49 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.240.114\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}} {multus Update v1 2021-08-23 21:51:49 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-84wmt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-84wmt,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-84wmt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.149.248.25,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c34,c19,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-wbkg9,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-08-23 21:51:44 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-08-23 21:51:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-08-23 21:51:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-08-23 21:51:44 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.149.248.25,PodIP:172.30.240.114,StartTime:2021-08-23 21:51:44 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-08-23 21:51:49 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:6feb0ea7b0967367da66e8d58ba813fde32bdb92f63bfc21a9e170d211539db4,ContainerID:cri-o://f210a3eac18b30fb39c93f913acc72bd6324ca227b770a9b48873d2488589f4f,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.240.114,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 23 21:52:00.552: INFO: Pod "webserver-deployment-dd94f59b7-5q7ll" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-5q7ll webserver-deployment-dd94f59b7- deployment-9669 /api/v1/namespaces/deployment-9669/pods/webserver-deployment-dd94f59b7-5q7ll f9b8116a-f2f9-4509-a787-d7b952cbf6ec 60036 0 2021-08-23 21:51:58 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 b9385c37-f291-464c-8a9c-f62597365c25 0xc0014069f7 0xc0014069f8}] []  [{kube-controller-manager Update v1 2021-08-23 21:51:58 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b9385c37-f291-464c-8a9c-f62597365c25\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{".":{},"f:capabilities":{"f:drop":{}}},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{".":{},"f:seLinuxOptions":{"f:level":{}}},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-08-23 21:51:58 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-84wmt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-84wmt,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-84wmt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.149.248.24,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c34,c19,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-wbkg9,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-08-23 21:51:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-08-23 21:51:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-08-23 21:51:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-08-23 21:51:58 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.149.248.24,PodIP:,StartTime:2021-08-23 21:51:58 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 23 21:52:00.552: INFO: Pod "webserver-deployment-dd94f59b7-7dxvn" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-7dxvn webserver-deployment-dd94f59b7- deployment-9669 /api/v1/namespaces/deployment-9669/pods/webserver-deployment-dd94f59b7-7dxvn 6ad839f6-e213-486e-adba-1f40fdd21d81 59848 0 2021-08-23 21:51:44 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[cni.projectcalico.org/podIP:172.30.210.179/32 cni.projectcalico.org/podIPs:172.30.210.179/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "",
    "ips": [
        "172.30.210.179"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "",
    "ips": [
        "172.30.210.179"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 b9385c37-f291-464c-8a9c-f62597365c25 0xc001406ba7 0xc001406ba8}] []  [{kube-controller-manager Update v1 2021-08-23 21:51:44 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b9385c37-f291-464c-8a9c-f62597365c25\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{".":{},"f:capabilities":{"f:drop":{}}},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{".":{},"f:seLinuxOptions":{"f:level":{}}},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-08-23 21:51:51 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {multus Update v1 2021-08-23 21:51:51 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}}} {kubelet Update v1 2021-08-23 21:51:52 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.210.179\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-84wmt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-84wmt,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-84wmt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.149.248.9,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c34,c19,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-wbkg9,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-08-23 21:51:44 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-08-23 21:51:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-08-23 21:51:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-08-23 21:51:44 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.149.248.9,PodIP:172.30.210.179,StartTime:2021-08-23 21:51:44 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-08-23 21:51:51 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:6feb0ea7b0967367da66e8d58ba813fde32bdb92f63bfc21a9e170d211539db4,ContainerID:cri-o://6fe2d8a54815aa4a239a2fa617fd5c9bcfa3e07c241e54c807237b2f45591aeb,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.210.179,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 23 21:52:00.552: INFO: Pod "webserver-deployment-dd94f59b7-7qz6m" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-7qz6m webserver-deployment-dd94f59b7- deployment-9669 /api/v1/namespaces/deployment-9669/pods/webserver-deployment-dd94f59b7-7qz6m a0a2bdec-9cb3-4806-bc50-95e13b272dab 60028 0 2021-08-23 21:51:58 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 b9385c37-f291-464c-8a9c-f62597365c25 0xc001406da7 0xc001406da8}] []  [{kube-controller-manager Update v1 2021-08-23 21:51:58 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b9385c37-f291-464c-8a9c-f62597365c25\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{".":{},"f:capabilities":{"f:drop":{}}},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{".":{},"f:seLinuxOptions":{"f:level":{}}},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-08-23 21:51:58 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-84wmt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-84wmt,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-84wmt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.149.248.9,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c34,c19,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-wbkg9,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-08-23 21:51:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-08-23 21:51:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-08-23 21:51:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-08-23 21:51:58 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.149.248.9,PodIP:,StartTime:2021-08-23 21:51:58 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 23 21:52:00.552: INFO: Pod "webserver-deployment-dd94f59b7-7w8ht" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-7w8ht webserver-deployment-dd94f59b7- deployment-9669 /api/v1/namespaces/deployment-9669/pods/webserver-deployment-dd94f59b7-7w8ht 9f69038a-f0d6-4a7a-bdf0-5853159aeddb 59812 0 2021-08-23 21:51:44 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[cni.projectcalico.org/podIP:172.30.210.177/32 cni.projectcalico.org/podIPs:172.30.210.177/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "",
    "ips": [
        "172.30.210.177"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "",
    "ips": [
        "172.30.210.177"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 b9385c37-f291-464c-8a9c-f62597365c25 0xc001406f57 0xc001406f58}] []  [{kube-controller-manager Update v1 2021-08-23 21:51:44 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b9385c37-f291-464c-8a9c-f62597365c25\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{".":{},"f:capabilities":{"f:drop":{}}},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{".":{},"f:seLinuxOptions":{"f:level":{}}},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-08-23 21:51:49 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {multus Update v1 2021-08-23 21:51:49 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}}} {kubelet Update v1 2021-08-23 21:51:50 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.210.177\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-84wmt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-84wmt,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-84wmt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.149.248.9,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c34,c19,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-wbkg9,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-08-23 21:51:44 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-08-23 21:51:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-08-23 21:51:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-08-23 21:51:44 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.149.248.9,PodIP:172.30.210.177,StartTime:2021-08-23 21:51:44 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-08-23 21:51:50 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:6feb0ea7b0967367da66e8d58ba813fde32bdb92f63bfc21a9e170d211539db4,ContainerID:cri-o://ed03b296b465ade0950f8b46ea530025db1e42a94db0998c85db5a8a005bb87b,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.210.177,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 23 21:52:00.552: INFO: Pod "webserver-deployment-dd94f59b7-b2ftp" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-b2ftp webserver-deployment-dd94f59b7- deployment-9669 /api/v1/namespaces/deployment-9669/pods/webserver-deployment-dd94f59b7-b2ftp 6c07d882-32af-436a-b1f0-addd2c34bfb4 60005 0 2021-08-23 21:51:58 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 b9385c37-f291-464c-8a9c-f62597365c25 0xc001407157 0xc001407158}] []  [{kube-controller-manager Update v1 2021-08-23 21:51:58 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b9385c37-f291-464c-8a9c-f62597365c25\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{".":{},"f:capabilities":{"f:drop":{}}},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{".":{},"f:seLinuxOptions":{"f:level":{}}},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-08-23 21:51:58 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-84wmt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-84wmt,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-84wmt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.149.248.25,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c34,c19,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-wbkg9,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-08-23 21:51:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-08-23 21:51:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-08-23 21:51:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-08-23 21:51:58 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.149.248.25,PodIP:,StartTime:2021-08-23 21:51:58 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 23 21:52:00.553: INFO: Pod "webserver-deployment-dd94f59b7-bt2t4" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-bt2t4 webserver-deployment-dd94f59b7- deployment-9669 /api/v1/namespaces/deployment-9669/pods/webserver-deployment-dd94f59b7-bt2t4 d72f1c9f-114a-4e3a-ac5b-843f73eb9baa 59817 0 2021-08-23 21:51:44 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[cni.projectcalico.org/podIP:172.30.87.119/32 cni.projectcalico.org/podIPs:172.30.87.119/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "",
    "ips": [
        "172.30.87.119"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "",
    "ips": [
        "172.30.87.119"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 b9385c37-f291-464c-8a9c-f62597365c25 0xc001407327 0xc001407328}] []  [{kube-controller-manager Update v1 2021-08-23 21:51:44 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b9385c37-f291-464c-8a9c-f62597365c25\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{".":{},"f:capabilities":{"f:drop":{}}},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{".":{},"f:seLinuxOptions":{"f:level":{}}},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-08-23 21:51:49 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {multus Update v1 2021-08-23 21:51:49 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}}} {kubelet Update v1 2021-08-23 21:51:50 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.87.119\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-84wmt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-84wmt,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-84wmt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.149.248.24,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c34,c19,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-wbkg9,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-08-23 21:51:44 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-08-23 21:51:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-08-23 21:51:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-08-23 21:51:44 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.149.248.24,PodIP:172.30.87.119,StartTime:2021-08-23 21:51:44 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-08-23 21:51:49 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:6feb0ea7b0967367da66e8d58ba813fde32bdb92f63bfc21a9e170d211539db4,ContainerID:cri-o://038e2c982c16f1f244b006b0cc4300f667b39143bbbf7bd2d061aa5ef9949010,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.87.119,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 23 21:52:00.553: INFO: Pod "webserver-deployment-dd94f59b7-jz4dw" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-jz4dw webserver-deployment-dd94f59b7- deployment-9669 /api/v1/namespaces/deployment-9669/pods/webserver-deployment-dd94f59b7-jz4dw b388eb25-d3c0-4bd1-9e59-27be46f590ee 59978 0 2021-08-23 21:51:58 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 b9385c37-f291-464c-8a9c-f62597365c25 0xc001407527 0xc001407528}] []  [{kube-controller-manager Update v1 2021-08-23 21:51:58 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b9385c37-f291-464c-8a9c-f62597365c25\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{".":{},"f:capabilities":{"f:drop":{}}},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{".":{},"f:seLinuxOptions":{"f:level":{}}},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-08-23 21:51:58 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-84wmt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-84wmt,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-84wmt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.149.248.25,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c34,c19,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-wbkg9,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-08-23 21:51:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-08-23 21:51:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-08-23 21:51:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-08-23 21:51:58 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.149.248.25,PodIP:,StartTime:2021-08-23 21:51:58 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 23 21:52:00.553: INFO: Pod "webserver-deployment-dd94f59b7-l75sf" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-l75sf webserver-deployment-dd94f59b7- deployment-9669 /api/v1/namespaces/deployment-9669/pods/webserver-deployment-dd94f59b7-l75sf b1f8349a-3e55-450b-8747-9624dececc5e 59725 0 2021-08-23 21:51:44 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[cni.projectcalico.org/podIP:172.30.87.116/32 cni.projectcalico.org/podIPs:172.30.87.116/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "",
    "ips": [
        "172.30.87.116"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "",
    "ips": [
        "172.30.87.116"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 b9385c37-f291-464c-8a9c-f62597365c25 0xc0014076f7 0xc0014076f8}] []  [{kube-controller-manager Update v1 2021-08-23 21:51:44 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b9385c37-f291-464c-8a9c-f62597365c25\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{".":{},"f:capabilities":{"f:drop":{}}},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{".":{},"f:seLinuxOptions":{"f:level":{}}},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-08-23 21:51:47 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {multus Update v1 2021-08-23 21:51:47 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}}} {kubelet Update v1 2021-08-23 21:51:48 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.87.116\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-84wmt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-84wmt,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-84wmt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.149.248.24,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c34,c19,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-wbkg9,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-08-23 21:51:44 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-08-23 21:51:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-08-23 21:51:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-08-23 21:51:44 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.149.248.24,PodIP:172.30.87.116,StartTime:2021-08-23 21:51:44 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-08-23 21:51:47 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:6feb0ea7b0967367da66e8d58ba813fde32bdb92f63bfc21a9e170d211539db4,ContainerID:cri-o://64c58edb3ce1513b6906769cfe3fde2be7a5e7e5562258006fee56d88ac0a3f1,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.87.116,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 23 21:52:00.553: INFO: Pod "webserver-deployment-dd94f59b7-lkz7f" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-lkz7f webserver-deployment-dd94f59b7- deployment-9669 /api/v1/namespaces/deployment-9669/pods/webserver-deployment-dd94f59b7-lkz7f 840a5b43-1caf-4d7a-a5c6-777e77cca62d 59992 0 2021-08-23 21:51:58 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 b9385c37-f291-464c-8a9c-f62597365c25 0xc0014078f7 0xc0014078f8}] []  [{kube-controller-manager Update v1 2021-08-23 21:51:58 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b9385c37-f291-464c-8a9c-f62597365c25\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{".":{},"f:capabilities":{"f:drop":{}}},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{".":{},"f:seLinuxOptions":{"f:level":{}}},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-08-23 21:51:58 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-84wmt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-84wmt,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-84wmt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.149.248.9,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c34,c19,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-wbkg9,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-08-23 21:51:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-08-23 21:51:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-08-23 21:51:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-08-23 21:51:58 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.149.248.9,PodIP:,StartTime:2021-08-23 21:51:58 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 23 21:52:00.554: INFO: Pod "webserver-deployment-dd94f59b7-lpd8n" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-lpd8n webserver-deployment-dd94f59b7- deployment-9669 /api/v1/namespaces/deployment-9669/pods/webserver-deployment-dd94f59b7-lpd8n 81c3eda0-94ca-4e11-8e6c-b965caa17793 59736 0 2021-08-23 21:51:44 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[cni.projectcalico.org/podIP:172.30.87.118/32 cni.projectcalico.org/podIPs:172.30.87.118/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "",
    "ips": [
        "172.30.87.118"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "",
    "ips": [
        "172.30.87.118"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 b9385c37-f291-464c-8a9c-f62597365c25 0xc001407ac7 0xc001407ac8}] []  [{kube-controller-manager Update v1 2021-08-23 21:51:44 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b9385c37-f291-464c-8a9c-f62597365c25\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{".":{},"f:capabilities":{"f:drop":{}}},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{".":{},"f:seLinuxOptions":{"f:level":{}}},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-08-23 21:51:47 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {multus Update v1 2021-08-23 21:51:47 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}}} {kubelet Update v1 2021-08-23 21:51:48 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.87.118\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-84wmt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-84wmt,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-84wmt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.149.248.24,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c34,c19,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-wbkg9,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-08-23 21:51:44 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-08-23 21:51:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-08-23 21:51:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-08-23 21:51:44 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.149.248.24,PodIP:172.30.87.118,StartTime:2021-08-23 21:51:44 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-08-23 21:51:48 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:6feb0ea7b0967367da66e8d58ba813fde32bdb92f63bfc21a9e170d211539db4,ContainerID:cri-o://a9f6f4279c3322ee56a4516216dd6d1187dae41e069a4c5ccc9c50b20ceb2143,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.87.118,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 23 21:52:00.554: INFO: Pod "webserver-deployment-dd94f59b7-m7s5j" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-m7s5j webserver-deployment-dd94f59b7- deployment-9669 /api/v1/namespaces/deployment-9669/pods/webserver-deployment-dd94f59b7-m7s5j 900ab13f-a214-4f85-8791-49216a658d3e 60032 0 2021-08-23 21:51:58 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 b9385c37-f291-464c-8a9c-f62597365c25 0xc001407cc7 0xc001407cc8}] []  [{kube-controller-manager Update v1 2021-08-23 21:51:58 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b9385c37-f291-464c-8a9c-f62597365c25\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{".":{},"f:capabilities":{"f:drop":{}}},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{".":{},"f:seLinuxOptions":{"f:level":{}}},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-08-23 21:51:58 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-84wmt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-84wmt,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-84wmt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.149.248.25,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c34,c19,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-wbkg9,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-08-23 21:51:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-08-23 21:51:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-08-23 21:51:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-08-23 21:51:58 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.149.248.25,PodIP:,StartTime:2021-08-23 21:51:58 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 23 21:52:00.554: INFO: Pod "webserver-deployment-dd94f59b7-mp6bl" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-mp6bl webserver-deployment-dd94f59b7- deployment-9669 /api/v1/namespaces/deployment-9669/pods/webserver-deployment-dd94f59b7-mp6bl a4edc49a-32fe-4350-8529-0401b2ed3245 60022 0 2021-08-23 21:51:58 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 b9385c37-f291-464c-8a9c-f62597365c25 0xc001407e77 0xc001407e78}] []  [{kube-controller-manager Update v1 2021-08-23 21:51:58 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b9385c37-f291-464c-8a9c-f62597365c25\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{".":{},"f:capabilities":{"f:drop":{}}},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{".":{},"f:seLinuxOptions":{"f:level":{}}},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-08-23 21:51:58 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-84wmt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-84wmt,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-84wmt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.149.248.25,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c34,c19,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-wbkg9,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-08-23 21:51:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-08-23 21:51:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-08-23 21:51:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-08-23 21:51:58 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.149.248.25,PodIP:,StartTime:2021-08-23 21:51:58 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 23 21:52:00.554: INFO: Pod "webserver-deployment-dd94f59b7-ngrht" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-ngrht webserver-deployment-dd94f59b7- deployment-9669 /api/v1/namespaces/deployment-9669/pods/webserver-deployment-dd94f59b7-ngrht b099eec5-5f05-4383-84fa-a858042d7d8b 60058 0 2021-08-23 21:51:58 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 b9385c37-f291-464c-8a9c-f62597365c25 0xc00072e0e7 0xc00072e0e8}] []  [{kube-controller-manager Update v1 2021-08-23 21:51:58 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b9385c37-f291-464c-8a9c-f62597365c25\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{".":{},"f:capabilities":{"f:drop":{}}},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{".":{},"f:seLinuxOptions":{"f:level":{}}},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-08-23 21:51:58 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-84wmt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-84wmt,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-84wmt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.149.248.24,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c34,c19,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-wbkg9,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-08-23 21:51:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-08-23 21:51:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-08-23 21:51:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-08-23 21:51:58 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.149.248.24,PodIP:,StartTime:2021-08-23 21:51:58 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 23 21:52:00.555: INFO: Pod "webserver-deployment-dd94f59b7-p9zxh" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-p9zxh webserver-deployment-dd94f59b7- deployment-9669 /api/v1/namespaces/deployment-9669/pods/webserver-deployment-dd94f59b7-p9zxh a780ef7d-3745-4b53-8edc-5190f7b18f19 59829 0 2021-08-23 21:51:44 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[cni.projectcalico.org/podIP:172.30.210.178/32 cni.projectcalico.org/podIPs:172.30.210.178/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "",
    "ips": [
        "172.30.210.178"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "",
    "ips": [
        "172.30.210.178"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 b9385c37-f291-464c-8a9c-f62597365c25 0xc00072e2e7 0xc00072e2e8}] []  [{kube-controller-manager Update v1 2021-08-23 21:51:44 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b9385c37-f291-464c-8a9c-f62597365c25\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{".":{},"f:capabilities":{"f:drop":{}}},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{".":{},"f:seLinuxOptions":{"f:level":{}}},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-08-23 21:51:49 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {multus Update v1 2021-08-23 21:51:50 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}}} {kubelet Update v1 2021-08-23 21:51:51 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.210.178\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-84wmt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-84wmt,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-84wmt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.149.248.9,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c34,c19,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-wbkg9,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-08-23 21:51:44 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-08-23 21:51:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-08-23 21:51:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-08-23 21:51:44 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.149.248.9,PodIP:172.30.210.178,StartTime:2021-08-23 21:51:44 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-08-23 21:51:50 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:6feb0ea7b0967367da66e8d58ba813fde32bdb92f63bfc21a9e170d211539db4,ContainerID:cri-o://08adda06b6f1c781b1adea950eb11e5f0e822d8c4f9258afc575f7f7e3749200,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.210.178,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 23 21:52:00.555: INFO: Pod "webserver-deployment-dd94f59b7-rwnh9" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-rwnh9 webserver-deployment-dd94f59b7- deployment-9669 /api/v1/namespaces/deployment-9669/pods/webserver-deployment-dd94f59b7-rwnh9 789e0f70-b550-4345-8ed4-db5cddfedef9 59701 0 2021-08-23 21:51:44 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[cni.projectcalico.org/podIP:172.30.240.113/32 cni.projectcalico.org/podIPs:172.30.240.113/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "",
    "ips": [
        "172.30.240.113"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "",
    "ips": [
        "172.30.240.113"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 b9385c37-f291-464c-8a9c-f62597365c25 0xc00072e537 0xc00072e538}] []  [{kube-controller-manager Update v1 2021-08-23 21:51:44 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b9385c37-f291-464c-8a9c-f62597365c25\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{".":{},"f:capabilities":{"f:drop":{}}},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{".":{},"f:seLinuxOptions":{"f:level":{}}},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-08-23 21:51:46 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {multus Update v1 2021-08-23 21:51:46 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}}} {kubelet Update v1 2021-08-23 21:51:47 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.240.113\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-84wmt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-84wmt,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-84wmt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.149.248.25,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c34,c19,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-wbkg9,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-08-23 21:51:44 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-08-23 21:51:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-08-23 21:51:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-08-23 21:51:44 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.149.248.25,PodIP:172.30.240.113,StartTime:2021-08-23 21:51:44 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-08-23 21:51:47 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:6feb0ea7b0967367da66e8d58ba813fde32bdb92f63bfc21a9e170d211539db4,ContainerID:cri-o://9cbc0cb0ee574f62ed8d3b1de95ee5814db2114424bf0c41df52ce90a648de07,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.240.113,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 23 21:52:00.555: INFO: Pod "webserver-deployment-dd94f59b7-s6dpn" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-s6dpn webserver-deployment-dd94f59b7- deployment-9669 /api/v1/namespaces/deployment-9669/pods/webserver-deployment-dd94f59b7-s6dpn 951a23dd-857c-4638-aa44-6e517a2ee642 60075 0 2021-08-23 21:51:58 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 b9385c37-f291-464c-8a9c-f62597365c25 0xc00072e777 0xc00072e778}] []  [{kube-controller-manager Update v1 2021-08-23 21:51:58 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b9385c37-f291-464c-8a9c-f62597365c25\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{".":{},"f:capabilities":{"f:drop":{}}},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{".":{},"f:seLinuxOptions":{"f:level":{}}},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-08-23 21:51:58 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-84wmt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-84wmt,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-84wmt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.149.248.24,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c34,c19,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-wbkg9,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-08-23 21:51:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-08-23 21:51:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-08-23 21:51:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-08-23 21:51:58 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.149.248.24,PodIP:,StartTime:2021-08-23 21:51:58 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 23 21:52:00.555: INFO: Pod "webserver-deployment-dd94f59b7-vhmgg" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-vhmgg webserver-deployment-dd94f59b7- deployment-9669 /api/v1/namespaces/deployment-9669/pods/webserver-deployment-dd94f59b7-vhmgg 0738d418-3318-419c-9a29-c8ac9848c57e 60081 0 2021-08-23 21:51:58 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 b9385c37-f291-464c-8a9c-f62597365c25 0xc00072e957 0xc00072e958}] []  [{kube-controller-manager Update v1 2021-08-23 21:51:58 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b9385c37-f291-464c-8a9c-f62597365c25\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{".":{},"f:capabilities":{"f:drop":{}}},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{".":{},"f:seLinuxOptions":{"f:level":{}}},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-08-23 21:51:58 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-84wmt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-84wmt,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-84wmt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.149.248.24,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c34,c19,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-wbkg9,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-08-23 21:51:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-08-23 21:51:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-08-23 21:51:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-08-23 21:51:58 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.149.248.24,PodIP:,StartTime:2021-08-23 21:51:58 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 23 21:52:00.556: INFO: Pod "webserver-deployment-dd94f59b7-xbsc8" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-xbsc8 webserver-deployment-dd94f59b7- deployment-9669 /api/v1/namespaces/deployment-9669/pods/webserver-deployment-dd94f59b7-xbsc8 b34bbc19-87b9-4599-9cf5-6e68f019747a 59995 0 2021-08-23 21:51:58 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 b9385c37-f291-464c-8a9c-f62597365c25 0xc00072ec47 0xc00072ec48}] []  [{kube-controller-manager Update v1 2021-08-23 21:51:58 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b9385c37-f291-464c-8a9c-f62597365c25\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{".":{},"f:capabilities":{"f:drop":{}}},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{".":{},"f:seLinuxOptions":{"f:level":{}}},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-08-23 21:51:58 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-84wmt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-84wmt,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-84wmt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.149.248.25,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c34,c19,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-wbkg9,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-08-23 21:51:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-08-23 21:51:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-08-23 21:51:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-08-23 21:51:58 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.149.248.25,PodIP:,StartTime:2021-08-23 21:51:58 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 21:52:00.556: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-9669" for this suite.

• [SLOW TEST:17.399 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support proportional scaling [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should support proportional scaling [Conformance]","total":311,"completed":30,"skipped":423,"failed":0}
SSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 21:52:00.763: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:53
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating pod liveness-0c697d28-be23-4f13-9c98-19866079ec3a in namespace container-probe-3675
Aug 23 21:52:07.233: INFO: Started pod liveness-0c697d28-be23-4f13-9c98-19866079ec3a in namespace container-probe-3675
STEP: checking the pod's current state and verifying that restartCount is present
Aug 23 21:52:07.291: INFO: Initial restart count of pod liveness-0c697d28-be23-4f13-9c98-19866079ec3a is 0
Aug 23 21:52:24.179: INFO: Restart count of pod container-probe-3675/liveness-0c697d28-be23-4f13-9c98-19866079ec3a is now 1 (16.88216302s elapsed)
Aug 23 21:52:44.386: INFO: Restart count of pod container-probe-3675/liveness-0c697d28-be23-4f13-9c98-19866079ec3a is now 2 (37.088651104s elapsed)
Aug 23 21:53:04.010: INFO: Restart count of pod container-probe-3675/liveness-0c697d28-be23-4f13-9c98-19866079ec3a is now 3 (56.713105248s elapsed)
Aug 23 21:53:24.670: INFO: Restart count of pod container-probe-3675/liveness-0c697d28-be23-4f13-9c98-19866079ec3a is now 4 (1m17.373017943s elapsed)
Aug 23 21:54:34.685: INFO: Restart count of pod container-probe-3675/liveness-0c697d28-be23-4f13-9c98-19866079ec3a is now 5 (2m27.387321443s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 21:54:34.738: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-3675" for this suite.

• [SLOW TEST:154.189 seconds]
[k8s.io] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Probing container should have monotonically increasing restart count [NodeConformance] [Conformance]","total":311,"completed":31,"skipped":436,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 21:54:34.953: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating service in namespace services-2296
STEP: creating service affinity-clusterip-transition in namespace services-2296
STEP: creating replication controller affinity-clusterip-transition in namespace services-2296
I0823 21:54:35.208755      25 runners.go:190] Created replication controller with name: affinity-clusterip-transition, namespace: services-2296, replica count: 3
I0823 21:54:38.259129      25 runners.go:190] affinity-clusterip-transition Pods: 3 out of 3 created, 0 running, 3 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0823 21:54:41.259341      25 runners.go:190] affinity-clusterip-transition Pods: 3 out of 3 created, 1 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0823 21:54:44.259659      25 runners.go:190] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Aug 23 21:54:44.689: INFO: Creating new exec pod
Aug 23 21:54:50.450: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585719795 --namespace=services-2296 exec execpod-affinitydb6pl -- /bin/sh -x -c nc -zv -t -w 2 affinity-clusterip-transition 80'
Aug 23 21:54:52.164: INFO: stderr: "+ nc -zv -t -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
Aug 23 21:54:52.164: INFO: stdout: ""
Aug 23 21:54:52.165: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585719795 --namespace=services-2296 exec execpod-affinitydb6pl -- /bin/sh -x -c nc -zv -t -w 2 172.21.30.208 80'
Aug 23 21:54:52.878: INFO: stderr: "+ nc -zv -t -w 2 172.21.30.208 80\nConnection to 172.21.30.208 80 port [tcp/http] succeeded!\n"
Aug 23 21:54:52.878: INFO: stdout: ""
Aug 23 21:54:53.143: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585719795 --namespace=services-2296 exec execpod-affinitydb6pl -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.21.30.208:80/ ; done'
Aug 23 21:54:55.186: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.30.208:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.30.208:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.30.208:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.30.208:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.30.208:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.30.208:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.30.208:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.30.208:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.30.208:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.30.208:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.30.208:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.30.208:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.30.208:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.30.208:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.30.208:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.30.208:80/\n"
Aug 23 21:54:55.186: INFO: stdout: "\naffinity-clusterip-transition-ckzdh\naffinity-clusterip-transition-ckzdh\naffinity-clusterip-transition-zvg2l\naffinity-clusterip-transition-ckzdh\naffinity-clusterip-transition-t9s78\naffinity-clusterip-transition-t9s78\naffinity-clusterip-transition-zvg2l\naffinity-clusterip-transition-zvg2l\naffinity-clusterip-transition-zvg2l\naffinity-clusterip-transition-ckzdh\naffinity-clusterip-transition-zvg2l\naffinity-clusterip-transition-ckzdh\naffinity-clusterip-transition-zvg2l\naffinity-clusterip-transition-zvg2l\naffinity-clusterip-transition-ckzdh\naffinity-clusterip-transition-zvg2l"
Aug 23 21:54:55.186: INFO: Received response from host: affinity-clusterip-transition-ckzdh
Aug 23 21:54:55.186: INFO: Received response from host: affinity-clusterip-transition-ckzdh
Aug 23 21:54:55.186: INFO: Received response from host: affinity-clusterip-transition-zvg2l
Aug 23 21:54:55.186: INFO: Received response from host: affinity-clusterip-transition-ckzdh
Aug 23 21:54:55.186: INFO: Received response from host: affinity-clusterip-transition-t9s78
Aug 23 21:54:55.186: INFO: Received response from host: affinity-clusterip-transition-t9s78
Aug 23 21:54:55.186: INFO: Received response from host: affinity-clusterip-transition-zvg2l
Aug 23 21:54:55.186: INFO: Received response from host: affinity-clusterip-transition-zvg2l
Aug 23 21:54:55.186: INFO: Received response from host: affinity-clusterip-transition-zvg2l
Aug 23 21:54:55.186: INFO: Received response from host: affinity-clusterip-transition-ckzdh
Aug 23 21:54:55.186: INFO: Received response from host: affinity-clusterip-transition-zvg2l
Aug 23 21:54:55.186: INFO: Received response from host: affinity-clusterip-transition-ckzdh
Aug 23 21:54:55.186: INFO: Received response from host: affinity-clusterip-transition-zvg2l
Aug 23 21:54:55.187: INFO: Received response from host: affinity-clusterip-transition-zvg2l
Aug 23 21:54:55.187: INFO: Received response from host: affinity-clusterip-transition-ckzdh
Aug 23 21:54:55.187: INFO: Received response from host: affinity-clusterip-transition-zvg2l
Aug 23 21:54:55.464: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585719795 --namespace=services-2296 exec execpod-affinitydb6pl -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.21.30.208:80/ ; done'
Aug 23 21:54:56.538: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.30.208:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.30.208:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.30.208:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.30.208:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.30.208:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.30.208:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.30.208:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.30.208:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.30.208:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.30.208:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.30.208:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.30.208:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.30.208:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.30.208:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.30.208:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.30.208:80/\n"
Aug 23 21:54:56.538: INFO: stdout: "\naffinity-clusterip-transition-zvg2l\naffinity-clusterip-transition-zvg2l\naffinity-clusterip-transition-zvg2l\naffinity-clusterip-transition-zvg2l\naffinity-clusterip-transition-zvg2l\naffinity-clusterip-transition-zvg2l\naffinity-clusterip-transition-zvg2l\naffinity-clusterip-transition-zvg2l\naffinity-clusterip-transition-zvg2l\naffinity-clusterip-transition-zvg2l\naffinity-clusterip-transition-zvg2l\naffinity-clusterip-transition-zvg2l\naffinity-clusterip-transition-zvg2l\naffinity-clusterip-transition-zvg2l\naffinity-clusterip-transition-zvg2l\naffinity-clusterip-transition-zvg2l"
Aug 23 21:54:56.538: INFO: Received response from host: affinity-clusterip-transition-zvg2l
Aug 23 21:54:56.538: INFO: Received response from host: affinity-clusterip-transition-zvg2l
Aug 23 21:54:56.538: INFO: Received response from host: affinity-clusterip-transition-zvg2l
Aug 23 21:54:56.538: INFO: Received response from host: affinity-clusterip-transition-zvg2l
Aug 23 21:54:56.538: INFO: Received response from host: affinity-clusterip-transition-zvg2l
Aug 23 21:54:56.538: INFO: Received response from host: affinity-clusterip-transition-zvg2l
Aug 23 21:54:56.538: INFO: Received response from host: affinity-clusterip-transition-zvg2l
Aug 23 21:54:56.538: INFO: Received response from host: affinity-clusterip-transition-zvg2l
Aug 23 21:54:56.538: INFO: Received response from host: affinity-clusterip-transition-zvg2l
Aug 23 21:54:56.538: INFO: Received response from host: affinity-clusterip-transition-zvg2l
Aug 23 21:54:56.538: INFO: Received response from host: affinity-clusterip-transition-zvg2l
Aug 23 21:54:56.538: INFO: Received response from host: affinity-clusterip-transition-zvg2l
Aug 23 21:54:56.538: INFO: Received response from host: affinity-clusterip-transition-zvg2l
Aug 23 21:54:56.538: INFO: Received response from host: affinity-clusterip-transition-zvg2l
Aug 23 21:54:56.538: INFO: Received response from host: affinity-clusterip-transition-zvg2l
Aug 23 21:54:56.538: INFO: Received response from host: affinity-clusterip-transition-zvg2l
Aug 23 21:54:56.538: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-2296, will wait for the garbage collector to delete the pods
Aug 23 21:54:57.328: INFO: Deleting ReplicationController affinity-clusterip-transition took: 116.677481ms
Aug 23 21:54:57.528: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 200.156981ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 21:55:16.742: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-2296" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749

• [SLOW TEST:43.215 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","total":311,"completed":32,"skipped":459,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 21:55:18.170: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-2367
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a new StatefulSet
Aug 23 21:55:19.408: INFO: Found 1 stateful pods, waiting for 3
Aug 23 21:55:29.432: INFO: Found 2 stateful pods, waiting for 3
Aug 23 21:55:39.436: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Aug 23 21:55:39.436: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Aug 23 21:55:39.436: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from docker.io/library/httpd:2.4.38-alpine to docker.io/library/httpd:2.4.39-alpine
Aug 23 21:55:39.556: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Not applying an update when the partition is greater than the number of replicas
STEP: Performing a canary update
Aug 23 21:55:50.480: INFO: Updating stateful set ss2
Aug 23 21:55:50.765: INFO: Waiting for Pod statefulset-2367/ss2-2 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
STEP: Restoring Pods to the correct revision when they are deleted
Aug 23 21:56:01.542: INFO: Found 2 stateful pods, waiting for 3
Aug 23 21:56:11.672: INFO: Found 2 stateful pods, waiting for 3
Aug 23 21:56:21.813: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Aug 23 21:56:21.813: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Aug 23 21:56:21.813: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Pending - Ready=false
Aug 23 21:56:31.787: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Aug 23 21:56:31.787: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Aug 23 21:56:31.787: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update
Aug 23 21:56:32.384: INFO: Updating stateful set ss2
Aug 23 21:56:32.667: INFO: Waiting for Pod statefulset-2367/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Aug 23 21:56:42.824: INFO: Updating stateful set ss2
Aug 23 21:56:42.892: INFO: Waiting for StatefulSet statefulset-2367/ss2 to complete update
Aug 23 21:56:42.892: INFO: Waiting for Pod statefulset-2367/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Aug 23 21:56:52.968: INFO: Waiting for StatefulSet statefulset-2367/ss2 to complete update
Aug 23 21:56:52.968: INFO: Waiting for Pod statefulset-2367/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Aug 23 21:57:03.316: INFO: Waiting for StatefulSet statefulset-2367/ss2 to complete update
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
Aug 23 21:57:13.260: INFO: Deleting all statefulset in ns statefulset-2367
Aug 23 21:57:13.421: INFO: Scaling statefulset ss2 to 0
Aug 23 21:57:54.378: INFO: Waiting for statefulset status.replicas updated to 0
Aug 23 21:57:54.397: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 21:57:54.468: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-2367" for this suite.

• [SLOW TEST:156.385 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should perform canary updates and phased rolling updates of template modifications [Conformance]","total":311,"completed":33,"skipped":492,"failed":0}
SS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 21:57:54.556: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name projected-configmap-test-volume-7b8d1cb8-0397-4313-a46d-0c3873c3dd90
STEP: Creating a pod to test consume configMaps
Aug 23 21:57:54.850: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-4ddb28a3-eeae-4bb0-bd5a-f85eaac6a258" in namespace "projected-8104" to be "Succeeded or Failed"
Aug 23 21:57:54.877: INFO: Pod "pod-projected-configmaps-4ddb28a3-eeae-4bb0-bd5a-f85eaac6a258": Phase="Pending", Reason="", readiness=false. Elapsed: 27.076367ms
Aug 23 21:57:56.936: INFO: Pod "pod-projected-configmaps-4ddb28a3-eeae-4bb0-bd5a-f85eaac6a258": Phase="Pending", Reason="", readiness=false. Elapsed: 2.085737392s
Aug 23 21:57:58.965: INFO: Pod "pod-projected-configmaps-4ddb28a3-eeae-4bb0-bd5a-f85eaac6a258": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.114980204s
STEP: Saw pod success
Aug 23 21:57:58.965: INFO: Pod "pod-projected-configmaps-4ddb28a3-eeae-4bb0-bd5a-f85eaac6a258" satisfied condition "Succeeded or Failed"
Aug 23 21:57:58.994: INFO: Trying to get logs from node 10.149.248.9 pod pod-projected-configmaps-4ddb28a3-eeae-4bb0-bd5a-f85eaac6a258 container agnhost-container: <nil>
STEP: delete the pod
Aug 23 21:57:59.151: INFO: Waiting for pod pod-projected-configmaps-4ddb28a3-eeae-4bb0-bd5a-f85eaac6a258 to disappear
Aug 23 21:57:59.183: INFO: Pod pod-projected-configmaps-4ddb28a3-eeae-4bb0-bd5a-f85eaac6a258 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 21:57:59.183: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8104" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":34,"skipped":494,"failed":0}
SSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 21:57:59.469: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating pod pod-subpath-test-downwardapi-9n2r
STEP: Creating a pod to test atomic-volume-subpath
Aug 23 21:57:59.756: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-9n2r" in namespace "subpath-562" to be "Succeeded or Failed"
Aug 23 21:57:59.779: INFO: Pod "pod-subpath-test-downwardapi-9n2r": Phase="Pending", Reason="", readiness=false. Elapsed: 23.269146ms
Aug 23 21:58:01.794: INFO: Pod "pod-subpath-test-downwardapi-9n2r": Phase="Pending", Reason="", readiness=false. Elapsed: 2.038651143s
Aug 23 21:58:03.882: INFO: Pod "pod-subpath-test-downwardapi-9n2r": Phase="Pending", Reason="", readiness=false. Elapsed: 4.125911622s
Aug 23 21:58:06.042: INFO: Pod "pod-subpath-test-downwardapi-9n2r": Phase="Running", Reason="", readiness=true. Elapsed: 6.286664593s
Aug 23 21:58:08.163: INFO: Pod "pod-subpath-test-downwardapi-9n2r": Phase="Running", Reason="", readiness=true. Elapsed: 8.406819681s
Aug 23 21:58:10.358: INFO: Pod "pod-subpath-test-downwardapi-9n2r": Phase="Running", Reason="", readiness=true. Elapsed: 10.602593514s
Aug 23 21:58:12.484: INFO: Pod "pod-subpath-test-downwardapi-9n2r": Phase="Running", Reason="", readiness=true. Elapsed: 12.727743788s
Aug 23 21:58:14.591: INFO: Pod "pod-subpath-test-downwardapi-9n2r": Phase="Running", Reason="", readiness=true. Elapsed: 14.835455627s
Aug 23 21:58:16.700: INFO: Pod "pod-subpath-test-downwardapi-9n2r": Phase="Running", Reason="", readiness=true. Elapsed: 16.944341261s
Aug 23 21:58:18.797: INFO: Pod "pod-subpath-test-downwardapi-9n2r": Phase="Running", Reason="", readiness=true. Elapsed: 19.041628965s
Aug 23 21:58:20.925: INFO: Pod "pod-subpath-test-downwardapi-9n2r": Phase="Running", Reason="", readiness=true. Elapsed: 21.169142076s
Aug 23 21:58:23.062: INFO: Pod "pod-subpath-test-downwardapi-9n2r": Phase="Running", Reason="", readiness=true. Elapsed: 23.306234842s
Aug 23 21:58:25.192: INFO: Pod "pod-subpath-test-downwardapi-9n2r": Phase="Running", Reason="", readiness=true. Elapsed: 25.436114667s
Aug 23 21:58:27.329: INFO: Pod "pod-subpath-test-downwardapi-9n2r": Phase="Succeeded", Reason="", readiness=false. Elapsed: 27.573142479s
STEP: Saw pod success
Aug 23 21:58:27.329: INFO: Pod "pod-subpath-test-downwardapi-9n2r" satisfied condition "Succeeded or Failed"
Aug 23 21:58:27.469: INFO: Trying to get logs from node 10.149.248.9 pod pod-subpath-test-downwardapi-9n2r container test-container-subpath-downwardapi-9n2r: <nil>
STEP: delete the pod
Aug 23 21:58:27.762: INFO: Waiting for pod pod-subpath-test-downwardapi-9n2r to disappear
Aug 23 21:58:27.900: INFO: Pod pod-subpath-test-downwardapi-9n2r no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-9n2r
Aug 23 21:58:27.900: INFO: Deleting pod "pod-subpath-test-downwardapi-9n2r" in namespace "subpath-562"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 21:58:28.036: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-562" for this suite.

• [SLOW TEST:30.246 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with downward pod [LinuxOnly] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with downward pod [LinuxOnly] [Conformance]","total":311,"completed":35,"skipped":497,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 21:58:29.715: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test override command
Aug 23 21:58:30.370: INFO: Waiting up to 5m0s for pod "client-containers-0118eab0-b141-4eda-986b-9bba6a240fc9" in namespace "containers-3877" to be "Succeeded or Failed"
Aug 23 21:58:30.483: INFO: Pod "client-containers-0118eab0-b141-4eda-986b-9bba6a240fc9": Phase="Pending", Reason="", readiness=false. Elapsed: 113.224684ms
Aug 23 21:58:32.595: INFO: Pod "client-containers-0118eab0-b141-4eda-986b-9bba6a240fc9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.225611591s
Aug 23 21:58:34.712: INFO: Pod "client-containers-0118eab0-b141-4eda-986b-9bba6a240fc9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.342368956s
Aug 23 21:58:36.822: INFO: Pod "client-containers-0118eab0-b141-4eda-986b-9bba6a240fc9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.452450621s
STEP: Saw pod success
Aug 23 21:58:36.822: INFO: Pod "client-containers-0118eab0-b141-4eda-986b-9bba6a240fc9" satisfied condition "Succeeded or Failed"
Aug 23 21:58:36.943: INFO: Trying to get logs from node 10.149.248.9 pod client-containers-0118eab0-b141-4eda-986b-9bba6a240fc9 container agnhost-container: <nil>
STEP: delete the pod
Aug 23 21:58:37.218: INFO: Waiting for pod client-containers-0118eab0-b141-4eda-986b-9bba6a240fc9 to disappear
Aug 23 21:58:37.326: INFO: Pod client-containers-0118eab0-b141-4eda-986b-9bba6a240fc9 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 21:58:37.326: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-3877" for this suite.

• [SLOW TEST:8.198 seconds]
[k8s.io] Docker Containers
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Docker Containers should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]","total":311,"completed":36,"skipped":517,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: udp [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 21:58:37.913: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Performing setup for networking test in namespace pod-network-test-285
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Aug 23 21:58:38.460: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Aug 23 21:58:40.228: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Aug 23 21:58:42.391: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Aug 23 21:58:44.373: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Aug 23 21:58:46.331: INFO: The status of Pod netserver-0 is Running (Ready = false)
Aug 23 21:58:48.327: INFO: The status of Pod netserver-0 is Running (Ready = false)
Aug 23 21:58:50.323: INFO: The status of Pod netserver-0 is Running (Ready = false)
Aug 23 21:58:52.384: INFO: The status of Pod netserver-0 is Running (Ready = false)
Aug 23 21:58:54.374: INFO: The status of Pod netserver-0 is Running (Ready = false)
Aug 23 21:58:56.271: INFO: The status of Pod netserver-0 is Running (Ready = false)
Aug 23 21:58:58.262: INFO: The status of Pod netserver-0 is Running (Ready = false)
Aug 23 21:59:00.255: INFO: The status of Pod netserver-0 is Running (Ready = false)
Aug 23 21:59:02.245: INFO: The status of Pod netserver-0 is Running (Ready = false)
Aug 23 21:59:04.245: INFO: The status of Pod netserver-0 is Running (Ready = true)
Aug 23 21:59:04.280: INFO: The status of Pod netserver-1 is Running (Ready = true)
Aug 23 21:59:04.328: INFO: The status of Pod netserver-2 is Running (Ready = true)
STEP: Creating test pods
Aug 23 21:59:10.516: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
Aug 23 21:59:10.516: INFO: Breadth first check of 172.30.87.99 on host 10.149.248.24...
Aug 23 21:59:10.553: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.210.149:9080/dial?request=hostname&protocol=udp&host=172.30.87.99&port=8081&tries=1'] Namespace:pod-network-test-285 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 23 21:59:10.553: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
Aug 23 21:59:11.811: INFO: Waiting for responses: map[]
Aug 23 21:59:11.811: INFO: reached 172.30.87.99 after 0/1 tries
Aug 23 21:59:11.811: INFO: Breadth first check of 172.30.240.90 on host 10.149.248.25...
Aug 23 21:59:11.879: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.210.149:9080/dial?request=hostname&protocol=udp&host=172.30.240.90&port=8081&tries=1'] Namespace:pod-network-test-285 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 23 21:59:11.880: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
Aug 23 21:59:12.061: INFO: Waiting for responses: map[]
Aug 23 21:59:12.062: INFO: reached 172.30.240.90 after 0/1 tries
Aug 23 21:59:12.062: INFO: Breadth first check of 172.30.210.148 on host 10.149.248.9...
Aug 23 21:59:12.111: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.210.149:9080/dial?request=hostname&protocol=udp&host=172.30.210.148&port=8081&tries=1'] Namespace:pod-network-test-285 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 23 21:59:12.111: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
Aug 23 21:59:12.945: INFO: Waiting for responses: map[]
Aug 23 21:59:12.945: INFO: reached 172.30.210.148 after 0/1 tries
Aug 23 21:59:12.945: INFO: Going to retry 0 out of 3 pods....
[AfterEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 21:59:12.945: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-285" for this suite.

• [SLOW TEST:35.336 seconds]
[sig-network] Networking
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:27
  Granular Checks: Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:30
    should function for intra-pod communication: udp [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: udp [NodeConformance] [Conformance]","total":311,"completed":37,"skipped":532,"failed":0}
SSS
------------------------------
[sig-auth] ServiceAccounts 
  should run through the lifecycle of a ServiceAccount [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 21:59:13.250: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run through the lifecycle of a ServiceAccount [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a ServiceAccount
STEP: watching for the ServiceAccount to be added
STEP: patching the ServiceAccount
STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector)
STEP: deleting the ServiceAccount
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 21:59:13.982: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-6794" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should run through the lifecycle of a ServiceAccount [Conformance]","total":311,"completed":38,"skipped":535,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should delete a collection of pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 21:59:14.469: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:187
[It] should delete a collection of pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Create set of pods
Aug 23 21:59:15.406: INFO: created test-pod-1
Aug 23 21:59:15.599: INFO: created test-pod-2
Aug 23 21:59:15.749: INFO: created test-pod-3
STEP: waiting for all 3 pods to be located
STEP: waiting for all pods to be deleted
[AfterEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 21:59:16.117: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-9253" for this suite.
•{"msg":"PASSED [k8s.io] Pods should delete a collection of pods [Conformance]","total":311,"completed":39,"skipped":554,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 21:59:16.455: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:41
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating the pod
Aug 23 21:59:22.606: INFO: Successfully updated pod "labelsupdatea7a15213-ddc7-4518-8459-cd789a90c898"
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 21:59:24.930: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2237" for this suite.

• [SLOW TEST:9.014 seconds]
[sig-storage] Downward API volume
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:36
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should update labels on modification [NodeConformance] [Conformance]","total":311,"completed":40,"skipped":564,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for ExternalName services [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 21:59:25.470: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for ExternalName services [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a test externalName service
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-1951.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-1951.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-1951.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-1951.svc.cluster.local; sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Aug 23 21:59:33.509: INFO: DNS probes using dns-test-cc7d345d-b89c-42a7-9a6a-ee40116714ff succeeded

STEP: deleting the pod
STEP: changing the externalName to bar.example.com
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-1951.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-1951.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-1951.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-1951.svc.cluster.local; sleep 1; done

STEP: creating a second pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Aug 23 21:59:38.548: INFO: DNS probes using dns-test-ce030218-4409-4ed5-a1d7-95bbdc046a3c succeeded

STEP: deleting the pod
STEP: changing the service to type=ClusterIP
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-1951.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-1951.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-1951.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-1951.svc.cluster.local; sleep 1; done

STEP: creating a third pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Aug 23 21:59:51.651: INFO: File jessie_udp@dns-test-service-3.dns-1951.svc.cluster.local from pod  dns-1951/dns-test-a4ea779c-3882-459e-b981-e09e1d081690 contains '' instead of '172.21.152.249'
Aug 23 21:59:51.651: INFO: Lookups using dns-1951/dns-test-a4ea779c-3882-459e-b981-e09e1d081690 failed for: [jessie_udp@dns-test-service-3.dns-1951.svc.cluster.local]

Aug 23 21:59:56.772: INFO: DNS probes using dns-test-a4ea779c-3882-459e-b981-e09e1d081690 succeeded

STEP: deleting the pod
STEP: deleting the test externalName service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 21:59:56.975: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-1951" for this suite.

• [SLOW TEST:31.736 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for ExternalName services [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for ExternalName services [Conformance]","total":311,"completed":41,"skipped":590,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 21:59:57.209: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a service externalname-service with the type=ExternalName in namespace services-976
STEP: changing the ExternalName service to type=NodePort
STEP: creating replication controller externalname-service in namespace services-976
I0823 21:59:57.656343      25 runners.go:190] Created replication controller with name: externalname-service, namespace: services-976, replica count: 2
I0823 22:00:00.706671      25 runners.go:190] externalname-service Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0823 22:00:03.707055      25 runners.go:190] externalname-service Pods: 2 out of 2 created, 1 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0823 22:00:06.707656      25 runners.go:190] externalname-service Pods: 2 out of 2 created, 1 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0823 22:00:09.708057      25 runners.go:190] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Aug 23 22:00:09.708: INFO: Creating new exec pod
Aug 23 22:00:20.659: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585719795 --namespace=services-976 exec execpod9q87n -- /bin/sh -x -c nc -zv -t -w 2 externalname-service 80'
Aug 23 22:00:22.934: INFO: stderr: "+ nc -zv -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Aug 23 22:00:22.934: INFO: stdout: ""
Aug 23 22:00:22.934: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585719795 --namespace=services-976 exec execpod9q87n -- /bin/sh -x -c nc -zv -t -w 2 172.21.219.212 80'
Aug 23 22:00:24.948: INFO: stderr: "+ nc -zv -t -w 2 172.21.219.212 80\nConnection to 172.21.219.212 80 port [tcp/http] succeeded!\n"
Aug 23 22:00:24.948: INFO: stdout: ""
Aug 23 22:00:24.948: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585719795 --namespace=services-976 exec execpod9q87n -- /bin/sh -x -c nc -zv -t -w 2 10.149.248.24 32338'
Aug 23 22:00:26.767: INFO: stderr: "+ nc -zv -t -w 2 10.149.248.24 32338\nConnection to 10.149.248.24 32338 port [tcp/32338] succeeded!\n"
Aug 23 22:00:26.767: INFO: stdout: ""
Aug 23 22:00:26.767: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585719795 --namespace=services-976 exec execpod9q87n -- /bin/sh -x -c nc -zv -t -w 2 10.149.248.9 32338'
Aug 23 22:00:32.358: INFO: stderr: "+ nc -zv -t -w 2 10.149.248.9 32338\nConnection to 10.149.248.9 32338 port [tcp/32338] succeeded!\n"
Aug 23 22:00:32.358: INFO: stdout: ""
Aug 23 22:00:32.358: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585719795 --namespace=services-976 exec execpod9q87n -- /bin/sh -x -c nc -zv -t -w 2 169.45.209.222 32338'
Aug 23 22:00:34.252: INFO: stderr: "+ nc -zv -t -w 2 169.45.209.222 32338\nConnection to 169.45.209.222 32338 port [tcp/32338] succeeded!\n"
Aug 23 22:00:34.252: INFO: stdout: ""
Aug 23 22:00:34.252: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585719795 --namespace=services-976 exec execpod9q87n -- /bin/sh -x -c nc -zv -t -w 2 169.45.209.221 32338'
Aug 23 22:00:35.434: INFO: stderr: "+ nc -zv -t -w 2 169.45.209.221 32338\nConnection to 169.45.209.221 32338 port [tcp/32338] succeeded!\n"
Aug 23 22:00:35.434: INFO: stdout: ""
Aug 23 22:00:35.434: INFO: Cleaning up the ExternalName to NodePort test service
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 22:00:35.622: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-976" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749

• [SLOW TEST:39.214 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to NodePort [Conformance]","total":311,"completed":42,"skipped":607,"failed":0}
SS
------------------------------
[k8s.io] Pods 
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 22:00:36.423: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:187
[It] should be submitted and removed [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating the pod
STEP: setting up watch
STEP: submitting the pod to kubernetes
Aug 23 22:00:37.538: INFO: observed the pod list
STEP: verifying the pod is in kubernetes
STEP: verifying pod creation was observed
STEP: deleting the pod gracefully
STEP: verifying pod deletion was observed
[AfterEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 22:00:53.578: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-5035" for this suite.

• [SLOW TEST:17.985 seconds]
[k8s.io] Pods
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Pods should be submitted and removed [NodeConformance] [Conformance]","total":311,"completed":43,"skipped":609,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 22:00:54.408: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir 0666 on tmpfs
Aug 23 22:00:55.385: INFO: Waiting up to 5m0s for pod "pod-04ca0a35-87fb-49dc-8f09-78a1deab3a04" in namespace "emptydir-2595" to be "Succeeded or Failed"
Aug 23 22:00:55.574: INFO: Pod "pod-04ca0a35-87fb-49dc-8f09-78a1deab3a04": Phase="Pending", Reason="", readiness=false. Elapsed: 188.827324ms
Aug 23 22:00:57.684: INFO: Pod "pod-04ca0a35-87fb-49dc-8f09-78a1deab3a04": Phase="Pending", Reason="", readiness=false. Elapsed: 2.298767614s
Aug 23 22:00:59.803: INFO: Pod "pod-04ca0a35-87fb-49dc-8f09-78a1deab3a04": Phase="Pending", Reason="", readiness=false. Elapsed: 4.417709901s
Aug 23 22:01:01.926: INFO: Pod "pod-04ca0a35-87fb-49dc-8f09-78a1deab3a04": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.541293153s
STEP: Saw pod success
Aug 23 22:01:01.926: INFO: Pod "pod-04ca0a35-87fb-49dc-8f09-78a1deab3a04" satisfied condition "Succeeded or Failed"
Aug 23 22:01:02.036: INFO: Trying to get logs from node 10.149.248.9 pod pod-04ca0a35-87fb-49dc-8f09-78a1deab3a04 container test-container: <nil>
STEP: delete the pod
Aug 23 22:01:02.323: INFO: Waiting for pod pod-04ca0a35-87fb-49dc-8f09-78a1deab3a04 to disappear
Aug 23 22:01:02.461: INFO: Pod pod-04ca0a35-87fb-49dc-8f09-78a1deab3a04 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 22:01:02.461: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2595" for this suite.

• [SLOW TEST:8.798 seconds]
[sig-storage] EmptyDir volumes
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:45
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":44,"skipped":625,"failed":0}
SSSSSSS
------------------------------
[sig-storage] Secrets 
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 22:01:03.209: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating secret with name secret-test-d984f42d-6bd5-4994-98d5-8d2cb19745bd
STEP: Creating a pod to test consume secrets
Aug 23 22:01:04.612: INFO: Waiting up to 5m0s for pod "pod-secrets-772ff0c3-d87b-47e7-b406-59e95b0a4437" in namespace "secrets-1109" to be "Succeeded or Failed"
Aug 23 22:01:04.744: INFO: Pod "pod-secrets-772ff0c3-d87b-47e7-b406-59e95b0a4437": Phase="Pending", Reason="", readiness=false. Elapsed: 132.577912ms
Aug 23 22:01:06.806: INFO: Pod "pod-secrets-772ff0c3-d87b-47e7-b406-59e95b0a4437": Phase="Pending", Reason="", readiness=false. Elapsed: 2.193876385s
Aug 23 22:01:08.939: INFO: Pod "pod-secrets-772ff0c3-d87b-47e7-b406-59e95b0a4437": Phase="Pending", Reason="", readiness=false. Elapsed: 4.3272048s
Aug 23 22:01:11.000: INFO: Pod "pod-secrets-772ff0c3-d87b-47e7-b406-59e95b0a4437": Phase="Pending", Reason="", readiness=false. Elapsed: 6.387697884s
Aug 23 22:01:13.059: INFO: Pod "pod-secrets-772ff0c3-d87b-47e7-b406-59e95b0a4437": Phase="Pending", Reason="", readiness=false. Elapsed: 8.446950693s
Aug 23 22:01:15.205: INFO: Pod "pod-secrets-772ff0c3-d87b-47e7-b406-59e95b0a4437": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.592876609s
STEP: Saw pod success
Aug 23 22:01:15.205: INFO: Pod "pod-secrets-772ff0c3-d87b-47e7-b406-59e95b0a4437" satisfied condition "Succeeded or Failed"
Aug 23 22:01:15.373: INFO: Trying to get logs from node 10.149.248.9 pod pod-secrets-772ff0c3-d87b-47e7-b406-59e95b0a4437 container secret-volume-test: <nil>
STEP: delete the pod
Aug 23 22:01:15.734: INFO: Waiting for pod pod-secrets-772ff0c3-d87b-47e7-b406-59e95b0a4437 to disappear
Aug 23 22:01:15.869: INFO: Pod pod-secrets-772ff0c3-d87b-47e7-b406-59e95b0a4437 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 22:01:15.869: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1109" for this suite.
STEP: Destroying namespace "secret-namespace-5256" for this suite.

• [SLOW TEST:13.375 seconds]
[sig-storage] Secrets
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:36
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Secrets should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]","total":311,"completed":45,"skipped":632,"failed":0}
SSSSSSSSS
------------------------------
[sig-scheduling] LimitRange 
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-scheduling] LimitRange
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 22:01:16.584: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename limitrange
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a LimitRange
STEP: Setting up watch
STEP: Submitting a LimitRange
Aug 23 22:01:17.529: INFO: observed the limitRanges list
STEP: Verifying LimitRange creation was observed
STEP: Fetching the LimitRange to ensure it has proper values
Aug 23 22:01:17.756: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Aug 23 22:01:17.756: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with no resource requirements
STEP: Ensuring Pod has resource requirements applied from LimitRange
Aug 23 22:01:17.949: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Aug 23 22:01:17.949: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with partial resource requirements
STEP: Ensuring Pod has merged resource requirements applied from LimitRange
Aug 23 22:01:18.173: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
Aug 23 22:01:18.173: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Failing to create a Pod with less than min resources
STEP: Failing to create a Pod with more than max resources
STEP: Updating a LimitRange
STEP: Verifying LimitRange updating is effective
STEP: Creating a Pod with less than former min resources
STEP: Failing to create a Pod with more than max resources
STEP: Deleting a LimitRange
STEP: Verifying the LimitRange was deleted
Aug 23 22:01:26.330: INFO: limitRange is already deleted
STEP: Creating a Pod with more than former max resources
[AfterEach] [sig-scheduling] LimitRange
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 22:01:26.462: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "limitrange-2943" for this suite.

• [SLOW TEST:10.691 seconds]
[sig-scheduling] LimitRange
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-scheduling] LimitRange should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]","total":311,"completed":46,"skipped":641,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 22:01:27.276: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-5731 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-5731;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-5731 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-5731;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-5731.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-5731.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-5731.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-5731.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-5731.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-5731.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-5731.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-5731.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-5731.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-5731.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-5731.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-5731.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-5731.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 175.5.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.5.175_udp@PTR;check="$$(dig +tcp +noall +answer +search 175.5.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.5.175_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-5731 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-5731;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-5731 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-5731;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-5731.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-5731.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-5731.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-5731.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-5731.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-5731.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-5731.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-5731.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-5731.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-5731.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-5731.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-5731.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-5731.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 175.5.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.5.175_udp@PTR;check="$$(dig +tcp +noall +answer +search 175.5.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.5.175_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Aug 23 22:01:38.934: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-5731/dns-test-b3c6a694-d3c7-40ac-9dd8-bc644b3fa6ca: the server could not find the requested resource (get pods dns-test-b3c6a694-d3c7-40ac-9dd8-bc644b3fa6ca)
Aug 23 22:01:39.039: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-5731/dns-test-b3c6a694-d3c7-40ac-9dd8-bc644b3fa6ca: the server could not find the requested resource (get pods dns-test-b3c6a694-d3c7-40ac-9dd8-bc644b3fa6ca)
Aug 23 22:01:39.160: INFO: Unable to read wheezy_udp@dns-test-service.dns-5731 from pod dns-5731/dns-test-b3c6a694-d3c7-40ac-9dd8-bc644b3fa6ca: the server could not find the requested resource (get pods dns-test-b3c6a694-d3c7-40ac-9dd8-bc644b3fa6ca)
Aug 23 22:01:39.383: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5731 from pod dns-5731/dns-test-b3c6a694-d3c7-40ac-9dd8-bc644b3fa6ca: the server could not find the requested resource (get pods dns-test-b3c6a694-d3c7-40ac-9dd8-bc644b3fa6ca)
Aug 23 22:01:39.542: INFO: Unable to read wheezy_udp@dns-test-service.dns-5731.svc from pod dns-5731/dns-test-b3c6a694-d3c7-40ac-9dd8-bc644b3fa6ca: the server could not find the requested resource (get pods dns-test-b3c6a694-d3c7-40ac-9dd8-bc644b3fa6ca)
Aug 23 22:01:39.690: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5731.svc from pod dns-5731/dns-test-b3c6a694-d3c7-40ac-9dd8-bc644b3fa6ca: the server could not find the requested resource (get pods dns-test-b3c6a694-d3c7-40ac-9dd8-bc644b3fa6ca)
Aug 23 22:01:39.848: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-5731.svc from pod dns-5731/dns-test-b3c6a694-d3c7-40ac-9dd8-bc644b3fa6ca: the server could not find the requested resource (get pods dns-test-b3c6a694-d3c7-40ac-9dd8-bc644b3fa6ca)
Aug 23 22:01:39.998: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-5731.svc from pod dns-5731/dns-test-b3c6a694-d3c7-40ac-9dd8-bc644b3fa6ca: the server could not find the requested resource (get pods dns-test-b3c6a694-d3c7-40ac-9dd8-bc644b3fa6ca)
Aug 23 22:01:41.003: INFO: Unable to read jessie_udp@dns-test-service from pod dns-5731/dns-test-b3c6a694-d3c7-40ac-9dd8-bc644b3fa6ca: the server could not find the requested resource (get pods dns-test-b3c6a694-d3c7-40ac-9dd8-bc644b3fa6ca)
Aug 23 22:01:41.150: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-5731/dns-test-b3c6a694-d3c7-40ac-9dd8-bc644b3fa6ca: the server could not find the requested resource (get pods dns-test-b3c6a694-d3c7-40ac-9dd8-bc644b3fa6ca)
Aug 23 22:01:41.294: INFO: Unable to read jessie_udp@dns-test-service.dns-5731 from pod dns-5731/dns-test-b3c6a694-d3c7-40ac-9dd8-bc644b3fa6ca: the server could not find the requested resource (get pods dns-test-b3c6a694-d3c7-40ac-9dd8-bc644b3fa6ca)
Aug 23 22:01:41.419: INFO: Unable to read jessie_tcp@dns-test-service.dns-5731 from pod dns-5731/dns-test-b3c6a694-d3c7-40ac-9dd8-bc644b3fa6ca: the server could not find the requested resource (get pods dns-test-b3c6a694-d3c7-40ac-9dd8-bc644b3fa6ca)
Aug 23 22:01:41.540: INFO: Unable to read jessie_udp@dns-test-service.dns-5731.svc from pod dns-5731/dns-test-b3c6a694-d3c7-40ac-9dd8-bc644b3fa6ca: the server could not find the requested resource (get pods dns-test-b3c6a694-d3c7-40ac-9dd8-bc644b3fa6ca)
Aug 23 22:01:41.642: INFO: Unable to read jessie_tcp@dns-test-service.dns-5731.svc from pod dns-5731/dns-test-b3c6a694-d3c7-40ac-9dd8-bc644b3fa6ca: the server could not find the requested resource (get pods dns-test-b3c6a694-d3c7-40ac-9dd8-bc644b3fa6ca)
Aug 23 22:01:41.759: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-5731.svc from pod dns-5731/dns-test-b3c6a694-d3c7-40ac-9dd8-bc644b3fa6ca: the server could not find the requested resource (get pods dns-test-b3c6a694-d3c7-40ac-9dd8-bc644b3fa6ca)
Aug 23 22:01:41.880: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-5731.svc from pod dns-5731/dns-test-b3c6a694-d3c7-40ac-9dd8-bc644b3fa6ca: the server could not find the requested resource (get pods dns-test-b3c6a694-d3c7-40ac-9dd8-bc644b3fa6ca)
Aug 23 22:01:42.670: INFO: Lookups using dns-5731/dns-test-b3c6a694-d3c7-40ac-9dd8-bc644b3fa6ca failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-5731 wheezy_tcp@dns-test-service.dns-5731 wheezy_udp@dns-test-service.dns-5731.svc wheezy_tcp@dns-test-service.dns-5731.svc wheezy_udp@_http._tcp.dns-test-service.dns-5731.svc wheezy_tcp@_http._tcp.dns-test-service.dns-5731.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-5731 jessie_tcp@dns-test-service.dns-5731 jessie_udp@dns-test-service.dns-5731.svc jessie_tcp@dns-test-service.dns-5731.svc jessie_udp@_http._tcp.dns-test-service.dns-5731.svc jessie_tcp@_http._tcp.dns-test-service.dns-5731.svc]

Aug 23 22:01:51.350: INFO: DNS probes using dns-5731/dns-test-b3c6a694-d3c7-40ac-9dd8-bc644b3fa6ca succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 22:01:51.800: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-5731" for this suite.

• [SLOW TEST:24.967 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] DNS should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]","total":311,"completed":47,"skipped":659,"failed":0}
SSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 22:01:52.242: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:162
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating the pod
Aug 23 22:01:52.850: INFO: PodSpec: initContainers in spec.initContainers
Aug 23 22:02:46.312: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-7872f44c-8293-427b-97da-e3c0f6fb6727", GenerateName:"", Namespace:"init-container-7956", SelfLink:"/api/v1/namespaces/init-container-7956/pods/pod-init-7872f44c-8293-427b-97da-e3c0f6fb6727", UID:"6da3211f-e226-4767-9291-9b531a384b2f", ResourceVersion:"66342", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63765352912, loc:(*time.Location)(0x7975ee0)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"850792447"}, Annotations:map[string]string{"cni.projectcalico.org/podIP":"172.30.87.103/32", "cni.projectcalico.org/podIPs":"172.30.87.103/32", "k8s.v1.cni.cncf.io/network-status":"[{\n    \"name\": \"\",\n    \"ips\": [\n        \"172.30.87.103\"\n    ],\n    \"default\": true,\n    \"dns\": {}\n}]", "k8s.v1.cni.cncf.io/networks-status":"[{\n    \"name\": \"\",\n    \"ips\": [\n        \"172.30.87.103\"\n    ],\n    \"default\": true,\n    \"dns\": {}\n}]", "openshift.io/scc":"anyuid"}, OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:(*v1.Time)(0xc006dee8e0), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc006dee900)}, v1.ManagedFieldsEntry{Manager:"calico", Operation:"Update", APIVersion:"v1", Time:(*v1.Time)(0xc006dee920), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc006dee940)}, v1.ManagedFieldsEntry{Manager:"multus", Operation:"Update", APIVersion:"v1", Time:(*v1.Time)(0xc006dee960), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc006dee980)}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:(*v1.Time)(0xc006dee9a0), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc006dee9c0)}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"default-token-6xnq6", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(0xc0028cccc0), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-6xnq6", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(0xc0036c6420), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-6xnq6", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(0xc0036c6480), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"k8s.gcr.io/pause:3.2", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-6xnq6", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(0xc0036c63c0), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc00883fc90), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"10.149.248.24", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc002e84fc0), ImagePullSecrets:[]v1.LocalObjectReference{v1.LocalObjectReference{Name:"default-dockercfg-rs4p8"}}, Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc00883fd40)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc00883fd60)}, v1.Toleration{Key:"node.kubernetes.io/memory-pressure", Operator:"Exists", Value:"", Effect:"NoSchedule", TolerationSeconds:(*int64)(nil)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc00883fd7c), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc00883fd80), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc0056865d0), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63765352913, loc:(*time.Location)(0x7975ee0)}}, Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63765352913, loc:(*time.Location)(0x7975ee0)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63765352913, loc:(*time.Location)(0x7975ee0)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63765352912, loc:(*time.Location)(0x7975ee0)}}, Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"10.149.248.24", PodIP:"172.30.87.103", PodIPs:[]v1.PodIP{v1.PodIP{IP:"172.30.87.103"}}, StartTime:(*v1.Time)(0xc006dee9e0), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc002e850a0)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc002e85110)}, Ready:false, RestartCount:3, Image:"docker.io/library/busybox:1.29", ImageID:"docker.io/library/busybox@sha256:8ccbac733d19c0dd4d70b4f0c1e12245b5fa3ad24758a11035ee505c629c0796", ContainerID:"cri-o://648cf7c0855947a712688dc78e0a40a2773f7a01624c2f41f188fa8dddfdf037", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc006deea20), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"docker.io/library/busybox:1.29", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc006deea00), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/pause:3.2", ImageID:"", ContainerID:"", Started:(*bool)(0xc00883fdf4)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 22:02:46.312: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-7956" for this suite.

• [SLOW TEST:54.324 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should not start app containers if init containers fail on a RestartAlways pod [Conformance]","total":311,"completed":48,"skipped":671,"failed":0}
SS
------------------------------
[sig-instrumentation] Events API 
  should delete a collection of events [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 22:02:46.566: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/instrumentation/events.go:81
[It] should delete a collection of events [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Create set of events
STEP: get a list of Events with a label in the current namespace
STEP: delete a list of events
Aug 23 22:02:47.238: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity
[AfterEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 22:02:47.404: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-7147" for this suite.
•{"msg":"PASSED [sig-instrumentation] Events API should delete a collection of events [Conformance]","total":311,"completed":49,"skipped":673,"failed":0}
SSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a read only busybox container 
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 22:02:47.656: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[AfterEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 22:02:54.711: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-1947" for this suite.

• [SLOW TEST:7.539 seconds]
[k8s.io] Kubelet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  when scheduling a read only busybox container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:188
    should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Kubelet when scheduling a read only busybox container should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":50,"skipped":685,"failed":0}
SSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 22:02:55.197: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name projected-configmap-test-volume-d32e74b5-b0fa-4081-b15b-2e8de63f73da
STEP: Creating a pod to test consume configMaps
Aug 23 22:02:56.115: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-3fa0605a-0af1-4d8a-b0fd-62e22ca724c7" in namespace "projected-2360" to be "Succeeded or Failed"
Aug 23 22:02:56.247: INFO: Pod "pod-projected-configmaps-3fa0605a-0af1-4d8a-b0fd-62e22ca724c7": Phase="Pending", Reason="", readiness=false. Elapsed: 131.95078ms
Aug 23 22:02:58.358: INFO: Pod "pod-projected-configmaps-3fa0605a-0af1-4d8a-b0fd-62e22ca724c7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.243112065s
Aug 23 22:03:00.467: INFO: Pod "pod-projected-configmaps-3fa0605a-0af1-4d8a-b0fd-62e22ca724c7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.351985354s
STEP: Saw pod success
Aug 23 22:03:00.467: INFO: Pod "pod-projected-configmaps-3fa0605a-0af1-4d8a-b0fd-62e22ca724c7" satisfied condition "Succeeded or Failed"
Aug 23 22:03:00.581: INFO: Trying to get logs from node 10.149.248.25 pod pod-projected-configmaps-3fa0605a-0af1-4d8a-b0fd-62e22ca724c7 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Aug 23 22:03:01.017: INFO: Waiting for pod pod-projected-configmaps-3fa0605a-0af1-4d8a-b0fd-62e22ca724c7 to disappear
Aug 23 22:03:01.144: INFO: Pod pod-projected-configmaps-3fa0605a-0af1-4d8a-b0fd-62e22ca724c7 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 22:03:01.144: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2360" for this suite.

• [SLOW TEST:6.544 seconds]
[sig-storage] Projected configMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:36
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","total":311,"completed":51,"skipped":689,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 22:03:01.741: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a watch on configmaps with a certain label
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: changing the label value of the configmap
STEP: Expecting to observe a delete notification for the watched object
Aug 23 22:03:03.660: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-531 /api/v1/namespaces/watch-531/configmaps/e2e-watch-test-label-changed fd766538-d753-4a58-879a-bd9bdb2c15e1 66707 0 2021-08-23 22:03:03 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-08-23 22:03:03 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Aug 23 22:03:03.660: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-531 /api/v1/namespaces/watch-531/configmaps/e2e-watch-test-label-changed fd766538-d753-4a58-879a-bd9bdb2c15e1 66710 0 2021-08-23 22:03:03 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-08-23 22:03:03 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Aug 23 22:03:03.660: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-531 /api/v1/namespaces/watch-531/configmaps/e2e-watch-test-label-changed fd766538-d753-4a58-879a-bd9bdb2c15e1 66716 0 2021-08-23 22:03:03 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-08-23 22:03:03 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements
STEP: changing the label value of the configmap back
STEP: modifying the configmap a third time
STEP: deleting the configmap
STEP: Expecting to observe an add notification for the watched object when the label value was restored
Aug 23 22:03:14.570: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-531 /api/v1/namespaces/watch-531/configmaps/e2e-watch-test-label-changed fd766538-d753-4a58-879a-bd9bdb2c15e1 66802 0 2021-08-23 22:03:03 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-08-23 22:03:03 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Aug 23 22:03:14.571: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-531 /api/v1/namespaces/watch-531/configmaps/e2e-watch-test-label-changed fd766538-d753-4a58-879a-bd9bdb2c15e1 66803 0 2021-08-23 22:03:03 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-08-23 22:03:03 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
Aug 23 22:03:14.571: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-531 /api/v1/namespaces/watch-531/configmaps/e2e-watch-test-label-changed fd766538-d753-4a58-879a-bd9bdb2c15e1 66804 0 2021-08-23 22:03:03 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-08-23 22:03:03 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 22:03:14.571: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-531" for this suite.

• [SLOW TEST:13.248 seconds]
[sig-api-machinery] Watchers
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should observe an object deletion if it stops meeting the requirements of the selector [Conformance]","total":311,"completed":52,"skipped":705,"failed":0}
SSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 22:03:14.989: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Aug 23 22:03:15.379: INFO: Creating ReplicaSet my-hostname-basic-594f7adb-16b9-447e-9fb9-cc7c39eba766
Aug 23 22:03:15.493: INFO: Pod name my-hostname-basic-594f7adb-16b9-447e-9fb9-cc7c39eba766: Found 1 pods out of 1
Aug 23 22:03:15.493: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-594f7adb-16b9-447e-9fb9-cc7c39eba766" is running
Aug 23 22:03:21.647: INFO: Pod "my-hostname-basic-594f7adb-16b9-447e-9fb9-cc7c39eba766-6vnt5" is running (conditions: [])
Aug 23 22:03:21.648: INFO: Trying to dial the pod
Aug 23 22:03:26.998: INFO: Controller my-hostname-basic-594f7adb-16b9-447e-9fb9-cc7c39eba766: Got expected result from replica 1 [my-hostname-basic-594f7adb-16b9-447e-9fb9-cc7c39eba766-6vnt5]: "my-hostname-basic-594f7adb-16b9-447e-9fb9-cc7c39eba766-6vnt5", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 22:03:26.998: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-825" for this suite.

• [SLOW TEST:12.421 seconds]
[sig-apps] ReplicaSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet should serve a basic image on each replica with a public image  [Conformance]","total":311,"completed":53,"skipped":713,"failed":0}
SSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 22:03:27.412: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the container
STEP: wait for the container to reach Failed
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Aug 23 22:03:33.876: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 22:03:34.100: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-3234" for this suite.

• [SLOW TEST:7.129 seconds]
[k8s.io] Container Runtime
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  blackbox test
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:41
    on terminated container
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:134
      should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":311,"completed":54,"skipped":716,"failed":0}
SS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing validating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 22:03:34.541: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Aug 23 22:03:36.328: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63765353016, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63765353016, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63765353016, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63765353015, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 23 22:03:38.421: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63765353016, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63765353016, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63765353016, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63765353015, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Aug 23 22:03:41.545: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing validating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 22:03:44.041: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6187" for this suite.
STEP: Destroying namespace "webhook-6187-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

• [SLOW TEST:10.504 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  listing validating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing validating webhooks should work [Conformance]","total":311,"completed":55,"skipped":718,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 22:03:45.045: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name configmap-test-volume-9032249c-dca9-4915-aa10-033bbb2cafea
STEP: Creating a pod to test consume configMaps
Aug 23 22:03:45.891: INFO: Waiting up to 5m0s for pod "pod-configmaps-849d18ed-0917-417e-9914-ed5d23e53b83" in namespace "configmap-920" to be "Succeeded or Failed"
Aug 23 22:03:45.996: INFO: Pod "pod-configmaps-849d18ed-0917-417e-9914-ed5d23e53b83": Phase="Pending", Reason="", readiness=false. Elapsed: 104.452321ms
Aug 23 22:03:48.137: INFO: Pod "pod-configmaps-849d18ed-0917-417e-9914-ed5d23e53b83": Phase="Pending", Reason="", readiness=false. Elapsed: 2.245910498s
Aug 23 22:03:50.232: INFO: Pod "pod-configmaps-849d18ed-0917-417e-9914-ed5d23e53b83": Phase="Pending", Reason="", readiness=false. Elapsed: 4.340998781s
Aug 23 22:03:52.270: INFO: Pod "pod-configmaps-849d18ed-0917-417e-9914-ed5d23e53b83": Phase="Pending", Reason="", readiness=false. Elapsed: 6.378869211s
Aug 23 22:03:54.369: INFO: Pod "pod-configmaps-849d18ed-0917-417e-9914-ed5d23e53b83": Phase="Pending", Reason="", readiness=false. Elapsed: 8.477616174s
Aug 23 22:03:56.444: INFO: Pod "pod-configmaps-849d18ed-0917-417e-9914-ed5d23e53b83": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.552420125s
STEP: Saw pod success
Aug 23 22:03:56.444: INFO: Pod "pod-configmaps-849d18ed-0917-417e-9914-ed5d23e53b83" satisfied condition "Succeeded or Failed"
Aug 23 22:03:56.516: INFO: Trying to get logs from node 10.149.248.9 pod pod-configmaps-849d18ed-0917-417e-9914-ed5d23e53b83 container configmap-volume-test: <nil>
STEP: delete the pod
Aug 23 22:03:57.205: INFO: Waiting for pod pod-configmaps-849d18ed-0917-417e-9914-ed5d23e53b83 to disappear
Aug 23 22:03:57.237: INFO: Pod pod-configmaps-849d18ed-0917-417e-9914-ed5d23e53b83 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 22:03:57.237: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-920" for this suite.

• [SLOW TEST:12.323 seconds]
[sig-storage] ConfigMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:36
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","total":311,"completed":56,"skipped":761,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should succeed in writing subpaths in container [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 22:03:57.368: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should succeed in writing subpaths in container [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating the pod
STEP: waiting for pod running
STEP: creating a file in subpath
Aug 23 22:04:04.192: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-371 PodName:var-expansion-79ecf962-fb88-4e80-8084-43f4ef0ed011 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 23 22:04:04.192: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: test for file in mounted path
Aug 23 22:04:06.260: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-371 PodName:var-expansion-79ecf962-fb88-4e80-8084-43f4ef0ed011 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 23 22:04:06.261: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: updating the annotation value
Aug 23 22:04:08.933: INFO: Successfully updated pod "var-expansion-79ecf962-fb88-4e80-8084-43f4ef0ed011"
STEP: waiting for annotated pod running
STEP: deleting the pod gracefully
Aug 23 22:04:09.076: INFO: Deleting pod "var-expansion-79ecf962-fb88-4e80-8084-43f4ef0ed011" in namespace "var-expansion-371"
Aug 23 22:04:09.211: INFO: Wait up to 5m0s for pod "var-expansion-79ecf962-fb88-4e80-8084-43f4ef0ed011" to be fully deleted
[AfterEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 22:04:55.403: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-371" for this suite.

• [SLOW TEST:58.149 seconds]
[k8s.io] Variable Expansion
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should succeed in writing subpaths in container [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Variable Expansion should succeed in writing subpaths in container [sig-storage][Slow] [Conformance]","total":311,"completed":57,"skipped":801,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 22:04:55.517: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:41
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Aug 23 22:04:55.798: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e899c5a2-9ff5-484e-be3d-a84ace21f708" in namespace "projected-7772" to be "Succeeded or Failed"
Aug 23 22:04:55.827: INFO: Pod "downwardapi-volume-e899c5a2-9ff5-484e-be3d-a84ace21f708": Phase="Pending", Reason="", readiness=false. Elapsed: 28.357413ms
Aug 23 22:04:57.850: INFO: Pod "downwardapi-volume-e899c5a2-9ff5-484e-be3d-a84ace21f708": Phase="Pending", Reason="", readiness=false. Elapsed: 2.051569926s
Aug 23 22:04:59.899: INFO: Pod "downwardapi-volume-e899c5a2-9ff5-484e-be3d-a84ace21f708": Phase="Pending", Reason="", readiness=false. Elapsed: 4.101179318s
Aug 23 22:05:01.924: INFO: Pod "downwardapi-volume-e899c5a2-9ff5-484e-be3d-a84ace21f708": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.126202491s
STEP: Saw pod success
Aug 23 22:05:01.925: INFO: Pod "downwardapi-volume-e899c5a2-9ff5-484e-be3d-a84ace21f708" satisfied condition "Succeeded or Failed"
Aug 23 22:05:01.956: INFO: Trying to get logs from node 10.149.248.9 pod downwardapi-volume-e899c5a2-9ff5-484e-be3d-a84ace21f708 container client-container: <nil>
STEP: delete the pod
Aug 23 22:05:02.087: INFO: Waiting for pod downwardapi-volume-e899c5a2-9ff5-484e-be3d-a84ace21f708 to disappear
Aug 23 22:05:02.133: INFO: Pod downwardapi-volume-e899c5a2-9ff5-484e-be3d-a84ace21f708 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 22:05:02.133: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7772" for this suite.

• [SLOW TEST:6.801 seconds]
[sig-storage] Projected downwardAPI
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:35
  should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide podname only [NodeConformance] [Conformance]","total":311,"completed":58,"skipped":813,"failed":0}
SSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop complex daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 22:05:02.319: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:129
[It] should run and stop complex daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Aug 23 22:05:02.790: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes.
Aug 23 22:05:02.836: INFO: Number of nodes with available pods: 0
Aug 23 22:05:02.837: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Change node label to blue, check that daemon pod is launched.
Aug 23 22:05:03.079: INFO: Number of nodes with available pods: 0
Aug 23 22:05:03.079: INFO: Node 10.149.248.24 is running more than one daemon pod
Aug 23 22:05:04.104: INFO: Number of nodes with available pods: 0
Aug 23 22:05:04.104: INFO: Node 10.149.248.24 is running more than one daemon pod
Aug 23 22:05:05.144: INFO: Number of nodes with available pods: 0
Aug 23 22:05:05.144: INFO: Node 10.149.248.24 is running more than one daemon pod
Aug 23 22:05:06.114: INFO: Number of nodes with available pods: 0
Aug 23 22:05:06.114: INFO: Node 10.149.248.24 is running more than one daemon pod
Aug 23 22:05:07.126: INFO: Number of nodes with available pods: 0
Aug 23 22:05:07.126: INFO: Node 10.149.248.24 is running more than one daemon pod
Aug 23 22:05:08.140: INFO: Number of nodes with available pods: 0
Aug 23 22:05:08.140: INFO: Node 10.149.248.24 is running more than one daemon pod
Aug 23 22:05:09.115: INFO: Number of nodes with available pods: 0
Aug 23 22:05:09.115: INFO: Node 10.149.248.24 is running more than one daemon pod
Aug 23 22:05:10.106: INFO: Number of nodes with available pods: 1
Aug 23 22:05:10.106: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Update the node label to green, and wait for daemons to be unscheduled
Aug 23 22:05:10.437: INFO: Number of nodes with available pods: 0
Aug 23 22:05:10.437: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate
Aug 23 22:05:10.569: INFO: Number of nodes with available pods: 0
Aug 23 22:05:10.569: INFO: Node 10.149.248.24 is running more than one daemon pod
Aug 23 22:05:11.668: INFO: Number of nodes with available pods: 0
Aug 23 22:05:11.668: INFO: Node 10.149.248.24 is running more than one daemon pod
Aug 23 22:05:12.698: INFO: Number of nodes with available pods: 0
Aug 23 22:05:12.698: INFO: Node 10.149.248.24 is running more than one daemon pod
Aug 23 22:05:13.797: INFO: Number of nodes with available pods: 0
Aug 23 22:05:13.797: INFO: Node 10.149.248.24 is running more than one daemon pod
Aug 23 22:05:14.777: INFO: Number of nodes with available pods: 0
Aug 23 22:05:14.777: INFO: Node 10.149.248.24 is running more than one daemon pod
Aug 23 22:05:15.825: INFO: Number of nodes with available pods: 0
Aug 23 22:05:15.825: INFO: Node 10.149.248.24 is running more than one daemon pod
Aug 23 22:05:16.797: INFO: Number of nodes with available pods: 0
Aug 23 22:05:16.797: INFO: Node 10.149.248.24 is running more than one daemon pod
Aug 23 22:05:17.793: INFO: Number of nodes with available pods: 0
Aug 23 22:05:17.793: INFO: Node 10.149.248.24 is running more than one daemon pod
Aug 23 22:05:18.758: INFO: Number of nodes with available pods: 0
Aug 23 22:05:18.758: INFO: Node 10.149.248.24 is running more than one daemon pod
Aug 23 22:05:19.778: INFO: Number of nodes with available pods: 0
Aug 23 22:05:19.778: INFO: Node 10.149.248.24 is running more than one daemon pod
Aug 23 22:05:20.776: INFO: Number of nodes with available pods: 1
Aug 23 22:05:20.777: INFO: Number of running nodes: 1, number of available pods: 1
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:95
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-6853, will wait for the garbage collector to delete the pods
Aug 23 22:05:21.800: INFO: Deleting DaemonSet.extensions daemon-set took: 196.766187ms
Aug 23 22:05:22.000: INFO: Terminating DaemonSet.extensions daemon-set pods took: 200.334699ms
Aug 23 22:05:33.730: INFO: Number of nodes with available pods: 0
Aug 23 22:05:33.730: INFO: Number of running nodes: 0, number of available pods: 0
Aug 23 22:05:33.878: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-6853/daemonsets","resourceVersion":"68212"},"items":null}

Aug 23 22:05:34.007: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-6853/pods","resourceVersion":"68213"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 22:05:35.895: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-6853" for this suite.

• [SLOW TEST:34.218 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop complex daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop complex daemon [Conformance]","total":311,"completed":59,"skipped":823,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 22:05:36.541: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward api env vars
Aug 23 22:05:37.491: INFO: Waiting up to 5m0s for pod "downward-api-4253cfc6-6c43-4bb9-a41a-491db951f270" in namespace "downward-api-611" to be "Succeeded or Failed"
Aug 23 22:05:37.599: INFO: Pod "downward-api-4253cfc6-6c43-4bb9-a41a-491db951f270": Phase="Pending", Reason="", readiness=false. Elapsed: 107.878636ms
Aug 23 22:05:39.728: INFO: Pod "downward-api-4253cfc6-6c43-4bb9-a41a-491db951f270": Phase="Pending", Reason="", readiness=false. Elapsed: 2.236929683s
Aug 23 22:05:41.817: INFO: Pod "downward-api-4253cfc6-6c43-4bb9-a41a-491db951f270": Phase="Pending", Reason="", readiness=false. Elapsed: 4.326301447s
Aug 23 22:05:43.934: INFO: Pod "downward-api-4253cfc6-6c43-4bb9-a41a-491db951f270": Phase="Pending", Reason="", readiness=false. Elapsed: 6.442926408s
Aug 23 22:05:46.062: INFO: Pod "downward-api-4253cfc6-6c43-4bb9-a41a-491db951f270": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.570974091s
STEP: Saw pod success
Aug 23 22:05:46.062: INFO: Pod "downward-api-4253cfc6-6c43-4bb9-a41a-491db951f270" satisfied condition "Succeeded or Failed"
Aug 23 22:05:46.180: INFO: Trying to get logs from node 10.149.248.24 pod downward-api-4253cfc6-6c43-4bb9-a41a-491db951f270 container dapi-container: <nil>
STEP: delete the pod
Aug 23 22:05:46.680: INFO: Waiting for pod downward-api-4253cfc6-6c43-4bb9-a41a-491db951f270 to disappear
Aug 23 22:05:46.785: INFO: Pod downward-api-4253cfc6-6c43-4bb9-a41a-491db951f270 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 22:05:46.785: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-611" for this suite.

• [SLOW TEST:10.837 seconds]
[sig-node] Downward API
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:34
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-node] Downward API should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]","total":311,"completed":60,"skipped":935,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 22:05:47.378: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:187
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Aug 23 22:05:48.007: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 22:05:52.906: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-8571" for this suite.

• [SLOW TEST:6.094 seconds]
[k8s.io] Pods
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Pods should support retrieving logs from the container over websockets [NodeConformance] [Conformance]","total":311,"completed":61,"skipped":952,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-apps] Job 
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 22:05:53.472: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a job
STEP: Ensuring job reaches completions
[AfterEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 22:06:06.415: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-9836" for this suite.

• [SLOW TEST:13.061 seconds]
[sig-apps] Job
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] Job should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]","total":311,"completed":62,"skipped":963,"failed":0}
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should have an terminated reason [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 22:06:06.533: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:82
[It] should have an terminated reason [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[AfterEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 22:06:15.892: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-7658" for this suite.

• [SLOW TEST:9.811 seconds]
[k8s.io] Kubelet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  when scheduling a busybox command that always fails in a pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:79
    should have an terminated reason [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox command that always fails in a pod should have an terminated reason [NodeConformance] [Conformance]","total":311,"completed":63,"skipped":963,"failed":0}
S
------------------------------
[sig-node] PodTemplates 
  should delete a collection of pod templates [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-node] PodTemplates
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 22:06:16.346: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename podtemplate
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a collection of pod templates [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Create set of pod templates
Aug 23 22:06:16.826: INFO: created test-podtemplate-1
Aug 23 22:06:16.925: INFO: created test-podtemplate-2
Aug 23 22:06:16.994: INFO: created test-podtemplate-3
STEP: get a list of pod templates with a label in the current namespace
STEP: delete collection of pod templates
Aug 23 22:06:17.042: INFO: requesting DeleteCollection of pod templates
STEP: check that the list of pod templates matches the requested quantity
Aug 23 22:06:17.154: INFO: requesting list of pod templates to confirm quantity
[AfterEach] [sig-node] PodTemplates
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 22:06:17.206: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-4766" for this suite.
•{"msg":"PASSED [sig-node] PodTemplates should delete a collection of pod templates [Conformance]","total":311,"completed":64,"skipped":964,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of different groups [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 22:06:17.492: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of different groups [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation
Aug 23 22:06:17.885: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
Aug 23 22:06:29.703: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 22:07:10.590: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-6017" for this suite.

• [SLOW TEST:53.254 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of different groups [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of different groups [Conformance]","total":311,"completed":65,"skipped":989,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 22:07:10.747: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:41
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Aug 23 22:07:11.256: INFO: Waiting up to 5m0s for pod "downwardapi-volume-31ffaed6-83fd-418a-84b5-60d769ccf3c3" in namespace "projected-6943" to be "Succeeded or Failed"
Aug 23 22:07:11.358: INFO: Pod "downwardapi-volume-31ffaed6-83fd-418a-84b5-60d769ccf3c3": Phase="Pending", Reason="", readiness=false. Elapsed: 102.22031ms
Aug 23 22:07:13.383: INFO: Pod "downwardapi-volume-31ffaed6-83fd-418a-84b5-60d769ccf3c3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.126595553s
Aug 23 22:07:15.400: INFO: Pod "downwardapi-volume-31ffaed6-83fd-418a-84b5-60d769ccf3c3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.143931518s
STEP: Saw pod success
Aug 23 22:07:15.400: INFO: Pod "downwardapi-volume-31ffaed6-83fd-418a-84b5-60d769ccf3c3" satisfied condition "Succeeded or Failed"
Aug 23 22:07:15.421: INFO: Trying to get logs from node 10.149.248.9 pod downwardapi-volume-31ffaed6-83fd-418a-84b5-60d769ccf3c3 container client-container: <nil>
STEP: delete the pod
Aug 23 22:07:15.517: INFO: Waiting for pod downwardapi-volume-31ffaed6-83fd-418a-84b5-60d769ccf3c3 to disappear
Aug 23 22:07:15.537: INFO: Pod downwardapi-volume-31ffaed6-83fd-418a-84b5-60d769ccf3c3 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 22:07:15.538: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6943" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","total":311,"completed":66,"skipped":1001,"failed":0}
SS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 22:07:15.601: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:41
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Aug 23 22:07:15.858: INFO: Waiting up to 5m0s for pod "downwardapi-volume-71fdbc5a-c04d-4ca8-ae91-eb2dc7227a52" in namespace "projected-553" to be "Succeeded or Failed"
Aug 23 22:07:15.879: INFO: Pod "downwardapi-volume-71fdbc5a-c04d-4ca8-ae91-eb2dc7227a52": Phase="Pending", Reason="", readiness=false. Elapsed: 20.872769ms
Aug 23 22:07:17.926: INFO: Pod "downwardapi-volume-71fdbc5a-c04d-4ca8-ae91-eb2dc7227a52": Phase="Pending", Reason="", readiness=false. Elapsed: 2.067974028s
Aug 23 22:07:20.069: INFO: Pod "downwardapi-volume-71fdbc5a-c04d-4ca8-ae91-eb2dc7227a52": Phase="Pending", Reason="", readiness=false. Elapsed: 4.210815693s
Aug 23 22:07:22.168: INFO: Pod "downwardapi-volume-71fdbc5a-c04d-4ca8-ae91-eb2dc7227a52": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.309839177s
STEP: Saw pod success
Aug 23 22:07:22.168: INFO: Pod "downwardapi-volume-71fdbc5a-c04d-4ca8-ae91-eb2dc7227a52" satisfied condition "Succeeded or Failed"
Aug 23 22:07:22.254: INFO: Trying to get logs from node 10.149.248.9 pod downwardapi-volume-71fdbc5a-c04d-4ca8-ae91-eb2dc7227a52 container client-container: <nil>
STEP: delete the pod
Aug 23 22:07:22.438: INFO: Waiting for pod downwardapi-volume-71fdbc5a-c04d-4ca8-ae91-eb2dc7227a52 to disappear
Aug 23 22:07:22.495: INFO: Pod downwardapi-volume-71fdbc5a-c04d-4ca8-ae91-eb2dc7227a52 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 22:07:22.495: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-553" for this suite.

• [SLOW TEST:7.103 seconds]
[sig-storage] Projected downwardAPI
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:35
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","total":311,"completed":67,"skipped":1003,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Security Context When creating a pod with privileged 
  should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 22:07:22.705: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:41
[It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Aug 23 22:07:23.220: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-40836a77-248f-4415-ac1b-9ad4bfd41ae5" in namespace "security-context-test-4859" to be "Succeeded or Failed"
Aug 23 22:07:23.294: INFO: Pod "busybox-privileged-false-40836a77-248f-4415-ac1b-9ad4bfd41ae5": Phase="Pending", Reason="", readiness=false. Elapsed: 74.858385ms
Aug 23 22:07:25.330: INFO: Pod "busybox-privileged-false-40836a77-248f-4415-ac1b-9ad4bfd41ae5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.110234049s
Aug 23 22:07:27.349: INFO: Pod "busybox-privileged-false-40836a77-248f-4415-ac1b-9ad4bfd41ae5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.129901406s
Aug 23 22:07:29.387: INFO: Pod "busybox-privileged-false-40836a77-248f-4415-ac1b-9ad4bfd41ae5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.167480019s
Aug 23 22:07:29.387: INFO: Pod "busybox-privileged-false-40836a77-248f-4415-ac1b-9ad4bfd41ae5" satisfied condition "Succeeded or Failed"
Aug 23 22:07:29.465: INFO: Got logs for pod "busybox-privileged-false-40836a77-248f-4415-ac1b-9ad4bfd41ae5": "ip: RTNETLINK answers: Operation not permitted\n"
[AfterEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 22:07:29.465: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-4859" for this suite.

• [SLOW TEST:6.909 seconds]
[k8s.io] Security Context
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  When creating a pod with privileged
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:227
    should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Security Context When creating a pod with privileged should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":68,"skipped":1028,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 22:07:29.614: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir 0644 on node default medium
Aug 23 22:07:29.866: INFO: Waiting up to 5m0s for pod "pod-e957d0cb-d33d-4e7a-a510-edc41672ac9f" in namespace "emptydir-5114" to be "Succeeded or Failed"
Aug 23 22:07:29.893: INFO: Pod "pod-e957d0cb-d33d-4e7a-a510-edc41672ac9f": Phase="Pending", Reason="", readiness=false. Elapsed: 26.349135ms
Aug 23 22:07:31.955: INFO: Pod "pod-e957d0cb-d33d-4e7a-a510-edc41672ac9f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.088364029s
Aug 23 22:07:34.034: INFO: Pod "pod-e957d0cb-d33d-4e7a-a510-edc41672ac9f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.167245921s
Aug 23 22:07:36.164: INFO: Pod "pod-e957d0cb-d33d-4e7a-a510-edc41672ac9f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.297541653s
STEP: Saw pod success
Aug 23 22:07:36.164: INFO: Pod "pod-e957d0cb-d33d-4e7a-a510-edc41672ac9f" satisfied condition "Succeeded or Failed"
Aug 23 22:07:36.338: INFO: Trying to get logs from node 10.149.248.9 pod pod-e957d0cb-d33d-4e7a-a510-edc41672ac9f container test-container: <nil>
STEP: delete the pod
Aug 23 22:07:36.729: INFO: Waiting for pod pod-e957d0cb-d33d-4e7a-a510-edc41672ac9f to disappear
Aug 23 22:07:36.889: INFO: Pod pod-e957d0cb-d33d-4e7a-a510-edc41672ac9f no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 22:07:36.889: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5114" for this suite.

• [SLOW TEST:7.821 seconds]
[sig-storage] EmptyDir volumes
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:45
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":69,"skipped":1081,"failed":0}
SSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 22:07:37.437: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name configmap-test-volume-map-8b02276b-5049-403b-8ee3-57407b02f574
STEP: Creating a pod to test consume configMaps
Aug 23 22:07:38.117: INFO: Waiting up to 5m0s for pod "pod-configmaps-884e6214-cc18-496c-b517-5387d9a528b8" in namespace "configmap-3922" to be "Succeeded or Failed"
Aug 23 22:07:38.188: INFO: Pod "pod-configmaps-884e6214-cc18-496c-b517-5387d9a528b8": Phase="Pending", Reason="", readiness=false. Elapsed: 70.962915ms
Aug 23 22:07:40.383: INFO: Pod "pod-configmaps-884e6214-cc18-496c-b517-5387d9a528b8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.266378859s
Aug 23 22:07:42.488: INFO: Pod "pod-configmaps-884e6214-cc18-496c-b517-5387d9a528b8": Phase="Pending", Reason="", readiness=false. Elapsed: 4.371471461s
Aug 23 22:07:44.681: INFO: Pod "pod-configmaps-884e6214-cc18-496c-b517-5387d9a528b8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.564121929s
STEP: Saw pod success
Aug 23 22:07:44.681: INFO: Pod "pod-configmaps-884e6214-cc18-496c-b517-5387d9a528b8" satisfied condition "Succeeded or Failed"
Aug 23 22:07:44.883: INFO: Trying to get logs from node 10.149.248.9 pod pod-configmaps-884e6214-cc18-496c-b517-5387d9a528b8 container agnhost-container: <nil>
STEP: delete the pod
Aug 23 22:07:45.310: INFO: Waiting for pod pod-configmaps-884e6214-cc18-496c-b517-5387d9a528b8 to disappear
Aug 23 22:07:45.483: INFO: Pod pod-configmaps-884e6214-cc18-496c-b517-5387d9a528b8 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 22:07:45.484: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3922" for this suite.

• [SLOW TEST:8.505 seconds]
[sig-storage] ConfigMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:36
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":311,"completed":70,"skipped":1085,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 22:07:45.959: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test override all
Aug 23 22:07:46.644: INFO: Waiting up to 5m0s for pod "client-containers-3e27e2e7-749b-4231-b534-0e4587177895" in namespace "containers-996" to be "Succeeded or Failed"
Aug 23 22:07:46.768: INFO: Pod "client-containers-3e27e2e7-749b-4231-b534-0e4587177895": Phase="Pending", Reason="", readiness=false. Elapsed: 124.148219ms
Aug 23 22:07:48.912: INFO: Pod "client-containers-3e27e2e7-749b-4231-b534-0e4587177895": Phase="Pending", Reason="", readiness=false. Elapsed: 2.268228813s
Aug 23 22:07:51.056: INFO: Pod "client-containers-3e27e2e7-749b-4231-b534-0e4587177895": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.412427725s
STEP: Saw pod success
Aug 23 22:07:51.056: INFO: Pod "client-containers-3e27e2e7-749b-4231-b534-0e4587177895" satisfied condition "Succeeded or Failed"
Aug 23 22:07:51.197: INFO: Trying to get logs from node 10.149.248.9 pod client-containers-3e27e2e7-749b-4231-b534-0e4587177895 container agnhost-container: <nil>
STEP: delete the pod
Aug 23 22:07:51.470: INFO: Waiting for pod client-containers-3e27e2e7-749b-4231-b534-0e4587177895 to disappear
Aug 23 22:07:51.595: INFO: Pod client-containers-3e27e2e7-749b-4231-b534-0e4587177895 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 22:07:51.595: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-996" for this suite.

• [SLOW TEST:6.102 seconds]
[k8s.io] Docker Containers
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Docker Containers should be able to override the image's default command and arguments [NodeConformance] [Conformance]","total":311,"completed":71,"skipped":1158,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 22:07:52.064: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a watch on configmaps with label A
STEP: creating a watch on configmaps with label B
STEP: creating a watch on configmaps with label A or B
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification
Aug 23 22:07:53.127: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-6685 /api/v1/namespaces/watch-6685/configmaps/e2e-watch-test-configmap-a e0dee261-a237-4964-acb0-e89c1283ed6f 70102 0 2021-08-23 22:07:53 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-08-23 22:07:53 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Aug 23 22:07:53.128: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-6685 /api/v1/namespaces/watch-6685/configmaps/e2e-watch-test-configmap-a e0dee261-a237-4964-acb0-e89c1283ed6f 70102 0 2021-08-23 22:07:53 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-08-23 22:07:53 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A and ensuring the correct watchers observe the notification
Aug 23 22:08:03.461: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-6685 /api/v1/namespaces/watch-6685/configmaps/e2e-watch-test-configmap-a e0dee261-a237-4964-acb0-e89c1283ed6f 70188 0 2021-08-23 22:07:53 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-08-23 22:08:03 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Aug 23 22:08:03.461: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-6685 /api/v1/namespaces/watch-6685/configmaps/e2e-watch-test-configmap-a e0dee261-a237-4964-acb0-e89c1283ed6f 70188 0 2021-08-23 22:07:53 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-08-23 22:08:03 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification
Aug 23 22:08:13.648: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-6685 /api/v1/namespaces/watch-6685/configmaps/e2e-watch-test-configmap-a e0dee261-a237-4964-acb0-e89c1283ed6f 70235 0 2021-08-23 22:07:53 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-08-23 22:08:03 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Aug 23 22:08:13.648: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-6685 /api/v1/namespaces/watch-6685/configmaps/e2e-watch-test-configmap-a e0dee261-a237-4964-acb0-e89c1283ed6f 70235 0 2021-08-23 22:07:53 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-08-23 22:08:03 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap A and ensuring the correct watchers observe the notification
Aug 23 22:08:23.694: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-6685 /api/v1/namespaces/watch-6685/configmaps/e2e-watch-test-configmap-a e0dee261-a237-4964-acb0-e89c1283ed6f 70286 0 2021-08-23 22:07:53 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-08-23 22:08:03 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Aug 23 22:08:23.694: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-6685 /api/v1/namespaces/watch-6685/configmaps/e2e-watch-test-configmap-a e0dee261-a237-4964-acb0-e89c1283ed6f 70286 0 2021-08-23 22:07:53 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-08-23 22:08:03 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification
Aug 23 22:08:33.826: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-6685 /api/v1/namespaces/watch-6685/configmaps/e2e-watch-test-configmap-b 7f1a1cd4-42f8-4296-b750-6260400a80fa 70335 0 2021-08-23 22:08:33 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2021-08-23 22:08:33 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Aug 23 22:08:33.826: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-6685 /api/v1/namespaces/watch-6685/configmaps/e2e-watch-test-configmap-b 7f1a1cd4-42f8-4296-b750-6260400a80fa 70335 0 2021-08-23 22:08:33 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2021-08-23 22:08:33 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap B and ensuring the correct watchers observe the notification
Aug 23 22:08:43.936: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-6685 /api/v1/namespaces/watch-6685/configmaps/e2e-watch-test-configmap-b 7f1a1cd4-42f8-4296-b750-6260400a80fa 70381 0 2021-08-23 22:08:33 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2021-08-23 22:08:33 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Aug 23 22:08:43.936: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-6685 /api/v1/namespaces/watch-6685/configmaps/e2e-watch-test-configmap-b 7f1a1cd4-42f8-4296-b750-6260400a80fa 70381 0 2021-08-23 22:08:33 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2021-08-23 22:08:33 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 22:08:53.937: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-6685" for this suite.

• [SLOW TEST:62.165 seconds]
[sig-api-machinery] Watchers
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should observe add, update, and delete watch notifications on configmaps [Conformance]","total":311,"completed":72,"skipped":1186,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-network] Services 
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 22:08:54.230: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating service in namespace services-6593
STEP: creating service affinity-clusterip in namespace services-6593
STEP: creating replication controller affinity-clusterip in namespace services-6593
I0823 22:08:55.070275      25 runners.go:190] Created replication controller with name: affinity-clusterip, namespace: services-6593, replica count: 3
I0823 22:08:58.170631      25 runners.go:190] affinity-clusterip Pods: 3 out of 3 created, 0 running, 3 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0823 22:09:01.170827      25 runners.go:190] affinity-clusterip Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0823 22:09:04.171024      25 runners.go:190] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Aug 23 22:09:04.347: INFO: Creating new exec pod
Aug 23 22:09:13.873: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585719795 --namespace=services-6593 exec execpod-affinitygn7cx -- /bin/sh -x -c nc -zv -t -w 2 affinity-clusterip 80'
Aug 23 22:09:15.768: INFO: stderr: "+ nc -zv -t -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
Aug 23 22:09:15.768: INFO: stdout: ""
Aug 23 22:09:15.768: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585719795 --namespace=services-6593 exec execpod-affinitygn7cx -- /bin/sh -x -c nc -zv -t -w 2 172.21.133.195 80'
Aug 23 22:09:17.122: INFO: stderr: "+ nc -zv -t -w 2 172.21.133.195 80\nConnection to 172.21.133.195 80 port [tcp/http] succeeded!\n"
Aug 23 22:09:17.122: INFO: stdout: ""
Aug 23 22:09:17.122: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585719795 --namespace=services-6593 exec execpod-affinitygn7cx -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.21.133.195:80/ ; done'
Aug 23 22:09:19.784: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.133.195:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.133.195:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.133.195:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.133.195:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.133.195:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.133.195:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.133.195:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.133.195:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.133.195:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.133.195:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.133.195:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.133.195:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.133.195:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.133.195:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.133.195:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.133.195:80/\n"
Aug 23 22:09:19.784: INFO: stdout: "\naffinity-clusterip-zznb6\naffinity-clusterip-zznb6\naffinity-clusterip-zznb6\naffinity-clusterip-zznb6\naffinity-clusterip-zznb6\naffinity-clusterip-zznb6\naffinity-clusterip-zznb6\naffinity-clusterip-zznb6\naffinity-clusterip-zznb6\naffinity-clusterip-zznb6\naffinity-clusterip-zznb6\naffinity-clusterip-zznb6\naffinity-clusterip-zznb6\naffinity-clusterip-zznb6\naffinity-clusterip-zznb6\naffinity-clusterip-zznb6"
Aug 23 22:09:19.784: INFO: Received response from host: affinity-clusterip-zznb6
Aug 23 22:09:19.784: INFO: Received response from host: affinity-clusterip-zznb6
Aug 23 22:09:19.784: INFO: Received response from host: affinity-clusterip-zznb6
Aug 23 22:09:19.784: INFO: Received response from host: affinity-clusterip-zznb6
Aug 23 22:09:19.784: INFO: Received response from host: affinity-clusterip-zznb6
Aug 23 22:09:19.784: INFO: Received response from host: affinity-clusterip-zznb6
Aug 23 22:09:19.784: INFO: Received response from host: affinity-clusterip-zznb6
Aug 23 22:09:19.784: INFO: Received response from host: affinity-clusterip-zznb6
Aug 23 22:09:19.784: INFO: Received response from host: affinity-clusterip-zznb6
Aug 23 22:09:19.784: INFO: Received response from host: affinity-clusterip-zznb6
Aug 23 22:09:19.784: INFO: Received response from host: affinity-clusterip-zznb6
Aug 23 22:09:19.784: INFO: Received response from host: affinity-clusterip-zznb6
Aug 23 22:09:19.784: INFO: Received response from host: affinity-clusterip-zznb6
Aug 23 22:09:19.784: INFO: Received response from host: affinity-clusterip-zznb6
Aug 23 22:09:19.784: INFO: Received response from host: affinity-clusterip-zznb6
Aug 23 22:09:19.784: INFO: Received response from host: affinity-clusterip-zznb6
Aug 23 22:09:19.784: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip in namespace services-6593, will wait for the garbage collector to delete the pods
Aug 23 22:09:20.156: INFO: Deleting ReplicationController affinity-clusterip took: 95.577586ms
Aug 23 22:09:20.256: INFO: Terminating ReplicationController affinity-clusterip pods took: 100.236531ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 22:09:36.241: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6593" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749

• [SLOW TEST:42.129 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]","total":311,"completed":73,"skipped":1197,"failed":0}
SS
------------------------------
[sig-storage] EmptyDir volumes 
  pod should support shared volumes between containers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 22:09:36.359: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] pod should support shared volumes between containers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating Pod
STEP: Reading file content from the nginx-container
Aug 23 22:09:40.912: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-6095 PodName:pod-sharedvolume-34f778b4-1958-4186-bcde-5827d1479fd5 ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 23 22:09:40.912: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
Aug 23 22:09:41.409: INFO: Exec stderr: ""
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 22:09:41.409: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6095" for this suite.

• [SLOW TEST:5.154 seconds]
[sig-storage] EmptyDir volumes
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:45
  pod should support shared volumes between containers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes pod should support shared volumes between containers [Conformance]","total":311,"completed":74,"skipped":1199,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 22:09:41.513: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating secret with name secret-test-45580db9-14cb-4b44-b890-f6d492befd31
STEP: Creating a pod to test consume secrets
Aug 23 22:09:41.795: INFO: Waiting up to 5m0s for pod "pod-secrets-52747bac-aac4-460a-be9e-66cf0f7cbd5d" in namespace "secrets-4318" to be "Succeeded or Failed"
Aug 23 22:09:41.814: INFO: Pod "pod-secrets-52747bac-aac4-460a-be9e-66cf0f7cbd5d": Phase="Pending", Reason="", readiness=false. Elapsed: 19.072075ms
Aug 23 22:09:43.865: INFO: Pod "pod-secrets-52747bac-aac4-460a-be9e-66cf0f7cbd5d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.069791307s
Aug 23 22:09:45.920: INFO: Pod "pod-secrets-52747bac-aac4-460a-be9e-66cf0f7cbd5d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.12481037s
Aug 23 22:09:47.940: INFO: Pod "pod-secrets-52747bac-aac4-460a-be9e-66cf0f7cbd5d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.144463697s
STEP: Saw pod success
Aug 23 22:09:47.940: INFO: Pod "pod-secrets-52747bac-aac4-460a-be9e-66cf0f7cbd5d" satisfied condition "Succeeded or Failed"
Aug 23 22:09:47.962: INFO: Trying to get logs from node 10.149.248.9 pod pod-secrets-52747bac-aac4-460a-be9e-66cf0f7cbd5d container secret-volume-test: <nil>
STEP: delete the pod
Aug 23 22:09:48.179: INFO: Waiting for pod pod-secrets-52747bac-aac4-460a-be9e-66cf0f7cbd5d to disappear
Aug 23 22:09:48.197: INFO: Pod pod-secrets-52747bac-aac4-460a-be9e-66cf0f7cbd5d no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 22:09:48.197: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4318" for this suite.

• [SLOW TEST:6.748 seconds]
[sig-storage] Secrets
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:36
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":75,"skipped":1223,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should patch a Namespace [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 22:09:48.261: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should patch a Namespace [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a Namespace
STEP: patching the Namespace
STEP: get the Namespace and ensuring it has the label
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 22:09:48.700: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-4402" for this suite.
STEP: Destroying namespace "nspatchtest-9cbc821e-f39d-469c-bf01-6b01011afb3a-107" for this suite.
•{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should patch a Namespace [Conformance]","total":311,"completed":76,"skipped":1257,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 22:09:48.803: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create ConfigMap with empty key [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap that has name configmap-test-emptyKey-fd36c3e8-1440-4017-b4a0-cd1b132ad263
[AfterEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 22:09:49.046: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9822" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should fail to create ConfigMap with empty key [Conformance]","total":311,"completed":77,"skipped":1268,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group but different versions [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 22:09:49.385: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of same group but different versions [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation
Aug 23 22:09:49.642: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation
Aug 23 22:10:40.735: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
Aug 23 22:10:50.022: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 22:11:33.944: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-5126" for this suite.

• [SLOW TEST:104.896 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group but different versions [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group but different versions [Conformance]","total":311,"completed":78,"skipped":1280,"failed":0}
SSSSSS
------------------------------
[sig-network] DNS 
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 22:11:34.281: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-2912.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-2912.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-2912.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-2912.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-2912.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-2912.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe /etc/hosts
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Aug 23 22:11:42.682: INFO: DNS probes using dns-2912/dns-test-40b4f74b-407e-40df-ad25-77d1547fcf60 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 22:11:42.834: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-2912" for this suite.

• [SLOW TEST:8.970 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] DNS should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]","total":311,"completed":79,"skipped":1286,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 22:11:43.251: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Aug 23 22:11:45.640: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63765353505, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63765353505, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63765353505, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63765353505, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 23 22:11:47.674: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63765353505, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63765353505, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63765353505, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63765353505, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 23 22:11:49.678: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63765353505, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63765353505, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63765353505, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63765353505, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Aug 23 22:11:52.720: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Creating a dummy validating-webhook-configuration object
STEP: Deleting the validating-webhook-configuration, which should be possible to remove
STEP: Creating a dummy mutating-webhook-configuration object
STEP: Deleting the mutating-webhook-configuration, which should be possible to remove
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 22:11:53.576: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6637" for this suite.
STEP: Destroying namespace "webhook-6637-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

• [SLOW TEST:10.670 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]","total":311,"completed":80,"skipped":1299,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial] 
  removing taint cancels eviction [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 22:11:53.921: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename taint-single-pod
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/taints.go:164
Aug 23 22:11:54.108: INFO: Waiting up to 1m0s for all nodes to be ready
Aug 23 22:12:55.359: INFO: Waiting for terminating namespaces to be deleted...
[It] removing taint cancels eviction [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Aug 23 22:12:55.524: INFO: Starting informer...
STEP: Starting pod...
Aug 23 22:12:55.666: INFO: Pod is running on 10.149.248.24. Tainting Node
STEP: Trying to apply a taint on the Node
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting short time to make sure Pod is queued for deletion
Aug 23 22:12:55.912: INFO: Pod wasn't evicted. Proceeding
Aug 23 22:12:55.912: INFO: Removing taint from Node
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting some time to make sure that toleration time passed.
Aug 23 22:14:11.197: INFO: Pod wasn't evicted. Test successful
[AfterEach] [k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 22:14:11.197: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-single-pod-373" for this suite.

• [SLOW TEST:137.401 seconds]
[k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  removing taint cancels eviction [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial] removing taint cancels eviction [Disruptive] [Conformance]","total":311,"completed":81,"skipped":1322,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not conflict [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 22:14:11.323: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not conflict [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Cleaning up the secret
STEP: Cleaning up the configmap
STEP: Cleaning up the pod
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 22:14:16.750: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-9912" for this suite.

• [SLOW TEST:5.744 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  should not conflict [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not conflict [Conformance]","total":311,"completed":82,"skipped":1348,"failed":0}
S
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 22:14:17.067: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir 0644 on node default medium
Aug 23 22:14:17.493: INFO: Waiting up to 5m0s for pod "pod-297a4cc9-3417-4b52-b34c-8ea0efb6ec9d" in namespace "emptydir-9" to be "Succeeded or Failed"
Aug 23 22:14:17.621: INFO: Pod "pod-297a4cc9-3417-4b52-b34c-8ea0efb6ec9d": Phase="Pending", Reason="", readiness=false. Elapsed: 128.435311ms
Aug 23 22:14:19.763: INFO: Pod "pod-297a4cc9-3417-4b52-b34c-8ea0efb6ec9d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.270041063s
Aug 23 22:14:21.866: INFO: Pod "pod-297a4cc9-3417-4b52-b34c-8ea0efb6ec9d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.373010279s
Aug 23 22:14:23.959: INFO: Pod "pod-297a4cc9-3417-4b52-b34c-8ea0efb6ec9d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.466150842s
STEP: Saw pod success
Aug 23 22:14:23.959: INFO: Pod "pod-297a4cc9-3417-4b52-b34c-8ea0efb6ec9d" satisfied condition "Succeeded or Failed"
Aug 23 22:14:24.070: INFO: Trying to get logs from node 10.149.248.24 pod pod-297a4cc9-3417-4b52-b34c-8ea0efb6ec9d container test-container: <nil>
STEP: delete the pod
Aug 23 22:14:24.340: INFO: Waiting for pod pod-297a4cc9-3417-4b52-b34c-8ea0efb6ec9d to disappear
Aug 23 22:14:24.422: INFO: Pod pod-297a4cc9-3417-4b52-b34c-8ea0efb6ec9d no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 22:14:24.422: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9" for this suite.

• [SLOW TEST:7.561 seconds]
[sig-storage] EmptyDir volumes
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:45
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":83,"skipped":1349,"failed":0}
[k8s.io] Variable Expansion 
  should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 22:14:24.628: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Aug 23 22:14:31.308: INFO: Deleting pod "var-expansion-1511e612-1067-48ca-82d4-3a7df8ef15de" in namespace "var-expansion-2983"
Aug 23 22:14:31.414: INFO: Wait up to 5m0s for pod "var-expansion-1511e612-1067-48ca-82d4-3a7df8ef15de" to be fully deleted
[AfterEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 22:14:45.612: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-2983" for this suite.

• [SLOW TEST:21.383 seconds]
[k8s.io] Variable Expansion
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]","total":311,"completed":84,"skipped":1349,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 22:14:46.013: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir 0777 on node default medium
Aug 23 22:14:46.674: INFO: Waiting up to 5m0s for pod "pod-5e59f824-fdcc-4544-9e1d-2db9040817e4" in namespace "emptydir-5905" to be "Succeeded or Failed"
Aug 23 22:14:46.804: INFO: Pod "pod-5e59f824-fdcc-4544-9e1d-2db9040817e4": Phase="Pending", Reason="", readiness=false. Elapsed: 129.565599ms
Aug 23 22:14:48.923: INFO: Pod "pod-5e59f824-fdcc-4544-9e1d-2db9040817e4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.248716946s
Aug 23 22:14:51.063: INFO: Pod "pod-5e59f824-fdcc-4544-9e1d-2db9040817e4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.38849692s
Aug 23 22:14:53.174: INFO: Pod "pod-5e59f824-fdcc-4544-9e1d-2db9040817e4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.49966502s
STEP: Saw pod success
Aug 23 22:14:53.174: INFO: Pod "pod-5e59f824-fdcc-4544-9e1d-2db9040817e4" satisfied condition "Succeeded or Failed"
Aug 23 22:14:53.295: INFO: Trying to get logs from node 10.149.248.24 pod pod-5e59f824-fdcc-4544-9e1d-2db9040817e4 container test-container: <nil>
STEP: delete the pod
Aug 23 22:14:53.593: INFO: Waiting for pod pod-5e59f824-fdcc-4544-9e1d-2db9040817e4 to disappear
Aug 23 22:14:53.757: INFO: Pod pod-5e59f824-fdcc-4544-9e1d-2db9040817e4 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 22:14:53.757: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5905" for this suite.

• [SLOW TEST:8.235 seconds]
[sig-storage] EmptyDir volumes
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:45
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":85,"skipped":1361,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 22:14:54.249: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test env composition
Aug 23 22:14:54.882: INFO: Waiting up to 5m0s for pod "var-expansion-b808e9bc-4d0a-4022-96f5-b215e760edb4" in namespace "var-expansion-8" to be "Succeeded or Failed"
Aug 23 22:14:54.978: INFO: Pod "var-expansion-b808e9bc-4d0a-4022-96f5-b215e760edb4": Phase="Pending", Reason="", readiness=false. Elapsed: 96.50182ms
Aug 23 22:14:57.047: INFO: Pod "var-expansion-b808e9bc-4d0a-4022-96f5-b215e760edb4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.165456653s
Aug 23 22:14:59.144: INFO: Pod "var-expansion-b808e9bc-4d0a-4022-96f5-b215e760edb4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.262427414s
Aug 23 22:15:01.291: INFO: Pod "var-expansion-b808e9bc-4d0a-4022-96f5-b215e760edb4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.409377692s
STEP: Saw pod success
Aug 23 22:15:01.291: INFO: Pod "var-expansion-b808e9bc-4d0a-4022-96f5-b215e760edb4" satisfied condition "Succeeded or Failed"
Aug 23 22:15:01.441: INFO: Trying to get logs from node 10.149.248.24 pod var-expansion-b808e9bc-4d0a-4022-96f5-b215e760edb4 container dapi-container: <nil>
STEP: delete the pod
Aug 23 22:15:01.724: INFO: Waiting for pod var-expansion-b808e9bc-4d0a-4022-96f5-b215e760edb4 to disappear
Aug 23 22:15:01.804: INFO: Pod var-expansion-b808e9bc-4d0a-4022-96f5-b215e760edb4 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 22:15:01.804: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-8" for this suite.

• [SLOW TEST:7.831 seconds]
[k8s.io] Variable Expansion
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Variable Expansion should allow composing env vars into new env vars [NodeConformance] [Conformance]","total":311,"completed":86,"skipped":1375,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 22:15:02.080: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Aug 23 22:15:06.785: INFO: Deleting pod "var-expansion-e6035b20-a10f-4efb-a80b-8517d3bf7490" in namespace "var-expansion-9782"
Aug 23 22:15:06.850: INFO: Wait up to 5m0s for pod "var-expansion-e6035b20-a10f-4efb-a80b-8517d3bf7490" to be fully deleted
[AfterEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 22:15:20.940: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-9782" for this suite.

• [SLOW TEST:19.027 seconds]
[k8s.io] Variable Expansion
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Variable Expansion should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]","total":311,"completed":87,"skipped":1405,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD with validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 22:15:21.110: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD with validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Aug 23 22:15:21.447: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: client-side validation (kubectl create and apply) allows request with known and required properties
Aug 23 22:15:32.951: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585719795 --namespace=crd-publish-openapi-8576 --namespace=crd-publish-openapi-8576 create -f -'
Aug 23 22:15:37.218: INFO: stderr: ""
Aug 23 22:15:37.218: INFO: stdout: "e2e-test-crd-publish-openapi-1494-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Aug 23 22:15:37.218: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585719795 --namespace=crd-publish-openapi-8576 --namespace=crd-publish-openapi-8576 delete e2e-test-crd-publish-openapi-1494-crds test-foo'
Aug 23 22:15:37.395: INFO: stderr: ""
Aug 23 22:15:37.395: INFO: stdout: "e2e-test-crd-publish-openapi-1494-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
Aug 23 22:15:37.395: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585719795 --namespace=crd-publish-openapi-8576 --namespace=crd-publish-openapi-8576 apply -f -'
Aug 23 22:15:38.940: INFO: stderr: ""
Aug 23 22:15:38.940: INFO: stdout: "e2e-test-crd-publish-openapi-1494-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Aug 23 22:15:38.940: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585719795 --namespace=crd-publish-openapi-8576 --namespace=crd-publish-openapi-8576 delete e2e-test-crd-publish-openapi-1494-crds test-foo'
Aug 23 22:15:39.599: INFO: stderr: ""
Aug 23 22:15:39.599: INFO: stdout: "e2e-test-crd-publish-openapi-1494-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
STEP: client-side validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema
Aug 23 22:15:39.599: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585719795 --namespace=crd-publish-openapi-8576 --namespace=crd-publish-openapi-8576 create -f -'
Aug 23 22:15:40.498: INFO: rc: 1
Aug 23 22:15:40.498: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585719795 --namespace=crd-publish-openapi-8576 --namespace=crd-publish-openapi-8576 apply -f -'
Aug 23 22:15:41.302: INFO: rc: 1
STEP: client-side validation (kubectl create and apply) rejects request without required properties
Aug 23 22:15:41.302: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585719795 --namespace=crd-publish-openapi-8576 --namespace=crd-publish-openapi-8576 create -f -'
Aug 23 22:15:42.122: INFO: rc: 1
Aug 23 22:15:42.122: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585719795 --namespace=crd-publish-openapi-8576 --namespace=crd-publish-openapi-8576 apply -f -'
Aug 23 22:15:42.838: INFO: rc: 1
STEP: kubectl explain works to explain CR properties
Aug 23 22:15:42.838: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585719795 --namespace=crd-publish-openapi-8576 explain e2e-test-crd-publish-openapi-1494-crds'
Aug 23 22:15:45.337: INFO: stderr: ""
Aug 23 22:15:45.337: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-1494-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
STEP: kubectl explain works to explain CR properties recursively
Aug 23 22:15:45.338: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585719795 --namespace=crd-publish-openapi-8576 explain e2e-test-crd-publish-openapi-1494-crds.metadata'
Aug 23 22:15:46.790: INFO: stderr: ""
Aug 23 22:15:46.790: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-1494-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   clusterName\t<string>\n     The name of the cluster which the object belongs to. This is used to\n     distinguish resources with same name and namespace in different clusters.\n     This field is not set anywhere right now and apiserver is going to ignore\n     it if set in create or update request.\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC.\n\n     Populated by the system. Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested.\n\n     Populated by the system when a graceful deletion is requested. Read-only.\n     More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server.\n\n     If this field is specified and the generated name exists, the server will\n     NOT return a 409 - instead, it will either return 201 Created or 500 with\n     Reason ServerTimeout indicating a unique name could not be found in the\n     time allotted, and the client should retry (optionally after the time\n     indicated in the Retry-After header).\n\n     Applied only if Name is not specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within which each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty.\n\n     Must be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources.\n\n     Populated by the system. Read-only. Value must be treated as opaque by\n     clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     SelfLink is a URL representing this object. Populated by the system.\n     Read-only.\n\n     DEPRECATED Kubernetes will stop propagating this field in 1.20 release and\n     the field is planned to be removed in 1.21 release.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations.\n\n     Populated by the system. Read-only. More info:\n     http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
Aug 23 22:15:46.790: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585719795 --namespace=crd-publish-openapi-8576 explain e2e-test-crd-publish-openapi-1494-crds.spec'
Aug 23 22:15:47.202: INFO: stderr: ""
Aug 23 22:15:47.202: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-1494-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
Aug 23 22:15:47.202: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585719795 --namespace=crd-publish-openapi-8576 explain e2e-test-crd-publish-openapi-1494-crds.spec.bars'
Aug 23 22:15:48.386: INFO: stderr: ""
Aug 23 22:15:48.386: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-1494-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
STEP: kubectl explain works to return error when explain is called on property that doesn't exist
Aug 23 22:15:48.386: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585719795 --namespace=crd-publish-openapi-8576 explain e2e-test-crd-publish-openapi-1494-crds.spec.bars2'
Aug 23 22:15:50.220: INFO: rc: 1
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 22:16:01.649: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-8576" for this suite.

• [SLOW TEST:40.966 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD with validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD with validation schema [Conformance]","total":311,"completed":88,"skipped":1426,"failed":0}
SSS
------------------------------
[sig-network] Services 
  should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 22:16:02.076: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a service nodeport-service with the type=NodePort in namespace services-4915
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-4915
STEP: creating replication controller externalsvc in namespace services-4915
I0823 22:16:03.118332      25 runners.go:190] Created replication controller with name: externalsvc, namespace: services-4915, replica count: 2
I0823 22:16:06.268696      25 runners.go:190] externalsvc Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0823 22:16:09.268972      25 runners.go:190] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the NodePort service to type=ExternalName
Aug 23 22:16:09.624: INFO: Creating new exec pod
Aug 23 22:16:17.828: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585719795 --namespace=services-4915 exec execpod946m6 -- /bin/sh -x -c nslookup nodeport-service.services-4915.svc.cluster.local'
Aug 23 22:16:19.608: INFO: stderr: "+ nslookup nodeport-service.services-4915.svc.cluster.local\n"
Aug 23 22:16:19.608: INFO: stdout: "Server:\t\t172.21.0.10\nAddress:\t172.21.0.10#53\n\nnodeport-service.services-4915.svc.cluster.local\tcanonical name = externalsvc.services-4915.svc.cluster.local.\nName:\texternalsvc.services-4915.svc.cluster.local\nAddress: 172.21.133.22\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-4915, will wait for the garbage collector to delete the pods
Aug 23 22:16:19.838: INFO: Deleting ReplicationController externalsvc took: 78.188649ms
Aug 23 22:16:20.038: INFO: Terminating ReplicationController externalsvc pods took: 200.448707ms
Aug 23 22:16:36.223: INFO: Cleaning up the NodePort to ExternalName test service
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 22:16:36.282: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4915" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749

• [SLOW TEST:34.345 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from NodePort to ExternalName [Conformance]","total":311,"completed":89,"skipped":1429,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 22:16:36.421: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:41
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Aug 23 22:16:36.709: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d1267702-c925-42a9-9a6d-e5acfa9ee135" in namespace "downward-api-1212" to be "Succeeded or Failed"
Aug 23 22:16:36.745: INFO: Pod "downwardapi-volume-d1267702-c925-42a9-9a6d-e5acfa9ee135": Phase="Pending", Reason="", readiness=false. Elapsed: 36.654853ms
Aug 23 22:16:38.824: INFO: Pod "downwardapi-volume-d1267702-c925-42a9-9a6d-e5acfa9ee135": Phase="Pending", Reason="", readiness=false. Elapsed: 2.114925251s
Aug 23 22:16:40.966: INFO: Pod "downwardapi-volume-d1267702-c925-42a9-9a6d-e5acfa9ee135": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.257020804s
STEP: Saw pod success
Aug 23 22:16:40.966: INFO: Pod "downwardapi-volume-d1267702-c925-42a9-9a6d-e5acfa9ee135" satisfied condition "Succeeded or Failed"
Aug 23 22:16:41.098: INFO: Trying to get logs from node 10.149.248.24 pod downwardapi-volume-d1267702-c925-42a9-9a6d-e5acfa9ee135 container client-container: <nil>
STEP: delete the pod
Aug 23 22:16:41.414: INFO: Waiting for pod downwardapi-volume-d1267702-c925-42a9-9a6d-e5acfa9ee135 to disappear
Aug 23 22:16:41.528: INFO: Pod downwardapi-volume-d1267702-c925-42a9-9a6d-e5acfa9ee135 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 22:16:41.528: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1212" for this suite.

• [SLOW TEST:5.471 seconds]
[sig-storage] Downward API volume
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:36
  should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should provide podname only [NodeConformance] [Conformance]","total":311,"completed":90,"skipped":1437,"failed":0}
SS
------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 22:16:41.892: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a service externalname-service with the type=ExternalName in namespace services-3450
STEP: changing the ExternalName service to type=ClusterIP
STEP: creating replication controller externalname-service in namespace services-3450
I0823 22:16:43.034537      25 runners.go:190] Created replication controller with name: externalname-service, namespace: services-3450, replica count: 2
I0823 22:16:46.184946      25 runners.go:190] externalname-service Pods: 2 out of 2 created, 1 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0823 22:16:49.185185      25 runners.go:190] externalname-service Pods: 2 out of 2 created, 1 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Aug 23 22:16:52.185: INFO: Creating new exec pod
I0823 22:16:52.185425      25 runners.go:190] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Aug 23 22:16:57.747: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585719795 --namespace=services-3450 exec execpod6nj9k -- /bin/sh -x -c nc -zv -t -w 2 externalname-service 80'
Aug 23 22:16:58.388: INFO: stderr: "+ nc -zv -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Aug 23 22:16:58.388: INFO: stdout: ""
Aug 23 22:16:58.389: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585719795 --namespace=services-3450 exec execpod6nj9k -- /bin/sh -x -c nc -zv -t -w 2 172.21.114.119 80'
Aug 23 22:16:59.114: INFO: stderr: "+ nc -zv -t -w 2 172.21.114.119 80\nConnection to 172.21.114.119 80 port [tcp/http] succeeded!\n"
Aug 23 22:16:59.114: INFO: stdout: ""
Aug 23 22:16:59.114: INFO: Cleaning up the ExternalName to ClusterIP test service
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 22:16:59.228: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3450" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749

• [SLOW TEST:17.661 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to ClusterIP [Conformance]","total":311,"completed":91,"skipped":1439,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl server-side dry-run 
  should check if kubectl can dry-run update Pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 22:16:59.553: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should check if kubectl can dry-run update Pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Aug 23 22:16:59.935: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585719795 --namespace=kubectl-2188 run e2e-test-httpd-pod --image=docker.io/library/httpd:2.4.38-alpine --labels=run=e2e-test-httpd-pod'
Aug 23 22:17:00.777: INFO: stderr: ""
Aug 23 22:17:00.777: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: replace the image in the pod with server-side dry-run
Aug 23 22:17:00.777: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585719795 --namespace=kubectl-2188 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "docker.io/library/busybox:1.29"}]}} --dry-run=server'
Aug 23 22:17:02.059: INFO: stderr: ""
Aug 23 22:17:02.059: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image docker.io/library/httpd:2.4.38-alpine
Aug 23 22:17:02.169: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585719795 --namespace=kubectl-2188 delete pods e2e-test-httpd-pod'
Aug 23 22:17:10.688: INFO: stderr: ""
Aug 23 22:17:10.688: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 22:17:10.688: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2188" for this suite.

• [SLOW TEST:11.661 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl server-side dry-run
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:909
    should check if kubectl can dry-run update Pods [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl server-side dry-run should check if kubectl can dry-run update Pods [Conformance]","total":311,"completed":92,"skipped":1450,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 22:17:11.215: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:41
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Aug 23 22:17:11.973: INFO: Waiting up to 5m0s for pod "downwardapi-volume-47fdea68-678c-44ef-901e-f1ba1df51511" in namespace "downward-api-8898" to be "Succeeded or Failed"
Aug 23 22:17:12.112: INFO: Pod "downwardapi-volume-47fdea68-678c-44ef-901e-f1ba1df51511": Phase="Pending", Reason="", readiness=false. Elapsed: 138.893581ms
Aug 23 22:17:14.282: INFO: Pod "downwardapi-volume-47fdea68-678c-44ef-901e-f1ba1df51511": Phase="Pending", Reason="", readiness=false. Elapsed: 2.309672174s
Aug 23 22:17:16.415: INFO: Pod "downwardapi-volume-47fdea68-678c-44ef-901e-f1ba1df51511": Phase="Pending", Reason="", readiness=false. Elapsed: 4.442633016s
Aug 23 22:17:18.527: INFO: Pod "downwardapi-volume-47fdea68-678c-44ef-901e-f1ba1df51511": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.554439325s
STEP: Saw pod success
Aug 23 22:17:18.527: INFO: Pod "downwardapi-volume-47fdea68-678c-44ef-901e-f1ba1df51511" satisfied condition "Succeeded or Failed"
Aug 23 22:17:18.669: INFO: Trying to get logs from node 10.149.248.24 pod downwardapi-volume-47fdea68-678c-44ef-901e-f1ba1df51511 container client-container: <nil>
STEP: delete the pod
Aug 23 22:17:18.979: INFO: Waiting for pod downwardapi-volume-47fdea68-678c-44ef-901e-f1ba1df51511 to disappear
Aug 23 22:17:19.100: INFO: Pod downwardapi-volume-47fdea68-678c-44ef-901e-f1ba1df51511 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 22:17:19.100: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8898" for this suite.

• [SLOW TEST:8.377 seconds]
[sig-storage] Downward API volume
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:36
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory request [NodeConformance] [Conformance]","total":311,"completed":93,"skipped":1496,"failed":0}
S
------------------------------
[sig-cli] Kubectl client Kubectl replace 
  should update a single-container pod's image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 22:17:19.592: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[BeforeEach] Kubectl replace
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1554
[It] should update a single-container pod's image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Aug 23 22:17:20.343: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585719795 --namespace=kubectl-1357 run e2e-test-httpd-pod --image=docker.io/library/httpd:2.4.38-alpine --labels=run=e2e-test-httpd-pod'
Aug 23 22:17:20.502: INFO: stderr: ""
Aug 23 22:17:20.502: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod is running
STEP: verifying the pod e2e-test-httpd-pod was created
Aug 23 22:17:30.652: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585719795 --namespace=kubectl-1357 get pod e2e-test-httpd-pod -o json'
Aug 23 22:17:31.414: INFO: stderr: ""
Aug 23 22:17:31.414: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"cni.projectcalico.org/podIP\": \"172.30.87.74/32\",\n            \"cni.projectcalico.org/podIPs\": \"172.30.87.74/32\",\n            \"k8s.v1.cni.cncf.io/network-status\": \"[{\\n    \\\"name\\\": \\\"\\\",\\n    \\\"ips\\\": [\\n        \\\"172.30.87.74\\\"\\n    ],\\n    \\\"default\\\": true,\\n    \\\"dns\\\": {}\\n}]\",\n            \"k8s.v1.cni.cncf.io/networks-status\": \"[{\\n    \\\"name\\\": \\\"\\\",\\n    \\\"ips\\\": [\\n        \\\"172.30.87.74\\\"\\n    ],\\n    \\\"default\\\": true,\\n    \\\"dns\\\": {}\\n}]\",\n            \"openshift.io/scc\": \"anyuid\"\n        },\n        \"creationTimestamp\": \"2021-08-23T22:17:20Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"managedFields\": [\n            {\n                \"apiVersion\": \"v1\",\n                \"fieldsType\": \"FieldsV1\",\n                \"fieldsV1\": {\n                    \"f:metadata\": {\n                        \"f:labels\": {\n                            \".\": {},\n                            \"f:run\": {}\n                        }\n                    },\n                    \"f:spec\": {\n                        \"f:containers\": {\n                            \"k:{\\\"name\\\":\\\"e2e-test-httpd-pod\\\"}\": {\n                                \".\": {},\n                                \"f:image\": {},\n                                \"f:imagePullPolicy\": {},\n                                \"f:name\": {},\n                                \"f:resources\": {},\n                                \"f:terminationMessagePath\": {},\n                                \"f:terminationMessagePolicy\": {}\n                            }\n                        },\n                        \"f:dnsPolicy\": {},\n                        \"f:enableServiceLinks\": {},\n                        \"f:restartPolicy\": {},\n                        \"f:schedulerName\": {},\n                        \"f:securityContext\": {\n                            \".\": {},\n                            \"f:seLinuxOptions\": {\n                                \"f:level\": {}\n                            }\n                        },\n                        \"f:terminationGracePeriodSeconds\": {}\n                    }\n                },\n                \"manager\": \"kubectl-run\",\n                \"operation\": \"Update\",\n                \"time\": \"2021-08-23T22:17:20Z\"\n            },\n            {\n                \"apiVersion\": \"v1\",\n                \"fieldsType\": \"FieldsV1\",\n                \"fieldsV1\": {\n                    \"f:metadata\": {\n                        \"f:annotations\": {\n                            \"f:cni.projectcalico.org/podIP\": {},\n                            \"f:cni.projectcalico.org/podIPs\": {}\n                        }\n                    }\n                },\n                \"manager\": \"calico\",\n                \"operation\": \"Update\",\n                \"time\": \"2021-08-23T22:17:24Z\"\n            },\n            {\n                \"apiVersion\": \"v1\",\n                \"fieldsType\": \"FieldsV1\",\n                \"fieldsV1\": {\n                    \"f:metadata\": {\n                        \"f:annotations\": {\n                            \"f:k8s.v1.cni.cncf.io/network-status\": {},\n                            \"f:k8s.v1.cni.cncf.io/networks-status\": {}\n                        }\n                    }\n                },\n                \"manager\": \"multus\",\n                \"operation\": \"Update\",\n                \"time\": \"2021-08-23T22:17:25Z\"\n            },\n            {\n                \"apiVersion\": \"v1\",\n                \"fieldsType\": \"FieldsV1\",\n                \"fieldsV1\": {\n                    \"f:status\": {\n                        \"f:conditions\": {\n                            \"k:{\\\"type\\\":\\\"ContainersReady\\\"}\": {\n                                \".\": {},\n                                \"f:lastProbeTime\": {},\n                                \"f:lastTransitionTime\": {},\n                                \"f:status\": {},\n                                \"f:type\": {}\n                            },\n                            \"k:{\\\"type\\\":\\\"Initialized\\\"}\": {\n                                \".\": {},\n                                \"f:lastProbeTime\": {},\n                                \"f:lastTransitionTime\": {},\n                                \"f:status\": {},\n                                \"f:type\": {}\n                            },\n                            \"k:{\\\"type\\\":\\\"Ready\\\"}\": {\n                                \".\": {},\n                                \"f:lastProbeTime\": {},\n                                \"f:lastTransitionTime\": {},\n                                \"f:status\": {},\n                                \"f:type\": {}\n                            }\n                        },\n                        \"f:containerStatuses\": {},\n                        \"f:hostIP\": {},\n                        \"f:phase\": {},\n                        \"f:podIP\": {},\n                        \"f:podIPs\": {\n                            \".\": {},\n                            \"k:{\\\"ip\\\":\\\"172.30.87.74\\\"}\": {\n                                \".\": {},\n                                \"f:ip\": {}\n                            }\n                        },\n                        \"f:startTime\": {}\n                    }\n                },\n                \"manager\": \"kubelet\",\n                \"operation\": \"Update\",\n                \"time\": \"2021-08-23T22:17:26Z\"\n            }\n        ],\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-1357\",\n        \"resourceVersion\": \"76791\",\n        \"selfLink\": \"/api/v1/namespaces/kubectl-1357/pods/e2e-test-httpd-pod\",\n        \"uid\": \"8fdd11fd-5ba1-493a-a335-2c2d8663b628\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"docker.io/library/httpd:2.4.38-alpine\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"securityContext\": {\n                    \"capabilities\": {\n                        \"drop\": [\n                            \"MKNOD\"\n                        ]\n                    }\n                },\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"default-token-hwgvn\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"imagePullSecrets\": [\n            {\n                \"name\": \"default-dockercfg-fmjg4\"\n            }\n        ],\n        \"nodeName\": \"10.149.248.24\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {\n            \"seLinuxOptions\": {\n                \"level\": \"s0:c43,c17\"\n            }\n        },\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"default-token-hwgvn\",\n                \"secret\": {\n                    \"defaultMode\": 420,\n                    \"secretName\": \"default-token-hwgvn\"\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2021-08-23T22:17:20Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2021-08-23T22:17:26Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2021-08-23T22:17:26Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2021-08-23T22:17:20Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"cri-o://191ec879b3f31897d7906ff362e5c2d533ee92acf531800e7276298028912a3c\",\n                \"image\": \"docker.io/library/httpd:2.4.38-alpine\",\n                \"imageID\": \"docker.io/library/httpd@sha256:6feb0ea7b0967367da66e8d58ba813fde32bdb92f63bfc21a9e170d211539db4\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2021-08-23T22:17:25Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"10.149.248.24\",\n        \"phase\": \"Running\",\n        \"podIP\": \"172.30.87.74\",\n        \"podIPs\": [\n            {\n                \"ip\": \"172.30.87.74\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2021-08-23T22:17:20Z\"\n    }\n}\n"
STEP: replace the image in the pod
Aug 23 22:17:31.414: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585719795 --namespace=kubectl-1357 replace -f -'
Aug 23 22:17:32.265: INFO: stderr: ""
Aug 23 22:17:32.265: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image docker.io/library/busybox:1.29
[AfterEach] Kubectl replace
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1558
Aug 23 22:17:32.471: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585719795 --namespace=kubectl-1357 delete pods e2e-test-httpd-pod'
Aug 23 22:17:43.794: INFO: stderr: ""
Aug 23 22:17:43.794: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 22:17:43.794: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1357" for this suite.

• [SLOW TEST:24.743 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl replace
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1551
    should update a single-container pod's image  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl replace should update a single-container pod's image  [Conformance]","total":311,"completed":94,"skipped":1497,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should get a host IP [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 22:17:44.335: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:187
[It] should get a host IP [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating pod
Aug 23 22:17:54.011: INFO: Pod pod-hostip-a83c05d9-1335-41e6-8c64-457da3f7b665 has hostIP: 10.149.248.24
[AfterEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 22:17:54.011: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-5429" for this suite.

• [SLOW TEST:10.347 seconds]
[k8s.io] Pods
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should get a host IP [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Pods should get a host IP [NodeConformance] [Conformance]","total":311,"completed":95,"skipped":1531,"failed":0}
SSSSSSSSSSSSS
------------------------------
[k8s.io] Security Context When creating a pod with readOnlyRootFilesystem 
  should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 22:17:54.683: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:41
[It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Aug 23 22:17:55.687: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-acf12a29-6fad-4fc5-a4b7-99e4cfe8be34" in namespace "security-context-test-5115" to be "Succeeded or Failed"
Aug 23 22:17:55.892: INFO: Pod "busybox-readonly-false-acf12a29-6fad-4fc5-a4b7-99e4cfe8be34": Phase="Pending", Reason="", readiness=false. Elapsed: 204.608078ms
Aug 23 22:17:58.048: INFO: Pod "busybox-readonly-false-acf12a29-6fad-4fc5-a4b7-99e4cfe8be34": Phase="Pending", Reason="", readiness=false. Elapsed: 2.360611815s
Aug 23 22:18:00.186: INFO: Pod "busybox-readonly-false-acf12a29-6fad-4fc5-a4b7-99e4cfe8be34": Phase="Pending", Reason="", readiness=false. Elapsed: 4.499014289s
Aug 23 22:18:02.314: INFO: Pod "busybox-readonly-false-acf12a29-6fad-4fc5-a4b7-99e4cfe8be34": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.626910078s
Aug 23 22:18:02.314: INFO: Pod "busybox-readonly-false-acf12a29-6fad-4fc5-a4b7-99e4cfe8be34" satisfied condition "Succeeded or Failed"
[AfterEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 22:18:02.314: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-5115" for this suite.

• [SLOW TEST:8.040 seconds]
[k8s.io] Security Context
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  When creating a pod with readOnlyRootFilesystem
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:166
    should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Security Context When creating a pod with readOnlyRootFilesystem should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]","total":311,"completed":96,"skipped":1544,"failed":0}
SS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 22:18:02.723: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:41
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Aug 23 22:18:03.537: INFO: Waiting up to 5m0s for pod "downwardapi-volume-bdf81ee9-0a63-4bc5-862e-2b48c5c47d62" in namespace "projected-7189" to be "Succeeded or Failed"
Aug 23 22:18:03.678: INFO: Pod "downwardapi-volume-bdf81ee9-0a63-4bc5-862e-2b48c5c47d62": Phase="Pending", Reason="", readiness=false. Elapsed: 140.885288ms
Aug 23 22:18:05.810: INFO: Pod "downwardapi-volume-bdf81ee9-0a63-4bc5-862e-2b48c5c47d62": Phase="Pending", Reason="", readiness=false. Elapsed: 2.273248765s
Aug 23 22:18:07.946: INFO: Pod "downwardapi-volume-bdf81ee9-0a63-4bc5-862e-2b48c5c47d62": Phase="Pending", Reason="", readiness=false. Elapsed: 4.408612963s
Aug 23 22:18:10.083: INFO: Pod "downwardapi-volume-bdf81ee9-0a63-4bc5-862e-2b48c5c47d62": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.545783705s
STEP: Saw pod success
Aug 23 22:18:10.083: INFO: Pod "downwardapi-volume-bdf81ee9-0a63-4bc5-862e-2b48c5c47d62" satisfied condition "Succeeded or Failed"
Aug 23 22:18:10.243: INFO: Trying to get logs from node 10.149.248.24 pod downwardapi-volume-bdf81ee9-0a63-4bc5-862e-2b48c5c47d62 container client-container: <nil>
STEP: delete the pod
Aug 23 22:18:10.604: INFO: Waiting for pod downwardapi-volume-bdf81ee9-0a63-4bc5-862e-2b48c5c47d62 to disappear
Aug 23 22:18:10.779: INFO: Pod downwardapi-volume-bdf81ee9-0a63-4bc5-862e-2b48c5c47d62 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 22:18:10.779: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7189" for this suite.

• [SLOW TEST:8.593 seconds]
[sig-storage] Projected downwardAPI
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:35
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory limit [NodeConformance] [Conformance]","total":311,"completed":97,"skipped":1546,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 22:18:11.317: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Aug 23 22:18:13.734: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63765353893, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63765353893, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63765353893, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63765353893, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 23 22:18:15.861: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63765353893, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63765353893, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63765353893, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63765353893, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Aug 23 22:18:18.969: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Aug 23 22:18:19.054: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Registering the custom resource webhook via the AdmissionRegistration API
STEP: Creating a custom resource that should be denied by the webhook
STEP: Creating a custom resource whose deletion would be denied by the webhook
STEP: Updating the custom resource with disallowed data should be denied
STEP: Deleting the custom resource should be denied
STEP: Remove the offending key and value from the custom resource data
STEP: Deleting the updated custom resource should be successful
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 22:18:20.782: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2147" for this suite.
STEP: Destroying namespace "webhook-2147-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

• [SLOW TEST:10.211 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny custom resource creation, update and deletion [Conformance]","total":311,"completed":98,"skipped":1561,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] Aggregator 
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 22:18:21.529: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename aggregator
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:76
Aug 23 22:18:22.180: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
[It] Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Registering the sample API server.
Aug 23 22:18:24.004: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63765353903, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63765353903, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63765353903, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63765353902, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 23 22:18:26.133: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63765353903, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63765353903, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63765353903, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63765353902, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 23 22:18:28.165: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63765353903, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63765353903, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63765353903, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63765353902, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 23 22:18:30.081: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63765353903, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63765353903, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63765353903, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63765353902, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 23 22:18:32.032: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63765353903, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63765353903, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63765353903, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63765353902, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 23 22:18:34.022: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63765353903, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63765353903, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63765353903, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63765353902, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 23 22:18:36.045: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63765353903, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63765353903, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63765353903, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63765353902, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 23 22:18:40.819: INFO: Waited 2.773240212s for the sample-apiserver to be ready to handle requests.
I0823 22:18:42.236997      25 request.go:655] Throttling request took 1.003035126s, request: GET:https://172.21.0.1:443/apis/ingress.operator.openshift.io/v1?timeout=32s
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:67
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 22:18:46.350: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "aggregator-6896" for this suite.

• [SLOW TEST:25.179 seconds]
[sig-api-machinery] Aggregator
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] Aggregator Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]","total":311,"completed":99,"skipped":1566,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should be possible to delete [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 22:18:46.711: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:82
[It] should be possible to delete [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[AfterEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 22:18:47.577: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-6111" for this suite.
•{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox command that always fails in a pod should be possible to delete [NodeConformance] [Conformance]","total":311,"completed":100,"skipped":1593,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 22:18:48.028: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:53
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[AfterEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 22:19:48.711: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-4871" for this suite.

• [SLOW TEST:61.604 seconds]
[k8s.io] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Probing container with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]","total":311,"completed":101,"skipped":1616,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 22:19:49.632: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating pod pod-subpath-test-configmap-g4cg
STEP: Creating a pod to test atomic-volume-subpath
Aug 23 22:19:50.338: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-g4cg" in namespace "subpath-6324" to be "Succeeded or Failed"
Aug 23 22:19:50.450: INFO: Pod "pod-subpath-test-configmap-g4cg": Phase="Pending", Reason="", readiness=false. Elapsed: 112.31219ms
Aug 23 22:19:52.583: INFO: Pod "pod-subpath-test-configmap-g4cg": Phase="Pending", Reason="", readiness=false. Elapsed: 2.245277213s
Aug 23 22:19:54.736: INFO: Pod "pod-subpath-test-configmap-g4cg": Phase="Pending", Reason="", readiness=false. Elapsed: 4.398887725s
Aug 23 22:19:56.865: INFO: Pod "pod-subpath-test-configmap-g4cg": Phase="Running", Reason="", readiness=true. Elapsed: 6.527677129s
Aug 23 22:19:58.989: INFO: Pod "pod-subpath-test-configmap-g4cg": Phase="Running", Reason="", readiness=true. Elapsed: 8.651156396s
Aug 23 22:20:01.098: INFO: Pod "pod-subpath-test-configmap-g4cg": Phase="Running", Reason="", readiness=true. Elapsed: 10.760379481s
Aug 23 22:20:03.252: INFO: Pod "pod-subpath-test-configmap-g4cg": Phase="Running", Reason="", readiness=true. Elapsed: 12.913956144s
Aug 23 22:20:05.359: INFO: Pod "pod-subpath-test-configmap-g4cg": Phase="Running", Reason="", readiness=true. Elapsed: 15.020979416s
Aug 23 22:20:07.492: INFO: Pod "pod-subpath-test-configmap-g4cg": Phase="Running", Reason="", readiness=true. Elapsed: 17.154012121s
Aug 23 22:20:09.714: INFO: Pod "pod-subpath-test-configmap-g4cg": Phase="Running", Reason="", readiness=true. Elapsed: 19.375935995s
Aug 23 22:20:11.881: INFO: Pod "pod-subpath-test-configmap-g4cg": Phase="Running", Reason="", readiness=true. Elapsed: 21.543890009s
Aug 23 22:20:14.032: INFO: Pod "pod-subpath-test-configmap-g4cg": Phase="Running", Reason="", readiness=true. Elapsed: 23.69473347s
Aug 23 22:20:16.133: INFO: Pod "pod-subpath-test-configmap-g4cg": Phase="Succeeded", Reason="", readiness=false. Elapsed: 25.795503425s
STEP: Saw pod success
Aug 23 22:20:16.133: INFO: Pod "pod-subpath-test-configmap-g4cg" satisfied condition "Succeeded or Failed"
Aug 23 22:20:16.252: INFO: Trying to get logs from node 10.149.248.24 pod pod-subpath-test-configmap-g4cg container test-container-subpath-configmap-g4cg: <nil>
STEP: delete the pod
Aug 23 22:20:16.865: INFO: Waiting for pod pod-subpath-test-configmap-g4cg to disappear
Aug 23 22:20:16.997: INFO: Pod pod-subpath-test-configmap-g4cg no longer exists
STEP: Deleting pod pod-subpath-test-configmap-g4cg
Aug 23 22:20:16.997: INFO: Deleting pod "pod-subpath-test-configmap-g4cg" in namespace "subpath-6324"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 22:20:17.154: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-6324" for this suite.

• [SLOW TEST:28.486 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]","total":311,"completed":102,"skipped":1639,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 22:20:18.119: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name projected-configmap-test-volume-map-02a43a53-206d-4203-a4c9-de11c0e8b85e
STEP: Creating a pod to test consume configMaps
Aug 23 22:20:19.547: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-9da887b9-9c24-4a6c-af82-5e3f83038516" in namespace "projected-3148" to be "Succeeded or Failed"
Aug 23 22:20:19.713: INFO: Pod "pod-projected-configmaps-9da887b9-9c24-4a6c-af82-5e3f83038516": Phase="Pending", Reason="", readiness=false. Elapsed: 166.289611ms
Aug 23 22:20:21.911: INFO: Pod "pod-projected-configmaps-9da887b9-9c24-4a6c-af82-5e3f83038516": Phase="Pending", Reason="", readiness=false. Elapsed: 2.364162891s
Aug 23 22:20:24.090: INFO: Pod "pod-projected-configmaps-9da887b9-9c24-4a6c-af82-5e3f83038516": Phase="Pending", Reason="", readiness=false. Elapsed: 4.542694925s
Aug 23 22:20:26.209: INFO: Pod "pod-projected-configmaps-9da887b9-9c24-4a6c-af82-5e3f83038516": Phase="Pending", Reason="", readiness=false. Elapsed: 6.662426585s
Aug 23 22:20:28.322: INFO: Pod "pod-projected-configmaps-9da887b9-9c24-4a6c-af82-5e3f83038516": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.775579248s
STEP: Saw pod success
Aug 23 22:20:28.323: INFO: Pod "pod-projected-configmaps-9da887b9-9c24-4a6c-af82-5e3f83038516" satisfied condition "Succeeded or Failed"
Aug 23 22:20:28.430: INFO: Trying to get logs from node 10.149.248.24 pod pod-projected-configmaps-9da887b9-9c24-4a6c-af82-5e3f83038516 container agnhost-container: <nil>
STEP: delete the pod
Aug 23 22:20:28.734: INFO: Waiting for pod pod-projected-configmaps-9da887b9-9c24-4a6c-af82-5e3f83038516 to disappear
Aug 23 22:20:28.855: INFO: Pod pod-projected-configmaps-9da887b9-9c24-4a6c-af82-5e3f83038516 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 22:20:28.856: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3148" for this suite.

• [SLOW TEST:11.470 seconds]
[sig-storage] Projected configMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:36
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","total":311,"completed":103,"skipped":1657,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a validating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 22:20:29.591: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Aug 23 22:20:31.939: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63765354031, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63765354031, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63765354031, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63765354031, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 23 22:20:34.072: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63765354031, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63765354031, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63765354031, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63765354031, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 23 22:20:35.962: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63765354031, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63765354031, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63765354031, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63765354031, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 23 22:20:37.957: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63765354031, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63765354031, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63765354031, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63765354031, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Aug 23 22:20:41.001: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a validating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a validating webhook configuration
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Updating a validating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Patching a validating webhook configuration's rules to include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 22:20:41.631: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2635" for this suite.
STEP: Destroying namespace "webhook-2635-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

• [SLOW TEST:12.501 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  patching/updating a validating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a validating webhook should work [Conformance]","total":311,"completed":104,"skipped":1674,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Should recreate evicted statefulset [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 22:20:42.092: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-9123
[It] Should recreate evicted statefulset [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Looking for a node to schedule stateful set and pod
STEP: Creating pod with conflicting port in namespace statefulset-9123
STEP: Creating statefulset with conflicting port in namespace statefulset-9123
STEP: Waiting until pod test-pod will start running in namespace statefulset-9123
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-9123
Aug 23 22:20:49.045: INFO: Observed stateful pod in namespace: statefulset-9123, name: ss-0, uid: 7f5b3fb0-055f-4d89-bbfa-ae632f5e4b04, status phase: Failed. Waiting for statefulset controller to delete.
Aug 23 22:20:49.059: INFO: Observed stateful pod in namespace: statefulset-9123, name: ss-0, uid: 7f5b3fb0-055f-4d89-bbfa-ae632f5e4b04, status phase: Failed. Waiting for statefulset controller to delete.
Aug 23 22:20:49.074: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-9123
STEP: Removing pod with conflicting port in namespace statefulset-9123
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-9123 and will be in running state
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
Aug 23 22:20:53.255: INFO: Deleting all statefulset in ns statefulset-9123
Aug 23 22:20:53.316: INFO: Scaling statefulset ss to 0
Aug 23 22:21:03.609: INFO: Waiting for statefulset status.replicas updated to 0
Aug 23 22:21:03.712: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 22:21:04.032: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-9123" for this suite.

• [SLOW TEST:22.645 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
    Should recreate evicted statefulset [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] Should recreate evicted statefulset [Conformance]","total":311,"completed":105,"skipped":1687,"failed":0}
[sig-api-machinery] Events 
  should delete a collection of events [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 22:21:04.737: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a collection of events [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Create set of events
Aug 23 22:21:05.573: INFO: created test-event-1
Aug 23 22:21:05.700: INFO: created test-event-2
Aug 23 22:21:05.843: INFO: created test-event-3
STEP: get a list of Events with a label in the current namespace
STEP: delete collection of events
Aug 23 22:21:05.990: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity
Aug 23 22:21:06.204: INFO: requesting list of events to confirm quantity
[AfterEach] [sig-api-machinery] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 22:21:06.314: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-897" for this suite.
•{"msg":"PASSED [sig-api-machinery] Events should delete a collection of events [Conformance]","total":311,"completed":106,"skipped":1687,"failed":0}
SSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 22:21:06.870: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:53
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating pod liveness-3e8221ee-3e6c-4c88-a97e-e8c7f846c802 in namespace container-probe-8460
Aug 23 22:21:15.836: INFO: Started pod liveness-3e8221ee-3e6c-4c88-a97e-e8c7f846c802 in namespace container-probe-8460
STEP: checking the pod's current state and verifying that restartCount is present
Aug 23 22:21:15.929: INFO: Initial restart count of pod liveness-3e8221ee-3e6c-4c88-a97e-e8c7f846c802 is 0
Aug 23 22:21:37.271: INFO: Restart count of pod container-probe-8460/liveness-3e8221ee-3e6c-4c88-a97e-e8c7f846c802 is now 1 (21.341881046s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 22:21:37.408: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-8460" for this suite.

• [SLOW TEST:31.006 seconds]
[k8s.io] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Probing container should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","total":311,"completed":107,"skipped":1696,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should delete old replica sets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 22:21:37.876: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:85
[It] deployment should delete old replica sets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Aug 23 22:21:38.393: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Aug 23 22:21:48.514: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:79
Aug 23 22:21:54.934: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:{test-cleanup-deployment  deployment-479 /apis/apps/v1/namespaces/deployment-479/deployments/test-cleanup-deployment c9a00244-8456-45c4-b7eb-b3e7e4ba93f0 79823 1 2021-08-23 22:21:48 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[deployment.kubernetes.io/revision:1] [] []  [{e2e.test Update apps/v1 2021-08-23 22:21:48 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-08-23 22:21:54 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}}}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.21 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00328bb28 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2021-08-23 22:21:48 +0000 UTC,LastTransitionTime:2021-08-23 22:21:48 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-cleanup-deployment-685c4f8568" has successfully progressed.,LastUpdateTime:2021-08-23 22:21:54 +0000 UTC,LastTransitionTime:2021-08-23 22:21:48 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Aug 23 22:21:55.034: INFO: New ReplicaSet "test-cleanup-deployment-685c4f8568" of Deployment "test-cleanup-deployment":
&ReplicaSet{ObjectMeta:{test-cleanup-deployment-685c4f8568  deployment-479 /apis/apps/v1/namespaces/deployment-479/replicasets/test-cleanup-deployment-685c4f8568 1fda9c5b-a702-4190-96f9-dbd0ed9e6231 79813 1 2021-08-23 22:21:48 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:685c4f8568] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-cleanup-deployment c9a00244-8456-45c4-b7eb-b3e7e4ba93f0 0xc0074a7987 0xc0074a7988}] []  [{kube-controller-manager Update apps/v1 2021-08-23 22:21:54 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c9a00244-8456-45c4-b7eb-b3e7e4ba93f0\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 685c4f8568,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:685c4f8568] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.21 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0074a7a18 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Aug 23 22:21:55.132: INFO: Pod "test-cleanup-deployment-685c4f8568-jfpvq" is available:
&Pod{ObjectMeta:{test-cleanup-deployment-685c4f8568-jfpvq test-cleanup-deployment-685c4f8568- deployment-479 /api/v1/namespaces/deployment-479/pods/test-cleanup-deployment-685c4f8568-jfpvq 8518d0fd-a74c-43f9-ba23-18bf47a0e9ef 79811 0 2021-08-23 22:21:48 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:685c4f8568] map[cni.projectcalico.org/podIP:172.30.87.64/32 cni.projectcalico.org/podIPs:172.30.87.64/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "",
    "ips": [
        "172.30.87.64"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "",
    "ips": [
        "172.30.87.64"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet test-cleanup-deployment-685c4f8568 1fda9c5b-a702-4190-96f9-dbd0ed9e6231 0xc0074a7da7 0xc0074a7da8}] []  [{kube-controller-manager Update v1 2021-08-23 22:21:48 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1fda9c5b-a702-4190-96f9-dbd0ed9e6231\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{".":{},"f:capabilities":{"f:drop":{}}},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{".":{},"f:seLinuxOptions":{"f:level":{}}},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-08-23 22:21:53 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {multus Update v1 2021-08-23 22:21:53 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}}} {kubelet Update v1 2021-08-23 22:21:54 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.87.64\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-kjsmn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-kjsmn,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:k8s.gcr.io/e2e-test-images/agnhost:2.21,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-kjsmn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.149.248.24,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c45,c10,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-8hgg7,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-08-23 22:21:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-08-23 22:21:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-08-23 22:21:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-08-23 22:21:48 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.149.248.24,PodIP:172.30.87.64,StartTime:2021-08-23 22:21:48 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-08-23 22:21:54 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.21,ImageID:k8s.gcr.io/e2e-test-images/agnhost@sha256:ab055cd3d45f50b90732c14593a5bf50f210871bb4f91994c756fc22db6d922a,ContainerID:cri-o://3108d563c5831e37ef6de4b1a157996db81d582e6792166edd04420d4d0e703e,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.87.64,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 22:21:55.132: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-479" for this suite.

• [SLOW TEST:17.679 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should delete old replica sets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should delete old replica sets [Conformance]","total":311,"completed":108,"skipped":1708,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl logs 
  should be able to retrieve and filter logs  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 22:21:55.556: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[BeforeEach] Kubectl logs
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1392
STEP: creating an pod
Aug 23 22:21:56.105: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585719795 --namespace=kubectl-5703 run logs-generator --image=k8s.gcr.io/e2e-test-images/agnhost:2.21 --restart=Never -- logs-generator --log-lines-total 100 --run-duration 20s'
Aug 23 22:21:56.703: INFO: stderr: ""
Aug 23 22:21:56.703: INFO: stdout: "pod/logs-generator created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Waiting for log generator to start.
Aug 23 22:21:56.703: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
Aug 23 22:21:56.703: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-5703" to be "running and ready, or succeeded"
Aug 23 22:21:56.806: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 102.707164ms
Aug 23 22:21:58.905: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.201521712s
Aug 23 22:22:01.042: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 4.339033563s
Aug 23 22:22:03.147: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 6.4442139s
Aug 23 22:22:05.304: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 8.600871001s
Aug 23 22:22:05.304: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
Aug 23 22:22:05.304: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
STEP: checking for a matching strings
Aug 23 22:22:05.304: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585719795 --namespace=kubectl-5703 logs logs-generator logs-generator'
Aug 23 22:22:05.473: INFO: stderr: ""
Aug 23 22:22:05.473: INFO: stdout: "I0823 22:22:03.686282       1 logs_generator.go:76] 0 PUT /api/v1/namespaces/default/pods/mf6 526\nI0823 22:22:03.885653       1 logs_generator.go:76] 1 PUT /api/v1/namespaces/default/pods/5dbg 587\nI0823 22:22:04.088895       1 logs_generator.go:76] 2 GET /api/v1/namespaces/default/pods/99s 410\nI0823 22:22:04.285758       1 logs_generator.go:76] 3 PUT /api/v1/namespaces/kube-system/pods/9lnm 528\nI0823 22:22:04.485818       1 logs_generator.go:76] 4 PUT /api/v1/namespaces/kube-system/pods/svv9 417\nI0823 22:22:04.685890       1 logs_generator.go:76] 5 POST /api/v1/namespaces/ns/pods/kpr 255\nI0823 22:22:04.885732       1 logs_generator.go:76] 6 GET /api/v1/namespaces/default/pods/xrv 422\nI0823 22:22:05.085784       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/default/pods/2zj 301\nI0823 22:22:05.285890       1 logs_generator.go:76] 8 POST /api/v1/namespaces/kube-system/pods/fhg2 566\n"
STEP: limiting log lines
Aug 23 22:22:05.473: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585719795 --namespace=kubectl-5703 logs logs-generator logs-generator --tail=1'
Aug 23 22:22:05.667: INFO: stderr: ""
Aug 23 22:22:05.667: INFO: stdout: "I0823 22:22:05.485787       1 logs_generator.go:76] 9 GET /api/v1/namespaces/kube-system/pods/4j2 354\n"
Aug 23 22:22:05.667: INFO: got output "I0823 22:22:05.485787       1 logs_generator.go:76] 9 GET /api/v1/namespaces/kube-system/pods/4j2 354\n"
STEP: limiting log bytes
Aug 23 22:22:05.667: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585719795 --namespace=kubectl-5703 logs logs-generator logs-generator --limit-bytes=1'
Aug 23 22:22:06.608: INFO: stderr: ""
Aug 23 22:22:06.608: INFO: stdout: "I"
Aug 23 22:22:06.608: INFO: got output "I"
STEP: exposing timestamps
Aug 23 22:22:06.608: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585719795 --namespace=kubectl-5703 logs logs-generator logs-generator --tail=1 --timestamps'
Aug 23 22:22:06.822: INFO: stderr: ""
Aug 23 22:22:06.822: INFO: stdout: "2021-08-23T17:22:06.685931223-05:00 I0823 22:22:06.685781       1 logs_generator.go:76] 15 POST /api/v1/namespaces/ns/pods/pl7 405\n"
Aug 23 22:22:06.822: INFO: got output "2021-08-23T17:22:06.685931223-05:00 I0823 22:22:06.685781       1 logs_generator.go:76] 15 POST /api/v1/namespaces/ns/pods/pl7 405\n"
STEP: restricting to a time range
Aug 23 22:22:09.322: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585719795 --namespace=kubectl-5703 logs logs-generator logs-generator --since=1s'
Aug 23 22:22:10.139: INFO: stderr: ""
Aug 23 22:22:10.139: INFO: stdout: "I0823 22:22:09.085814       1 logs_generator.go:76] 27 PUT /api/v1/namespaces/default/pods/mvhz 386\nI0823 22:22:09.285892       1 logs_generator.go:76] 28 POST /api/v1/namespaces/kube-system/pods/kn2w 365\nI0823 22:22:09.485817       1 logs_generator.go:76] 29 POST /api/v1/namespaces/ns/pods/h5g 433\nI0823 22:22:09.685793       1 logs_generator.go:76] 30 POST /api/v1/namespaces/ns/pods/fqg4 202\nI0823 22:22:09.885905       1 logs_generator.go:76] 31 GET /api/v1/namespaces/kube-system/pods/q9p 569\n"
Aug 23 22:22:10.139: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585719795 --namespace=kubectl-5703 logs logs-generator logs-generator --since=24h'
Aug 23 22:22:14.188: INFO: stderr: ""
Aug 23 22:22:14.188: INFO: stdout: "I0823 22:22:03.686282       1 logs_generator.go:76] 0 PUT /api/v1/namespaces/default/pods/mf6 526\nI0823 22:22:03.885653       1 logs_generator.go:76] 1 PUT /api/v1/namespaces/default/pods/5dbg 587\nI0823 22:22:04.088895       1 logs_generator.go:76] 2 GET /api/v1/namespaces/default/pods/99s 410\nI0823 22:22:04.285758       1 logs_generator.go:76] 3 PUT /api/v1/namespaces/kube-system/pods/9lnm 528\nI0823 22:22:04.485818       1 logs_generator.go:76] 4 PUT /api/v1/namespaces/kube-system/pods/svv9 417\nI0823 22:22:04.685890       1 logs_generator.go:76] 5 POST /api/v1/namespaces/ns/pods/kpr 255\nI0823 22:22:04.885732       1 logs_generator.go:76] 6 GET /api/v1/namespaces/default/pods/xrv 422\nI0823 22:22:05.085784       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/default/pods/2zj 301\nI0823 22:22:05.285890       1 logs_generator.go:76] 8 POST /api/v1/namespaces/kube-system/pods/fhg2 566\nI0823 22:22:05.485787       1 logs_generator.go:76] 9 GET /api/v1/namespaces/kube-system/pods/4j2 354\nI0823 22:22:05.685799       1 logs_generator.go:76] 10 GET /api/v1/namespaces/kube-system/pods/5jc 369\nI0823 22:22:05.885833       1 logs_generator.go:76] 11 GET /api/v1/namespaces/kube-system/pods/j7p 456\nI0823 22:22:06.085645       1 logs_generator.go:76] 12 GET /api/v1/namespaces/kube-system/pods/8z6 254\nI0823 22:22:06.285870       1 logs_generator.go:76] 13 POST /api/v1/namespaces/default/pods/6b88 406\nI0823 22:22:06.485940       1 logs_generator.go:76] 14 PUT /api/v1/namespaces/ns/pods/5cr 550\nI0823 22:22:06.685781       1 logs_generator.go:76] 15 POST /api/v1/namespaces/ns/pods/pl7 405\nI0823 22:22:06.886426       1 logs_generator.go:76] 16 GET /api/v1/namespaces/default/pods/wdq 387\nI0823 22:22:07.085756       1 logs_generator.go:76] 17 GET /api/v1/namespaces/kube-system/pods/vsc 241\nI0823 22:22:07.285986       1 logs_generator.go:76] 18 GET /api/v1/namespaces/default/pods/gbll 564\nI0823 22:22:07.485963       1 logs_generator.go:76] 19 POST /api/v1/namespaces/default/pods/ct28 227\nI0823 22:22:07.685937       1 logs_generator.go:76] 20 PUT /api/v1/namespaces/ns/pods/4xw7 487\nI0823 22:22:07.886067       1 logs_generator.go:76] 21 PUT /api/v1/namespaces/kube-system/pods/szt 433\nI0823 22:22:08.085798       1 logs_generator.go:76] 22 POST /api/v1/namespaces/default/pods/lbt 277\nI0823 22:22:08.285900       1 logs_generator.go:76] 23 GET /api/v1/namespaces/ns/pods/9q6k 392\nI0823 22:22:08.485839       1 logs_generator.go:76] 24 GET /api/v1/namespaces/default/pods/rnc 273\nI0823 22:22:08.685774       1 logs_generator.go:76] 25 GET /api/v1/namespaces/ns/pods/wnk 390\nI0823 22:22:08.885788       1 logs_generator.go:76] 26 GET /api/v1/namespaces/kube-system/pods/cbgm 236\nI0823 22:22:09.085814       1 logs_generator.go:76] 27 PUT /api/v1/namespaces/default/pods/mvhz 386\nI0823 22:22:09.285892       1 logs_generator.go:76] 28 POST /api/v1/namespaces/kube-system/pods/kn2w 365\nI0823 22:22:09.485817       1 logs_generator.go:76] 29 POST /api/v1/namespaces/ns/pods/h5g 433\nI0823 22:22:09.685793       1 logs_generator.go:76] 30 POST /api/v1/namespaces/ns/pods/fqg4 202\nI0823 22:22:09.885905       1 logs_generator.go:76] 31 GET /api/v1/namespaces/kube-system/pods/q9p 569\nI0823 22:22:10.085800       1 logs_generator.go:76] 32 POST /api/v1/namespaces/default/pods/t54 519\nI0823 22:22:10.285820       1 logs_generator.go:76] 33 POST /api/v1/namespaces/ns/pods/9b4 402\nI0823 22:22:10.485817       1 logs_generator.go:76] 34 POST /api/v1/namespaces/ns/pods/j6pw 325\nI0823 22:22:10.685809       1 logs_generator.go:76] 35 POST /api/v1/namespaces/kube-system/pods/qvtv 443\nI0823 22:22:10.885750       1 logs_generator.go:76] 36 PUT /api/v1/namespaces/default/pods/ttb 395\nI0823 22:22:11.085807       1 logs_generator.go:76] 37 GET /api/v1/namespaces/kube-system/pods/cfj5 219\nI0823 22:22:11.285927       1 logs_generator.go:76] 38 POST /api/v1/namespaces/default/pods/jj7 481\nI0823 22:22:11.485823       1 logs_generator.go:76] 39 PUT /api/v1/namespaces/ns/pods/z7q 206\nI0823 22:22:11.685936       1 logs_generator.go:76] 40 POST /api/v1/namespaces/default/pods/vbzz 480\nI0823 22:22:11.885775       1 logs_generator.go:76] 41 PUT /api/v1/namespaces/ns/pods/xmr 324\nI0823 22:22:12.085707       1 logs_generator.go:76] 42 POST /api/v1/namespaces/ns/pods/r74 390\nI0823 22:22:12.285842       1 logs_generator.go:76] 43 POST /api/v1/namespaces/kube-system/pods/zjjv 302\nI0823 22:22:12.485775       1 logs_generator.go:76] 44 PUT /api/v1/namespaces/ns/pods/c9d 509\nI0823 22:22:12.685839       1 logs_generator.go:76] 45 POST /api/v1/namespaces/ns/pods/jx4k 233\nI0823 22:22:12.885640       1 logs_generator.go:76] 46 PUT /api/v1/namespaces/default/pods/s5mn 535\nI0823 22:22:13.085832       1 logs_generator.go:76] 47 GET /api/v1/namespaces/ns/pods/zz4 273\nI0823 22:22:13.285824       1 logs_generator.go:76] 48 PUT /api/v1/namespaces/kube-system/pods/gwt 449\nI0823 22:22:13.485882       1 logs_generator.go:76] 49 GET /api/v1/namespaces/kube-system/pods/7cv 252\nI0823 22:22:13.685744       1 logs_generator.go:76] 50 POST /api/v1/namespaces/ns/pods/5zp 207\nI0823 22:22:13.885910       1 logs_generator.go:76] 51 GET /api/v1/namespaces/kube-system/pods/s56 462\nI0823 22:22:14.085786       1 logs_generator.go:76] 52 GET /api/v1/namespaces/default/pods/m84l 394\n"
[AfterEach] Kubectl logs
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1397
Aug 23 22:22:14.189: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585719795 --namespace=kubectl-5703 delete pod logs-generator'
Aug 23 22:22:19.605: INFO: stderr: ""
Aug 23 22:22:19.605: INFO: stdout: "pod \"logs-generator\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 22:22:19.605: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5703" for this suite.

• [SLOW TEST:24.656 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl logs
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1389
    should be able to retrieve and filter logs  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl logs should be able to retrieve and filter logs  [Conformance]","total":311,"completed":109,"skipped":1721,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 22:22:20.213: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating secret with name secret-test-map-89132b8a-cb8f-4c69-bdc4-2adb3d5b27ed
STEP: Creating a pod to test consume secrets
Aug 23 22:22:21.229: INFO: Waiting up to 5m0s for pod "pod-secrets-af078ffe-7cf5-44c1-b8a2-7deb597ba5a3" in namespace "secrets-6389" to be "Succeeded or Failed"
Aug 23 22:22:21.366: INFO: Pod "pod-secrets-af078ffe-7cf5-44c1-b8a2-7deb597ba5a3": Phase="Pending", Reason="", readiness=false. Elapsed: 136.396566ms
Aug 23 22:22:23.474: INFO: Pod "pod-secrets-af078ffe-7cf5-44c1-b8a2-7deb597ba5a3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.244896226s
Aug 23 22:22:25.614: INFO: Pod "pod-secrets-af078ffe-7cf5-44c1-b8a2-7deb597ba5a3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.384511178s
Aug 23 22:22:27.741: INFO: Pod "pod-secrets-af078ffe-7cf5-44c1-b8a2-7deb597ba5a3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.51159371s
STEP: Saw pod success
Aug 23 22:22:27.741: INFO: Pod "pod-secrets-af078ffe-7cf5-44c1-b8a2-7deb597ba5a3" satisfied condition "Succeeded or Failed"
Aug 23 22:22:27.849: INFO: Trying to get logs from node 10.149.248.24 pod pod-secrets-af078ffe-7cf5-44c1-b8a2-7deb597ba5a3 container secret-volume-test: <nil>
STEP: delete the pod
Aug 23 22:22:28.103: INFO: Waiting for pod pod-secrets-af078ffe-7cf5-44c1-b8a2-7deb597ba5a3 to disappear
Aug 23 22:22:28.190: INFO: Pod pod-secrets-af078ffe-7cf5-44c1-b8a2-7deb597ba5a3 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 22:22:28.190: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6389" for this suite.

• [SLOW TEST:8.426 seconds]
[sig-storage] Secrets
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:36
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":311,"completed":110,"skipped":1756,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 22:22:28.641: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:162
[It] should invoke init containers on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating the pod
Aug 23 22:22:29.121: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 22:22:36.756: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-3215" for this suite.

• [SLOW TEST:8.667 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should invoke init containers on a RestartNever pod [Conformance]","total":311,"completed":111,"skipped":1772,"failed":0}
SSS
------------------------------
[sig-storage] Projected combined 
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected combined
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 22:22:37.308: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name configmap-projected-all-test-volume-c36da710-03a9-4586-8876-d667771d9936
STEP: Creating secret with name secret-projected-all-test-volume-f2e4f254-b03a-4615-8511-a780ff61c5e8
STEP: Creating a pod to test Check all projections for projected volume plugin
Aug 23 22:22:38.380: INFO: Waiting up to 5m0s for pod "projected-volume-8dbcd8ee-c9b7-46cb-9096-a518450b8d48" in namespace "projected-4454" to be "Succeeded or Failed"
Aug 23 22:22:38.505: INFO: Pod "projected-volume-8dbcd8ee-c9b7-46cb-9096-a518450b8d48": Phase="Pending", Reason="", readiness=false. Elapsed: 124.752014ms
Aug 23 22:22:40.591: INFO: Pod "projected-volume-8dbcd8ee-c9b7-46cb-9096-a518450b8d48": Phase="Pending", Reason="", readiness=false. Elapsed: 2.210270309s
Aug 23 22:22:42.647: INFO: Pod "projected-volume-8dbcd8ee-c9b7-46cb-9096-a518450b8d48": Phase="Pending", Reason="", readiness=false. Elapsed: 4.266188842s
Aug 23 22:22:44.749: INFO: Pod "projected-volume-8dbcd8ee-c9b7-46cb-9096-a518450b8d48": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.368443917s
STEP: Saw pod success
Aug 23 22:22:44.749: INFO: Pod "projected-volume-8dbcd8ee-c9b7-46cb-9096-a518450b8d48" satisfied condition "Succeeded or Failed"
Aug 23 22:22:44.846: INFO: Trying to get logs from node 10.149.248.24 pod projected-volume-8dbcd8ee-c9b7-46cb-9096-a518450b8d48 container projected-all-volume-test: <nil>
STEP: delete the pod
Aug 23 22:22:45.047: INFO: Waiting for pod projected-volume-8dbcd8ee-c9b7-46cb-9096-a518450b8d48 to disappear
Aug 23 22:22:45.108: INFO: Pod projected-volume-8dbcd8ee-c9b7-46cb-9096-a518450b8d48 no longer exists
[AfterEach] [sig-storage] Projected combined
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 22:22:45.108: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4454" for this suite.

• [SLOW TEST:8.062 seconds]
[sig-storage] Projected combined
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_combined.go:32
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Projected combined should project all components that make up the projection API [Projection][NodeConformance] [Conformance]","total":311,"completed":112,"skipped":1775,"failed":0}
SSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command in a pod 
  should print the output to logs [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 22:22:45.370: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[It] should print the output to logs [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[AfterEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 22:22:52.411: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-4885" for this suite.

• [SLOW TEST:7.611 seconds]
[k8s.io] Kubelet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  when scheduling a busybox command in a pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:41
    should print the output to logs [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox command in a pod should print the output to logs [NodeConformance] [Conformance]","total":311,"completed":113,"skipped":1781,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 22:22:52.982: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating secret secrets-2000/secret-test-21cab203-206a-4129-8765-f3ab848f7b10
STEP: Creating a pod to test consume secrets
Aug 23 22:22:53.303: INFO: Waiting up to 5m0s for pod "pod-configmaps-9ac5f6a2-c155-45e8-8f5c-0ce9d0a8fe17" in namespace "secrets-2000" to be "Succeeded or Failed"
Aug 23 22:22:53.333: INFO: Pod "pod-configmaps-9ac5f6a2-c155-45e8-8f5c-0ce9d0a8fe17": Phase="Pending", Reason="", readiness=false. Elapsed: 30.279132ms
Aug 23 22:22:55.350: INFO: Pod "pod-configmaps-9ac5f6a2-c155-45e8-8f5c-0ce9d0a8fe17": Phase="Pending", Reason="", readiness=false. Elapsed: 2.047218469s
Aug 23 22:22:57.393: INFO: Pod "pod-configmaps-9ac5f6a2-c155-45e8-8f5c-0ce9d0a8fe17": Phase="Pending", Reason="", readiness=false. Elapsed: 4.090524888s
Aug 23 22:22:59.475: INFO: Pod "pod-configmaps-9ac5f6a2-c155-45e8-8f5c-0ce9d0a8fe17": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.172439786s
STEP: Saw pod success
Aug 23 22:22:59.475: INFO: Pod "pod-configmaps-9ac5f6a2-c155-45e8-8f5c-0ce9d0a8fe17" satisfied condition "Succeeded or Failed"
Aug 23 22:22:59.526: INFO: Trying to get logs from node 10.149.248.24 pod pod-configmaps-9ac5f6a2-c155-45e8-8f5c-0ce9d0a8fe17 container env-test: <nil>
STEP: delete the pod
Aug 23 22:22:59.694: INFO: Waiting for pod pod-configmaps-9ac5f6a2-c155-45e8-8f5c-0ce9d0a8fe17 to disappear
Aug 23 22:22:59.749: INFO: Pod pod-configmaps-9ac5f6a2-c155-45e8-8f5c-0ce9d0a8fe17 no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 22:22:59.749: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2000" for this suite.

• [SLOW TEST:7.030 seconds]
[sig-api-machinery] Secrets
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:36
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] Secrets should be consumable via the environment [NodeConformance] [Conformance]","total":311,"completed":114,"skipped":1812,"failed":0}
SS
------------------------------
[sig-apps] Deployment 
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 22:23:00.013: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:85
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Aug 23 22:23:00.368: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Aug 23 22:23:00.473: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Aug 23 22:23:06.606: INFO: Creating deployment "test-rolling-update-deployment"
Aug 23 22:23:06.684: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Aug 23 22:23:06.842: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Aug 23 22:23:06.917: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63765354186, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63765354186, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63765354186, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63765354186, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-6b6bf9df46\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 23 22:23:08.956: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63765354186, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63765354186, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63765354186, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63765354186, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-6b6bf9df46\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 23 22:23:10.943: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63765354186, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63765354186, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63765354186, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63765354186, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-6b6bf9df46\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 23 22:23:12.995: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:79
Aug 23 22:23:13.305: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-5543 /apis/apps/v1/namespaces/deployment-5543/deployments/test-rolling-update-deployment 1e697700-dad4-49ce-ae8d-1d7fe1034342 80882 1 2021-08-23 22:23:06 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] []  [{e2e.test Update apps/v1 2021-08-23 22:23:06 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-08-23 22:23:11 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}}}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.21 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc006ae7eb8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2021-08-23 22:23:06 +0000 UTC,LastTransitionTime:2021-08-23 22:23:06 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-6b6bf9df46" has successfully progressed.,LastUpdateTime:2021-08-23 22:23:11 +0000 UTC,LastTransitionTime:2021-08-23 22:23:06 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Aug 23 22:23:13.408: INFO: New ReplicaSet "test-rolling-update-deployment-6b6bf9df46" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:{test-rolling-update-deployment-6b6bf9df46  deployment-5543 /apis/apps/v1/namespaces/deployment-5543/replicasets/test-rolling-update-deployment-6b6bf9df46 c58d5388-7399-4d6b-8cd8-df1b81d4d539 80870 1 2021-08-23 22:23:06 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:6b6bf9df46] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment 1e697700-dad4-49ce-ae8d-1d7fe1034342 0xc003820387 0xc003820388}] []  [{kube-controller-manager Update apps/v1 2021-08-23 22:23:11 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1e697700-dad4-49ce-ae8d-1d7fe1034342\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 6b6bf9df46,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:6b6bf9df46] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.21 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003820418 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Aug 23 22:23:13.409: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Aug 23 22:23:13.409: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-5543 /apis/apps/v1/namespaces/deployment-5543/replicasets/test-rolling-update-controller 3dcaf693-b809-4f32-b83d-12f7a36b2872 80880 2 2021-08-23 22:23:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment 1e697700-dad4-49ce-ae8d-1d7fe1034342 0xc003820277 0xc003820278}] []  [{e2e.test Update apps/v1 2021-08-23 22:23:00 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-08-23 22:23:11 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1e697700-dad4-49ce-ae8d-1d7fe1034342\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{}},"f:status":{"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc003820318 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Aug 23 22:23:13.498: INFO: Pod "test-rolling-update-deployment-6b6bf9df46-jmzbs" is available:
&Pod{ObjectMeta:{test-rolling-update-deployment-6b6bf9df46-jmzbs test-rolling-update-deployment-6b6bf9df46- deployment-5543 /api/v1/namespaces/deployment-5543/pods/test-rolling-update-deployment-6b6bf9df46-jmzbs e9e2496c-0411-4aaf-a4f2-cc4632753938 80869 0 2021-08-23 22:23:06 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:6b6bf9df46] map[cni.projectcalico.org/podIP:172.30.210.191/32 cni.projectcalico.org/podIPs:172.30.210.191/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "",
    "ips": [
        "172.30.210.191"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "",
    "ips": [
        "172.30.210.191"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet test-rolling-update-deployment-6b6bf9df46 c58d5388-7399-4d6b-8cd8-df1b81d4d539 0xc003820847 0xc003820848}] []  [{kube-controller-manager Update v1 2021-08-23 22:23:06 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c58d5388-7399-4d6b-8cd8-df1b81d4d539\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{".":{},"f:capabilities":{"f:drop":{}}},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{".":{},"f:seLinuxOptions":{"f:level":{}}},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-08-23 22:23:10 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {multus Update v1 2021-08-23 22:23:10 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}}} {kubelet Update v1 2021-08-23 22:23:11 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.210.191\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-cctzt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-cctzt,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:k8s.gcr.io/e2e-test-images/agnhost:2.21,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-cctzt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.149.248.9,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c46,c0,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-5f72m,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-08-23 22:23:06 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-08-23 22:23:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-08-23 22:23:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-08-23 22:23:06 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.149.248.9,PodIP:172.30.210.191,StartTime:2021-08-23 22:23:06 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-08-23 22:23:10 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.21,ImageID:k8s.gcr.io/e2e-test-images/agnhost@sha256:ab055cd3d45f50b90732c14593a5bf50f210871bb4f91994c756fc22db6d922a,ContainerID:cri-o://768a29520083128147dd054cfc85271881e14ccc3dbc3626911bc133b65d8364,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.210.191,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 22:23:13.499: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-5543" for this suite.

• [SLOW TEST:14.011 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] Deployment RollingUpdateDeployment should delete old pods and create new ones [Conformance]","total":311,"completed":115,"skipped":1814,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Ingress API 
  should support creating Ingress API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Ingress API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 22:23:14.025: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename ingress
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support creating Ingress API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: getting /apis
STEP: getting /apis/networking.k8s.io
STEP: getting /apis/networking.k8s.iov1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Aug 23 22:23:16.058: INFO: starting watch
STEP: cluster-wide listing
STEP: cluster-wide watching
Aug 23 22:23:16.252: INFO: starting watch
STEP: patching
STEP: updating
Aug 23 22:23:16.708: INFO: waiting for watch events with expected annotations
Aug 23 22:23:16.709: INFO: saw patched and updated annotations
STEP: patching /status
STEP: updating /status
STEP: get /status
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-network] Ingress API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 22:23:17.742: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ingress-3880" for this suite.
•{"msg":"PASSED [sig-network] Ingress API should support creating Ingress API operations [Conformance]","total":311,"completed":116,"skipped":1841,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should adopt matching pods on creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 22:23:18.271: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should adopt matching pods on creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Given a Pod with a 'name' label pod-adoption is created
STEP: When a replication controller with a matching selector is created
STEP: Then the orphan pod is adopted
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 22:23:24.378: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-1276" for this suite.

• [SLOW TEST:6.897 seconds]
[sig-apps] ReplicationController
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching pods on creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should adopt matching pods on creation [Conformance]","total":311,"completed":117,"skipped":1854,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve multiport endpoints from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 22:23:25.168: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should serve multiport endpoints from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating service multi-endpoint-test in namespace services-464
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-464 to expose endpoints map[]
Aug 23 22:23:26.203: INFO: successfully validated that service multi-endpoint-test in namespace services-464 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-464
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-464 to expose endpoints map[pod1:[100]]
Aug 23 22:23:30.625: INFO: Unexpected endpoints: found map[], expected map[pod1:[100]], will retry
Aug 23 22:23:33.008: INFO: successfully validated that service multi-endpoint-test in namespace services-464 exposes endpoints map[pod1:[100]]
STEP: Creating pod pod2 in namespace services-464
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-464 to expose endpoints map[pod1:[100] pod2:[101]]
Aug 23 22:23:37.975: INFO: Unexpected endpoints: found map[31832736-52ef-4507-a165-5576ecd5680c:[100]], expected map[pod1:[100] pod2:[101]], will retry
Aug 23 22:23:39.579: INFO: successfully validated that service multi-endpoint-test in namespace services-464 exposes endpoints map[pod1:[100] pod2:[101]]
STEP: Deleting pod pod1 in namespace services-464
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-464 to expose endpoints map[pod2:[101]]
Aug 23 22:23:41.829: INFO: successfully validated that service multi-endpoint-test in namespace services-464 exposes endpoints map[pod2:[101]]
STEP: Deleting pod pod2 in namespace services-464
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-464 to expose endpoints map[]
Aug 23 22:23:42.906: INFO: successfully validated that service multi-endpoint-test in namespace services-464 exposes endpoints map[]
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 22:23:43.138: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-464" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749

• [SLOW TEST:19.130 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should serve multiport endpoints from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Services should serve multiport endpoints from pods  [Conformance]","total":311,"completed":118,"skipped":1871,"failed":0}
SSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 22:23:44.298: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:41
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Aug 23 22:23:45.295: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a439abe5-f9f4-4bc1-8a94-a9d678e42b32" in namespace "projected-9330" to be "Succeeded or Failed"
Aug 23 22:23:45.432: INFO: Pod "downwardapi-volume-a439abe5-f9f4-4bc1-8a94-a9d678e42b32": Phase="Pending", Reason="", readiness=false. Elapsed: 136.962351ms
Aug 23 22:23:47.553: INFO: Pod "downwardapi-volume-a439abe5-f9f4-4bc1-8a94-a9d678e42b32": Phase="Pending", Reason="", readiness=false. Elapsed: 2.257918339s
Aug 23 22:23:49.695: INFO: Pod "downwardapi-volume-a439abe5-f9f4-4bc1-8a94-a9d678e42b32": Phase="Pending", Reason="", readiness=false. Elapsed: 4.399346301s
Aug 23 22:23:51.776: INFO: Pod "downwardapi-volume-a439abe5-f9f4-4bc1-8a94-a9d678e42b32": Phase="Pending", Reason="", readiness=false. Elapsed: 6.481105024s
Aug 23 22:23:53.865: INFO: Pod "downwardapi-volume-a439abe5-f9f4-4bc1-8a94-a9d678e42b32": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.569670006s
STEP: Saw pod success
Aug 23 22:23:53.865: INFO: Pod "downwardapi-volume-a439abe5-f9f4-4bc1-8a94-a9d678e42b32" satisfied condition "Succeeded or Failed"
Aug 23 22:23:53.945: INFO: Trying to get logs from node 10.149.248.24 pod downwardapi-volume-a439abe5-f9f4-4bc1-8a94-a9d678e42b32 container client-container: <nil>
STEP: delete the pod
Aug 23 22:23:54.151: INFO: Waiting for pod downwardapi-volume-a439abe5-f9f4-4bc1-8a94-a9d678e42b32 to disappear
Aug 23 22:23:54.239: INFO: Pod downwardapi-volume-a439abe5-f9f4-4bc1-8a94-a9d678e42b32 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 22:23:54.240: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9330" for this suite.

• [SLOW TEST:10.402 seconds]
[sig-storage] Projected downwardAPI
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:35
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu limit [NodeConformance] [Conformance]","total":311,"completed":119,"skipped":1876,"failed":0}
SS
------------------------------
[sig-network] DNS 
  should support configurable pod DNS nameservers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 22:23:54.701: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support configurable pod DNS nameservers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod with dnsPolicy=None and customized dnsConfig...
Aug 23 22:23:55.089: INFO: Created pod &Pod{ObjectMeta:{test-dns-nameservers  dns-9340 /api/v1/namespaces/dns-9340/pods/test-dns-nameservers e3fabc5f-fff0-4007-a335-f560ed20f532 81539 0 2021-08-23 22:23:55 +0000 UTC <nil> <nil> map[] map[openshift.io/scc:anyuid] [] []  [{e2e.test Update v1 2021-08-23 22:23:55 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost-container\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-bbdrl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-bbdrl,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost-container,Image:k8s.gcr.io/e2e-test-images/agnhost:2.21,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-bbdrl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c46,c25,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-kgft4,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 23 22:23:55.145: INFO: The status of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
Aug 23 22:23:57.280: INFO: The status of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
Aug 23 22:23:59.240: INFO: The status of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
Aug 23 22:24:01.167: INFO: The status of Pod test-dns-nameservers is Running (Ready = true)
STEP: Verifying customized DNS suffix list is configured on pod...
Aug 23 22:24:01.167: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-9340 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 23 22:24:01.167: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Verifying customized DNS server is configured on pod...
Aug 23 22:24:03.338: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-9340 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 23 22:24:03.338: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
Aug 23 22:24:03.590: INFO: Deleting pod test-dns-nameservers...
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 22:24:03.650: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-9340" for this suite.

• [SLOW TEST:9.038 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should support configurable pod DNS nameservers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] DNS should support configurable pod DNS nameservers [Conformance]","total":311,"completed":120,"skipped":1878,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] 
  should include custom resource definition resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 22:24:03.740: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] should include custom resource definition resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: fetching the /apis discovery document
STEP: finding the apiextensions.k8s.io API group in the /apis discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/apiextensions.k8s.io discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document
STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document
STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 22:24:03.969: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-5641" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] should include custom resource definition resources in discovery documents [Conformance]","total":311,"completed":121,"skipped":1897,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints 
  verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 22:24:04.088: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename sched-preemption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:90
Aug 23 22:24:04.381: INFO: Waiting up to 1m0s for all nodes to be ready
Aug 23 22:25:04.882: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PriorityClass endpoints
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 22:25:04.929: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename sched-preemption-path
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] PriorityClass endpoints
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:679
[It] verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Aug 23 22:25:05.254: INFO: PriorityClass.scheduling.k8s.io "p1" is invalid: Value: Forbidden: may not be changed in an update.
Aug 23 22:25:05.292: INFO: PriorityClass.scheduling.k8s.io "p2" is invalid: Value: Forbidden: may not be changed in an update.
[AfterEach] PriorityClass endpoints
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 22:25:05.480: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-path-7279" for this suite.
[AfterEach] PriorityClass endpoints
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:693
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 22:25:05.647: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-5211" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:78

• [SLOW TEST:61.954 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  PriorityClass endpoints
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:673
    verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]","total":311,"completed":122,"skipped":2038,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate pod and apply defaults after mutation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 22:25:06.042: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Aug 23 22:25:07.220: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63765354307, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63765354307, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63765354307, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63765354307, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 23 22:25:09.248: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63765354307, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63765354307, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63765354307, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63765354307, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 23 22:25:11.297: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63765354307, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63765354307, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63765354307, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63765354307, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 23 22:25:13.280: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63765354307, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63765354307, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63765354307, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63765354307, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Aug 23 22:25:16.316: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate pod and apply defaults after mutation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Registering the mutating pod webhook via the AdmissionRegistration API
STEP: create a pod that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 22:25:16.578: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1995" for this suite.
STEP: Destroying namespace "webhook-1995-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

• [SLOW TEST:10.815 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate pod and apply defaults after mutation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate pod and apply defaults after mutation [Conformance]","total":311,"completed":123,"skipped":2049,"failed":0}
SS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 22:25:16.857: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:41
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Aug 23 22:25:17.158: INFO: Waiting up to 5m0s for pod "downwardapi-volume-00c352ab-19d4-45e1-a93a-b6041da53180" in namespace "projected-8601" to be "Succeeded or Failed"
Aug 23 22:25:17.179: INFO: Pod "downwardapi-volume-00c352ab-19d4-45e1-a93a-b6041da53180": Phase="Pending", Reason="", readiness=false. Elapsed: 21.028627ms
Aug 23 22:25:19.214: INFO: Pod "downwardapi-volume-00c352ab-19d4-45e1-a93a-b6041da53180": Phase="Pending", Reason="", readiness=false. Elapsed: 2.055457272s
Aug 23 22:25:21.343: INFO: Pod "downwardapi-volume-00c352ab-19d4-45e1-a93a-b6041da53180": Phase="Pending", Reason="", readiness=false. Elapsed: 4.184806398s
Aug 23 22:25:23.474: INFO: Pod "downwardapi-volume-00c352ab-19d4-45e1-a93a-b6041da53180": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.315583013s
STEP: Saw pod success
Aug 23 22:25:23.474: INFO: Pod "downwardapi-volume-00c352ab-19d4-45e1-a93a-b6041da53180" satisfied condition "Succeeded or Failed"
Aug 23 22:25:23.590: INFO: Trying to get logs from node 10.149.248.24 pod downwardapi-volume-00c352ab-19d4-45e1-a93a-b6041da53180 container client-container: <nil>
STEP: delete the pod
Aug 23 22:25:24.209: INFO: Waiting for pod downwardapi-volume-00c352ab-19d4-45e1-a93a-b6041da53180 to disappear
Aug 23 22:25:24.376: INFO: Pod downwardapi-volume-00c352ab-19d4-45e1-a93a-b6041da53180 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 22:25:24.376: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8601" for this suite.

• [SLOW TEST:7.835 seconds]
[sig-storage] Projected downwardAPI
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:35
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory request [NodeConformance] [Conformance]","total":311,"completed":124,"skipped":2051,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Subdomain [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 22:25:24.692: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for pods for Subdomain [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-7039.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-7039.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-7039.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7039.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-7039.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-7039.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-7039.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-7039.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-7039.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-7039.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-7039.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-7039.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-7039.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-7039.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-7039.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-7039.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-7039.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-7039.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Aug 23 22:25:33.545: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-7039.svc.cluster.local from pod dns-7039/dns-test-02cfbb59-afe9-4de7-83e2-a57cda582711: the server could not find the requested resource (get pods dns-test-02cfbb59-afe9-4de7-83e2-a57cda582711)
Aug 23 22:25:33.697: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7039.svc.cluster.local from pod dns-7039/dns-test-02cfbb59-afe9-4de7-83e2-a57cda582711: the server could not find the requested resource (get pods dns-test-02cfbb59-afe9-4de7-83e2-a57cda582711)
Aug 23 22:25:33.850: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-7039.svc.cluster.local from pod dns-7039/dns-test-02cfbb59-afe9-4de7-83e2-a57cda582711: the server could not find the requested resource (get pods dns-test-02cfbb59-afe9-4de7-83e2-a57cda582711)
Aug 23 22:25:33.973: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-7039.svc.cluster.local from pod dns-7039/dns-test-02cfbb59-afe9-4de7-83e2-a57cda582711: the server could not find the requested resource (get pods dns-test-02cfbb59-afe9-4de7-83e2-a57cda582711)
Aug 23 22:25:34.382: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-7039.svc.cluster.local from pod dns-7039/dns-test-02cfbb59-afe9-4de7-83e2-a57cda582711: the server could not find the requested resource (get pods dns-test-02cfbb59-afe9-4de7-83e2-a57cda582711)
Aug 23 22:25:34.696: INFO: Unable to read jessie_udp@dns-test-service-2.dns-7039.svc.cluster.local from pod dns-7039/dns-test-02cfbb59-afe9-4de7-83e2-a57cda582711: the server could not find the requested resource (get pods dns-test-02cfbb59-afe9-4de7-83e2-a57cda582711)
Aug 23 22:25:35.243: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-7039.svc.cluster.local from pod dns-7039/dns-test-02cfbb59-afe9-4de7-83e2-a57cda582711: the server could not find the requested resource (get pods dns-test-02cfbb59-afe9-4de7-83e2-a57cda582711)
Aug 23 22:25:35.476: INFO: Lookups using dns-7039/dns-test-02cfbb59-afe9-4de7-83e2-a57cda582711 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-7039.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7039.svc.cluster.local wheezy_udp@dns-test-service-2.dns-7039.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-7039.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-7039.svc.cluster.local jessie_udp@dns-test-service-2.dns-7039.svc.cluster.local jessie_tcp@dns-test-service-2.dns-7039.svc.cluster.local]

Aug 23 22:25:43.025: INFO: DNS probes using dns-7039/dns-test-02cfbb59-afe9-4de7-83e2-a57cda582711 succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 22:25:43.622: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-7039" for this suite.

• [SLOW TEST:19.665 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for pods for Subdomain [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Subdomain [Conformance]","total":311,"completed":125,"skipped":2084,"failed":0}
SSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 22:25:44.358: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir 0777 on tmpfs
Aug 23 22:25:45.153: INFO: Waiting up to 5m0s for pod "pod-7710483c-cc8e-4ff2-aac8-f93e4dbd08f7" in namespace "emptydir-1997" to be "Succeeded or Failed"
Aug 23 22:25:45.289: INFO: Pod "pod-7710483c-cc8e-4ff2-aac8-f93e4dbd08f7": Phase="Pending", Reason="", readiness=false. Elapsed: 135.983787ms
Aug 23 22:25:47.413: INFO: Pod "pod-7710483c-cc8e-4ff2-aac8-f93e4dbd08f7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.259881391s
Aug 23 22:25:49.560: INFO: Pod "pod-7710483c-cc8e-4ff2-aac8-f93e4dbd08f7": Phase="Pending", Reason="", readiness=false. Elapsed: 4.406091385s
Aug 23 22:25:51.696: INFO: Pod "pod-7710483c-cc8e-4ff2-aac8-f93e4dbd08f7": Phase="Pending", Reason="", readiness=false. Elapsed: 6.542942133s
Aug 23 22:25:53.840: INFO: Pod "pod-7710483c-cc8e-4ff2-aac8-f93e4dbd08f7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.686546554s
STEP: Saw pod success
Aug 23 22:25:53.840: INFO: Pod "pod-7710483c-cc8e-4ff2-aac8-f93e4dbd08f7" satisfied condition "Succeeded or Failed"
Aug 23 22:25:53.989: INFO: Trying to get logs from node 10.149.248.24 pod pod-7710483c-cc8e-4ff2-aac8-f93e4dbd08f7 container test-container: <nil>
STEP: delete the pod
Aug 23 22:25:54.325: INFO: Waiting for pod pod-7710483c-cc8e-4ff2-aac8-f93e4dbd08f7 to disappear
Aug 23 22:25:54.451: INFO: Pod pod-7710483c-cc8e-4ff2-aac8-f93e4dbd08f7 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 22:25:54.452: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1997" for this suite.

• [SLOW TEST:10.833 seconds]
[sig-storage] EmptyDir volumes
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:45
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":126,"skipped":2087,"failed":0}
SSSSSS
------------------------------
[sig-storage] ConfigMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 22:25:55.191: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Aug 23 22:25:56.424: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
STEP: Creating configMap with name configmap-test-upd-f0d4e68c-c868-4ac2-8c48-7b650d3e46d5
STEP: Creating the pod
STEP: Updating configmap configmap-test-upd-f0d4e68c-c868-4ac2-8c48-7b650d3e46d5
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 22:27:34.827: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-47" for this suite.

• [SLOW TEST:99.815 seconds]
[sig-storage] ConfigMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:36
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap updates should be reflected in volume [NodeConformance] [Conformance]","total":311,"completed":127,"skipped":2093,"failed":0}
SSSSSSSSSSS
------------------------------
[k8s.io] Security Context when creating containers with AllowPrivilegeEscalation 
  should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 22:27:35.007: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:41
[It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Aug 23 22:27:35.289: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-1d9e4019-6a38-4af1-b329-a7e33c9ab07b" in namespace "security-context-test-8914" to be "Succeeded or Failed"
Aug 23 22:27:35.334: INFO: Pod "alpine-nnp-false-1d9e4019-6a38-4af1-b329-a7e33c9ab07b": Phase="Pending", Reason="", readiness=false. Elapsed: 44.764818ms
Aug 23 22:27:37.349: INFO: Pod "alpine-nnp-false-1d9e4019-6a38-4af1-b329-a7e33c9ab07b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.060282339s
Aug 23 22:27:39.363: INFO: Pod "alpine-nnp-false-1d9e4019-6a38-4af1-b329-a7e33c9ab07b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.074416675s
Aug 23 22:27:41.399: INFO: Pod "alpine-nnp-false-1d9e4019-6a38-4af1-b329-a7e33c9ab07b": Phase="Pending", Reason="", readiness=false. Elapsed: 6.110070653s
Aug 23 22:27:43.457: INFO: Pod "alpine-nnp-false-1d9e4019-6a38-4af1-b329-a7e33c9ab07b": Phase="Pending", Reason="", readiness=false. Elapsed: 8.167921722s
Aug 23 22:27:45.546: INFO: Pod "alpine-nnp-false-1d9e4019-6a38-4af1-b329-a7e33c9ab07b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.256885492s
Aug 23 22:27:45.546: INFO: Pod "alpine-nnp-false-1d9e4019-6a38-4af1-b329-a7e33c9ab07b" satisfied condition "Succeeded or Failed"
[AfterEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 22:27:45.642: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-8914" for this suite.

• [SLOW TEST:11.039 seconds]
[k8s.io] Security Context
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  when creating containers with AllowPrivilegeEscalation
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:291
    should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Security Context when creating containers with AllowPrivilegeEscalation should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":128,"skipped":2104,"failed":0}
SSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing mutating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 22:27:46.046: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Aug 23 22:27:48.335: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63765354468, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63765354468, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63765354468, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63765354468, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 23 22:27:50.448: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63765354468, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63765354468, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63765354468, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63765354468, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 23 22:27:52.548: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63765354468, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63765354468, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63765354468, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63765354468, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Aug 23 22:27:55.660: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing mutating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that should be mutated
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that should not be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 22:28:01.111: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1343" for this suite.
STEP: Destroying namespace "webhook-1343-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

• [SLOW TEST:17.348 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  listing mutating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing mutating webhooks should work [Conformance]","total":311,"completed":129,"skipped":2111,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class 
  should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 22:28:03.394: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods Set QOS Class
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:150
[It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying QOS class is set on the pod
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 22:28:04.364: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-3692" for this suite.
•{"msg":"PASSED [k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]","total":311,"completed":130,"skipped":2130,"failed":0}
SSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 22:28:05.362: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan pods created by rc if delete options say so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods
STEP: Gathering metrics
W0823 22:28:47.817942      25 metrics_grabber.go:98] Can't find kube-scheduler pod. Grabbing metrics from kube-scheduler is disabled.
W0823 22:28:47.817962      25 metrics_grabber.go:102] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
W0823 22:28:47.817971      25 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
Aug 23 22:28:47.818: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

Aug 23 22:28:47.818: INFO: Deleting pod "simpletest.rc-4w6mv" in namespace "gc-1263"
Aug 23 22:28:47.876: INFO: Deleting pod "simpletest.rc-72nxm" in namespace "gc-1263"
Aug 23 22:28:47.916: INFO: Deleting pod "simpletest.rc-8qcgq" in namespace "gc-1263"
Aug 23 22:28:47.963: INFO: Deleting pod "simpletest.rc-cc8jd" in namespace "gc-1263"
Aug 23 22:28:48.005: INFO: Deleting pod "simpletest.rc-h4hv6" in namespace "gc-1263"
Aug 23 22:28:48.051: INFO: Deleting pod "simpletest.rc-hkzgl" in namespace "gc-1263"
Aug 23 22:28:48.099: INFO: Deleting pod "simpletest.rc-m6t7l" in namespace "gc-1263"
Aug 23 22:28:48.140: INFO: Deleting pod "simpletest.rc-prm2c" in namespace "gc-1263"
Aug 23 22:28:48.186: INFO: Deleting pod "simpletest.rc-vbvkg" in namespace "gc-1263"
Aug 23 22:28:48.233: INFO: Deleting pod "simpletest.rc-wbns6" in namespace "gc-1263"
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 22:28:48.272: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-1263" for this suite.

• [SLOW TEST:42.972 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan pods created by rc if delete options say so [Conformance]","total":311,"completed":131,"skipped":2140,"failed":0}
SS
------------------------------
[sig-apps] ReplicationController 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 22:28:48.334: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating replication controller my-hostname-basic-39ce55a9-618e-4a89-a287-12959a78c598
Aug 23 22:28:48.667: INFO: Pod name my-hostname-basic-39ce55a9-618e-4a89-a287-12959a78c598: Found 0 pods out of 1
Aug 23 22:28:53.693: INFO: Pod name my-hostname-basic-39ce55a9-618e-4a89-a287-12959a78c598: Found 1 pods out of 1
Aug 23 22:28:53.693: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-39ce55a9-618e-4a89-a287-12959a78c598" are running
Aug 23 22:28:55.789: INFO: Pod "my-hostname-basic-39ce55a9-618e-4a89-a287-12959a78c598-6w2vk" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-08-23 22:28:48 +0000 UTC Reason: Message:} {Type:Ready Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-08-23 22:28:48 +0000 UTC Reason:ContainersNotReady Message:containers with unready status: [my-hostname-basic-39ce55a9-618e-4a89-a287-12959a78c598]} {Type:ContainersReady Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-08-23 22:28:48 +0000 UTC Reason:ContainersNotReady Message:containers with unready status: [my-hostname-basic-39ce55a9-618e-4a89-a287-12959a78c598]} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-08-23 22:28:48 +0000 UTC Reason: Message:}])
Aug 23 22:28:55.790: INFO: Trying to dial the pod
Aug 23 22:29:01.141: INFO: Controller my-hostname-basic-39ce55a9-618e-4a89-a287-12959a78c598: Got expected result from replica 1 [my-hostname-basic-39ce55a9-618e-4a89-a287-12959a78c598-6w2vk]: "my-hostname-basic-39ce55a9-618e-4a89-a287-12959a78c598-6w2vk", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 22:29:01.141: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-3231" for this suite.

• [SLOW TEST:13.427 seconds]
[sig-apps] ReplicationController
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should serve a basic image on each replica with a public image  [Conformance]","total":311,"completed":132,"skipped":2142,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 22:29:01.761: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward api env vars
Aug 23 22:29:02.512: INFO: Waiting up to 5m0s for pod "downward-api-d06bb593-5a07-4b19-9343-89801184b371" in namespace "downward-api-5966" to be "Succeeded or Failed"
Aug 23 22:29:02.641: INFO: Pod "downward-api-d06bb593-5a07-4b19-9343-89801184b371": Phase="Pending", Reason="", readiness=false. Elapsed: 129.094703ms
Aug 23 22:29:04.785: INFO: Pod "downward-api-d06bb593-5a07-4b19-9343-89801184b371": Phase="Pending", Reason="", readiness=false. Elapsed: 2.273420818s
Aug 23 22:29:06.907: INFO: Pod "downward-api-d06bb593-5a07-4b19-9343-89801184b371": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.394904957s
STEP: Saw pod success
Aug 23 22:29:06.907: INFO: Pod "downward-api-d06bb593-5a07-4b19-9343-89801184b371" satisfied condition "Succeeded or Failed"
Aug 23 22:29:07.051: INFO: Trying to get logs from node 10.149.248.24 pod downward-api-d06bb593-5a07-4b19-9343-89801184b371 container dapi-container: <nil>
STEP: delete the pod
Aug 23 22:29:07.340: INFO: Waiting for pod downward-api-d06bb593-5a07-4b19-9343-89801184b371 to disappear
Aug 23 22:29:07.455: INFO: Pod downward-api-d06bb593-5a07-4b19-9343-89801184b371 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 22:29:07.455: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5966" for this suite.

• [SLOW TEST:6.353 seconds]
[sig-node] Downward API
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:34
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-node] Downward API should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]","total":311,"completed":133,"skipped":2159,"failed":0}
SSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 22:29:08.115: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating secret with name secret-test-map-ff491237-940c-4d85-b367-ee2174a813af
STEP: Creating a pod to test consume secrets
Aug 23 22:29:09.030: INFO: Waiting up to 5m0s for pod "pod-secrets-a579d964-03ea-4c1f-ae30-4e3d79f34835" in namespace "secrets-2072" to be "Succeeded or Failed"
Aug 23 22:29:09.152: INFO: Pod "pod-secrets-a579d964-03ea-4c1f-ae30-4e3d79f34835": Phase="Pending", Reason="", readiness=false. Elapsed: 122.519964ms
Aug 23 22:29:11.319: INFO: Pod "pod-secrets-a579d964-03ea-4c1f-ae30-4e3d79f34835": Phase="Pending", Reason="", readiness=false. Elapsed: 2.288571561s
Aug 23 22:29:13.468: INFO: Pod "pod-secrets-a579d964-03ea-4c1f-ae30-4e3d79f34835": Phase="Pending", Reason="", readiness=false. Elapsed: 4.438321646s
Aug 23 22:29:15.605: INFO: Pod "pod-secrets-a579d964-03ea-4c1f-ae30-4e3d79f34835": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.574638558s
STEP: Saw pod success
Aug 23 22:29:15.605: INFO: Pod "pod-secrets-a579d964-03ea-4c1f-ae30-4e3d79f34835" satisfied condition "Succeeded or Failed"
Aug 23 22:29:15.755: INFO: Trying to get logs from node 10.149.248.24 pod pod-secrets-a579d964-03ea-4c1f-ae30-4e3d79f34835 container secret-volume-test: <nil>
STEP: delete the pod
Aug 23 22:29:16.098: INFO: Waiting for pod pod-secrets-a579d964-03ea-4c1f-ae30-4e3d79f34835 to disappear
Aug 23 22:29:16.282: INFO: Pod pod-secrets-a579d964-03ea-4c1f-ae30-4e3d79f34835 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 22:29:16.283: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2072" for this suite.

• [SLOW TEST:8.930 seconds]
[sig-storage] Secrets
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:36
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":134,"skipped":2162,"failed":0}
S
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 22:29:17.045: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test override arguments
Aug 23 22:29:17.956: INFO: Waiting up to 5m0s for pod "client-containers-403a56af-1ece-4f1e-abda-927a28cb22bf" in namespace "containers-2869" to be "Succeeded or Failed"
Aug 23 22:29:18.109: INFO: Pod "client-containers-403a56af-1ece-4f1e-abda-927a28cb22bf": Phase="Pending", Reason="", readiness=false. Elapsed: 152.93779ms
Aug 23 22:29:20.236: INFO: Pod "client-containers-403a56af-1ece-4f1e-abda-927a28cb22bf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.27907853s
Aug 23 22:29:22.380: INFO: Pod "client-containers-403a56af-1ece-4f1e-abda-927a28cb22bf": Phase="Pending", Reason="", readiness=false. Elapsed: 4.424001406s
Aug 23 22:29:24.474: INFO: Pod "client-containers-403a56af-1ece-4f1e-abda-927a28cb22bf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.517597163s
STEP: Saw pod success
Aug 23 22:29:24.474: INFO: Pod "client-containers-403a56af-1ece-4f1e-abda-927a28cb22bf" satisfied condition "Succeeded or Failed"
Aug 23 22:29:24.566: INFO: Trying to get logs from node 10.149.248.24 pod client-containers-403a56af-1ece-4f1e-abda-927a28cb22bf container agnhost-container: <nil>
STEP: delete the pod
Aug 23 22:29:24.780: INFO: Waiting for pod client-containers-403a56af-1ece-4f1e-abda-927a28cb22bf to disappear
Aug 23 22:29:24.871: INFO: Pod client-containers-403a56af-1ece-4f1e-abda-927a28cb22bf no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 22:29:24.871: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-2869" for this suite.

• [SLOW TEST:8.222 seconds]
[k8s.io] Docker Containers
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Docker Containers should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]","total":311,"completed":135,"skipped":2163,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  updates the published spec when one version gets renamed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 22:29:25.268: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates the published spec when one version gets renamed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: set up a multi version CRD
Aug 23 22:29:25.700: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: rename a version
STEP: check the new version name is served
STEP: check the old version name is removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 22:30:23.801: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-6740" for this suite.

• [SLOW TEST:59.237 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  updates the published spec when one version gets renamed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] updates the published spec when one version gets renamed [Conformance]","total":311,"completed":136,"skipped":2193,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 22:30:24.505: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating secret with name secret-test-df7c2a3a-b783-42a5-aa50-8f27bd5c4bdd
STEP: Creating a pod to test consume secrets
Aug 23 22:30:25.648: INFO: Waiting up to 5m0s for pod "pod-secrets-5482ac37-536e-4885-ba5b-08ce229d4181" in namespace "secrets-7898" to be "Succeeded or Failed"
Aug 23 22:30:25.855: INFO: Pod "pod-secrets-5482ac37-536e-4885-ba5b-08ce229d4181": Phase="Pending", Reason="", readiness=false. Elapsed: 206.475356ms
Aug 23 22:30:28.041: INFO: Pod "pod-secrets-5482ac37-536e-4885-ba5b-08ce229d4181": Phase="Pending", Reason="", readiness=false. Elapsed: 2.392790658s
Aug 23 22:30:30.198: INFO: Pod "pod-secrets-5482ac37-536e-4885-ba5b-08ce229d4181": Phase="Pending", Reason="", readiness=false. Elapsed: 4.550417617s
Aug 23 22:30:32.298: INFO: Pod "pod-secrets-5482ac37-536e-4885-ba5b-08ce229d4181": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.650319103s
STEP: Saw pod success
Aug 23 22:30:32.298: INFO: Pod "pod-secrets-5482ac37-536e-4885-ba5b-08ce229d4181" satisfied condition "Succeeded or Failed"
Aug 23 22:30:32.413: INFO: Trying to get logs from node 10.149.248.24 pod pod-secrets-5482ac37-536e-4885-ba5b-08ce229d4181 container secret-volume-test: <nil>
STEP: delete the pod
Aug 23 22:30:32.706: INFO: Waiting for pod pod-secrets-5482ac37-536e-4885-ba5b-08ce229d4181 to disappear
Aug 23 22:30:32.821: INFO: Pod pod-secrets-5482ac37-536e-4885-ba5b-08ce229d4181 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 22:30:32.822: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-7898" for this suite.

• [SLOW TEST:8.711 seconds]
[sig-storage] Secrets
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:36
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":137,"skipped":2236,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-api-machinery] Servers with support for Table transformation 
  should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 22:30:33.216: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename tables
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/table_conversion.go:47
[It] should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[AfterEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 22:30:34.226: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "tables-2470" for this suite.
•{"msg":"PASSED [sig-api-machinery] Servers with support for Table transformation should return a 406 for a backend which does not implement metadata [Conformance]","total":311,"completed":138,"skipped":2247,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl cluster-info 
  should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 22:30:34.706: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: validating cluster-info
Aug 23 22:30:35.602: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585719795 --namespace=kubectl-1078 cluster-info'
Aug 23 22:30:36.002: INFO: stderr: ""
Aug 23 22:30:36.002: INFO: stdout: "\x1b[0;32mKubernetes control plane\x1b[0m is running at \x1b[0;33mhttps://172.21.0.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 22:30:36.002: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1078" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes control plane services is included in cluster-info  [Conformance]","total":311,"completed":139,"skipped":2259,"failed":0}
SSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 22:30:36.539: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap configmap-1350/configmap-test-76e44f11-15e7-43fe-819f-878c09a4dbbc
STEP: Creating a pod to test consume configMaps
Aug 23 22:30:37.825: INFO: Waiting up to 5m0s for pod "pod-configmaps-645c6157-beee-4c3d-8e32-560860161686" in namespace "configmap-1350" to be "Succeeded or Failed"
Aug 23 22:30:37.992: INFO: Pod "pod-configmaps-645c6157-beee-4c3d-8e32-560860161686": Phase="Pending", Reason="", readiness=false. Elapsed: 166.997192ms
Aug 23 22:30:40.119: INFO: Pod "pod-configmaps-645c6157-beee-4c3d-8e32-560860161686": Phase="Pending", Reason="", readiness=false. Elapsed: 2.294221543s
Aug 23 22:30:42.265: INFO: Pod "pod-configmaps-645c6157-beee-4c3d-8e32-560860161686": Phase="Pending", Reason="", readiness=false. Elapsed: 4.439723386s
Aug 23 22:30:44.369: INFO: Pod "pod-configmaps-645c6157-beee-4c3d-8e32-560860161686": Phase="Pending", Reason="", readiness=false. Elapsed: 6.543639933s
Aug 23 22:30:46.471: INFO: Pod "pod-configmaps-645c6157-beee-4c3d-8e32-560860161686": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.645741898s
STEP: Saw pod success
Aug 23 22:30:46.471: INFO: Pod "pod-configmaps-645c6157-beee-4c3d-8e32-560860161686" satisfied condition "Succeeded or Failed"
Aug 23 22:30:46.560: INFO: Trying to get logs from node 10.149.248.24 pod pod-configmaps-645c6157-beee-4c3d-8e32-560860161686 container env-test: <nil>
STEP: delete the pod
Aug 23 22:30:46.818: INFO: Waiting for pod pod-configmaps-645c6157-beee-4c3d-8e32-560860161686 to disappear
Aug 23 22:30:46.903: INFO: Pod pod-configmaps-645c6157-beee-4c3d-8e32-560860161686 no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 22:30:46.903: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1350" for this suite.

• [SLOW TEST:10.625 seconds]
[sig-node] ConfigMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:34
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-node] ConfigMap should be consumable via environment variable [NodeConformance] [Conformance]","total":311,"completed":140,"skipped":2264,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 22:30:47.164: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap configmap-9453/configmap-test-54973c2e-c867-48c3-b3d5-18986ddad5eb
STEP: Creating a pod to test consume configMaps
Aug 23 22:30:47.942: INFO: Waiting up to 5m0s for pod "pod-configmaps-a4064b3e-2c7d-455e-b320-2a43a437362c" in namespace "configmap-9453" to be "Succeeded or Failed"
Aug 23 22:30:48.075: INFO: Pod "pod-configmaps-a4064b3e-2c7d-455e-b320-2a43a437362c": Phase="Pending", Reason="", readiness=false. Elapsed: 132.707407ms
Aug 23 22:30:50.272: INFO: Pod "pod-configmaps-a4064b3e-2c7d-455e-b320-2a43a437362c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.32915899s
Aug 23 22:30:52.406: INFO: Pod "pod-configmaps-a4064b3e-2c7d-455e-b320-2a43a437362c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.463719987s
Aug 23 22:30:54.521: INFO: Pod "pod-configmaps-a4064b3e-2c7d-455e-b320-2a43a437362c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.578758503s
STEP: Saw pod success
Aug 23 22:30:54.521: INFO: Pod "pod-configmaps-a4064b3e-2c7d-455e-b320-2a43a437362c" satisfied condition "Succeeded or Failed"
Aug 23 22:30:54.625: INFO: Trying to get logs from node 10.149.248.24 pod pod-configmaps-a4064b3e-2c7d-455e-b320-2a43a437362c container env-test: <nil>
STEP: delete the pod
Aug 23 22:30:54.866: INFO: Waiting for pod pod-configmaps-a4064b3e-2c7d-455e-b320-2a43a437362c to disappear
Aug 23 22:30:54.974: INFO: Pod pod-configmaps-a4064b3e-2c7d-455e-b320-2a43a437362c no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 22:30:54.974: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9453" for this suite.

• [SLOW TEST:8.154 seconds]
[sig-node] ConfigMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:34
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-node] ConfigMap should be consumable via the environment [NodeConformance] [Conformance]","total":311,"completed":141,"skipped":2311,"failed":0}
SSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate configmap [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 22:30:55.318: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Aug 23 22:30:56.794: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63765354656, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63765354656, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63765354656, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63765354656, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 23 22:30:58.848: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63765354656, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63765354656, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63765354656, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63765354656, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 23 22:31:00.842: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63765354656, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63765354656, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63765354656, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63765354656, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Aug 23 22:31:03.857: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate configmap [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Registering the mutating configmap webhook via the AdmissionRegistration API
STEP: create a configmap that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 22:31:04.028: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5717" for this suite.
STEP: Destroying namespace "webhook-5717-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

• [SLOW TEST:8.961 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate configmap [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate configmap [Conformance]","total":311,"completed":142,"skipped":2315,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 22:31:04.280: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Service
STEP: Ensuring resource quota status captures service creation
STEP: Deleting a Service
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 22:31:15.869: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-7634" for this suite.

• [SLOW TEST:11.740 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a service. [Conformance]","total":311,"completed":143,"skipped":2399,"failed":0}
SSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should run through a ConfigMap lifecycle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 22:31:16.023: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run through a ConfigMap lifecycle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a ConfigMap
STEP: fetching the ConfigMap
STEP: patching the ConfigMap
STEP: listing all ConfigMaps in all namespaces with a label selector
STEP: deleting the ConfigMap by collection with a label selector
STEP: listing all ConfigMaps in test namespace
[AfterEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 22:31:16.636: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4370" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should run through a ConfigMap lifecycle [Conformance]","total":311,"completed":144,"skipped":2407,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 22:31:16.763: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating service in namespace services-8364
Aug 23 22:31:19.822: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585719795 --namespace=services-8364 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
Aug 23 22:31:22.379: INFO: rc: 7
Aug 23 22:31:22.690: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Aug 23 22:31:22.947: INFO: Pod kube-proxy-mode-detector no longer exists
Aug 23 22:31:22.947: INFO: Couldn't detect KubeProxy mode - test failure may be expected: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585719795 --namespace=services-8364 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode:
Command stdout:

stderr:
+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode
command terminated with exit code 7

error:
exit status 7
STEP: creating service affinity-nodeport-timeout in namespace services-8364
STEP: creating replication controller affinity-nodeport-timeout in namespace services-8364
I0823 22:31:23.408410      25 runners.go:190] Created replication controller with name: affinity-nodeport-timeout, namespace: services-8364, replica count: 3
I0823 22:31:26.608751      25 runners.go:190] affinity-nodeport-timeout Pods: 3 out of 3 created, 0 running, 3 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0823 22:31:29.608978      25 runners.go:190] affinity-nodeport-timeout Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0823 22:31:32.609189      25 runners.go:190] affinity-nodeport-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Aug 23 22:31:33.301: INFO: Creating new exec pod
Aug 23 22:31:40.087: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585719795 --namespace=services-8364 exec execpod-affinityvlt84 -- /bin/sh -x -c nc -zv -t -w 2 affinity-nodeport-timeout 80'
Aug 23 22:31:43.636: INFO: stderr: "+ nc -zv -t -w 2 affinity-nodeport-timeout 80\nConnection to affinity-nodeport-timeout 80 port [tcp/http] succeeded!\n"
Aug 23 22:31:43.636: INFO: stdout: ""
Aug 23 22:31:43.637: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585719795 --namespace=services-8364 exec execpod-affinityvlt84 -- /bin/sh -x -c nc -zv -t -w 2 172.21.26.60 80'
Aug 23 22:31:45.056: INFO: stderr: "+ nc -zv -t -w 2 172.21.26.60 80\nConnection to 172.21.26.60 80 port [tcp/http] succeeded!\n"
Aug 23 22:31:45.056: INFO: stdout: ""
Aug 23 22:31:45.056: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585719795 --namespace=services-8364 exec execpod-affinityvlt84 -- /bin/sh -x -c nc -zv -t -w 2 10.149.248.24 30274'
Aug 23 22:31:45.923: INFO: stderr: "+ nc -zv -t -w 2 10.149.248.24 30274\nConnection to 10.149.248.24 30274 port [tcp/30274] succeeded!\n"
Aug 23 22:31:45.923: INFO: stdout: ""
Aug 23 22:31:45.923: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585719795 --namespace=services-8364 exec execpod-affinityvlt84 -- /bin/sh -x -c nc -zv -t -w 2 10.149.248.25 30274'
Aug 23 22:31:47.733: INFO: stderr: "+ nc -zv -t -w 2 10.149.248.25 30274\nConnection to 10.149.248.25 30274 port [tcp/30274] succeeded!\n"
Aug 23 22:31:47.733: INFO: stdout: ""
Aug 23 22:31:47.733: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585719795 --namespace=services-8364 exec execpod-affinityvlt84 -- /bin/sh -x -c nc -zv -t -w 2 169.45.209.222 30274'
Aug 23 22:31:48.836: INFO: stderr: "+ nc -zv -t -w 2 169.45.209.222 30274\nConnection to 169.45.209.222 30274 port [tcp/30274] succeeded!\n"
Aug 23 22:31:48.836: INFO: stdout: ""
Aug 23 22:31:48.836: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585719795 --namespace=services-8364 exec execpod-affinityvlt84 -- /bin/sh -x -c nc -zv -t -w 2 169.45.232.204 30274'
Aug 23 22:31:49.604: INFO: stderr: "+ nc -zv -t -w 2 169.45.232.204 30274\nConnection to 169.45.232.204 30274 port [tcp/30274] succeeded!\n"
Aug 23 22:31:49.604: INFO: stdout: ""
Aug 23 22:31:49.604: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585719795 --namespace=services-8364 exec execpod-affinityvlt84 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.149.248.24:30274/ ; done'
Aug 23 22:31:52.569: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.149.248.24:30274/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.149.248.24:30274/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.149.248.24:30274/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.149.248.24:30274/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.149.248.24:30274/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.149.248.24:30274/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.149.248.24:30274/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.149.248.24:30274/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.149.248.24:30274/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.149.248.24:30274/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.149.248.24:30274/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.149.248.24:30274/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.149.248.24:30274/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.149.248.24:30274/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.149.248.24:30274/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.149.248.24:30274/\n"
Aug 23 22:31:52.569: INFO: stdout: "\naffinity-nodeport-timeout-286q2\naffinity-nodeport-timeout-286q2\naffinity-nodeport-timeout-286q2\naffinity-nodeport-timeout-286q2\naffinity-nodeport-timeout-286q2\naffinity-nodeport-timeout-286q2\naffinity-nodeport-timeout-286q2\naffinity-nodeport-timeout-286q2\naffinity-nodeport-timeout-286q2\naffinity-nodeport-timeout-286q2\naffinity-nodeport-timeout-286q2\naffinity-nodeport-timeout-286q2\naffinity-nodeport-timeout-286q2\naffinity-nodeport-timeout-286q2\naffinity-nodeport-timeout-286q2\naffinity-nodeport-timeout-286q2"
Aug 23 22:31:52.569: INFO: Received response from host: affinity-nodeport-timeout-286q2
Aug 23 22:31:52.569: INFO: Received response from host: affinity-nodeport-timeout-286q2
Aug 23 22:31:52.569: INFO: Received response from host: affinity-nodeport-timeout-286q2
Aug 23 22:31:52.569: INFO: Received response from host: affinity-nodeport-timeout-286q2
Aug 23 22:31:52.569: INFO: Received response from host: affinity-nodeport-timeout-286q2
Aug 23 22:31:52.569: INFO: Received response from host: affinity-nodeport-timeout-286q2
Aug 23 22:31:52.569: INFO: Received response from host: affinity-nodeport-timeout-286q2
Aug 23 22:31:52.569: INFO: Received response from host: affinity-nodeport-timeout-286q2
Aug 23 22:31:52.569: INFO: Received response from host: affinity-nodeport-timeout-286q2
Aug 23 22:31:52.569: INFO: Received response from host: affinity-nodeport-timeout-286q2
Aug 23 22:31:52.569: INFO: Received response from host: affinity-nodeport-timeout-286q2
Aug 23 22:31:52.569: INFO: Received response from host: affinity-nodeport-timeout-286q2
Aug 23 22:31:52.569: INFO: Received response from host: affinity-nodeport-timeout-286q2
Aug 23 22:31:52.569: INFO: Received response from host: affinity-nodeport-timeout-286q2
Aug 23 22:31:52.569: INFO: Received response from host: affinity-nodeport-timeout-286q2
Aug 23 22:31:52.569: INFO: Received response from host: affinity-nodeport-timeout-286q2
Aug 23 22:31:52.570: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585719795 --namespace=services-8364 exec execpod-affinityvlt84 -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.149.248.24:30274/'
Aug 23 22:31:53.720: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.149.248.24:30274/\n"
Aug 23 22:31:53.720: INFO: stdout: "affinity-nodeport-timeout-286q2"
Aug 23 22:32:13.720: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585719795 --namespace=services-8364 exec execpod-affinityvlt84 -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.149.248.24:30274/'
Aug 23 22:32:16.672: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.149.248.24:30274/\n"
Aug 23 22:32:16.672: INFO: stdout: "affinity-nodeport-timeout-5f6kt"
Aug 23 22:32:16.672: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-timeout in namespace services-8364, will wait for the garbage collector to delete the pods
Aug 23 22:32:17.279: INFO: Deleting ReplicationController affinity-nodeport-timeout took: 196.012846ms
Aug 23 22:32:17.479: INFO: Terminating ReplicationController affinity-nodeport-timeout pods took: 200.207565ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 22:32:34.509: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8364" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749

• [SLOW TEST:78.395 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]","total":311,"completed":145,"skipped":2444,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Discovery 
  should validate PreferredVersion for each APIGroup [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Discovery
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 22:32:35.158: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename discovery
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Discovery
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/discovery.go:39
STEP: Setting up server cert
[It] should validate PreferredVersion for each APIGroup [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Aug 23 22:32:36.344: INFO: Checking APIGroup: apiregistration.k8s.io
Aug 23 22:32:36.462: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
Aug 23 22:32:36.462: INFO: Versions found [{apiregistration.k8s.io/v1 v1} {apiregistration.k8s.io/v1beta1 v1beta1}]
Aug 23 22:32:36.462: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
Aug 23 22:32:36.462: INFO: Checking APIGroup: apps
Aug 23 22:32:36.577: INFO: PreferredVersion.GroupVersion: apps/v1
Aug 23 22:32:36.577: INFO: Versions found [{apps/v1 v1}]
Aug 23 22:32:36.577: INFO: apps/v1 matches apps/v1
Aug 23 22:32:36.577: INFO: Checking APIGroup: events.k8s.io
Aug 23 22:32:36.702: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
Aug 23 22:32:36.702: INFO: Versions found [{events.k8s.io/v1 v1} {events.k8s.io/v1beta1 v1beta1}]
Aug 23 22:32:36.702: INFO: events.k8s.io/v1 matches events.k8s.io/v1
Aug 23 22:32:36.702: INFO: Checking APIGroup: authentication.k8s.io
Aug 23 22:32:36.832: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
Aug 23 22:32:36.832: INFO: Versions found [{authentication.k8s.io/v1 v1} {authentication.k8s.io/v1beta1 v1beta1}]
Aug 23 22:32:36.832: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
Aug 23 22:32:36.832: INFO: Checking APIGroup: authorization.k8s.io
Aug 23 22:32:36.956: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
Aug 23 22:32:36.956: INFO: Versions found [{authorization.k8s.io/v1 v1} {authorization.k8s.io/v1beta1 v1beta1}]
Aug 23 22:32:36.956: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
Aug 23 22:32:36.956: INFO: Checking APIGroup: autoscaling
Aug 23 22:32:37.118: INFO: PreferredVersion.GroupVersion: autoscaling/v1
Aug 23 22:32:37.118: INFO: Versions found [{autoscaling/v1 v1} {autoscaling/v2beta1 v2beta1} {autoscaling/v2beta2 v2beta2}]
Aug 23 22:32:37.119: INFO: autoscaling/v1 matches autoscaling/v1
Aug 23 22:32:37.119: INFO: Checking APIGroup: batch
Aug 23 22:32:37.299: INFO: PreferredVersion.GroupVersion: batch/v1
Aug 23 22:32:37.299: INFO: Versions found [{batch/v1 v1} {batch/v1beta1 v1beta1}]
Aug 23 22:32:37.299: INFO: batch/v1 matches batch/v1
Aug 23 22:32:37.299: INFO: Checking APIGroup: certificates.k8s.io
Aug 23 22:32:37.469: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
Aug 23 22:32:37.469: INFO: Versions found [{certificates.k8s.io/v1 v1} {certificates.k8s.io/v1beta1 v1beta1}]
Aug 23 22:32:37.469: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
Aug 23 22:32:37.469: INFO: Checking APIGroup: networking.k8s.io
Aug 23 22:32:38.187: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
Aug 23 22:32:38.187: INFO: Versions found [{networking.k8s.io/v1 v1} {networking.k8s.io/v1beta1 v1beta1}]
Aug 23 22:32:38.187: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
Aug 23 22:32:38.187: INFO: Checking APIGroup: extensions
Aug 23 22:32:38.283: INFO: PreferredVersion.GroupVersion: extensions/v1beta1
Aug 23 22:32:38.283: INFO: Versions found [{extensions/v1beta1 v1beta1}]
Aug 23 22:32:38.283: INFO: extensions/v1beta1 matches extensions/v1beta1
Aug 23 22:32:38.283: INFO: Checking APIGroup: policy
Aug 23 22:32:38.372: INFO: PreferredVersion.GroupVersion: policy/v1beta1
Aug 23 22:32:38.372: INFO: Versions found [{policy/v1beta1 v1beta1}]
Aug 23 22:32:38.372: INFO: policy/v1beta1 matches policy/v1beta1
Aug 23 22:32:38.372: INFO: Checking APIGroup: rbac.authorization.k8s.io
Aug 23 22:32:38.477: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
Aug 23 22:32:38.477: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1} {rbac.authorization.k8s.io/v1beta1 v1beta1}]
Aug 23 22:32:38.477: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
Aug 23 22:32:38.477: INFO: Checking APIGroup: storage.k8s.io
Aug 23 22:32:38.587: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
Aug 23 22:32:38.587: INFO: Versions found [{storage.k8s.io/v1 v1} {storage.k8s.io/v1beta1 v1beta1}]
Aug 23 22:32:38.587: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
Aug 23 22:32:38.587: INFO: Checking APIGroup: admissionregistration.k8s.io
Aug 23 22:32:38.687: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
Aug 23 22:32:38.687: INFO: Versions found [{admissionregistration.k8s.io/v1 v1} {admissionregistration.k8s.io/v1beta1 v1beta1}]
Aug 23 22:32:38.687: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
Aug 23 22:32:38.687: INFO: Checking APIGroup: apiextensions.k8s.io
Aug 23 22:32:38.787: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
Aug 23 22:32:38.787: INFO: Versions found [{apiextensions.k8s.io/v1 v1} {apiextensions.k8s.io/v1beta1 v1beta1}]
Aug 23 22:32:38.787: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
Aug 23 22:32:38.787: INFO: Checking APIGroup: scheduling.k8s.io
Aug 23 22:32:38.894: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
Aug 23 22:32:38.894: INFO: Versions found [{scheduling.k8s.io/v1 v1} {scheduling.k8s.io/v1beta1 v1beta1}]
Aug 23 22:32:38.894: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
Aug 23 22:32:38.894: INFO: Checking APIGroup: coordination.k8s.io
Aug 23 22:32:39.015: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
Aug 23 22:32:39.015: INFO: Versions found [{coordination.k8s.io/v1 v1} {coordination.k8s.io/v1beta1 v1beta1}]
Aug 23 22:32:39.015: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
Aug 23 22:32:39.015: INFO: Checking APIGroup: node.k8s.io
Aug 23 22:32:39.148: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1
Aug 23 22:32:39.148: INFO: Versions found [{node.k8s.io/v1 v1} {node.k8s.io/v1beta1 v1beta1}]
Aug 23 22:32:39.148: INFO: node.k8s.io/v1 matches node.k8s.io/v1
Aug 23 22:32:39.148: INFO: Checking APIGroup: discovery.k8s.io
Aug 23 22:32:39.266: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1beta1
Aug 23 22:32:39.266: INFO: Versions found [{discovery.k8s.io/v1beta1 v1beta1}]
Aug 23 22:32:39.266: INFO: discovery.k8s.io/v1beta1 matches discovery.k8s.io/v1beta1
Aug 23 22:32:39.266: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
Aug 23 22:32:39.395: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1beta1
Aug 23 22:32:39.395: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1beta1 v1beta1} {flowcontrol.apiserver.k8s.io/v1alpha1 v1alpha1}]
Aug 23 22:32:39.395: INFO: flowcontrol.apiserver.k8s.io/v1beta1 matches flowcontrol.apiserver.k8s.io/v1beta1
Aug 23 22:32:39.395: INFO: Checking APIGroup: apps.openshift.io
Aug 23 22:32:39.524: INFO: PreferredVersion.GroupVersion: apps.openshift.io/v1
Aug 23 22:32:39.524: INFO: Versions found [{apps.openshift.io/v1 v1}]
Aug 23 22:32:39.524: INFO: apps.openshift.io/v1 matches apps.openshift.io/v1
Aug 23 22:32:39.524: INFO: Checking APIGroup: authorization.openshift.io
Aug 23 22:32:39.638: INFO: PreferredVersion.GroupVersion: authorization.openshift.io/v1
Aug 23 22:32:39.638: INFO: Versions found [{authorization.openshift.io/v1 v1}]
Aug 23 22:32:39.638: INFO: authorization.openshift.io/v1 matches authorization.openshift.io/v1
Aug 23 22:32:39.638: INFO: Checking APIGroup: build.openshift.io
Aug 23 22:32:39.761: INFO: PreferredVersion.GroupVersion: build.openshift.io/v1
Aug 23 22:32:39.761: INFO: Versions found [{build.openshift.io/v1 v1}]
Aug 23 22:32:39.761: INFO: build.openshift.io/v1 matches build.openshift.io/v1
Aug 23 22:32:39.762: INFO: Checking APIGroup: image.openshift.io
Aug 23 22:32:39.903: INFO: PreferredVersion.GroupVersion: image.openshift.io/v1
Aug 23 22:32:39.903: INFO: Versions found [{image.openshift.io/v1 v1}]
Aug 23 22:32:39.903: INFO: image.openshift.io/v1 matches image.openshift.io/v1
Aug 23 22:32:39.903: INFO: Checking APIGroup: oauth.openshift.io
Aug 23 22:32:40.039: INFO: PreferredVersion.GroupVersion: oauth.openshift.io/v1
Aug 23 22:32:40.040: INFO: Versions found [{oauth.openshift.io/v1 v1}]
Aug 23 22:32:40.040: INFO: oauth.openshift.io/v1 matches oauth.openshift.io/v1
Aug 23 22:32:40.040: INFO: Checking APIGroup: project.openshift.io
Aug 23 22:32:40.156: INFO: PreferredVersion.GroupVersion: project.openshift.io/v1
Aug 23 22:32:40.156: INFO: Versions found [{project.openshift.io/v1 v1}]
Aug 23 22:32:40.156: INFO: project.openshift.io/v1 matches project.openshift.io/v1
Aug 23 22:32:40.156: INFO: Checking APIGroup: quota.openshift.io
Aug 23 22:32:40.289: INFO: PreferredVersion.GroupVersion: quota.openshift.io/v1
Aug 23 22:32:40.289: INFO: Versions found [{quota.openshift.io/v1 v1}]
Aug 23 22:32:40.289: INFO: quota.openshift.io/v1 matches quota.openshift.io/v1
Aug 23 22:32:40.289: INFO: Checking APIGroup: route.openshift.io
Aug 23 22:32:40.442: INFO: PreferredVersion.GroupVersion: route.openshift.io/v1
Aug 23 22:32:40.442: INFO: Versions found [{route.openshift.io/v1 v1}]
Aug 23 22:32:40.442: INFO: route.openshift.io/v1 matches route.openshift.io/v1
Aug 23 22:32:40.442: INFO: Checking APIGroup: security.openshift.io
Aug 23 22:32:40.550: INFO: PreferredVersion.GroupVersion: security.openshift.io/v1
Aug 23 22:32:40.550: INFO: Versions found [{security.openshift.io/v1 v1}]
Aug 23 22:32:40.550: INFO: security.openshift.io/v1 matches security.openshift.io/v1
Aug 23 22:32:40.550: INFO: Checking APIGroup: template.openshift.io
Aug 23 22:32:40.694: INFO: PreferredVersion.GroupVersion: template.openshift.io/v1
Aug 23 22:32:40.694: INFO: Versions found [{template.openshift.io/v1 v1}]
Aug 23 22:32:40.694: INFO: template.openshift.io/v1 matches template.openshift.io/v1
Aug 23 22:32:40.694: INFO: Checking APIGroup: user.openshift.io
Aug 23 22:32:40.867: INFO: PreferredVersion.GroupVersion: user.openshift.io/v1
Aug 23 22:32:40.867: INFO: Versions found [{user.openshift.io/v1 v1}]
Aug 23 22:32:40.867: INFO: user.openshift.io/v1 matches user.openshift.io/v1
Aug 23 22:32:40.867: INFO: Checking APIGroup: packages.operators.coreos.com
Aug 23 22:32:41.027: INFO: PreferredVersion.GroupVersion: packages.operators.coreos.com/v1
Aug 23 22:32:41.027: INFO: Versions found [{packages.operators.coreos.com/v1 v1}]
Aug 23 22:32:41.027: INFO: packages.operators.coreos.com/v1 matches packages.operators.coreos.com/v1
Aug 23 22:32:41.027: INFO: Checking APIGroup: config.openshift.io
Aug 23 22:32:41.180: INFO: PreferredVersion.GroupVersion: config.openshift.io/v1
Aug 23 22:32:41.180: INFO: Versions found [{config.openshift.io/v1 v1}]
Aug 23 22:32:41.180: INFO: config.openshift.io/v1 matches config.openshift.io/v1
Aug 23 22:32:41.180: INFO: Checking APIGroup: operator.openshift.io
Aug 23 22:32:41.323: INFO: PreferredVersion.GroupVersion: operator.openshift.io/v1
Aug 23 22:32:41.323: INFO: Versions found [{operator.openshift.io/v1 v1} {operator.openshift.io/v1alpha1 v1alpha1}]
Aug 23 22:32:41.323: INFO: operator.openshift.io/v1 matches operator.openshift.io/v1
Aug 23 22:32:41.323: INFO: Checking APIGroup: cloudcredential.openshift.io
Aug 23 22:32:41.446: INFO: PreferredVersion.GroupVersion: cloudcredential.openshift.io/v1
Aug 23 22:32:41.446: INFO: Versions found [{cloudcredential.openshift.io/v1 v1}]
Aug 23 22:32:41.446: INFO: cloudcredential.openshift.io/v1 matches cloudcredential.openshift.io/v1
Aug 23 22:32:41.446: INFO: Checking APIGroup: console.openshift.io
Aug 23 22:32:41.544: INFO: PreferredVersion.GroupVersion: console.openshift.io/v1
Aug 23 22:32:41.544: INFO: Versions found [{console.openshift.io/v1 v1} {console.openshift.io/v1alpha1 v1alpha1}]
Aug 23 22:32:41.544: INFO: console.openshift.io/v1 matches console.openshift.io/v1
Aug 23 22:32:41.544: INFO: Checking APIGroup: crd.projectcalico.org
Aug 23 22:32:41.661: INFO: PreferredVersion.GroupVersion: crd.projectcalico.org/v1
Aug 23 22:32:41.661: INFO: Versions found [{crd.projectcalico.org/v1 v1}]
Aug 23 22:32:41.661: INFO: crd.projectcalico.org/v1 matches crd.projectcalico.org/v1
Aug 23 22:32:41.661: INFO: Checking APIGroup: imageregistry.operator.openshift.io
Aug 23 22:32:41.751: INFO: PreferredVersion.GroupVersion: imageregistry.operator.openshift.io/v1
Aug 23 22:32:41.751: INFO: Versions found [{imageregistry.operator.openshift.io/v1 v1}]
Aug 23 22:32:41.751: INFO: imageregistry.operator.openshift.io/v1 matches imageregistry.operator.openshift.io/v1
Aug 23 22:32:41.751: INFO: Checking APIGroup: ingress.operator.openshift.io
Aug 23 22:32:41.859: INFO: PreferredVersion.GroupVersion: ingress.operator.openshift.io/v1
Aug 23 22:32:41.859: INFO: Versions found [{ingress.operator.openshift.io/v1 v1}]
Aug 23 22:32:41.860: INFO: ingress.operator.openshift.io/v1 matches ingress.operator.openshift.io/v1
Aug 23 22:32:41.860: INFO: Checking APIGroup: k8s.cni.cncf.io
Aug 23 22:32:41.975: INFO: PreferredVersion.GroupVersion: k8s.cni.cncf.io/v1
Aug 23 22:32:41.975: INFO: Versions found [{k8s.cni.cncf.io/v1 v1}]
Aug 23 22:32:41.975: INFO: k8s.cni.cncf.io/v1 matches k8s.cni.cncf.io/v1
Aug 23 22:32:41.975: INFO: Checking APIGroup: machineconfiguration.openshift.io
Aug 23 22:32:42.093: INFO: PreferredVersion.GroupVersion: machineconfiguration.openshift.io/v1
Aug 23 22:32:42.093: INFO: Versions found [{machineconfiguration.openshift.io/v1 v1}]
Aug 23 22:32:42.093: INFO: machineconfiguration.openshift.io/v1 matches machineconfiguration.openshift.io/v1
Aug 23 22:32:42.093: INFO: Checking APIGroup: monitoring.coreos.com
Aug 23 22:32:42.223: INFO: PreferredVersion.GroupVersion: monitoring.coreos.com/v1
Aug 23 22:32:42.223: INFO: Versions found [{monitoring.coreos.com/v1 v1} {monitoring.coreos.com/v1alpha1 v1alpha1}]
Aug 23 22:32:42.223: INFO: monitoring.coreos.com/v1 matches monitoring.coreos.com/v1
Aug 23 22:32:42.223: INFO: Checking APIGroup: network.operator.openshift.io
Aug 23 22:32:42.343: INFO: PreferredVersion.GroupVersion: network.operator.openshift.io/v1
Aug 23 22:32:42.343: INFO: Versions found [{network.operator.openshift.io/v1 v1}]
Aug 23 22:32:42.343: INFO: network.operator.openshift.io/v1 matches network.operator.openshift.io/v1
Aug 23 22:32:42.343: INFO: Checking APIGroup: operator.tigera.io
Aug 23 22:32:42.471: INFO: PreferredVersion.GroupVersion: operator.tigera.io/v1
Aug 23 22:32:42.471: INFO: Versions found [{operator.tigera.io/v1 v1}]
Aug 23 22:32:42.471: INFO: operator.tigera.io/v1 matches operator.tigera.io/v1
Aug 23 22:32:42.471: INFO: Checking APIGroup: operators.coreos.com
Aug 23 22:32:42.584: INFO: PreferredVersion.GroupVersion: operators.coreos.com/v2
Aug 23 22:32:42.584: INFO: Versions found [{operators.coreos.com/v2 v2} {operators.coreos.com/v1 v1} {operators.coreos.com/v1alpha2 v1alpha2} {operators.coreos.com/v1alpha1 v1alpha1}]
Aug 23 22:32:42.584: INFO: operators.coreos.com/v2 matches operators.coreos.com/v2
Aug 23 22:32:42.584: INFO: Checking APIGroup: samples.operator.openshift.io
Aug 23 22:32:42.701: INFO: PreferredVersion.GroupVersion: samples.operator.openshift.io/v1
Aug 23 22:32:42.701: INFO: Versions found [{samples.operator.openshift.io/v1 v1}]
Aug 23 22:32:42.701: INFO: samples.operator.openshift.io/v1 matches samples.operator.openshift.io/v1
Aug 23 22:32:42.701: INFO: Checking APIGroup: security.internal.openshift.io
Aug 23 22:32:42.819: INFO: PreferredVersion.GroupVersion: security.internal.openshift.io/v1
Aug 23 22:32:42.819: INFO: Versions found [{security.internal.openshift.io/v1 v1}]
Aug 23 22:32:42.819: INFO: security.internal.openshift.io/v1 matches security.internal.openshift.io/v1
Aug 23 22:32:42.819: INFO: Checking APIGroup: snapshot.storage.k8s.io
Aug 23 22:32:42.949: INFO: PreferredVersion.GroupVersion: snapshot.storage.k8s.io/v1
Aug 23 22:32:42.949: INFO: Versions found [{snapshot.storage.k8s.io/v1 v1} {snapshot.storage.k8s.io/v1beta1 v1beta1}]
Aug 23 22:32:42.949: INFO: snapshot.storage.k8s.io/v1 matches snapshot.storage.k8s.io/v1
Aug 23 22:32:42.949: INFO: Checking APIGroup: tuned.openshift.io
Aug 23 22:32:43.079: INFO: PreferredVersion.GroupVersion: tuned.openshift.io/v1
Aug 23 22:32:43.079: INFO: Versions found [{tuned.openshift.io/v1 v1}]
Aug 23 22:32:43.079: INFO: tuned.openshift.io/v1 matches tuned.openshift.io/v1
Aug 23 22:32:43.079: INFO: Checking APIGroup: controlplane.operator.openshift.io
Aug 23 22:32:43.199: INFO: PreferredVersion.GroupVersion: controlplane.operator.openshift.io/v1alpha1
Aug 23 22:32:43.199: INFO: Versions found [{controlplane.operator.openshift.io/v1alpha1 v1alpha1}]
Aug 23 22:32:43.199: INFO: controlplane.operator.openshift.io/v1alpha1 matches controlplane.operator.openshift.io/v1alpha1
Aug 23 22:32:43.199: INFO: Checking APIGroup: ibm.com
Aug 23 22:32:43.297: INFO: PreferredVersion.GroupVersion: ibm.com/v1alpha1
Aug 23 22:32:43.297: INFO: Versions found [{ibm.com/v1alpha1 v1alpha1}]
Aug 23 22:32:43.297: INFO: ibm.com/v1alpha1 matches ibm.com/v1alpha1
Aug 23 22:32:43.297: INFO: Checking APIGroup: metal3.io
Aug 23 22:32:43.396: INFO: PreferredVersion.GroupVersion: metal3.io/v1alpha1
Aug 23 22:32:43.396: INFO: Versions found [{metal3.io/v1alpha1 v1alpha1}]
Aug 23 22:32:43.396: INFO: metal3.io/v1alpha1 matches metal3.io/v1alpha1
Aug 23 22:32:43.396: INFO: Checking APIGroup: migration.k8s.io
Aug 23 22:32:43.488: INFO: PreferredVersion.GroupVersion: migration.k8s.io/v1alpha1
Aug 23 22:32:43.488: INFO: Versions found [{migration.k8s.io/v1alpha1 v1alpha1}]
Aug 23 22:32:43.488: INFO: migration.k8s.io/v1alpha1 matches migration.k8s.io/v1alpha1
Aug 23 22:32:43.488: INFO: Checking APIGroup: whereabouts.cni.cncf.io
Aug 23 22:32:43.591: INFO: PreferredVersion.GroupVersion: whereabouts.cni.cncf.io/v1alpha1
Aug 23 22:32:43.591: INFO: Versions found [{whereabouts.cni.cncf.io/v1alpha1 v1alpha1}]
Aug 23 22:32:43.591: INFO: whereabouts.cni.cncf.io/v1alpha1 matches whereabouts.cni.cncf.io/v1alpha1
Aug 23 22:32:43.591: INFO: Checking APIGroup: helm.openshift.io
Aug 23 22:32:43.693: INFO: PreferredVersion.GroupVersion: helm.openshift.io/v1beta1
Aug 23 22:32:43.693: INFO: Versions found [{helm.openshift.io/v1beta1 v1beta1}]
Aug 23 22:32:43.693: INFO: helm.openshift.io/v1beta1 matches helm.openshift.io/v1beta1
Aug 23 22:32:43.693: INFO: Checking APIGroup: metrics.k8s.io
Aug 23 22:32:43.814: INFO: PreferredVersion.GroupVersion: metrics.k8s.io/v1beta1
Aug 23 22:32:43.814: INFO: Versions found [{metrics.k8s.io/v1beta1 v1beta1}]
Aug 23 22:32:43.814: INFO: metrics.k8s.io/v1beta1 matches metrics.k8s.io/v1beta1
[AfterEach] [sig-api-machinery] Discovery
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 22:32:43.814: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "discovery-3372" for this suite.

• [SLOW TEST:9.225 seconds]
[sig-api-machinery] Discovery
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should validate PreferredVersion for each APIGroup [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] Discovery should validate PreferredVersion for each APIGroup [Conformance]","total":311,"completed":146,"skipped":2460,"failed":0}
S
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 22:32:44.383: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating projection with secret that has name projected-secret-test-1b20509f-a3d0-45cc-aab9-bf1df91b494a
STEP: Creating a pod to test consume secrets
Aug 23 22:32:45.212: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-880e42a1-174c-4e83-a0a8-53b697b1ef56" in namespace "projected-2050" to be "Succeeded or Failed"
Aug 23 22:32:45.369: INFO: Pod "pod-projected-secrets-880e42a1-174c-4e83-a0a8-53b697b1ef56": Phase="Pending", Reason="", readiness=false. Elapsed: 156.881658ms
Aug 23 22:32:47.498: INFO: Pod "pod-projected-secrets-880e42a1-174c-4e83-a0a8-53b697b1ef56": Phase="Pending", Reason="", readiness=false. Elapsed: 2.286287363s
Aug 23 22:32:49.568: INFO: Pod "pod-projected-secrets-880e42a1-174c-4e83-a0a8-53b697b1ef56": Phase="Pending", Reason="", readiness=false. Elapsed: 4.356000995s
Aug 23 22:32:51.691: INFO: Pod "pod-projected-secrets-880e42a1-174c-4e83-a0a8-53b697b1ef56": Phase="Pending", Reason="", readiness=false. Elapsed: 6.478552418s
Aug 23 22:32:53.814: INFO: Pod "pod-projected-secrets-880e42a1-174c-4e83-a0a8-53b697b1ef56": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.602118034s
STEP: Saw pod success
Aug 23 22:32:53.814: INFO: Pod "pod-projected-secrets-880e42a1-174c-4e83-a0a8-53b697b1ef56" satisfied condition "Succeeded or Failed"
Aug 23 22:32:53.988: INFO: Trying to get logs from node 10.149.248.24 pod pod-projected-secrets-880e42a1-174c-4e83-a0a8-53b697b1ef56 container projected-secret-volume-test: <nil>
STEP: delete the pod
Aug 23 22:32:54.410: INFO: Waiting for pod pod-projected-secrets-880e42a1-174c-4e83-a0a8-53b697b1ef56 to disappear
Aug 23 22:32:54.566: INFO: Pod pod-projected-secrets-880e42a1-174c-4e83-a0a8-53b697b1ef56 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 22:32:54.566: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2050" for this suite.

• [SLOW TEST:10.787 seconds]
[sig-storage] Projected secret
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":147,"skipped":2461,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should honor timeout [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 22:32:55.170: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Aug 23 22:32:57.869: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63765354777, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63765354777, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63765354777, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63765354777, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 23 22:33:00.018: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63765354777, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63765354777, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63765354777, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63765354777, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 23 22:33:01.947: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63765354777, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63765354777, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63765354777, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63765354777, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Aug 23 22:33:05.125: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should honor timeout [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Setting timeout (1s) shorter than webhook latency (5s)
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s)
STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is longer than webhook latency
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is empty (defaulted to 10s in v1)
STEP: Registering slow webhook via the AdmissionRegistration API
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 22:33:19.147: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9169" for this suite.
STEP: Destroying namespace "webhook-9169-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

• [SLOW TEST:24.920 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should honor timeout [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should honor timeout [Conformance]","total":311,"completed":148,"skipped":2473,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl patch 
  should add annotations for pods in rc  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 22:33:20.090: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should add annotations for pods in rc  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating Agnhost RC
Aug 23 22:33:20.402: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585719795 --namespace=kubectl-4683 create -f -'
Aug 23 22:33:21.232: INFO: stderr: ""
Aug 23 22:33:21.232: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
Aug 23 22:33:22.334: INFO: Selector matched 1 pods for map[app:agnhost]
Aug 23 22:33:22.334: INFO: Found 0 / 1
Aug 23 22:33:23.402: INFO: Selector matched 1 pods for map[app:agnhost]
Aug 23 22:33:23.403: INFO: Found 0 / 1
Aug 23 22:33:24.371: INFO: Selector matched 1 pods for map[app:agnhost]
Aug 23 22:33:24.371: INFO: Found 0 / 1
Aug 23 22:33:25.325: INFO: Selector matched 1 pods for map[app:agnhost]
Aug 23 22:33:25.325: INFO: Found 0 / 1
Aug 23 22:33:26.327: INFO: Selector matched 1 pods for map[app:agnhost]
Aug 23 22:33:26.327: INFO: Found 0 / 1
Aug 23 22:33:27.303: INFO: Selector matched 1 pods for map[app:agnhost]
Aug 23 22:33:27.303: INFO: Found 1 / 1
Aug 23 22:33:27.303: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods
Aug 23 22:33:27.374: INFO: Selector matched 1 pods for map[app:agnhost]
Aug 23 22:33:27.374: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Aug 23 22:33:27.374: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585719795 --namespace=kubectl-4683 patch pod agnhost-primary-qpr6h -p {"metadata":{"annotations":{"x":"y"}}}'
Aug 23 22:33:28.090: INFO: stderr: ""
Aug 23 22:33:28.090: INFO: stdout: "pod/agnhost-primary-qpr6h patched\n"
STEP: checking annotations
Aug 23 22:33:28.229: INFO: Selector matched 1 pods for map[app:agnhost]
Aug 23 22:33:28.229: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 22:33:28.229: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4683" for this suite.

• [SLOW TEST:8.894 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl patch
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1466
    should add annotations for pods in rc  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl patch should add annotations for pods in rc  [Conformance]","total":311,"completed":149,"skipped":2507,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 22:33:28.985: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating pod pod-subpath-test-configmap-9vs8
STEP: Creating a pod to test atomic-volume-subpath
Aug 23 22:33:30.169: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-9vs8" in namespace "subpath-4940" to be "Succeeded or Failed"
Aug 23 22:33:30.275: INFO: Pod "pod-subpath-test-configmap-9vs8": Phase="Pending", Reason="", readiness=false. Elapsed: 105.558535ms
Aug 23 22:33:32.419: INFO: Pod "pod-subpath-test-configmap-9vs8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.249880442s
Aug 23 22:33:34.537: INFO: Pod "pod-subpath-test-configmap-9vs8": Phase="Pending", Reason="", readiness=false. Elapsed: 4.368315557s
Aug 23 22:33:36.647: INFO: Pod "pod-subpath-test-configmap-9vs8": Phase="Pending", Reason="", readiness=false. Elapsed: 6.478026708s
Aug 23 22:33:38.799: INFO: Pod "pod-subpath-test-configmap-9vs8": Phase="Running", Reason="", readiness=true. Elapsed: 8.629717394s
Aug 23 22:33:40.923: INFO: Pod "pod-subpath-test-configmap-9vs8": Phase="Running", Reason="", readiness=true. Elapsed: 10.754038311s
Aug 23 22:33:43.060: INFO: Pod "pod-subpath-test-configmap-9vs8": Phase="Running", Reason="", readiness=true. Elapsed: 12.891340584s
Aug 23 22:33:45.175: INFO: Pod "pod-subpath-test-configmap-9vs8": Phase="Running", Reason="", readiness=true. Elapsed: 15.005957647s
Aug 23 22:33:47.314: INFO: Pod "pod-subpath-test-configmap-9vs8": Phase="Running", Reason="", readiness=true. Elapsed: 17.145225688s
Aug 23 22:33:49.465: INFO: Pod "pod-subpath-test-configmap-9vs8": Phase="Running", Reason="", readiness=true. Elapsed: 19.296007629s
Aug 23 22:33:51.637: INFO: Pod "pod-subpath-test-configmap-9vs8": Phase="Running", Reason="", readiness=true. Elapsed: 21.468243052s
Aug 23 22:33:53.774: INFO: Pod "pod-subpath-test-configmap-9vs8": Phase="Running", Reason="", readiness=true. Elapsed: 23.605359296s
Aug 23 22:33:55.933: INFO: Pod "pod-subpath-test-configmap-9vs8": Phase="Running", Reason="", readiness=true. Elapsed: 25.763704s
Aug 23 22:33:58.059: INFO: Pod "pod-subpath-test-configmap-9vs8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 27.889945294s
STEP: Saw pod success
Aug 23 22:33:58.059: INFO: Pod "pod-subpath-test-configmap-9vs8" satisfied condition "Succeeded or Failed"
Aug 23 22:33:58.202: INFO: Trying to get logs from node 10.149.248.24 pod pod-subpath-test-configmap-9vs8 container test-container-subpath-configmap-9vs8: <nil>
STEP: delete the pod
Aug 23 22:33:58.512: INFO: Waiting for pod pod-subpath-test-configmap-9vs8 to disappear
Aug 23 22:33:58.646: INFO: Pod pod-subpath-test-configmap-9vs8 no longer exists
STEP: Deleting pod pod-subpath-test-configmap-9vs8
Aug 23 22:33:58.646: INFO: Deleting pod "pod-subpath-test-configmap-9vs8" in namespace "subpath-4940"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 22:33:58.779: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-4940" for this suite.

• [SLOW TEST:30.297 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod [LinuxOnly] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod [LinuxOnly] [Conformance]","total":311,"completed":150,"skipped":2532,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Events 
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] [sig-node] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 22:33:59.282: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: retrieving the pod
Aug 23 22:34:06.522: INFO: &Pod{ObjectMeta:{send-events-08503039-5c6b-4245-aaf4-708cf9a2186e  events-8222 /api/v1/namespaces/events-8222/pods/send-events-08503039-5c6b-4245-aaf4-708cf9a2186e 43bc69e2-cb10-47ab-ae27-af06d9fbb258 88024 0 2021-08-23 22:33:59 +0000 UTC <nil> <nil> map[name:foo time:851343380] map[cni.projectcalico.org/podIP:172.30.87.89/32 cni.projectcalico.org/podIPs:172.30.87.89/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "",
    "ips": [
        "172.30.87.89"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "",
    "ips": [
        "172.30.87.89"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [] []  [{e2e.test Update v1 2021-08-23 22:33:59 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:time":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"p\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":80,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{".":{},"f:seLinuxOptions":{"f:level":{}}},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-08-23 22:34:03 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {multus Update v1 2021-08-23 22:34:04 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}}} {kubelet Update v1 2021-08-23 22:34:05 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.87.89\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-8gbdc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-8gbdc,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:p,Image:k8s.gcr.io/e2e-test-images/agnhost:2.21,Command:[],Args:[serve-hostname],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:80,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-8gbdc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.149.248.24,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c50,c15,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-tqvxj,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-08-23 22:34:00 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-08-23 22:34:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-08-23 22:34:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-08-23 22:33:59 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.149.248.24,PodIP:172.30.87.89,StartTime:2021-08-23 22:34:00 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:p,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-08-23 22:34:04 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.21,ImageID:k8s.gcr.io/e2e-test-images/agnhost@sha256:ab055cd3d45f50b90732c14593a5bf50f210871bb4f91994c756fc22db6d922a,ContainerID:cri-o://3708cba42717fbac4326e33b7f431afd1ce990637fd1c9317dd2e647596dc326,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.87.89,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

STEP: checking for scheduler event about the pod
Aug 23 22:34:08.964: INFO: Saw scheduler event for our pod.
STEP: checking for kubelet event about the pod
Aug 23 22:34:11.088: INFO: Saw kubelet event for our pod.
STEP: deleting the pod
[AfterEach] [k8s.io] [sig-node] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 22:34:11.202: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-8222" for this suite.

• [SLOW TEST:12.409 seconds]
[k8s.io] [sig-node] Events
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] [sig-node] Events should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]","total":311,"completed":151,"skipped":2579,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should be able to update and delete ResourceQuota. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 22:34:11.691: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to update and delete ResourceQuota. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a ResourceQuota
STEP: Getting a ResourceQuota
STEP: Updating a ResourceQuota
STEP: Verifying a ResourceQuota was modified
STEP: Deleting a ResourceQuota
STEP: Verifying the deleted ResourceQuota
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 22:34:12.655: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-538" for this suite.
•{"msg":"PASSED [sig-api-machinery] ResourceQuota should be able to update and delete ResourceQuota. [Conformance]","total":311,"completed":152,"skipped":2598,"failed":0}
SSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 22:34:12.875: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Performing setup for networking test in namespace pod-network-test-3728
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Aug 23 22:34:13.061: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Aug 23 22:34:13.376: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Aug 23 22:34:15.399: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Aug 23 22:34:17.400: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Aug 23 22:34:19.407: INFO: The status of Pod netserver-0 is Running (Ready = false)
Aug 23 22:34:21.394: INFO: The status of Pod netserver-0 is Running (Ready = false)
Aug 23 22:34:23.466: INFO: The status of Pod netserver-0 is Running (Ready = false)
Aug 23 22:34:25.417: INFO: The status of Pod netserver-0 is Running (Ready = false)
Aug 23 22:34:27.396: INFO: The status of Pod netserver-0 is Running (Ready = false)
Aug 23 22:34:29.402: INFO: The status of Pod netserver-0 is Running (Ready = false)
Aug 23 22:34:31.418: INFO: The status of Pod netserver-0 is Running (Ready = true)
Aug 23 22:34:31.473: INFO: The status of Pod netserver-1 is Running (Ready = false)
Aug 23 22:34:33.496: INFO: The status of Pod netserver-1 is Running (Ready = false)
Aug 23 22:34:35.590: INFO: The status of Pod netserver-1 is Running (Ready = true)
Aug 23 22:34:35.823: INFO: The status of Pod netserver-2 is Running (Ready = true)
STEP: Creating test pods
Aug 23 22:34:40.637: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
Aug 23 22:34:40.637: INFO: Going to poll 172.30.87.95 on port 8081 at least 0 times, with a maximum of 39 tries before failing
Aug 23 22:34:40.755: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.30.87.95 8081 | grep -v '^\s*$'] Namespace:pod-network-test-3728 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 23 22:34:40.755: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
Aug 23 22:34:41.987: INFO: Found all 1 expected endpoints: [netserver-0]
Aug 23 22:34:41.987: INFO: Going to poll 172.30.240.105 on port 8081 at least 0 times, with a maximum of 39 tries before failing
Aug 23 22:34:42.091: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.30.240.105 8081 | grep -v '^\s*$'] Namespace:pod-network-test-3728 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 23 22:34:42.091: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
Aug 23 22:34:44.803: INFO: Found all 1 expected endpoints: [netserver-1]
Aug 23 22:34:44.803: INFO: Going to poll 172.30.210.154 on port 8081 at least 0 times, with a maximum of 39 tries before failing
Aug 23 22:34:44.935: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.30.210.154 8081 | grep -v '^\s*$'] Namespace:pod-network-test-3728 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 23 22:34:44.935: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
Aug 23 22:34:46.842: INFO: Found all 1 expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 22:34:46.842: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-3728" for this suite.

• [SLOW TEST:34.762 seconds]
[sig-network] Networking
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:27
  Granular Checks: Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:30
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":153,"skipped":2607,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should rollback without unnecessary restarts [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 22:34:47.637: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:129
[It] should rollback without unnecessary restarts [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Aug 23 22:34:50.138: INFO: Create a RollingUpdate DaemonSet
Aug 23 22:34:50.338: INFO: Check that daemon pods launch on every node of the cluster
Aug 23 22:34:50.865: INFO: Number of nodes with available pods: 0
Aug 23 22:34:50.865: INFO: Node 10.149.248.24 is running more than one daemon pod
Aug 23 22:34:52.488: INFO: Number of nodes with available pods: 0
Aug 23 22:34:52.488: INFO: Node 10.149.248.24 is running more than one daemon pod
Aug 23 22:34:53.234: INFO: Number of nodes with available pods: 0
Aug 23 22:34:53.234: INFO: Node 10.149.248.24 is running more than one daemon pod
Aug 23 22:34:54.467: INFO: Number of nodes with available pods: 1
Aug 23 22:34:54.467: INFO: Node 10.149.248.24 is running more than one daemon pod
Aug 23 22:34:55.427: INFO: Number of nodes with available pods: 2
Aug 23 22:34:55.427: INFO: Node 10.149.248.24 is running more than one daemon pod
Aug 23 22:34:56.362: INFO: Number of nodes with available pods: 3
Aug 23 22:34:56.362: INFO: Number of running nodes: 3, number of available pods: 3
Aug 23 22:34:56.362: INFO: Update the DaemonSet to trigger a rollout
Aug 23 22:34:56.658: INFO: Updating DaemonSet daemon-set
Aug 23 22:35:07.703: INFO: Roll back the DaemonSet before rollout is complete
Aug 23 22:35:08.210: INFO: Updating DaemonSet daemon-set
Aug 23 22:35:08.210: INFO: Make sure DaemonSet rollback is complete
Aug 23 22:35:08.493: INFO: Wrong image for pod: daemon-set-5cw6c. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Aug 23 22:35:08.493: INFO: Pod daemon-set-5cw6c is not available
Aug 23 22:35:10.518: INFO: Wrong image for pod: daemon-set-5cw6c. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Aug 23 22:35:10.518: INFO: Pod daemon-set-5cw6c is not available
Aug 23 22:35:11.564: INFO: Wrong image for pod: daemon-set-5cw6c. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Aug 23 22:35:11.569: INFO: Pod daemon-set-5cw6c is not available
Aug 23 22:35:13.495: INFO: Wrong image for pod: daemon-set-5cw6c. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Aug 23 22:35:13.495: INFO: Pod daemon-set-5cw6c is not available
Aug 23 22:35:14.505: INFO: Wrong image for pod: daemon-set-5cw6c. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Aug 23 22:35:14.505: INFO: Pod daemon-set-5cw6c is not available
Aug 23 22:35:15.486: INFO: Pod daemon-set-n4gl9 is not available
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:95
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-6845, will wait for the garbage collector to delete the pods
Aug 23 22:35:17.384: INFO: Deleting DaemonSet.extensions daemon-set took: 446.077677ms
Aug 23 22:35:17.585: INFO: Terminating DaemonSet.extensions daemon-set pods took: 200.331631ms
Aug 23 22:35:26.570: INFO: Number of nodes with available pods: 0
Aug 23 22:35:26.570: INFO: Number of running nodes: 0, number of available pods: 0
Aug 23 22:35:26.744: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-6845/daemonsets","resourceVersion":"88864"},"items":null}

Aug 23 22:35:26.901: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-6845/pods","resourceVersion":"88864"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 22:35:27.993: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-6845" for this suite.

• [SLOW TEST:40.988 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should rollback without unnecessary restarts [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","total":311,"completed":154,"skipped":2639,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 22:35:28.626: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Aug 23 22:35:29.697: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
STEP: Creating projection with configMap that has name projected-configmap-test-upd-f1e9c471-ad1c-4b55-a054-0b0598469f32
STEP: Creating the pod
STEP: Updating configmap projected-configmap-test-upd-f1e9c471-ad1c-4b55-a054-0b0598469f32
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 22:36:54.409: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5994" for this suite.

• [SLOW TEST:86.550 seconds]
[sig-storage] Projected configMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:36
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap updates should be reflected in volume [NodeConformance] [Conformance]","total":311,"completed":155,"skipped":2683,"failed":0}
SSSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount an API token into pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 22:36:55.175: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount an API token into pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: getting the auto-created API token
STEP: reading a file in the container
Aug 23 22:37:01.061: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-1134 pod-service-account-f63bde19-fbfa-4e18-9114-684f796eda0f -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container
Aug 23 22:37:03.293: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-1134 pod-service-account-f63bde19-fbfa-4e18-9114-684f796eda0f -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container
Aug 23 22:37:04.609: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-1134 pod-service-account-f63bde19-fbfa-4e18-9114-684f796eda0f -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 22:37:05.668: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-1134" for this suite.

• [SLOW TEST:11.080 seconds]
[sig-auth] ServiceAccounts
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:23
  should mount an API token into pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-auth] ServiceAccounts should mount an API token into pods  [Conformance]","total":311,"completed":156,"skipped":2687,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Lease 
  lease API should be available [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Lease
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 22:37:06.255: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename lease-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] lease API should be available [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[AfterEach] [k8s.io] Lease
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 22:37:09.307: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "lease-test-2134" for this suite.
•{"msg":"PASSED [k8s.io] Lease lease API should be available [Conformance]","total":311,"completed":157,"skipped":2705,"failed":0}
SSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should scale a replication controller  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 22:37:10.005: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[BeforeEach] Update Demo
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:299
[It] should scale a replication controller  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a replication controller
Aug 23 22:37:10.920: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585719795 --namespace=kubectl-8124 create -f -'
Aug 23 22:37:14.159: INFO: stderr: ""
Aug 23 22:37:14.159: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Aug 23 22:37:14.159: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585719795 --namespace=kubectl-8124 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Aug 23 22:37:14.282: INFO: stderr: ""
Aug 23 22:37:14.282: INFO: stdout: "update-demo-nautilus-qchsk update-demo-nautilus-vzk2g "
Aug 23 22:37:14.282: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585719795 --namespace=kubectl-8124 get pods update-demo-nautilus-qchsk -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Aug 23 22:37:14.815: INFO: stderr: ""
Aug 23 22:37:14.815: INFO: stdout: ""
Aug 23 22:37:14.815: INFO: update-demo-nautilus-qchsk is created but not running
Aug 23 22:37:19.816: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585719795 --namespace=kubectl-8124 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Aug 23 22:37:20.939: INFO: stderr: ""
Aug 23 22:37:20.939: INFO: stdout: "update-demo-nautilus-qchsk update-demo-nautilus-vzk2g "
Aug 23 22:37:20.939: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585719795 --namespace=kubectl-8124 get pods update-demo-nautilus-qchsk -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Aug 23 22:37:21.075: INFO: stderr: ""
Aug 23 22:37:21.075: INFO: stdout: "true"
Aug 23 22:37:21.075: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585719795 --namespace=kubectl-8124 get pods update-demo-nautilus-qchsk -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Aug 23 22:37:21.562: INFO: stderr: ""
Aug 23 22:37:21.562: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Aug 23 22:37:21.562: INFO: validating pod update-demo-nautilus-qchsk
Aug 23 22:37:21.685: INFO: got data: {
  "image": "nautilus.jpg"
}

Aug 23 22:37:21.685: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Aug 23 22:37:21.685: INFO: update-demo-nautilus-qchsk is verified up and running
Aug 23 22:37:21.685: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585719795 --namespace=kubectl-8124 get pods update-demo-nautilus-vzk2g -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Aug 23 22:37:22.169: INFO: stderr: ""
Aug 23 22:37:22.169: INFO: stdout: "true"
Aug 23 22:37:22.169: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585719795 --namespace=kubectl-8124 get pods update-demo-nautilus-vzk2g -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Aug 23 22:37:22.331: INFO: stderr: ""
Aug 23 22:37:22.332: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Aug 23 22:37:22.332: INFO: validating pod update-demo-nautilus-vzk2g
Aug 23 22:37:22.433: INFO: got data: {
  "image": "nautilus.jpg"
}

Aug 23 22:37:22.433: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Aug 23 22:37:22.433: INFO: update-demo-nautilus-vzk2g is verified up and running
STEP: scaling down the replication controller
Aug 23 22:37:22.437: INFO: scanned /root for discovery docs: <nil>
Aug 23 22:37:22.437: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585719795 --namespace=kubectl-8124 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
Aug 23 22:37:23.434: INFO: stderr: ""
Aug 23 22:37:23.434: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Aug 23 22:37:23.434: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585719795 --namespace=kubectl-8124 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Aug 23 22:37:24.258: INFO: stderr: ""
Aug 23 22:37:24.258: INFO: stdout: "update-demo-nautilus-qchsk update-demo-nautilus-vzk2g "
STEP: Replicas for name=update-demo: expected=1 actual=2
Aug 23 22:37:29.259: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585719795 --namespace=kubectl-8124 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Aug 23 22:37:30.277: INFO: stderr: ""
Aug 23 22:37:30.277: INFO: stdout: "update-demo-nautilus-vzk2g "
Aug 23 22:37:30.277: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585719795 --namespace=kubectl-8124 get pods update-demo-nautilus-vzk2g -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Aug 23 22:37:30.407: INFO: stderr: ""
Aug 23 22:37:30.407: INFO: stdout: "true"
Aug 23 22:37:30.407: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585719795 --namespace=kubectl-8124 get pods update-demo-nautilus-vzk2g -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Aug 23 22:37:30.976: INFO: stderr: ""
Aug 23 22:37:30.976: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Aug 23 22:37:30.976: INFO: validating pod update-demo-nautilus-vzk2g
Aug 23 22:37:31.087: INFO: got data: {
  "image": "nautilus.jpg"
}

Aug 23 22:37:31.087: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Aug 23 22:37:31.087: INFO: update-demo-nautilus-vzk2g is verified up and running
STEP: scaling up the replication controller
Aug 23 22:37:31.091: INFO: scanned /root for discovery docs: <nil>
Aug 23 22:37:31.091: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585719795 --namespace=kubectl-8124 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
Aug 23 22:37:32.168: INFO: stderr: ""
Aug 23 22:37:32.168: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Aug 23 22:37:32.168: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585719795 --namespace=kubectl-8124 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Aug 23 22:37:32.295: INFO: stderr: ""
Aug 23 22:37:32.295: INFO: stdout: "update-demo-nautilus-vzk2g update-demo-nautilus-wn4kz "
Aug 23 22:37:32.295: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585719795 --namespace=kubectl-8124 get pods update-demo-nautilus-vzk2g -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Aug 23 22:37:32.853: INFO: stderr: ""
Aug 23 22:37:32.853: INFO: stdout: "true"
Aug 23 22:37:32.853: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585719795 --namespace=kubectl-8124 get pods update-demo-nautilus-vzk2g -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Aug 23 22:37:33.829: INFO: stderr: ""
Aug 23 22:37:33.829: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Aug 23 22:37:33.829: INFO: validating pod update-demo-nautilus-vzk2g
Aug 23 22:37:33.975: INFO: got data: {
  "image": "nautilus.jpg"
}

Aug 23 22:37:33.975: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Aug 23 22:37:33.975: INFO: update-demo-nautilus-vzk2g is verified up and running
Aug 23 22:37:33.975: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585719795 --namespace=kubectl-8124 get pods update-demo-nautilus-wn4kz -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Aug 23 22:37:34.371: INFO: stderr: ""
Aug 23 22:37:34.371: INFO: stdout: ""
Aug 23 22:37:34.371: INFO: update-demo-nautilus-wn4kz is created but not running
Aug 23 22:37:39.371: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585719795 --namespace=kubectl-8124 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Aug 23 22:37:39.486: INFO: stderr: ""
Aug 23 22:37:39.486: INFO: stdout: "update-demo-nautilus-vzk2g update-demo-nautilus-wn4kz "
Aug 23 22:37:39.486: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585719795 --namespace=kubectl-8124 get pods update-demo-nautilus-vzk2g -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Aug 23 22:37:40.070: INFO: stderr: ""
Aug 23 22:37:40.070: INFO: stdout: "true"
Aug 23 22:37:40.070: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585719795 --namespace=kubectl-8124 get pods update-demo-nautilus-vzk2g -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Aug 23 22:37:41.680: INFO: stderr: ""
Aug 23 22:37:41.680: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Aug 23 22:37:41.680: INFO: validating pod update-demo-nautilus-vzk2g
Aug 23 22:37:41.766: INFO: got data: {
  "image": "nautilus.jpg"
}

Aug 23 22:37:41.766: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Aug 23 22:37:41.766: INFO: update-demo-nautilus-vzk2g is verified up and running
Aug 23 22:37:41.766: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585719795 --namespace=kubectl-8124 get pods update-demo-nautilus-wn4kz -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Aug 23 22:37:41.889: INFO: stderr: ""
Aug 23 22:37:41.889: INFO: stdout: "true"
Aug 23 22:37:41.889: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585719795 --namespace=kubectl-8124 get pods update-demo-nautilus-wn4kz -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Aug 23 22:37:42.098: INFO: stderr: ""
Aug 23 22:37:42.098: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Aug 23 22:37:42.098: INFO: validating pod update-demo-nautilus-wn4kz
Aug 23 22:37:42.244: INFO: got data: {
  "image": "nautilus.jpg"
}

Aug 23 22:37:42.245: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Aug 23 22:37:42.245: INFO: update-demo-nautilus-wn4kz is verified up and running
STEP: using delete to clean up resources
Aug 23 22:37:42.245: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585719795 --namespace=kubectl-8124 delete --grace-period=0 --force -f -'
Aug 23 22:37:42.947: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug 23 22:37:42.947: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Aug 23 22:37:42.947: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585719795 --namespace=kubectl-8124 get rc,svc -l name=update-demo --no-headers'
Aug 23 22:37:43.088: INFO: stderr: "No resources found in kubectl-8124 namespace.\n"
Aug 23 22:37:43.088: INFO: stdout: ""
Aug 23 22:37:43.088: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585719795 --namespace=kubectl-8124 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Aug 23 22:37:44.056: INFO: stderr: ""
Aug 23 22:37:44.056: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 22:37:44.056: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8124" for this suite.

• [SLOW TEST:34.767 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:297
    should scale a replication controller  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should scale a replication controller  [Conformance]","total":311,"completed":158,"skipped":2711,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 22:37:44.772: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Discovering how many secrets are in namespace by default
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Secret
STEP: Ensuring resource quota status captures secret creation
STEP: Deleting a secret
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 22:38:03.657: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-8435" for this suite.

• [SLOW TEST:19.180 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a secret. [Conformance]","total":311,"completed":159,"skipped":2719,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 22:38:03.953: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:41
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating the pod
Aug 23 22:38:11.857: INFO: Successfully updated pod "labelsupdated74d74ab-f6d4-4223-b991-8cfb677f51fa"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 22:38:14.157: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2850" for this suite.

• [SLOW TEST:10.841 seconds]
[sig-storage] Projected downwardAPI
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:35
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should update labels on modification [NodeConformance] [Conformance]","total":311,"completed":160,"skipped":2734,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch 
  watch on custom resource definition objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 22:38:14.794: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename crd-watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] watch on custom resource definition objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Aug 23 22:38:15.260: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Creating first CR 
Aug 23 22:38:16.282: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-08-23T22:38:16Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-08-23T22:38:16Z]] name:name1 resourceVersion:90357 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:351d8f20-c87d-4840-9efd-95609b4477ab] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Creating second CR
Aug 23 22:38:26.516: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-08-23T22:38:26Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-08-23T22:38:26Z]] name:name2 resourceVersion:90439 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:5919ac1b-29fe-4c5c-bc13-70edb6fdaba9] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying first CR
Aug 23 22:38:36.660: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-08-23T22:38:16Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-08-23T22:38:36Z]] name:name1 resourceVersion:90492 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:351d8f20-c87d-4840-9efd-95609b4477ab] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying second CR
Aug 23 22:38:46.808: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-08-23T22:38:26Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-08-23T22:38:46Z]] name:name2 resourceVersion:90539 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:5919ac1b-29fe-4c5c-bc13-70edb6fdaba9] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting first CR
Aug 23 22:38:56.892: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-08-23T22:38:16Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-08-23T22:38:36Z]] name:name1 resourceVersion:90585 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:351d8f20-c87d-4840-9efd-95609b4477ab] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting second CR
Aug 23 22:39:06.998: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-08-23T22:38:26Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-08-23T22:38:46Z]] name:name2 resourceVersion:90633 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:5919ac1b-29fe-4c5c-bc13-70edb6fdaba9] num:map[num1:9223372036854775807 num2:1000000]]}
[AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 22:39:17.720: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-watch-8336" for this suite.

• [SLOW TEST:63.275 seconds]
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  CustomResourceDefinition Watch
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_watch.go:42
    watch on custom resource definition objects [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch watch on custom resource definition objects [Conformance]","total":311,"completed":161,"skipped":2759,"failed":0}
S
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 22:39:18.070: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir 0666 on tmpfs
Aug 23 22:39:18.520: INFO: Waiting up to 5m0s for pod "pod-0bd1a5b9-e313-412d-b7f4-57ca3274c510" in namespace "emptydir-5575" to be "Succeeded or Failed"
Aug 23 22:39:18.592: INFO: Pod "pod-0bd1a5b9-e313-412d-b7f4-57ca3274c510": Phase="Pending", Reason="", readiness=false. Elapsed: 71.855034ms
Aug 23 22:39:20.663: INFO: Pod "pod-0bd1a5b9-e313-412d-b7f4-57ca3274c510": Phase="Pending", Reason="", readiness=false. Elapsed: 2.14350013s
Aug 23 22:39:22.764: INFO: Pod "pod-0bd1a5b9-e313-412d-b7f4-57ca3274c510": Phase="Pending", Reason="", readiness=false. Elapsed: 4.243684716s
Aug 23 22:39:24.976: INFO: Pod "pod-0bd1a5b9-e313-412d-b7f4-57ca3274c510": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.456436967s
STEP: Saw pod success
Aug 23 22:39:24.976: INFO: Pod "pod-0bd1a5b9-e313-412d-b7f4-57ca3274c510" satisfied condition "Succeeded or Failed"
Aug 23 22:39:25.173: INFO: Trying to get logs from node 10.149.248.24 pod pod-0bd1a5b9-e313-412d-b7f4-57ca3274c510 container test-container: <nil>
STEP: delete the pod
Aug 23 22:39:25.625: INFO: Waiting for pod pod-0bd1a5b9-e313-412d-b7f4-57ca3274c510 to disappear
Aug 23 22:39:26.262: INFO: Pod pod-0bd1a5b9-e313-412d-b7f4-57ca3274c510 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 22:39:26.262: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5575" for this suite.

• [SLOW TEST:9.066 seconds]
[sig-storage] EmptyDir volumes
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:45
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":162,"skipped":2760,"failed":0}
SSS
------------------------------
[k8s.io] Pods 
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 22:39:27.137: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:187
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Aug 23 22:39:35.323: INFO: Successfully updated pod "pod-update-activedeadlineseconds-da178c0e-c4eb-4b2a-b5e4-26050e09661b"
Aug 23 22:39:35.323: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-da178c0e-c4eb-4b2a-b5e4-26050e09661b" in namespace "pods-1337" to be "terminated due to deadline exceeded"
Aug 23 22:39:35.446: INFO: Pod "pod-update-activedeadlineseconds-da178c0e-c4eb-4b2a-b5e4-26050e09661b": Phase="Running", Reason="", readiness=true. Elapsed: 123.105303ms
Aug 23 22:39:37.595: INFO: Pod "pod-update-activedeadlineseconds-da178c0e-c4eb-4b2a-b5e4-26050e09661b": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 2.271655588s
Aug 23 22:39:37.595: INFO: Pod "pod-update-activedeadlineseconds-da178c0e-c4eb-4b2a-b5e4-26050e09661b" satisfied condition "terminated due to deadline exceeded"
[AfterEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 22:39:37.595: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-1337" for this suite.

• [SLOW TEST:11.196 seconds]
[k8s.io] Pods
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Pods should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]","total":311,"completed":163,"skipped":2763,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] Secrets 
  should fail to create secret due to empty secret key [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 22:39:38.334: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create secret due to empty secret key [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating projection with secret that has name secret-emptykey-test-16a00a94-42c2-4421-a7fe-7b8c8aa9a79b
[AfterEach] [sig-api-machinery] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 22:39:39.022: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5711" for this suite.
•{"msg":"PASSED [sig-api-machinery] Secrets should fail to create secret due to empty secret key [Conformance]","total":311,"completed":164,"skipped":2768,"failed":0}
SSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support --unix-socket=/path  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 22:39:39.585: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should support --unix-socket=/path  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Starting the proxy
Aug 23 22:39:40.363: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-585719795 --namespace=kubectl-5235 proxy --unix-socket=/tmp/kubectl-proxy-unix520693434/test'
STEP: retrieving proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 22:39:40.435: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5235" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support --unix-socket=/path  [Conformance]","total":311,"completed":165,"skipped":2778,"failed":0}
SSSSSS
------------------------------
[k8s.io] Pods 
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 22:39:41.201: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:187
[It] should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Aug 23 22:39:48.598: INFO: Waiting up to 5m0s for pod "client-envvars-a4a97d43-5fb0-4e7a-9f08-41b0d866877a" in namespace "pods-1730" to be "Succeeded or Failed"
Aug 23 22:39:48.714: INFO: Pod "client-envvars-a4a97d43-5fb0-4e7a-9f08-41b0d866877a": Phase="Pending", Reason="", readiness=false. Elapsed: 115.469084ms
Aug 23 22:39:50.865: INFO: Pod "client-envvars-a4a97d43-5fb0-4e7a-9f08-41b0d866877a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.266848875s
Aug 23 22:39:53.023: INFO: Pod "client-envvars-a4a97d43-5fb0-4e7a-9f08-41b0d866877a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.424891945s
STEP: Saw pod success
Aug 23 22:39:53.023: INFO: Pod "client-envvars-a4a97d43-5fb0-4e7a-9f08-41b0d866877a" satisfied condition "Succeeded or Failed"
Aug 23 22:39:53.146: INFO: Trying to get logs from node 10.149.248.24 pod client-envvars-a4a97d43-5fb0-4e7a-9f08-41b0d866877a container env3cont: <nil>
STEP: delete the pod
Aug 23 22:39:53.416: INFO: Waiting for pod client-envvars-a4a97d43-5fb0-4e7a-9f08-41b0d866877a to disappear
Aug 23 22:39:53.515: INFO: Pod client-envvars-a4a97d43-5fb0-4e7a-9f08-41b0d866877a no longer exists
[AfterEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 22:39:53.515: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-1730" for this suite.

• [SLOW TEST:12.662 seconds]
[k8s.io] Pods
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Pods should contain environment variables for services [NodeConformance] [Conformance]","total":311,"completed":166,"skipped":2784,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl diff 
  should check if kubectl diff finds a difference for Deployments [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 22:39:53.863: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should check if kubectl diff finds a difference for Deployments [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create deployment with httpd image
Aug 23 22:39:54.125: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585719795 --namespace=kubectl-1152 create -f -'
Aug 23 22:39:57.818: INFO: stderr: ""
Aug 23 22:39:57.818: INFO: stdout: "deployment.apps/httpd-deployment created\n"
STEP: verify diff finds difference between live and declared image
Aug 23 22:39:57.818: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585719795 --namespace=kubectl-1152 diff -f -'
Aug 23 22:39:58.543: INFO: rc: 1
Aug 23 22:39:58.543: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585719795 --namespace=kubectl-1152 delete -f -'
Aug 23 22:39:59.374: INFO: stderr: ""
Aug 23 22:39:59.374: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 22:39:59.374: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1152" for this suite.

• [SLOW TEST:6.240 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl diff
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:878
    should check if kubectl diff finds a difference for Deployments [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl diff should check if kubectl diff finds a difference for Deployments [Conformance]","total":311,"completed":167,"skipped":2803,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy through a service and a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] version v1
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 22:40:00.103: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy through a service and a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: starting an echo server on multiple ports
STEP: creating replication controller proxy-service-7zkrj in namespace proxy-1647
I0823 22:40:01.024897      25 runners.go:190] Created replication controller with name: proxy-service-7zkrj, namespace: proxy-1647, replica count: 1
I0823 22:40:02.175223      25 runners.go:190] proxy-service-7zkrj Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0823 22:40:03.175395      25 runners.go:190] proxy-service-7zkrj Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0823 22:40:04.175581      25 runners.go:190] proxy-service-7zkrj Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0823 22:40:05.175788      25 runners.go:190] proxy-service-7zkrj Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0823 22:40:06.175988      25 runners.go:190] proxy-service-7zkrj Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0823 22:40:07.176218      25 runners.go:190] proxy-service-7zkrj Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0823 22:40:08.176450      25 runners.go:190] proxy-service-7zkrj Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Aug 23 22:40:08.210: INFO: setup took 7.501660327s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts
Aug 23 22:40:08.291: INFO: (0) /api/v1/namespaces/proxy-1647/pods/proxy-service-7zkrj-ch8rk:160/proxy/: foo (200; 80.61086ms)
Aug 23 22:40:08.296: INFO: (0) /api/v1/namespaces/proxy-1647/pods/https:proxy-service-7zkrj-ch8rk:460/proxy/: tls baz (200; 85.85348ms)
Aug 23 22:40:08.296: INFO: (0) /api/v1/namespaces/proxy-1647/services/http:proxy-service-7zkrj:portname2/proxy/: bar (200; 86.505941ms)
Aug 23 22:40:08.336: INFO: (0) /api/v1/namespaces/proxy-1647/pods/http:proxy-service-7zkrj-ch8rk:162/proxy/: bar (200; 124.558242ms)
Aug 23 22:40:08.336: INFO: (0) /api/v1/namespaces/proxy-1647/pods/http:proxy-service-7zkrj-ch8rk:160/proxy/: foo (200; 125.332668ms)
Aug 23 22:40:08.344: INFO: (0) /api/v1/namespaces/proxy-1647/services/http:proxy-service-7zkrj:portname1/proxy/: foo (200; 133.651853ms)
Aug 23 22:40:08.344: INFO: (0) /api/v1/namespaces/proxy-1647/services/https:proxy-service-7zkrj:tlsportname1/proxy/: tls baz (200; 133.146634ms)
Aug 23 22:40:08.344: INFO: (0) /api/v1/namespaces/proxy-1647/pods/proxy-service-7zkrj-ch8rk/proxy/: <a href="/api/v1/namespaces/proxy-1647/pods/proxy-service-7zkrj-ch8rk/proxy/rewriteme">test</a> (200; 133.054145ms)
Aug 23 22:40:08.345: INFO: (0) /api/v1/namespaces/proxy-1647/pods/http:proxy-service-7zkrj-ch8rk:1080/proxy/: <a href="/api/v1/namespaces/proxy-1647/pods/http:proxy-service-7zkrj-ch8rk:1080/proxy/rewriteme">... (200; 133.775386ms)
Aug 23 22:40:08.345: INFO: (0) /api/v1/namespaces/proxy-1647/services/proxy-service-7zkrj:portname1/proxy/: foo (200; 133.795816ms)
Aug 23 22:40:08.345: INFO: (0) /api/v1/namespaces/proxy-1647/pods/proxy-service-7zkrj-ch8rk:162/proxy/: bar (200; 133.287251ms)
Aug 23 22:40:08.383: INFO: (0) /api/v1/namespaces/proxy-1647/services/https:proxy-service-7zkrj:tlsportname2/proxy/: tls qux (200; 171.555979ms)
Aug 23 22:40:08.383: INFO: (0) /api/v1/namespaces/proxy-1647/services/proxy-service-7zkrj:portname2/proxy/: bar (200; 172.196275ms)
Aug 23 22:40:08.383: INFO: (0) /api/v1/namespaces/proxy-1647/pods/https:proxy-service-7zkrj-ch8rk:443/proxy/: <a href="/api/v1/namespaces/proxy-1647/pods/https:proxy-service-7zkrj-ch8rk:443/proxy/tlsrewritem... (200; 171.78384ms)
Aug 23 22:40:08.383: INFO: (0) /api/v1/namespaces/proxy-1647/pods/https:proxy-service-7zkrj-ch8rk:462/proxy/: tls qux (200; 172.47628ms)
Aug 23 22:40:08.383: INFO: (0) /api/v1/namespaces/proxy-1647/pods/proxy-service-7zkrj-ch8rk:1080/proxy/: <a href="/api/v1/namespaces/proxy-1647/pods/proxy-service-7zkrj-ch8rk:1080/proxy/rewriteme">test<... (200; 172.176967ms)
Aug 23 22:40:08.437: INFO: (1) /api/v1/namespaces/proxy-1647/pods/http:proxy-service-7zkrj-ch8rk:1080/proxy/: <a href="/api/v1/namespaces/proxy-1647/pods/http:proxy-service-7zkrj-ch8rk:1080/proxy/rewriteme">... (200; 54.085313ms)
Aug 23 22:40:08.445: INFO: (1) /api/v1/namespaces/proxy-1647/pods/proxy-service-7zkrj-ch8rk:160/proxy/: foo (200; 61.61598ms)
Aug 23 22:40:08.445: INFO: (1) /api/v1/namespaces/proxy-1647/services/https:proxy-service-7zkrj:tlsportname2/proxy/: tls qux (200; 61.831397ms)
Aug 23 22:40:08.501: INFO: (1) /api/v1/namespaces/proxy-1647/pods/https:proxy-service-7zkrj-ch8rk:460/proxy/: tls baz (200; 117.095173ms)
Aug 23 22:40:08.501: INFO: (1) /api/v1/namespaces/proxy-1647/pods/proxy-service-7zkrj-ch8rk:162/proxy/: bar (200; 117.249203ms)
Aug 23 22:40:08.501: INFO: (1) /api/v1/namespaces/proxy-1647/pods/proxy-service-7zkrj-ch8rk/proxy/: <a href="/api/v1/namespaces/proxy-1647/pods/proxy-service-7zkrj-ch8rk/proxy/rewriteme">test</a> (200; 117.459499ms)
Aug 23 22:40:08.501: INFO: (1) /api/v1/namespaces/proxy-1647/pods/https:proxy-service-7zkrj-ch8rk:443/proxy/: <a href="/api/v1/namespaces/proxy-1647/pods/https:proxy-service-7zkrj-ch8rk:443/proxy/tlsrewritem... (200; 117.061736ms)
Aug 23 22:40:08.501: INFO: (1) /api/v1/namespaces/proxy-1647/pods/proxy-service-7zkrj-ch8rk:1080/proxy/: <a href="/api/v1/namespaces/proxy-1647/pods/proxy-service-7zkrj-ch8rk:1080/proxy/rewriteme">test<... (200; 117.161941ms)
Aug 23 22:40:08.501: INFO: (1) /api/v1/namespaces/proxy-1647/pods/https:proxy-service-7zkrj-ch8rk:462/proxy/: tls qux (200; 117.351728ms)
Aug 23 22:40:08.501: INFO: (1) /api/v1/namespaces/proxy-1647/pods/http:proxy-service-7zkrj-ch8rk:160/proxy/: foo (200; 117.2879ms)
Aug 23 22:40:08.508: INFO: (1) /api/v1/namespaces/proxy-1647/services/http:proxy-service-7zkrj:portname2/proxy/: bar (200; 124.151754ms)
Aug 23 22:40:08.508: INFO: (1) /api/v1/namespaces/proxy-1647/pods/http:proxy-service-7zkrj-ch8rk:162/proxy/: bar (200; 124.528481ms)
Aug 23 22:40:08.508: INFO: (1) /api/v1/namespaces/proxy-1647/services/proxy-service-7zkrj:portname1/proxy/: foo (200; 124.421043ms)
Aug 23 22:40:08.508: INFO: (1) /api/v1/namespaces/proxy-1647/services/https:proxy-service-7zkrj:tlsportname1/proxy/: tls baz (200; 124.345753ms)
Aug 23 22:40:08.508: INFO: (1) /api/v1/namespaces/proxy-1647/services/proxy-service-7zkrj:portname2/proxy/: bar (200; 124.426312ms)
Aug 23 22:40:08.508: INFO: (1) /api/v1/namespaces/proxy-1647/services/http:proxy-service-7zkrj:portname1/proxy/: foo (200; 124.58899ms)
Aug 23 22:40:08.566: INFO: (2) /api/v1/namespaces/proxy-1647/pods/https:proxy-service-7zkrj-ch8rk:443/proxy/: <a href="/api/v1/namespaces/proxy-1647/pods/https:proxy-service-7zkrj-ch8rk:443/proxy/tlsrewritem... (200; 57.717915ms)
Aug 23 22:40:08.582: INFO: (2) /api/v1/namespaces/proxy-1647/services/http:proxy-service-7zkrj:portname2/proxy/: bar (200; 73.259789ms)
Aug 23 22:40:08.582: INFO: (2) /api/v1/namespaces/proxy-1647/services/https:proxy-service-7zkrj:tlsportname2/proxy/: tls qux (200; 73.322927ms)
Aug 23 22:40:08.622: INFO: (2) /api/v1/namespaces/proxy-1647/pods/proxy-service-7zkrj-ch8rk:162/proxy/: bar (200; 113.338085ms)
Aug 23 22:40:08.631: INFO: (2) /api/v1/namespaces/proxy-1647/services/proxy-service-7zkrj:portname2/proxy/: bar (200; 122.166167ms)
Aug 23 22:40:08.631: INFO: (2) /api/v1/namespaces/proxy-1647/pods/https:proxy-service-7zkrj-ch8rk:460/proxy/: tls baz (200; 122.432916ms)
Aug 23 22:40:08.631: INFO: (2) /api/v1/namespaces/proxy-1647/pods/proxy-service-7zkrj-ch8rk:160/proxy/: foo (200; 122.553574ms)
Aug 23 22:40:08.632: INFO: (2) /api/v1/namespaces/proxy-1647/pods/https:proxy-service-7zkrj-ch8rk:462/proxy/: tls qux (200; 122.617086ms)
Aug 23 22:40:08.632: INFO: (2) /api/v1/namespaces/proxy-1647/pods/http:proxy-service-7zkrj-ch8rk:1080/proxy/: <a href="/api/v1/namespaces/proxy-1647/pods/http:proxy-service-7zkrj-ch8rk:1080/proxy/rewriteme">... (200; 122.619783ms)
Aug 23 22:40:08.632: INFO: (2) /api/v1/namespaces/proxy-1647/pods/proxy-service-7zkrj-ch8rk/proxy/: <a href="/api/v1/namespaces/proxy-1647/pods/proxy-service-7zkrj-ch8rk/proxy/rewriteme">test</a> (200; 123.137124ms)
Aug 23 22:40:08.632: INFO: (2) /api/v1/namespaces/proxy-1647/pods/http:proxy-service-7zkrj-ch8rk:160/proxy/: foo (200; 122.738032ms)
Aug 23 22:40:08.632: INFO: (2) /api/v1/namespaces/proxy-1647/pods/http:proxy-service-7zkrj-ch8rk:162/proxy/: bar (200; 123.093087ms)
Aug 23 22:40:08.632: INFO: (2) /api/v1/namespaces/proxy-1647/pods/proxy-service-7zkrj-ch8rk:1080/proxy/: <a href="/api/v1/namespaces/proxy-1647/pods/proxy-service-7zkrj-ch8rk:1080/proxy/rewriteme">test<... (200; 122.908244ms)
Aug 23 22:40:08.640: INFO: (2) /api/v1/namespaces/proxy-1647/services/http:proxy-service-7zkrj:portname1/proxy/: foo (200; 131.157418ms)
Aug 23 22:40:08.640: INFO: (2) /api/v1/namespaces/proxy-1647/services/https:proxy-service-7zkrj:tlsportname1/proxy/: tls baz (200; 131.146598ms)
Aug 23 22:40:08.640: INFO: (2) /api/v1/namespaces/proxy-1647/services/proxy-service-7zkrj:portname1/proxy/: foo (200; 131.487206ms)
Aug 23 22:40:08.704: INFO: (3) /api/v1/namespaces/proxy-1647/pods/https:proxy-service-7zkrj-ch8rk:462/proxy/: tls qux (200; 63.271381ms)
Aug 23 22:40:08.713: INFO: (3) /api/v1/namespaces/proxy-1647/pods/proxy-service-7zkrj-ch8rk/proxy/: <a href="/api/v1/namespaces/proxy-1647/pods/proxy-service-7zkrj-ch8rk/proxy/rewriteme">test</a> (200; 72.767243ms)
Aug 23 22:40:08.713: INFO: (3) /api/v1/namespaces/proxy-1647/pods/https:proxy-service-7zkrj-ch8rk:443/proxy/: <a href="/api/v1/namespaces/proxy-1647/pods/https:proxy-service-7zkrj-ch8rk:443/proxy/tlsrewritem... (200; 72.364997ms)
Aug 23 22:40:08.763: INFO: (3) /api/v1/namespaces/proxy-1647/pods/proxy-service-7zkrj-ch8rk:1080/proxy/: <a href="/api/v1/namespaces/proxy-1647/pods/proxy-service-7zkrj-ch8rk:1080/proxy/rewriteme">test<... (200; 122.22821ms)
Aug 23 22:40:08.772: INFO: (3) /api/v1/namespaces/proxy-1647/pods/http:proxy-service-7zkrj-ch8rk:1080/proxy/: <a href="/api/v1/namespaces/proxy-1647/pods/http:proxy-service-7zkrj-ch8rk:1080/proxy/rewriteme">... (200; 131.303947ms)
Aug 23 22:40:08.772: INFO: (3) /api/v1/namespaces/proxy-1647/pods/proxy-service-7zkrj-ch8rk:162/proxy/: bar (200; 131.434136ms)
Aug 23 22:40:08.772: INFO: (3) /api/v1/namespaces/proxy-1647/pods/http:proxy-service-7zkrj-ch8rk:160/proxy/: foo (200; 131.574517ms)
Aug 23 22:40:08.772: INFO: (3) /api/v1/namespaces/proxy-1647/pods/proxy-service-7zkrj-ch8rk:160/proxy/: foo (200; 131.398197ms)
Aug 23 22:40:08.772: INFO: (3) /api/v1/namespaces/proxy-1647/pods/http:proxy-service-7zkrj-ch8rk:162/proxy/: bar (200; 131.542111ms)
Aug 23 22:40:08.772: INFO: (3) /api/v1/namespaces/proxy-1647/pods/https:proxy-service-7zkrj-ch8rk:460/proxy/: tls baz (200; 131.504235ms)
Aug 23 22:40:08.772: INFO: (3) /api/v1/namespaces/proxy-1647/services/https:proxy-service-7zkrj:tlsportname1/proxy/: tls baz (200; 131.720972ms)
Aug 23 22:40:08.783: INFO: (3) /api/v1/namespaces/proxy-1647/services/http:proxy-service-7zkrj:portname1/proxy/: foo (200; 141.541162ms)
Aug 23 22:40:08.783: INFO: (3) /api/v1/namespaces/proxy-1647/services/https:proxy-service-7zkrj:tlsportname2/proxy/: tls qux (200; 141.644089ms)
Aug 23 22:40:08.783: INFO: (3) /api/v1/namespaces/proxy-1647/services/http:proxy-service-7zkrj:portname2/proxy/: bar (200; 141.720733ms)
Aug 23 22:40:08.823: INFO: (3) /api/v1/namespaces/proxy-1647/services/proxy-service-7zkrj:portname1/proxy/: foo (200; 182.186561ms)
Aug 23 22:40:08.823: INFO: (3) /api/v1/namespaces/proxy-1647/services/proxy-service-7zkrj:portname2/proxy/: bar (200; 182.197116ms)
Aug 23 22:40:08.931: INFO: (4) /api/v1/namespaces/proxy-1647/services/https:proxy-service-7zkrj:tlsportname1/proxy/: tls baz (200; 106.963376ms)
Aug 23 22:40:08.931: INFO: (4) /api/v1/namespaces/proxy-1647/pods/https:proxy-service-7zkrj-ch8rk:443/proxy/: <a href="/api/v1/namespaces/proxy-1647/pods/https:proxy-service-7zkrj-ch8rk:443/proxy/tlsrewritem... (200; 107.147993ms)
Aug 23 22:40:08.943: INFO: (4) /api/v1/namespaces/proxy-1647/pods/http:proxy-service-7zkrj-ch8rk:1080/proxy/: <a href="/api/v1/namespaces/proxy-1647/pods/http:proxy-service-7zkrj-ch8rk:1080/proxy/rewriteme">... (200; 119.596587ms)
Aug 23 22:40:09.051: INFO: (4) /api/v1/namespaces/proxy-1647/pods/proxy-service-7zkrj-ch8rk:160/proxy/: foo (200; 226.263644ms)
Aug 23 22:40:09.074: INFO: (4) /api/v1/namespaces/proxy-1647/services/http:proxy-service-7zkrj:portname2/proxy/: bar (200; 249.692928ms)
Aug 23 22:40:09.074: INFO: (4) /api/v1/namespaces/proxy-1647/pods/proxy-service-7zkrj-ch8rk:1080/proxy/: <a href="/api/v1/namespaces/proxy-1647/pods/proxy-service-7zkrj-ch8rk:1080/proxy/rewriteme">test<... (200; 250.125762ms)
Aug 23 22:40:09.074: INFO: (4) /api/v1/namespaces/proxy-1647/pods/https:proxy-service-7zkrj-ch8rk:462/proxy/: tls qux (200; 249.983611ms)
Aug 23 22:40:09.074: INFO: (4) /api/v1/namespaces/proxy-1647/pods/proxy-service-7zkrj-ch8rk:162/proxy/: bar (200; 249.901122ms)
Aug 23 22:40:09.074: INFO: (4) /api/v1/namespaces/proxy-1647/services/proxy-service-7zkrj:portname2/proxy/: bar (200; 249.567363ms)
Aug 23 22:40:09.074: INFO: (4) /api/v1/namespaces/proxy-1647/pods/https:proxy-service-7zkrj-ch8rk:460/proxy/: tls baz (200; 249.622943ms)
Aug 23 22:40:09.074: INFO: (4) /api/v1/namespaces/proxy-1647/pods/http:proxy-service-7zkrj-ch8rk:160/proxy/: foo (200; 250.149363ms)
Aug 23 22:40:09.074: INFO: (4) /api/v1/namespaces/proxy-1647/services/https:proxy-service-7zkrj:tlsportname2/proxy/: tls qux (200; 249.876863ms)
Aug 23 22:40:09.074: INFO: (4) /api/v1/namespaces/proxy-1647/pods/http:proxy-service-7zkrj-ch8rk:162/proxy/: bar (200; 249.983349ms)
Aug 23 22:40:09.074: INFO: (4) /api/v1/namespaces/proxy-1647/pods/proxy-service-7zkrj-ch8rk/proxy/: <a href="/api/v1/namespaces/proxy-1647/pods/proxy-service-7zkrj-ch8rk/proxy/rewriteme">test</a> (200; 249.874673ms)
Aug 23 22:40:09.074: INFO: (4) /api/v1/namespaces/proxy-1647/services/http:proxy-service-7zkrj:portname1/proxy/: foo (200; 250.115533ms)
Aug 23 22:40:09.074: INFO: (4) /api/v1/namespaces/proxy-1647/services/proxy-service-7zkrj:portname1/proxy/: foo (200; 249.813224ms)
Aug 23 22:40:09.164: INFO: (5) /api/v1/namespaces/proxy-1647/pods/proxy-service-7zkrj-ch8rk/proxy/: <a href="/api/v1/namespaces/proxy-1647/pods/proxy-service-7zkrj-ch8rk/proxy/rewriteme">test</a> (200; 90.074364ms)
Aug 23 22:40:09.172: INFO: (5) /api/v1/namespaces/proxy-1647/pods/https:proxy-service-7zkrj-ch8rk:443/proxy/: <a href="/api/v1/namespaces/proxy-1647/pods/https:proxy-service-7zkrj-ch8rk:443/proxy/tlsrewritem... (200; 98.191909ms)
Aug 23 22:40:09.172: INFO: (5) /api/v1/namespaces/proxy-1647/pods/http:proxy-service-7zkrj-ch8rk:162/proxy/: bar (200; 98.047301ms)
Aug 23 22:40:09.235: INFO: (5) /api/v1/namespaces/proxy-1647/pods/https:proxy-service-7zkrj-ch8rk:460/proxy/: tls baz (200; 159.650585ms)
Aug 23 22:40:09.235: INFO: (5) /api/v1/namespaces/proxy-1647/pods/http:proxy-service-7zkrj-ch8rk:160/proxy/: foo (200; 160.035934ms)
Aug 23 22:40:09.235: INFO: (5) /api/v1/namespaces/proxy-1647/pods/proxy-service-7zkrj-ch8rk:162/proxy/: bar (200; 159.920392ms)
Aug 23 22:40:09.235: INFO: (5) /api/v1/namespaces/proxy-1647/pods/https:proxy-service-7zkrj-ch8rk:462/proxy/: tls qux (200; 160.447687ms)
Aug 23 22:40:09.235: INFO: (5) /api/v1/namespaces/proxy-1647/pods/proxy-service-7zkrj-ch8rk:1080/proxy/: <a href="/api/v1/namespaces/proxy-1647/pods/proxy-service-7zkrj-ch8rk:1080/proxy/rewriteme">test<... (200; 160.140372ms)
Aug 23 22:40:09.235: INFO: (5) /api/v1/namespaces/proxy-1647/pods/http:proxy-service-7zkrj-ch8rk:1080/proxy/: <a href="/api/v1/namespaces/proxy-1647/pods/http:proxy-service-7zkrj-ch8rk:1080/proxy/rewriteme">... (200; 160.221191ms)
Aug 23 22:40:09.235: INFO: (5) /api/v1/namespaces/proxy-1647/pods/proxy-service-7zkrj-ch8rk:160/proxy/: foo (200; 160.066641ms)
Aug 23 22:40:09.246: INFO: (5) /api/v1/namespaces/proxy-1647/services/proxy-service-7zkrj:portname2/proxy/: bar (200; 171.0864ms)
Aug 23 22:40:09.246: INFO: (5) /api/v1/namespaces/proxy-1647/services/https:proxy-service-7zkrj:tlsportname2/proxy/: tls qux (200; 171.284426ms)
Aug 23 22:40:09.246: INFO: (5) /api/v1/namespaces/proxy-1647/services/http:proxy-service-7zkrj:portname1/proxy/: foo (200; 170.979324ms)
Aug 23 22:40:09.298: INFO: (5) /api/v1/namespaces/proxy-1647/services/proxy-service-7zkrj:portname1/proxy/: foo (200; 222.983577ms)
Aug 23 22:40:09.298: INFO: (5) /api/v1/namespaces/proxy-1647/services/http:proxy-service-7zkrj:portname2/proxy/: bar (200; 223.303007ms)
Aug 23 22:40:09.298: INFO: (5) /api/v1/namespaces/proxy-1647/services/https:proxy-service-7zkrj:tlsportname1/proxy/: tls baz (200; 222.927068ms)
Aug 23 22:40:09.387: INFO: (6) /api/v1/namespaces/proxy-1647/services/proxy-service-7zkrj:portname1/proxy/: foo (200; 89.18158ms)
Aug 23 22:40:09.387: INFO: (6) /api/v1/namespaces/proxy-1647/pods/http:proxy-service-7zkrj-ch8rk:1080/proxy/: <a href="/api/v1/namespaces/proxy-1647/pods/http:proxy-service-7zkrj-ch8rk:1080/proxy/rewriteme">... (200; 89.383687ms)
Aug 23 22:40:09.387: INFO: (6) /api/v1/namespaces/proxy-1647/pods/proxy-service-7zkrj-ch8rk:160/proxy/: foo (200; 89.62481ms)
Aug 23 22:40:09.464: INFO: (6) /api/v1/namespaces/proxy-1647/pods/http:proxy-service-7zkrj-ch8rk:160/proxy/: foo (200; 166.03965ms)
Aug 23 22:40:09.464: INFO: (6) /api/v1/namespaces/proxy-1647/pods/proxy-service-7zkrj-ch8rk/proxy/: <a href="/api/v1/namespaces/proxy-1647/pods/proxy-service-7zkrj-ch8rk/proxy/rewriteme">test</a> (200; 165.752449ms)
Aug 23 22:40:09.464: INFO: (6) /api/v1/namespaces/proxy-1647/pods/http:proxy-service-7zkrj-ch8rk:162/proxy/: bar (200; 165.580013ms)
Aug 23 22:40:09.464: INFO: (6) /api/v1/namespaces/proxy-1647/pods/proxy-service-7zkrj-ch8rk:1080/proxy/: <a href="/api/v1/namespaces/proxy-1647/pods/proxy-service-7zkrj-ch8rk:1080/proxy/rewriteme">test<... (200; 165.892851ms)
Aug 23 22:40:09.464: INFO: (6) /api/v1/namespaces/proxy-1647/pods/https:proxy-service-7zkrj-ch8rk:462/proxy/: tls qux (200; 165.322203ms)
Aug 23 22:40:09.464: INFO: (6) /api/v1/namespaces/proxy-1647/pods/proxy-service-7zkrj-ch8rk:162/proxy/: bar (200; 165.752812ms)
Aug 23 22:40:09.465: INFO: (6) /api/v1/namespaces/proxy-1647/pods/https:proxy-service-7zkrj-ch8rk:460/proxy/: tls baz (200; 165.442209ms)
Aug 23 22:40:09.465: INFO: (6) /api/v1/namespaces/proxy-1647/pods/https:proxy-service-7zkrj-ch8rk:443/proxy/: <a href="/api/v1/namespaces/proxy-1647/pods/https:proxy-service-7zkrj-ch8rk:443/proxy/tlsrewritem... (200; 166.143876ms)
Aug 23 22:40:09.465: INFO: (6) /api/v1/namespaces/proxy-1647/services/https:proxy-service-7zkrj:tlsportname1/proxy/: tls baz (200; 166.08129ms)
Aug 23 22:40:09.465: INFO: (6) /api/v1/namespaces/proxy-1647/services/http:proxy-service-7zkrj:portname1/proxy/: foo (200; 165.350347ms)
Aug 23 22:40:09.535: INFO: (6) /api/v1/namespaces/proxy-1647/services/proxy-service-7zkrj:portname2/proxy/: bar (200; 237.235898ms)
Aug 23 22:40:09.535: INFO: (6) /api/v1/namespaces/proxy-1647/services/https:proxy-service-7zkrj:tlsportname2/proxy/: tls qux (200; 236.485114ms)
Aug 23 22:40:09.535: INFO: (6) /api/v1/namespaces/proxy-1647/services/http:proxy-service-7zkrj:portname2/proxy/: bar (200; 236.598195ms)
Aug 23 22:40:09.605: INFO: (7) /api/v1/namespaces/proxy-1647/pods/http:proxy-service-7zkrj-ch8rk:160/proxy/: foo (200; 69.550256ms)
Aug 23 22:40:09.605: INFO: (7) /api/v1/namespaces/proxy-1647/services/proxy-service-7zkrj:portname1/proxy/: foo (200; 69.968791ms)
Aug 23 22:40:09.606: INFO: (7) /api/v1/namespaces/proxy-1647/pods/proxy-service-7zkrj-ch8rk:1080/proxy/: <a href="/api/v1/namespaces/proxy-1647/pods/proxy-service-7zkrj-ch8rk:1080/proxy/rewriteme">test<... (200; 69.778975ms)
Aug 23 22:40:09.656: INFO: (7) /api/v1/namespaces/proxy-1647/pods/proxy-service-7zkrj-ch8rk:162/proxy/: bar (200; 119.854252ms)
Aug 23 22:40:09.664: INFO: (7) /api/v1/namespaces/proxy-1647/pods/proxy-service-7zkrj-ch8rk/proxy/: <a href="/api/v1/namespaces/proxy-1647/pods/proxy-service-7zkrj-ch8rk/proxy/rewriteme">test</a> (200; 127.495836ms)
Aug 23 22:40:09.664: INFO: (7) /api/v1/namespaces/proxy-1647/pods/http:proxy-service-7zkrj-ch8rk:1080/proxy/: <a href="/api/v1/namespaces/proxy-1647/pods/http:proxy-service-7zkrj-ch8rk:1080/proxy/rewriteme">... (200; 127.395717ms)
Aug 23 22:40:09.664: INFO: (7) /api/v1/namespaces/proxy-1647/pods/http:proxy-service-7zkrj-ch8rk:162/proxy/: bar (200; 127.708609ms)
Aug 23 22:40:09.664: INFO: (7) /api/v1/namespaces/proxy-1647/pods/proxy-service-7zkrj-ch8rk:160/proxy/: foo (200; 127.580569ms)
Aug 23 22:40:09.664: INFO: (7) /api/v1/namespaces/proxy-1647/pods/https:proxy-service-7zkrj-ch8rk:462/proxy/: tls qux (200; 127.656453ms)
Aug 23 22:40:09.664: INFO: (7) /api/v1/namespaces/proxy-1647/services/https:proxy-service-7zkrj:tlsportname2/proxy/: tls qux (200; 127.729364ms)
Aug 23 22:40:09.664: INFO: (7) /api/v1/namespaces/proxy-1647/pods/https:proxy-service-7zkrj-ch8rk:460/proxy/: tls baz (200; 127.619516ms)
Aug 23 22:40:09.664: INFO: (7) /api/v1/namespaces/proxy-1647/pods/https:proxy-service-7zkrj-ch8rk:443/proxy/: <a href="/api/v1/namespaces/proxy-1647/pods/https:proxy-service-7zkrj-ch8rk:443/proxy/tlsrewritem... (200; 127.795089ms)
Aug 23 22:40:09.664: INFO: (7) /api/v1/namespaces/proxy-1647/services/http:proxy-service-7zkrj:portname1/proxy/: foo (200; 127.675828ms)
Aug 23 22:40:09.733: INFO: (7) /api/v1/namespaces/proxy-1647/services/http:proxy-service-7zkrj:portname2/proxy/: bar (200; 197.009016ms)
Aug 23 22:40:09.733: INFO: (7) /api/v1/namespaces/proxy-1647/services/proxy-service-7zkrj:portname2/proxy/: bar (200; 196.79251ms)
Aug 23 22:40:09.733: INFO: (7) /api/v1/namespaces/proxy-1647/services/https:proxy-service-7zkrj:tlsportname1/proxy/: tls baz (200; 197.023682ms)
Aug 23 22:40:09.790: INFO: (8) /api/v1/namespaces/proxy-1647/pods/http:proxy-service-7zkrj-ch8rk:162/proxy/: bar (200; 57.319989ms)
Aug 23 22:40:09.800: INFO: (8) /api/v1/namespaces/proxy-1647/services/https:proxy-service-7zkrj:tlsportname1/proxy/: tls baz (200; 66.739463ms)
Aug 23 22:40:09.800: INFO: (8) /api/v1/namespaces/proxy-1647/services/proxy-service-7zkrj:portname1/proxy/: foo (200; 66.565795ms)
Aug 23 22:40:09.846: INFO: (8) /api/v1/namespaces/proxy-1647/pods/proxy-service-7zkrj-ch8rk/proxy/: <a href="/api/v1/namespaces/proxy-1647/pods/proxy-service-7zkrj-ch8rk/proxy/rewriteme">test</a> (200; 112.309499ms)
Aug 23 22:40:09.846: INFO: (8) /api/v1/namespaces/proxy-1647/pods/http:proxy-service-7zkrj-ch8rk:160/proxy/: foo (200; 112.525472ms)
Aug 23 22:40:09.846: INFO: (8) /api/v1/namespaces/proxy-1647/services/http:proxy-service-7zkrj:portname2/proxy/: bar (200; 112.457819ms)
Aug 23 22:40:09.846: INFO: (8) /api/v1/namespaces/proxy-1647/pods/proxy-service-7zkrj-ch8rk:162/proxy/: bar (200; 112.348129ms)
Aug 23 22:40:09.856: INFO: (8) /api/v1/namespaces/proxy-1647/pods/proxy-service-7zkrj-ch8rk:160/proxy/: foo (200; 122.484066ms)
Aug 23 22:40:09.856: INFO: (8) /api/v1/namespaces/proxy-1647/pods/https:proxy-service-7zkrj-ch8rk:462/proxy/: tls qux (200; 122.550653ms)
Aug 23 22:40:09.856: INFO: (8) /api/v1/namespaces/proxy-1647/pods/https:proxy-service-7zkrj-ch8rk:460/proxy/: tls baz (200; 122.625836ms)
Aug 23 22:40:09.856: INFO: (8) /api/v1/namespaces/proxy-1647/pods/http:proxy-service-7zkrj-ch8rk:1080/proxy/: <a href="/api/v1/namespaces/proxy-1647/pods/http:proxy-service-7zkrj-ch8rk:1080/proxy/rewriteme">... (200; 122.201073ms)
Aug 23 22:40:09.856: INFO: (8) /api/v1/namespaces/proxy-1647/pods/https:proxy-service-7zkrj-ch8rk:443/proxy/: <a href="/api/v1/namespaces/proxy-1647/pods/https:proxy-service-7zkrj-ch8rk:443/proxy/tlsrewritem... (200; 122.505541ms)
Aug 23 22:40:09.856: INFO: (8) /api/v1/namespaces/proxy-1647/pods/proxy-service-7zkrj-ch8rk:1080/proxy/: <a href="/api/v1/namespaces/proxy-1647/pods/proxy-service-7zkrj-ch8rk:1080/proxy/rewriteme">test<... (200; 122.593942ms)
Aug 23 22:40:09.856: INFO: (8) /api/v1/namespaces/proxy-1647/services/http:proxy-service-7zkrj:portname1/proxy/: foo (200; 122.754171ms)
Aug 23 22:40:09.856: INFO: (8) /api/v1/namespaces/proxy-1647/services/proxy-service-7zkrj:portname2/proxy/: bar (200; 122.310275ms)
Aug 23 22:40:09.917: INFO: (8) /api/v1/namespaces/proxy-1647/services/https:proxy-service-7zkrj:tlsportname2/proxy/: tls qux (200; 183.923056ms)
Aug 23 22:40:09.974: INFO: (9) /api/v1/namespaces/proxy-1647/pods/https:proxy-service-7zkrj-ch8rk:462/proxy/: tls qux (200; 56.357975ms)
Aug 23 22:40:09.982: INFO: (9) /api/v1/namespaces/proxy-1647/pods/http:proxy-service-7zkrj-ch8rk:160/proxy/: foo (200; 64.477575ms)
Aug 23 22:40:09.982: INFO: (9) /api/v1/namespaces/proxy-1647/pods/proxy-service-7zkrj-ch8rk:1080/proxy/: <a href="/api/v1/namespaces/proxy-1647/pods/proxy-service-7zkrj-ch8rk:1080/proxy/rewriteme">test<... (200; 64.64644ms)
Aug 23 22:40:10.046: INFO: (9) /api/v1/namespaces/proxy-1647/pods/https:proxy-service-7zkrj-ch8rk:460/proxy/: tls baz (200; 128.664077ms)
Aug 23 22:40:10.047: INFO: (9) /api/v1/namespaces/proxy-1647/pods/proxy-service-7zkrj-ch8rk:162/proxy/: bar (200; 128.911163ms)
Aug 23 22:40:10.055: INFO: (9) /api/v1/namespaces/proxy-1647/services/http:proxy-service-7zkrj:portname1/proxy/: foo (200; 137.318564ms)
Aug 23 22:40:10.055: INFO: (9) /api/v1/namespaces/proxy-1647/pods/http:proxy-service-7zkrj-ch8rk:1080/proxy/: <a href="/api/v1/namespaces/proxy-1647/pods/http:proxy-service-7zkrj-ch8rk:1080/proxy/rewriteme">... (200; 137.238308ms)
Aug 23 22:40:10.055: INFO: (9) /api/v1/namespaces/proxy-1647/pods/proxy-service-7zkrj-ch8rk:160/proxy/: foo (200; 137.416056ms)
Aug 23 22:40:10.055: INFO: (9) /api/v1/namespaces/proxy-1647/pods/https:proxy-service-7zkrj-ch8rk:443/proxy/: <a href="/api/v1/namespaces/proxy-1647/pods/https:proxy-service-7zkrj-ch8rk:443/proxy/tlsrewritem... (200; 137.507733ms)
Aug 23 22:40:10.055: INFO: (9) /api/v1/namespaces/proxy-1647/pods/http:proxy-service-7zkrj-ch8rk:162/proxy/: bar (200; 137.530913ms)
Aug 23 22:40:10.055: INFO: (9) /api/v1/namespaces/proxy-1647/services/proxy-service-7zkrj:portname1/proxy/: foo (200; 137.482873ms)
Aug 23 22:40:10.055: INFO: (9) /api/v1/namespaces/proxy-1647/pods/proxy-service-7zkrj-ch8rk/proxy/: <a href="/api/v1/namespaces/proxy-1647/pods/proxy-service-7zkrj-ch8rk/proxy/rewriteme">test</a> (200; 137.738542ms)
Aug 23 22:40:10.055: INFO: (9) /api/v1/namespaces/proxy-1647/services/http:proxy-service-7zkrj:portname2/proxy/: bar (200; 137.683641ms)
Aug 23 22:40:10.056: INFO: (9) /api/v1/namespaces/proxy-1647/services/https:proxy-service-7zkrj:tlsportname2/proxy/: tls qux (200; 137.708601ms)
Aug 23 22:40:10.056: INFO: (9) /api/v1/namespaces/proxy-1647/services/https:proxy-service-7zkrj:tlsportname1/proxy/: tls baz (200; 137.712114ms)
Aug 23 22:40:10.056: INFO: (9) /api/v1/namespaces/proxy-1647/services/proxy-service-7zkrj:portname2/proxy/: bar (200; 137.840768ms)
Aug 23 22:40:10.119: INFO: (10) /api/v1/namespaces/proxy-1647/services/https:proxy-service-7zkrj:tlsportname1/proxy/: tls baz (200; 63.207948ms)
Aug 23 22:40:10.119: INFO: (10) /api/v1/namespaces/proxy-1647/pods/proxy-service-7zkrj-ch8rk/proxy/: <a href="/api/v1/namespaces/proxy-1647/pods/proxy-service-7zkrj-ch8rk/proxy/rewriteme">test</a> (200; 63.384087ms)
Aug 23 22:40:10.119: INFO: (10) /api/v1/namespaces/proxy-1647/pods/http:proxy-service-7zkrj-ch8rk:1080/proxy/: <a href="/api/v1/namespaces/proxy-1647/pods/http:proxy-service-7zkrj-ch8rk:1080/proxy/rewriteme">... (200; 63.601081ms)
Aug 23 22:40:10.171: INFO: (10) /api/v1/namespaces/proxy-1647/pods/http:proxy-service-7zkrj-ch8rk:162/proxy/: bar (200; 114.950754ms)
Aug 23 22:40:10.171: INFO: (10) /api/v1/namespaces/proxy-1647/pods/proxy-service-7zkrj-ch8rk:1080/proxy/: <a href="/api/v1/namespaces/proxy-1647/pods/proxy-service-7zkrj-ch8rk:1080/proxy/rewriteme">test<... (200; 115.237736ms)
Aug 23 22:40:10.171: INFO: (10) /api/v1/namespaces/proxy-1647/pods/https:proxy-service-7zkrj-ch8rk:462/proxy/: tls qux (200; 115.157831ms)
Aug 23 22:40:10.171: INFO: (10) /api/v1/namespaces/proxy-1647/pods/proxy-service-7zkrj-ch8rk:162/proxy/: bar (200; 115.348573ms)
Aug 23 22:40:10.179: INFO: (10) /api/v1/namespaces/proxy-1647/pods/http:proxy-service-7zkrj-ch8rk:160/proxy/: foo (200; 122.834514ms)
Aug 23 22:40:10.179: INFO: (10) /api/v1/namespaces/proxy-1647/services/https:proxy-service-7zkrj:tlsportname2/proxy/: tls qux (200; 122.858624ms)
Aug 23 22:40:10.179: INFO: (10) /api/v1/namespaces/proxy-1647/pods/proxy-service-7zkrj-ch8rk:160/proxy/: foo (200; 122.894603ms)
Aug 23 22:40:10.179: INFO: (10) /api/v1/namespaces/proxy-1647/pods/https:proxy-service-7zkrj-ch8rk:443/proxy/: <a href="/api/v1/namespaces/proxy-1647/pods/https:proxy-service-7zkrj-ch8rk:443/proxy/tlsrewritem... (200; 123.186615ms)
Aug 23 22:40:10.179: INFO: (10) /api/v1/namespaces/proxy-1647/pods/https:proxy-service-7zkrj-ch8rk:460/proxy/: tls baz (200; 123.235867ms)
Aug 23 22:40:10.180: INFO: (10) /api/v1/namespaces/proxy-1647/services/http:proxy-service-7zkrj:portname2/proxy/: bar (200; 123.43897ms)
Aug 23 22:40:10.221: INFO: (10) /api/v1/namespaces/proxy-1647/services/http:proxy-service-7zkrj:portname1/proxy/: foo (200; 164.584727ms)
Aug 23 22:40:10.221: INFO: (10) /api/v1/namespaces/proxy-1647/services/proxy-service-7zkrj:portname2/proxy/: bar (200; 164.644848ms)
Aug 23 22:40:10.221: INFO: (10) /api/v1/namespaces/proxy-1647/services/proxy-service-7zkrj:portname1/proxy/: foo (200; 164.704195ms)
Aug 23 22:40:10.289: INFO: (11) /api/v1/namespaces/proxy-1647/pods/https:proxy-service-7zkrj-ch8rk:460/proxy/: tls baz (200; 66.038898ms)
Aug 23 22:40:10.296: INFO: (11) /api/v1/namespaces/proxy-1647/services/https:proxy-service-7zkrj:tlsportname2/proxy/: tls qux (200; 73.182173ms)
Aug 23 22:40:10.296: INFO: (11) /api/v1/namespaces/proxy-1647/services/proxy-service-7zkrj:portname2/proxy/: bar (200; 73.182173ms)
Aug 23 22:40:10.337: INFO: (11) /api/v1/namespaces/proxy-1647/pods/https:proxy-service-7zkrj-ch8rk:462/proxy/: tls qux (200; 113.999102ms)
Aug 23 22:40:10.337: INFO: (11) /api/v1/namespaces/proxy-1647/pods/http:proxy-service-7zkrj-ch8rk:162/proxy/: bar (200; 113.914727ms)
Aug 23 22:40:10.337: INFO: (11) /api/v1/namespaces/proxy-1647/pods/proxy-service-7zkrj-ch8rk:1080/proxy/: <a href="/api/v1/namespaces/proxy-1647/pods/proxy-service-7zkrj-ch8rk:1080/proxy/rewriteme">test<... (200; 114.064973ms)
Aug 23 22:40:10.337: INFO: (11) /api/v1/namespaces/proxy-1647/pods/proxy-service-7zkrj-ch8rk:160/proxy/: foo (200; 114.127326ms)
Aug 23 22:40:10.337: INFO: (11) /api/v1/namespaces/proxy-1647/pods/proxy-service-7zkrj-ch8rk/proxy/: <a href="/api/v1/namespaces/proxy-1647/pods/proxy-service-7zkrj-ch8rk/proxy/rewriteme">test</a> (200; 114.004302ms)
Aug 23 22:40:10.337: INFO: (11) /api/v1/namespaces/proxy-1647/pods/http:proxy-service-7zkrj-ch8rk:1080/proxy/: <a href="/api/v1/namespaces/proxy-1647/pods/http:proxy-service-7zkrj-ch8rk:1080/proxy/rewriteme">... (200; 114.155191ms)
Aug 23 22:40:10.337: INFO: (11) /api/v1/namespaces/proxy-1647/pods/proxy-service-7zkrj-ch8rk:162/proxy/: bar (200; 114.104762ms)
Aug 23 22:40:10.337: INFO: (11) /api/v1/namespaces/proxy-1647/pods/https:proxy-service-7zkrj-ch8rk:443/proxy/: <a href="/api/v1/namespaces/proxy-1647/pods/https:proxy-service-7zkrj-ch8rk:443/proxy/tlsrewritem... (200; 114.204759ms)
Aug 23 22:40:10.345: INFO: (11) /api/v1/namespaces/proxy-1647/pods/http:proxy-service-7zkrj-ch8rk:160/proxy/: foo (200; 121.673778ms)
Aug 23 22:40:10.345: INFO: (11) /api/v1/namespaces/proxy-1647/services/http:proxy-service-7zkrj:portname2/proxy/: bar (200; 121.804404ms)
Aug 23 22:40:10.345: INFO: (11) /api/v1/namespaces/proxy-1647/services/https:proxy-service-7zkrj:tlsportname1/proxy/: tls baz (200; 121.8546ms)
Aug 23 22:40:10.345: INFO: (11) /api/v1/namespaces/proxy-1647/services/proxy-service-7zkrj:portname1/proxy/: foo (200; 121.971714ms)
Aug 23 22:40:10.353: INFO: (11) /api/v1/namespaces/proxy-1647/services/http:proxy-service-7zkrj:portname1/proxy/: foo (200; 130.08481ms)
Aug 23 22:40:10.428: INFO: (12) /api/v1/namespaces/proxy-1647/pods/proxy-service-7zkrj-ch8rk:1080/proxy/: <a href="/api/v1/namespaces/proxy-1647/pods/proxy-service-7zkrj-ch8rk:1080/proxy/rewriteme">test<... (200; 74.956046ms)
Aug 23 22:40:10.428: INFO: (12) /api/v1/namespaces/proxy-1647/pods/http:proxy-service-7zkrj-ch8rk:162/proxy/: bar (200; 75.181012ms)
Aug 23 22:40:10.435: INFO: (12) /api/v1/namespaces/proxy-1647/services/http:proxy-service-7zkrj:portname2/proxy/: bar (200; 82.343763ms)
Aug 23 22:40:10.481: INFO: (12) /api/v1/namespaces/proxy-1647/pods/http:proxy-service-7zkrj-ch8rk:1080/proxy/: <a href="/api/v1/namespaces/proxy-1647/pods/http:proxy-service-7zkrj-ch8rk:1080/proxy/rewriteme">... (200; 127.007615ms)
Aug 23 22:40:10.481: INFO: (12) /api/v1/namespaces/proxy-1647/pods/proxy-service-7zkrj-ch8rk/proxy/: <a href="/api/v1/namespaces/proxy-1647/pods/proxy-service-7zkrj-ch8rk/proxy/rewriteme">test</a> (200; 127.6214ms)
Aug 23 22:40:10.481: INFO: (12) /api/v1/namespaces/proxy-1647/pods/https:proxy-service-7zkrj-ch8rk:460/proxy/: tls baz (200; 127.31267ms)
Aug 23 22:40:10.481: INFO: (12) /api/v1/namespaces/proxy-1647/pods/https:proxy-service-7zkrj-ch8rk:462/proxy/: tls qux (200; 127.540074ms)
Aug 23 22:40:10.481: INFO: (12) /api/v1/namespaces/proxy-1647/pods/proxy-service-7zkrj-ch8rk:162/proxy/: bar (200; 127.686289ms)
Aug 23 22:40:10.481: INFO: (12) /api/v1/namespaces/proxy-1647/pods/https:proxy-service-7zkrj-ch8rk:443/proxy/: <a href="/api/v1/namespaces/proxy-1647/pods/https:proxy-service-7zkrj-ch8rk:443/proxy/tlsrewritem... (200; 127.794398ms)
Aug 23 22:40:10.489: INFO: (12) /api/v1/namespaces/proxy-1647/pods/proxy-service-7zkrj-ch8rk:160/proxy/: foo (200; 134.939515ms)
Aug 23 22:40:10.496: INFO: (12) /api/v1/namespaces/proxy-1647/pods/http:proxy-service-7zkrj-ch8rk:160/proxy/: foo (200; 143.005363ms)
Aug 23 22:40:10.496: INFO: (12) /api/v1/namespaces/proxy-1647/services/proxy-service-7zkrj:portname2/proxy/: bar (200; 142.601155ms)
Aug 23 22:40:10.496: INFO: (12) /api/v1/namespaces/proxy-1647/services/https:proxy-service-7zkrj:tlsportname2/proxy/: tls qux (200; 143.092051ms)
Aug 23 22:40:10.496: INFO: (12) /api/v1/namespaces/proxy-1647/services/https:proxy-service-7zkrj:tlsportname1/proxy/: tls baz (200; 142.399456ms)
Aug 23 22:40:10.527: INFO: (12) /api/v1/namespaces/proxy-1647/services/proxy-service-7zkrj:portname1/proxy/: foo (200; 173.183224ms)
Aug 23 22:40:10.527: INFO: (12) /api/v1/namespaces/proxy-1647/services/http:proxy-service-7zkrj:portname1/proxy/: foo (200; 173.356261ms)
Aug 23 22:40:10.575: INFO: (13) /api/v1/namespaces/proxy-1647/pods/https:proxy-service-7zkrj-ch8rk:462/proxy/: tls qux (200; 47.366534ms)
Aug 23 22:40:10.575: INFO: (13) /api/v1/namespaces/proxy-1647/pods/https:proxy-service-7zkrj-ch8rk:443/proxy/: <a href="/api/v1/namespaces/proxy-1647/pods/https:proxy-service-7zkrj-ch8rk:443/proxy/tlsrewritem... (200; 47.340616ms)
Aug 23 22:40:10.575: INFO: (13) /api/v1/namespaces/proxy-1647/services/proxy-service-7zkrj:portname1/proxy/: foo (200; 47.366482ms)
Aug 23 22:40:10.623: INFO: (13) /api/v1/namespaces/proxy-1647/pods/proxy-service-7zkrj-ch8rk:162/proxy/: bar (200; 95.032241ms)
Aug 23 22:40:10.623: INFO: (13) /api/v1/namespaces/proxy-1647/pods/http:proxy-service-7zkrj-ch8rk:162/proxy/: bar (200; 95.217029ms)
Aug 23 22:40:10.623: INFO: (13) /api/v1/namespaces/proxy-1647/pods/http:proxy-service-7zkrj-ch8rk:160/proxy/: foo (200; 95.226443ms)
Aug 23 22:40:10.623: INFO: (13) /api/v1/namespaces/proxy-1647/services/proxy-service-7zkrj:portname2/proxy/: bar (200; 95.308848ms)
Aug 23 22:40:10.623: INFO: (13) /api/v1/namespaces/proxy-1647/pods/proxy-service-7zkrj-ch8rk:160/proxy/: foo (200; 95.21409ms)
Aug 23 22:40:10.623: INFO: (13) /api/v1/namespaces/proxy-1647/pods/https:proxy-service-7zkrj-ch8rk:460/proxy/: tls baz (200; 95.226145ms)
Aug 23 22:40:10.623: INFO: (13) /api/v1/namespaces/proxy-1647/pods/proxy-service-7zkrj-ch8rk/proxy/: <a href="/api/v1/namespaces/proxy-1647/pods/proxy-service-7zkrj-ch8rk/proxy/rewriteme">test</a> (200; 95.42831ms)
Aug 23 22:40:10.623: INFO: (13) /api/v1/namespaces/proxy-1647/pods/http:proxy-service-7zkrj-ch8rk:1080/proxy/: <a href="/api/v1/namespaces/proxy-1647/pods/http:proxy-service-7zkrj-ch8rk:1080/proxy/rewriteme">... (200; 95.200357ms)
Aug 23 22:40:10.623: INFO: (13) /api/v1/namespaces/proxy-1647/pods/proxy-service-7zkrj-ch8rk:1080/proxy/: <a href="/api/v1/namespaces/proxy-1647/pods/proxy-service-7zkrj-ch8rk:1080/proxy/rewriteme">test<... (200; 95.434407ms)
Aug 23 22:40:10.624: INFO: (13) /api/v1/namespaces/proxy-1647/services/https:proxy-service-7zkrj:tlsportname2/proxy/: tls qux (200; 96.970022ms)
Aug 23 22:40:10.624: INFO: (13) /api/v1/namespaces/proxy-1647/services/http:proxy-service-7zkrj:portname1/proxy/: foo (200; 96.936671ms)
Aug 23 22:40:10.624: INFO: (13) /api/v1/namespaces/proxy-1647/services/http:proxy-service-7zkrj:portname2/proxy/: bar (200; 96.98972ms)
Aug 23 22:40:10.625: INFO: (13) /api/v1/namespaces/proxy-1647/services/https:proxy-service-7zkrj:tlsportname1/proxy/: tls baz (200; 97.12447ms)
Aug 23 22:40:10.644: INFO: (14) /api/v1/namespaces/proxy-1647/pods/http:proxy-service-7zkrj-ch8rk:162/proxy/: bar (200; 19.324407ms)
Aug 23 22:40:10.653: INFO: (14) /api/v1/namespaces/proxy-1647/services/proxy-service-7zkrj:portname1/proxy/: foo (200; 27.954011ms)
Aug 23 22:40:10.653: INFO: (14) /api/v1/namespaces/proxy-1647/services/https:proxy-service-7zkrj:tlsportname1/proxy/: tls baz (200; 28.125385ms)
Aug 23 22:40:10.666: INFO: (14) /api/v1/namespaces/proxy-1647/pods/proxy-service-7zkrj-ch8rk:162/proxy/: bar (200; 41.290461ms)
Aug 23 22:40:10.666: INFO: (14) /api/v1/namespaces/proxy-1647/pods/https:proxy-service-7zkrj-ch8rk:443/proxy/: <a href="/api/v1/namespaces/proxy-1647/pods/https:proxy-service-7zkrj-ch8rk:443/proxy/tlsrewritem... (200; 41.445467ms)
Aug 23 22:40:10.666: INFO: (14) /api/v1/namespaces/proxy-1647/pods/proxy-service-7zkrj-ch8rk:160/proxy/: foo (200; 41.34427ms)
Aug 23 22:40:10.666: INFO: (14) /api/v1/namespaces/proxy-1647/services/https:proxy-service-7zkrj:tlsportname2/proxy/: tls qux (200; 41.365869ms)
Aug 23 22:40:10.666: INFO: (14) /api/v1/namespaces/proxy-1647/pods/http:proxy-service-7zkrj-ch8rk:1080/proxy/: <a href="/api/v1/namespaces/proxy-1647/pods/http:proxy-service-7zkrj-ch8rk:1080/proxy/rewriteme">... (200; 41.654863ms)
Aug 23 22:40:10.667: INFO: (14) /api/v1/namespaces/proxy-1647/pods/proxy-service-7zkrj-ch8rk/proxy/: <a href="/api/v1/namespaces/proxy-1647/pods/proxy-service-7zkrj-ch8rk/proxy/rewriteme">test</a> (200; 41.487011ms)
Aug 23 22:40:10.667: INFO: (14) /api/v1/namespaces/proxy-1647/pods/http:proxy-service-7zkrj-ch8rk:160/proxy/: foo (200; 41.693864ms)
Aug 23 22:40:10.667: INFO: (14) /api/v1/namespaces/proxy-1647/pods/https:proxy-service-7zkrj-ch8rk:462/proxy/: tls qux (200; 41.462995ms)
Aug 23 22:40:10.667: INFO: (14) /api/v1/namespaces/proxy-1647/pods/https:proxy-service-7zkrj-ch8rk:460/proxy/: tls baz (200; 41.413067ms)
Aug 23 22:40:10.667: INFO: (14) /api/v1/namespaces/proxy-1647/pods/proxy-service-7zkrj-ch8rk:1080/proxy/: <a href="/api/v1/namespaces/proxy-1647/pods/proxy-service-7zkrj-ch8rk:1080/proxy/rewriteme">test<... (200; 41.701724ms)
Aug 23 22:40:10.681: INFO: (14) /api/v1/namespaces/proxy-1647/services/proxy-service-7zkrj:portname2/proxy/: bar (200; 55.782867ms)
Aug 23 22:40:10.681: INFO: (14) /api/v1/namespaces/proxy-1647/services/http:proxy-service-7zkrj:portname1/proxy/: foo (200; 55.492011ms)
Aug 23 22:40:10.681: INFO: (14) /api/v1/namespaces/proxy-1647/services/http:proxy-service-7zkrj:portname2/proxy/: bar (200; 55.66448ms)
Aug 23 22:40:10.722: INFO: (15) /api/v1/namespaces/proxy-1647/pods/https:proxy-service-7zkrj-ch8rk:462/proxy/: tls qux (200; 40.918773ms)
Aug 23 22:40:10.722: INFO: (15) /api/v1/namespaces/proxy-1647/services/proxy-service-7zkrj:portname1/proxy/: foo (200; 40.775983ms)
Aug 23 22:40:10.722: INFO: (15) /api/v1/namespaces/proxy-1647/services/proxy-service-7zkrj:portname2/proxy/: bar (200; 40.741561ms)
Aug 23 22:40:10.775: INFO: (15) /api/v1/namespaces/proxy-1647/pods/http:proxy-service-7zkrj-ch8rk:160/proxy/: foo (200; 93.702908ms)
Aug 23 22:40:10.783: INFO: (15) /api/v1/namespaces/proxy-1647/pods/http:proxy-service-7zkrj-ch8rk:162/proxy/: bar (200; 101.214881ms)
Aug 23 22:40:10.783: INFO: (15) /api/v1/namespaces/proxy-1647/pods/https:proxy-service-7zkrj-ch8rk:443/proxy/: <a href="/api/v1/namespaces/proxy-1647/pods/https:proxy-service-7zkrj-ch8rk:443/proxy/tlsrewritem... (200; 101.55437ms)
Aug 23 22:40:10.783: INFO: (15) /api/v1/namespaces/proxy-1647/pods/https:proxy-service-7zkrj-ch8rk:460/proxy/: tls baz (200; 101.091657ms)
Aug 23 22:40:10.792: INFO: (15) /api/v1/namespaces/proxy-1647/pods/http:proxy-service-7zkrj-ch8rk:1080/proxy/: <a href="/api/v1/namespaces/proxy-1647/pods/http:proxy-service-7zkrj-ch8rk:1080/proxy/rewriteme">... (200; 110.593394ms)
Aug 23 22:40:10.792: INFO: (15) /api/v1/namespaces/proxy-1647/services/http:proxy-service-7zkrj:portname2/proxy/: bar (200; 110.318711ms)
Aug 23 22:40:10.792: INFO: (15) /api/v1/namespaces/proxy-1647/services/https:proxy-service-7zkrj:tlsportname2/proxy/: tls qux (200; 110.284603ms)
Aug 23 22:40:10.792: INFO: (15) /api/v1/namespaces/proxy-1647/pods/proxy-service-7zkrj-ch8rk:162/proxy/: bar (200; 110.189329ms)
Aug 23 22:40:10.792: INFO: (15) /api/v1/namespaces/proxy-1647/services/https:proxy-service-7zkrj:tlsportname1/proxy/: tls baz (200; 110.608133ms)
Aug 23 22:40:10.792: INFO: (15) /api/v1/namespaces/proxy-1647/services/http:proxy-service-7zkrj:portname1/proxy/: foo (200; 110.070289ms)
Aug 23 22:40:10.792: INFO: (15) /api/v1/namespaces/proxy-1647/pods/proxy-service-7zkrj-ch8rk/proxy/: <a href="/api/v1/namespaces/proxy-1647/pods/proxy-service-7zkrj-ch8rk/proxy/rewriteme">test</a> (200; 110.319337ms)
Aug 23 22:40:10.792: INFO: (15) /api/v1/namespaces/proxy-1647/pods/proxy-service-7zkrj-ch8rk:160/proxy/: foo (200; 110.053885ms)
Aug 23 22:40:10.792: INFO: (15) /api/v1/namespaces/proxy-1647/pods/proxy-service-7zkrj-ch8rk:1080/proxy/: <a href="/api/v1/namespaces/proxy-1647/pods/proxy-service-7zkrj-ch8rk:1080/proxy/rewriteme">test<... (200; 110.636362ms)
Aug 23 22:40:10.855: INFO: (16) /api/v1/namespaces/proxy-1647/pods/proxy-service-7zkrj-ch8rk/proxy/: <a href="/api/v1/namespaces/proxy-1647/pods/proxy-service-7zkrj-ch8rk/proxy/rewriteme">test</a> (200; 62.463632ms)
Aug 23 22:40:10.855: INFO: (16) /api/v1/namespaces/proxy-1647/pods/proxy-service-7zkrj-ch8rk:162/proxy/: bar (200; 62.520361ms)
Aug 23 22:40:10.855: INFO: (16) /api/v1/namespaces/proxy-1647/pods/proxy-service-7zkrj-ch8rk:1080/proxy/: <a href="/api/v1/namespaces/proxy-1647/pods/proxy-service-7zkrj-ch8rk:1080/proxy/rewriteme">test<... (200; 62.803863ms)
Aug 23 22:40:10.913: INFO: (16) /api/v1/namespaces/proxy-1647/pods/https:proxy-service-7zkrj-ch8rk:443/proxy/: <a href="/api/v1/namespaces/proxy-1647/pods/https:proxy-service-7zkrj-ch8rk:443/proxy/tlsrewritem... (200; 120.260311ms)
Aug 23 22:40:10.921: INFO: (16) /api/v1/namespaces/proxy-1647/services/proxy-service-7zkrj:portname1/proxy/: foo (200; 127.920009ms)
Aug 23 22:40:10.928: INFO: (16) /api/v1/namespaces/proxy-1647/pods/https:proxy-service-7zkrj-ch8rk:462/proxy/: tls qux (200; 135.237054ms)
Aug 23 22:40:10.928: INFO: (16) /api/v1/namespaces/proxy-1647/pods/http:proxy-service-7zkrj-ch8rk:1080/proxy/: <a href="/api/v1/namespaces/proxy-1647/pods/http:proxy-service-7zkrj-ch8rk:1080/proxy/rewriteme">... (200; 135.158796ms)
Aug 23 22:40:10.928: INFO: (16) /api/v1/namespaces/proxy-1647/pods/proxy-service-7zkrj-ch8rk:160/proxy/: foo (200; 135.272084ms)
Aug 23 22:40:10.928: INFO: (16) /api/v1/namespaces/proxy-1647/pods/http:proxy-service-7zkrj-ch8rk:162/proxy/: bar (200; 135.769712ms)
Aug 23 22:40:10.928: INFO: (16) /api/v1/namespaces/proxy-1647/pods/https:proxy-service-7zkrj-ch8rk:460/proxy/: tls baz (200; 135.569344ms)
Aug 23 22:40:10.928: INFO: (16) /api/v1/namespaces/proxy-1647/pods/http:proxy-service-7zkrj-ch8rk:160/proxy/: foo (200; 135.228308ms)
Aug 23 22:40:10.975: INFO: (16) /api/v1/namespaces/proxy-1647/services/http:proxy-service-7zkrj:portname1/proxy/: foo (200; 182.36866ms)
Aug 23 22:40:10.975: INFO: (16) /api/v1/namespaces/proxy-1647/services/https:proxy-service-7zkrj:tlsportname2/proxy/: tls qux (200; 182.65335ms)
Aug 23 22:40:10.975: INFO: (16) /api/v1/namespaces/proxy-1647/services/https:proxy-service-7zkrj:tlsportname1/proxy/: tls baz (200; 182.178514ms)
Aug 23 22:40:10.976: INFO: (16) /api/v1/namespaces/proxy-1647/services/http:proxy-service-7zkrj:portname2/proxy/: bar (200; 182.933115ms)
Aug 23 22:40:10.976: INFO: (16) /api/v1/namespaces/proxy-1647/services/proxy-service-7zkrj:portname2/proxy/: bar (200; 182.475559ms)
Aug 23 22:40:11.099: INFO: (17) /api/v1/namespaces/proxy-1647/pods/proxy-service-7zkrj-ch8rk:160/proxy/: foo (200; 123.435572ms)
Aug 23 22:40:11.099: INFO: (17) /api/v1/namespaces/proxy-1647/pods/http:proxy-service-7zkrj-ch8rk:160/proxy/: foo (200; 123.617734ms)
Aug 23 22:40:11.099: INFO: (17) /api/v1/namespaces/proxy-1647/services/http:proxy-service-7zkrj:portname2/proxy/: bar (200; 123.475596ms)
Aug 23 22:40:11.174: INFO: (17) /api/v1/namespaces/proxy-1647/pods/http:proxy-service-7zkrj-ch8rk:1080/proxy/: <a href="/api/v1/namespaces/proxy-1647/pods/http:proxy-service-7zkrj-ch8rk:1080/proxy/rewriteme">... (200; 198.080019ms)
Aug 23 22:40:11.174: INFO: (17) /api/v1/namespaces/proxy-1647/pods/https:proxy-service-7zkrj-ch8rk:443/proxy/: <a href="/api/v1/namespaces/proxy-1647/pods/https:proxy-service-7zkrj-ch8rk:443/proxy/tlsrewritem... (200; 198.131852ms)
Aug 23 22:40:11.174: INFO: (17) /api/v1/namespaces/proxy-1647/pods/https:proxy-service-7zkrj-ch8rk:460/proxy/: tls baz (200; 198.281679ms)
Aug 23 22:40:11.174: INFO: (17) /api/v1/namespaces/proxy-1647/pods/proxy-service-7zkrj-ch8rk:1080/proxy/: <a href="/api/v1/namespaces/proxy-1647/pods/proxy-service-7zkrj-ch8rk:1080/proxy/rewriteme">test<... (200; 198.135528ms)
Aug 23 22:40:11.174: INFO: (17) /api/v1/namespaces/proxy-1647/pods/proxy-service-7zkrj-ch8rk:162/proxy/: bar (200; 198.349926ms)
Aug 23 22:40:11.174: INFO: (17) /api/v1/namespaces/proxy-1647/pods/http:proxy-service-7zkrj-ch8rk:162/proxy/: bar (200; 198.155908ms)
Aug 23 22:40:11.174: INFO: (17) /api/v1/namespaces/proxy-1647/pods/proxy-service-7zkrj-ch8rk/proxy/: <a href="/api/v1/namespaces/proxy-1647/pods/proxy-service-7zkrj-ch8rk/proxy/rewriteme">test</a> (200; 198.253026ms)
Aug 23 22:40:11.174: INFO: (17) /api/v1/namespaces/proxy-1647/pods/https:proxy-service-7zkrj-ch8rk:462/proxy/: tls qux (200; 198.248066ms)
Aug 23 22:40:11.182: INFO: (17) /api/v1/namespaces/proxy-1647/services/proxy-service-7zkrj:portname2/proxy/: bar (200; 206.281221ms)
Aug 23 22:40:11.182: INFO: (17) /api/v1/namespaces/proxy-1647/services/http:proxy-service-7zkrj:portname1/proxy/: foo (200; 206.531463ms)
Aug 23 22:40:11.182: INFO: (17) /api/v1/namespaces/proxy-1647/services/proxy-service-7zkrj:portname1/proxy/: foo (200; 206.374891ms)
Aug 23 22:40:11.182: INFO: (17) /api/v1/namespaces/proxy-1647/services/https:proxy-service-7zkrj:tlsportname2/proxy/: tls qux (200; 206.364795ms)
Aug 23 22:40:11.182: INFO: (17) /api/v1/namespaces/proxy-1647/services/https:proxy-service-7zkrj:tlsportname1/proxy/: tls baz (200; 206.325229ms)
Aug 23 22:40:11.270: INFO: (18) /api/v1/namespaces/proxy-1647/pods/https:proxy-service-7zkrj-ch8rk:443/proxy/: <a href="/api/v1/namespaces/proxy-1647/pods/https:proxy-service-7zkrj-ch8rk:443/proxy/tlsrewritem... (200; 87.486398ms)
Aug 23 22:40:11.270: INFO: (18) /api/v1/namespaces/proxy-1647/pods/http:proxy-service-7zkrj-ch8rk:1080/proxy/: <a href="/api/v1/namespaces/proxy-1647/pods/http:proxy-service-7zkrj-ch8rk:1080/proxy/rewriteme">... (200; 87.921971ms)
Aug 23 22:40:11.279: INFO: (18) /api/v1/namespaces/proxy-1647/services/proxy-service-7zkrj:portname1/proxy/: foo (200; 96.060143ms)
Aug 23 22:40:11.279: INFO: (18) /api/v1/namespaces/proxy-1647/services/https:proxy-service-7zkrj:tlsportname2/proxy/: tls qux (200; 96.359822ms)
Aug 23 22:40:11.319: INFO: (18) /api/v1/namespaces/proxy-1647/pods/https:proxy-service-7zkrj-ch8rk:460/proxy/: tls baz (200; 136.158559ms)
Aug 23 22:40:11.319: INFO: (18) /api/v1/namespaces/proxy-1647/pods/proxy-service-7zkrj-ch8rk:160/proxy/: foo (200; 136.340795ms)
Aug 23 22:40:11.319: INFO: (18) /api/v1/namespaces/proxy-1647/pods/proxy-service-7zkrj-ch8rk/proxy/: <a href="/api/v1/namespaces/proxy-1647/pods/proxy-service-7zkrj-ch8rk/proxy/rewriteme">test</a> (200; 136.235193ms)
Aug 23 22:40:11.319: INFO: (18) /api/v1/namespaces/proxy-1647/pods/https:proxy-service-7zkrj-ch8rk:462/proxy/: tls qux (200; 136.37673ms)
Aug 23 22:40:11.319: INFO: (18) /api/v1/namespaces/proxy-1647/pods/proxy-service-7zkrj-ch8rk:1080/proxy/: <a href="/api/v1/namespaces/proxy-1647/pods/proxy-service-7zkrj-ch8rk:1080/proxy/rewriteme">test<... (200; 136.331921ms)
Aug 23 22:40:11.326: INFO: (18) /api/v1/namespaces/proxy-1647/services/http:proxy-service-7zkrj:portname1/proxy/: foo (200; 143.171134ms)
Aug 23 22:40:11.326: INFO: (18) /api/v1/namespaces/proxy-1647/pods/http:proxy-service-7zkrj-ch8rk:162/proxy/: bar (200; 143.298946ms)
Aug 23 22:40:11.326: INFO: (18) /api/v1/namespaces/proxy-1647/pods/http:proxy-service-7zkrj-ch8rk:160/proxy/: foo (200; 143.464428ms)
Aug 23 22:40:11.326: INFO: (18) /api/v1/namespaces/proxy-1647/pods/proxy-service-7zkrj-ch8rk:162/proxy/: bar (200; 143.524911ms)
Aug 23 22:40:11.326: INFO: (18) /api/v1/namespaces/proxy-1647/services/https:proxy-service-7zkrj:tlsportname1/proxy/: tls baz (200; 143.769045ms)
Aug 23 22:40:11.333: INFO: (18) /api/v1/namespaces/proxy-1647/services/proxy-service-7zkrj:portname2/proxy/: bar (200; 149.699205ms)
Aug 23 22:40:11.333: INFO: (18) /api/v1/namespaces/proxy-1647/services/http:proxy-service-7zkrj:portname2/proxy/: bar (200; 149.868234ms)
Aug 23 22:40:11.399: INFO: (19) /api/v1/namespaces/proxy-1647/pods/http:proxy-service-7zkrj-ch8rk:1080/proxy/: <a href="/api/v1/namespaces/proxy-1647/pods/http:proxy-service-7zkrj-ch8rk:1080/proxy/rewriteme">... (200; 66.055721ms)
Aug 23 22:40:11.399: INFO: (19) /api/v1/namespaces/proxy-1647/pods/proxy-service-7zkrj-ch8rk:162/proxy/: bar (200; 66.413268ms)
Aug 23 22:40:11.399: INFO: (19) /api/v1/namespaces/proxy-1647/services/http:proxy-service-7zkrj:portname1/proxy/: foo (200; 66.265005ms)
Aug 23 22:40:11.399: INFO: (19) /api/v1/namespaces/proxy-1647/pods/proxy-service-7zkrj-ch8rk:160/proxy/: foo (200; 66.308835ms)
Aug 23 22:40:11.472: INFO: (19) /api/v1/namespaces/proxy-1647/pods/https:proxy-service-7zkrj-ch8rk:443/proxy/: <a href="/api/v1/namespaces/proxy-1647/pods/https:proxy-service-7zkrj-ch8rk:443/proxy/tlsrewritem... (200; 138.897882ms)
Aug 23 22:40:11.480: INFO: (19) /api/v1/namespaces/proxy-1647/pods/proxy-service-7zkrj-ch8rk/proxy/: <a href="/api/v1/namespaces/proxy-1647/pods/proxy-service-7zkrj-ch8rk/proxy/rewriteme">test</a> (200; 147.168098ms)
Aug 23 22:40:11.481: INFO: (19) /api/v1/namespaces/proxy-1647/pods/https:proxy-service-7zkrj-ch8rk:462/proxy/: tls qux (200; 147.457272ms)
Aug 23 22:40:11.481: INFO: (19) /api/v1/namespaces/proxy-1647/services/https:proxy-service-7zkrj:tlsportname1/proxy/: tls baz (200; 147.596067ms)
Aug 23 22:40:11.481: INFO: (19) /api/v1/namespaces/proxy-1647/services/proxy-service-7zkrj:portname1/proxy/: foo (200; 147.42201ms)
Aug 23 22:40:11.481: INFO: (19) /api/v1/namespaces/proxy-1647/pods/proxy-service-7zkrj-ch8rk:1080/proxy/: <a href="/api/v1/namespaces/proxy-1647/pods/proxy-service-7zkrj-ch8rk:1080/proxy/rewriteme">test<... (200; 147.481419ms)
Aug 23 22:40:11.481: INFO: (19) /api/v1/namespaces/proxy-1647/services/https:proxy-service-7zkrj:tlsportname2/proxy/: tls qux (200; 147.428162ms)
Aug 23 22:40:11.481: INFO: (19) /api/v1/namespaces/proxy-1647/services/http:proxy-service-7zkrj:portname2/proxy/: bar (200; 147.533402ms)
Aug 23 22:40:11.481: INFO: (19) /api/v1/namespaces/proxy-1647/pods/http:proxy-service-7zkrj-ch8rk:160/proxy/: foo (200; 147.593532ms)
Aug 23 22:40:11.481: INFO: (19) /api/v1/namespaces/proxy-1647/pods/http:proxy-service-7zkrj-ch8rk:162/proxy/: bar (200; 147.556768ms)
Aug 23 22:40:11.481: INFO: (19) /api/v1/namespaces/proxy-1647/pods/https:proxy-service-7zkrj-ch8rk:460/proxy/: tls baz (200; 147.745311ms)
Aug 23 22:40:11.561: INFO: (19) /api/v1/namespaces/proxy-1647/services/proxy-service-7zkrj:portname2/proxy/: bar (200; 227.743298ms)
STEP: deleting ReplicationController proxy-service-7zkrj in namespace proxy-1647, will wait for the garbage collector to delete the pods
Aug 23 22:40:11.831: INFO: Deleting ReplicationController proxy-service-7zkrj took: 94.815098ms
Aug 23 22:40:11.932: INFO: Terminating ReplicationController proxy-service-7zkrj pods took: 100.255829ms
[AfterEach] version v1
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 22:40:26.132: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-1647" for this suite.

• [SLOW TEST:26.361 seconds]
[sig-network] Proxy
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  version v1
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:59
    should proxy through a service and a pod  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Proxy version v1 should proxy through a service and a pod  [Conformance]","total":311,"completed":168,"skipped":2820,"failed":0}
SSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with pruning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 22:40:26.464: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Aug 23 22:40:27.989: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63765355227, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63765355227, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63765355227, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63765355227, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 23 22:40:30.074: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63765355227, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63765355227, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63765355227, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63765355227, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 23 22:40:32.062: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63765355227, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63765355227, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63765355227, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63765355227, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 23 22:40:34.117: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63765355227, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63765355227, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63765355227, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63765355227, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Aug 23 22:40:37.356: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with pruning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Aug 23 22:40:37.484: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-36-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 22:40:39.949: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3769" for this suite.
STEP: Destroying namespace "webhook-3769-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

• [SLOW TEST:15.564 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource with pruning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with pruning [Conformance]","total":311,"completed":169,"skipped":2827,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 22:40:42.028: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify that a failing subpath expansion can be modified during the lifecycle of a container [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating the pod with failed condition
STEP: updating the pod
Aug 23 22:42:43.860: INFO: Successfully updated pod "var-expansion-404fa1ba-1451-4fb8-95f1-06213e201605"
STEP: waiting for pod running
STEP: deleting the pod gracefully
Aug 23 22:42:46.026: INFO: Deleting pod "var-expansion-404fa1ba-1451-4fb8-95f1-06213e201605" in namespace "var-expansion-2314"
Aug 23 22:42:46.279: INFO: Wait up to 5m0s for pod "var-expansion-404fa1ba-1451-4fb8-95f1-06213e201605" to be fully deleted
[AfterEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 22:43:24.682: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-2314" for this suite.

• [SLOW TEST:163.666 seconds]
[k8s.io] Variable Expansion
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Variable Expansion should verify that a failing subpath expansion can be modified during the lifecycle of a container [sig-storage][Slow] [Conformance]","total":311,"completed":170,"skipped":2860,"failed":0}
SSS
------------------------------
[sig-apps] Job 
  should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 22:43:25.695: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: Orphaning one of the Job's Pods
Aug 23 22:43:35.965: INFO: Successfully updated pod "adopt-release-7tvjb"
STEP: Checking that the Job readopts the Pod
Aug 23 22:43:35.965: INFO: Waiting up to 15m0s for pod "adopt-release-7tvjb" in namespace "job-7739" to be "adopted"
Aug 23 22:43:36.124: INFO: Pod "adopt-release-7tvjb": Phase="Running", Reason="", readiness=true. Elapsed: 158.415898ms
Aug 23 22:43:36.124: INFO: Pod "adopt-release-7tvjb" satisfied condition "adopted"
STEP: Removing the labels from the Job's Pod
Aug 23 22:43:37.043: INFO: Successfully updated pod "adopt-release-7tvjb"
STEP: Checking that the Job releases the Pod
Aug 23 22:43:37.043: INFO: Waiting up to 15m0s for pod "adopt-release-7tvjb" in namespace "job-7739" to be "released"
Aug 23 22:43:37.255: INFO: Pod "adopt-release-7tvjb": Phase="Running", Reason="", readiness=true. Elapsed: 211.470005ms
Aug 23 22:43:37.255: INFO: Pod "adopt-release-7tvjb" satisfied condition "released"
[AfterEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 22:43:37.255: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-7739" for this suite.

• [SLOW TEST:12.745 seconds]
[sig-apps] Job
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] Job should adopt matching orphans and release non-matching pods [Conformance]","total":311,"completed":171,"skipped":2863,"failed":0}
SSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 22:43:38.440: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Aug 23 22:43:38.899: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
STEP: Creating rc "condition-test" that asks for more than the allowed pod quota
STEP: Checking rc "condition-test" has the desired failure condition set
STEP: Scaling down rc "condition-test" to satisfy pod quota
Aug 23 22:43:40.737: INFO: Updating replication controller "condition-test"
STEP: Checking rc "condition-test" has no failure condition set
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 22:43:40.887: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-7097" for this suite.
•{"msg":"PASSED [sig-apps] ReplicationController should surface a failure condition on a common issue like exceeded quota [Conformance]","total":311,"completed":172,"skipped":2870,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox Pod with hostAliases 
  should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 22:43:41.556: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[It] should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[AfterEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 22:43:49.583: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-2410" for this suite.

• [SLOW TEST:8.678 seconds]
[k8s.io] Kubelet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  when scheduling a busybox Pod with hostAliases
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:137
    should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox Pod with hostAliases should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":173,"skipped":2896,"failed":0}
S
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop simple daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 22:43:50.238: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:129
[It] should run and stop simple daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Aug 23 22:43:52.463: INFO: Number of nodes with available pods: 0
Aug 23 22:43:52.463: INFO: Node 10.149.248.24 is running more than one daemon pod
Aug 23 22:43:54.031: INFO: Number of nodes with available pods: 0
Aug 23 22:43:54.031: INFO: Node 10.149.248.24 is running more than one daemon pod
Aug 23 22:43:55.015: INFO: Number of nodes with available pods: 0
Aug 23 22:43:55.015: INFO: Node 10.149.248.24 is running more than one daemon pod
Aug 23 22:43:55.659: INFO: Number of nodes with available pods: 0
Aug 23 22:43:55.659: INFO: Node 10.149.248.24 is running more than one daemon pod
Aug 23 22:43:56.713: INFO: Number of nodes with available pods: 1
Aug 23 22:43:56.713: INFO: Node 10.149.248.24 is running more than one daemon pod
Aug 23 22:43:57.678: INFO: Number of nodes with available pods: 3
Aug 23 22:43:57.678: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Stop a daemon pod, check that the daemon pod is revived.
Aug 23 22:43:58.000: INFO: Number of nodes with available pods: 2
Aug 23 22:43:58.000: INFO: Node 10.149.248.9 is running more than one daemon pod
Aug 23 22:43:59.245: INFO: Number of nodes with available pods: 2
Aug 23 22:43:59.245: INFO: Node 10.149.248.9 is running more than one daemon pod
Aug 23 22:44:00.254: INFO: Number of nodes with available pods: 2
Aug 23 22:44:00.254: INFO: Node 10.149.248.9 is running more than one daemon pod
Aug 23 22:44:01.170: INFO: Number of nodes with available pods: 2
Aug 23 22:44:01.170: INFO: Node 10.149.248.9 is running more than one daemon pod
Aug 23 22:44:02.422: INFO: Number of nodes with available pods: 2
Aug 23 22:44:02.422: INFO: Node 10.149.248.9 is running more than one daemon pod
Aug 23 22:44:03.580: INFO: Number of nodes with available pods: 2
Aug 23 22:44:03.580: INFO: Node 10.149.248.9 is running more than one daemon pod
Aug 23 22:44:04.335: INFO: Number of nodes with available pods: 2
Aug 23 22:44:04.335: INFO: Node 10.149.248.9 is running more than one daemon pod
Aug 23 22:44:05.443: INFO: Number of nodes with available pods: 2
Aug 23 22:44:05.443: INFO: Node 10.149.248.9 is running more than one daemon pod
Aug 23 22:44:06.887: INFO: Number of nodes with available pods: 2
Aug 23 22:44:06.887: INFO: Node 10.149.248.9 is running more than one daemon pod
Aug 23 22:44:07.560: INFO: Number of nodes with available pods: 2
Aug 23 22:44:07.560: INFO: Node 10.149.248.9 is running more than one daemon pod
Aug 23 22:44:08.391: INFO: Number of nodes with available pods: 2
Aug 23 22:44:08.391: INFO: Node 10.149.248.9 is running more than one daemon pod
Aug 23 22:44:10.219: INFO: Number of nodes with available pods: 2
Aug 23 22:44:10.219: INFO: Node 10.149.248.9 is running more than one daemon pod
Aug 23 22:44:11.659: INFO: Number of nodes with available pods: 2
Aug 23 22:44:11.659: INFO: Node 10.149.248.9 is running more than one daemon pod
Aug 23 22:44:12.511: INFO: Number of nodes with available pods: 3
Aug 23 22:44:12.512: INFO: Number of running nodes: 3, number of available pods: 3
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:95
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-6977, will wait for the garbage collector to delete the pods
Aug 23 22:44:14.089: INFO: Deleting DaemonSet.extensions daemon-set took: 446.863984ms
Aug 23 22:44:14.190: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.277286ms
Aug 23 22:44:26.742: INFO: Number of nodes with available pods: 0
Aug 23 22:44:26.742: INFO: Number of running nodes: 0, number of available pods: 0
Aug 23 22:44:26.935: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-6977/daemonsets","resourceVersion":"93667"},"items":null}

Aug 23 22:44:27.144: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-6977/pods","resourceVersion":"93668"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 22:44:28.818: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-6977" for this suite.

• [SLOW TEST:39.257 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop simple daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop simple daemon [Conformance]","total":311,"completed":174,"skipped":2897,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 22:44:29.496: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name projected-configmap-test-volume-1ee10a2e-a610-4543-bf59-61f2f9d04cd9
STEP: Creating a pod to test consume configMaps
Aug 23 22:44:30.452: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-f0f67f89-dc3b-4e05-a108-08ebb5b21228" in namespace "projected-359" to be "Succeeded or Failed"
Aug 23 22:44:30.574: INFO: Pod "pod-projected-configmaps-f0f67f89-dc3b-4e05-a108-08ebb5b21228": Phase="Pending", Reason="", readiness=false. Elapsed: 122.161241ms
Aug 23 22:44:32.708: INFO: Pod "pod-projected-configmaps-f0f67f89-dc3b-4e05-a108-08ebb5b21228": Phase="Pending", Reason="", readiness=false. Elapsed: 2.25560699s
Aug 23 22:44:34.854: INFO: Pod "pod-projected-configmaps-f0f67f89-dc3b-4e05-a108-08ebb5b21228": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.401178254s
STEP: Saw pod success
Aug 23 22:44:34.854: INFO: Pod "pod-projected-configmaps-f0f67f89-dc3b-4e05-a108-08ebb5b21228" satisfied condition "Succeeded or Failed"
Aug 23 22:44:34.992: INFO: Trying to get logs from node 10.149.248.24 pod pod-projected-configmaps-f0f67f89-dc3b-4e05-a108-08ebb5b21228 container agnhost-container: <nil>
STEP: delete the pod
Aug 23 22:44:35.265: INFO: Waiting for pod pod-projected-configmaps-f0f67f89-dc3b-4e05-a108-08ebb5b21228 to disappear
Aug 23 22:44:35.439: INFO: Pod pod-projected-configmaps-f0f67f89-dc3b-4e05-a108-08ebb5b21228 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 22:44:35.439: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-359" for this suite.

• [SLOW TEST:6.419 seconds]
[sig-storage] Projected configMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:36
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","total":311,"completed":175,"skipped":2922,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 22:44:35.916: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 22:44:43.661: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-2248" for this suite.

• [SLOW TEST:8.083 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]","total":311,"completed":176,"skipped":2960,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should test the lifecycle of a ReplicationController [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 22:44:43.999: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should test the lifecycle of a ReplicationController [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a ReplicationController
STEP: waiting for RC to be added
STEP: waiting for available Replicas
STEP: patching ReplicationController
STEP: waiting for RC to be modified
STEP: patching ReplicationController status
STEP: waiting for RC to be modified
STEP: waiting for available Replicas
STEP: fetching ReplicationController status
STEP: patching ReplicationController scale
STEP: waiting for RC to be modified
STEP: waiting for ReplicationController's scale to be the max amount
STEP: fetching ReplicationController; ensuring that it's patched
STEP: updating ReplicationController status
STEP: waiting for RC to be modified
STEP: listing all ReplicationControllers
STEP: checking that ReplicationController has expected values
STEP: deleting ReplicationControllers by collection
STEP: waiting for ReplicationController to have a DELETED watchEvent
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 22:44:52.315: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-9398" for this suite.

• [SLOW TEST:8.667 seconds]
[sig-apps] ReplicationController
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should test the lifecycle of a ReplicationController [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should test the lifecycle of a ReplicationController [Conformance]","total":311,"completed":177,"skipped":2987,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with different stored version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 22:44:52.667: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Aug 23 22:44:54.165: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63765355493, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63765355493, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63765355493, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63765355493, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 23 22:44:56.420: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63765355493, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63765355493, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63765355493, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63765355493, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 23 22:44:58.353: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63765355493, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63765355493, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63765355493, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63765355493, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Aug 23 22:45:01.633: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with different stored version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Aug 23 22:45:01.749: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-5101-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource while v1 is storage version
STEP: Patching Custom Resource Definition to set v2 as storage
STEP: Patching the custom resource while v2 is storage version
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 22:45:04.175: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2619" for this suite.
STEP: Destroying namespace "webhook-2619-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

• [SLOW TEST:11.938 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource with different stored version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with different stored version [Conformance]","total":311,"completed":178,"skipped":3012,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 22:45:04.606: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-9719.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-9719.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-9719.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-9719.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-9719.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-9719.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Aug 23 22:45:17.134: INFO: DNS probes using dns-9719/dns-test-93fb158e-1bb7-4960-b9fe-7e22f4209c7f succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 22:45:17.549: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-9719" for this suite.

• [SLOW TEST:13.907 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Hostname [LinuxOnly] [Conformance]","total":311,"completed":179,"skipped":3023,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 22:45:18.514: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-1376
[It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating stateful set ss in namespace statefulset-1376
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-1376
Aug 23 22:45:19.630: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Pending - Ready=false
Aug 23 22:45:29.807: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod
Aug 23 22:45:29.967: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585719795 --namespace=statefulset-1376 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Aug 23 22:45:34.880: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Aug 23 22:45:34.880: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Aug 23 22:45:34.880: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Aug 23 22:45:35.023: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Aug 23 22:45:45.163: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Aug 23 22:45:45.163: INFO: Waiting for statefulset status.replicas updated to 0
Aug 23 22:45:45.794: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999998085s
Aug 23 22:45:46.939: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.85601104s
Aug 23 22:45:48.182: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.708759861s
Aug 23 22:45:49.508: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.468516104s
Aug 23 22:45:50.816: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.142334495s
Aug 23 22:45:51.917: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.834011381s
Aug 23 22:45:53.014: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.732660509s
Aug 23 22:45:54.146: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.634913301s
Aug 23 22:45:55.303: INFO: Verifying statefulset ss doesn't scale past 3 for another 503.950106ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-1376
Aug 23 22:45:56.423: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585719795 --namespace=statefulset-1376 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 23 22:45:58.399: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Aug 23 22:45:58.399: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Aug 23 22:45:58.399: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Aug 23 22:45:58.399: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585719795 --namespace=statefulset-1376 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 23 22:46:00.768: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Aug 23 22:46:00.768: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Aug 23 22:46:00.768: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Aug 23 22:46:00.768: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585719795 --namespace=statefulset-1376 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 23 22:46:02.086: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Aug 23 22:46:02.086: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Aug 23 22:46:02.086: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Aug 23 22:46:02.212: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Aug 23 22:46:02.212: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Aug 23 22:46:02.212: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod
Aug 23 22:46:02.347: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585719795 --namespace=statefulset-1376 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Aug 23 22:46:03.360: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Aug 23 22:46:03.360: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Aug 23 22:46:03.360: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Aug 23 22:46:03.360: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585719795 --namespace=statefulset-1376 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Aug 23 22:46:05.396: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Aug 23 22:46:05.396: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Aug 23 22:46:05.396: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Aug 23 22:46:05.396: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585719795 --namespace=statefulset-1376 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Aug 23 22:46:06.890: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Aug 23 22:46:06.890: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Aug 23 22:46:06.890: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Aug 23 22:46:06.890: INFO: Waiting for statefulset status.replicas updated to 0
Aug 23 22:46:06.994: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 1
Aug 23 22:46:17.051: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Aug 23 22:46:17.051: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Aug 23 22:46:17.051: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Aug 23 22:46:17.124: INFO: POD   NODE           PHASE    GRACE  CONDITIONS
Aug 23 22:46:17.124: INFO: ss-0  10.149.248.24  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-08-23 22:45:19 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-08-23 22:46:04 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-08-23 22:46:04 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-08-23 22:45:19 +0000 UTC  }]
Aug 23 22:46:17.124: INFO: ss-1  10.149.248.9   Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-08-23 22:45:45 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-08-23 22:46:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-08-23 22:46:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-08-23 22:45:45 +0000 UTC  }]
Aug 23 22:46:17.124: INFO: ss-2  10.149.248.25  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-08-23 22:45:45 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-08-23 22:46:07 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-08-23 22:46:07 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-08-23 22:45:45 +0000 UTC  }]
Aug 23 22:46:17.124: INFO: 
Aug 23 22:46:17.124: INFO: StatefulSet ss has not reached scale 0, at 3
Aug 23 22:46:18.175: INFO: POD   NODE           PHASE    GRACE  CONDITIONS
Aug 23 22:46:18.175: INFO: ss-0  10.149.248.24  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-08-23 22:45:19 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-08-23 22:46:04 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-08-23 22:46:04 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-08-23 22:45:19 +0000 UTC  }]
Aug 23 22:46:18.175: INFO: ss-1  10.149.248.9   Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-08-23 22:45:45 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-08-23 22:46:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-08-23 22:46:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-08-23 22:45:45 +0000 UTC  }]
Aug 23 22:46:18.175: INFO: ss-2  10.149.248.25  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-08-23 22:45:45 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-08-23 22:46:07 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-08-23 22:46:07 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-08-23 22:45:45 +0000 UTC  }]
Aug 23 22:46:18.175: INFO: 
Aug 23 22:46:18.175: INFO: StatefulSet ss has not reached scale 0, at 3
Aug 23 22:46:19.334: INFO: POD   NODE           PHASE    GRACE  CONDITIONS
Aug 23 22:46:19.334: INFO: ss-0  10.149.248.24  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-08-23 22:45:19 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-08-23 22:46:04 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-08-23 22:46:04 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-08-23 22:45:19 +0000 UTC  }]
Aug 23 22:46:19.334: INFO: ss-1  10.149.248.9   Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-08-23 22:45:45 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-08-23 22:46:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-08-23 22:46:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-08-23 22:45:45 +0000 UTC  }]
Aug 23 22:46:19.334: INFO: ss-2  10.149.248.25  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-08-23 22:45:45 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-08-23 22:46:07 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-08-23 22:46:07 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-08-23 22:45:45 +0000 UTC  }]
Aug 23 22:46:19.334: INFO: 
Aug 23 22:46:19.334: INFO: StatefulSet ss has not reached scale 0, at 3
Aug 23 22:46:20.575: INFO: POD   NODE           PHASE    GRACE  CONDITIONS
Aug 23 22:46:20.575: INFO: ss-0  10.149.248.24  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-08-23 22:45:19 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-08-23 22:46:04 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-08-23 22:46:04 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-08-23 22:45:19 +0000 UTC  }]
Aug 23 22:46:20.575: INFO: ss-1  10.149.248.9   Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-08-23 22:45:45 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-08-23 22:46:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-08-23 22:46:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-08-23 22:45:45 +0000 UTC  }]
Aug 23 22:46:20.575: INFO: ss-2  10.149.248.25  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-08-23 22:45:45 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-08-23 22:46:07 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-08-23 22:46:07 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-08-23 22:45:45 +0000 UTC  }]
Aug 23 22:46:20.575: INFO: 
Aug 23 22:46:20.575: INFO: StatefulSet ss has not reached scale 0, at 3
Aug 23 22:46:21.832: INFO: POD   NODE           PHASE    GRACE  CONDITIONS
Aug 23 22:46:21.832: INFO: ss-0  10.149.248.24  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-08-23 22:45:19 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-08-23 22:46:04 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-08-23 22:46:04 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-08-23 22:45:19 +0000 UTC  }]
Aug 23 22:46:21.832: INFO: ss-1  10.149.248.9   Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-08-23 22:45:45 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-08-23 22:46:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-08-23 22:46:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-08-23 22:45:45 +0000 UTC  }]
Aug 23 22:46:21.832: INFO: ss-2  10.149.248.25  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-08-23 22:45:45 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-08-23 22:46:07 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-08-23 22:46:07 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-08-23 22:45:45 +0000 UTC  }]
Aug 23 22:46:21.832: INFO: 
Aug 23 22:46:21.832: INFO: StatefulSet ss has not reached scale 0, at 3
Aug 23 22:46:22.909: INFO: POD   NODE           PHASE    GRACE  CONDITIONS
Aug 23 22:46:22.909: INFO: ss-1  10.149.248.9   Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-08-23 22:45:45 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-08-23 22:46:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-08-23 22:46:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-08-23 22:45:45 +0000 UTC  }]
Aug 23 22:46:22.909: INFO: ss-2  10.149.248.25  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-08-23 22:45:45 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-08-23 22:46:07 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-08-23 22:46:07 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-08-23 22:45:45 +0000 UTC  }]
Aug 23 22:46:22.909: INFO: 
Aug 23 22:46:22.909: INFO: StatefulSet ss has not reached scale 0, at 2
Aug 23 22:46:23.977: INFO: POD   NODE           PHASE    GRACE  CONDITIONS
Aug 23 22:46:23.977: INFO: ss-1  10.149.248.9   Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-08-23 22:45:45 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-08-23 22:46:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-08-23 22:46:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-08-23 22:45:45 +0000 UTC  }]
Aug 23 22:46:23.977: INFO: ss-2  10.149.248.25  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-08-23 22:45:45 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-08-23 22:46:07 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-08-23 22:46:07 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-08-23 22:45:45 +0000 UTC  }]
Aug 23 22:46:23.977: INFO: 
Aug 23 22:46:23.977: INFO: StatefulSet ss has not reached scale 0, at 2
Aug 23 22:46:25.151: INFO: POD   NODE           PHASE    GRACE  CONDITIONS
Aug 23 22:46:25.151: INFO: ss-1  10.149.248.9   Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-08-23 22:45:45 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-08-23 22:46:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-08-23 22:46:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-08-23 22:45:45 +0000 UTC  }]
Aug 23 22:46:25.151: INFO: ss-2  10.149.248.25  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-08-23 22:45:45 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-08-23 22:46:07 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-08-23 22:46:07 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-08-23 22:45:45 +0000 UTC  }]
Aug 23 22:46:25.151: INFO: 
Aug 23 22:46:25.151: INFO: StatefulSet ss has not reached scale 0, at 2
Aug 23 22:46:26.228: INFO: POD   NODE           PHASE    GRACE  CONDITIONS
Aug 23 22:46:26.228: INFO: ss-2  10.149.248.25  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-08-23 22:45:45 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-08-23 22:46:07 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-08-23 22:46:07 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-08-23 22:45:45 +0000 UTC  }]
Aug 23 22:46:26.228: INFO: 
Aug 23 22:46:26.228: INFO: StatefulSet ss has not reached scale 0, at 1
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-1376
Aug 23 22:46:27.287: INFO: Scaling statefulset ss to 0
Aug 23 22:46:27.496: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
Aug 23 22:46:27.571: INFO: Deleting all statefulset in ns statefulset-1376
Aug 23 22:46:27.656: INFO: Scaling statefulset ss to 0
Aug 23 22:46:28.066: INFO: Waiting for statefulset status.replicas updated to 0
Aug 23 22:46:28.189: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 22:46:28.610: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-1376" for this suite.

• [SLOW TEST:70.864 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
    Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]","total":311,"completed":180,"skipped":3039,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 22:46:29.378: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:41
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Aug 23 22:46:30.947: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d0c15c57-3148-43d9-a80a-aa3e58fd2797" in namespace "projected-787" to be "Succeeded or Failed"
Aug 23 22:46:31.178: INFO: Pod "downwardapi-volume-d0c15c57-3148-43d9-a80a-aa3e58fd2797": Phase="Pending", Reason="", readiness=false. Elapsed: 231.490725ms
Aug 23 22:46:33.304: INFO: Pod "downwardapi-volume-d0c15c57-3148-43d9-a80a-aa3e58fd2797": Phase="Pending", Reason="", readiness=false. Elapsed: 2.356998345s
Aug 23 22:46:35.451: INFO: Pod "downwardapi-volume-d0c15c57-3148-43d9-a80a-aa3e58fd2797": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.503951445s
STEP: Saw pod success
Aug 23 22:46:35.451: INFO: Pod "downwardapi-volume-d0c15c57-3148-43d9-a80a-aa3e58fd2797" satisfied condition "Succeeded or Failed"
Aug 23 22:46:35.608: INFO: Trying to get logs from node 10.149.248.24 pod downwardapi-volume-d0c15c57-3148-43d9-a80a-aa3e58fd2797 container client-container: <nil>
STEP: delete the pod
Aug 23 22:46:35.991: INFO: Waiting for pod downwardapi-volume-d0c15c57-3148-43d9-a80a-aa3e58fd2797 to disappear
Aug 23 22:46:36.166: INFO: Pod downwardapi-volume-d0c15c57-3148-43d9-a80a-aa3e58fd2797 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 22:46:36.166: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-787" for this suite.

• [SLOW TEST:8.387 seconds]
[sig-storage] Projected downwardAPI
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:35
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":181,"skipped":3060,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny attaching pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 22:46:37.766: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Aug 23 22:46:41.590: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63765355600, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63765355600, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63765355600, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63765355600, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 23 22:46:43.764: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63765355600, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63765355600, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63765355600, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63765355600, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 23 22:46:45.714: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63765355600, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63765355600, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63765355600, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63765355600, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Aug 23 22:46:49.022: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny attaching pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod
STEP: 'kubectl attach' the pod, should be denied by the webhook
Aug 23 22:46:55.734: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585719795 --namespace=webhook-9408 attach --namespace=webhook-9408 to-be-attached-pod -i -c=container1'
Aug 23 22:46:56.284: INFO: rc: 1
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 22:46:56.454: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9408" for this suite.
STEP: Destroying namespace "webhook-9408-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

• [SLOW TEST:21.030 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny attaching pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny attaching pod [Conformance]","total":311,"completed":182,"skipped":3066,"failed":0}
S
------------------------------
[sig-apps] ReplicationController 
  should release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 22:46:58.795: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Given a ReplicationController is created
STEP: When the matched label of one of its pods change
Aug 23 22:46:59.925: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 22:47:00.327: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-437" for this suite.
•{"msg":"PASSED [sig-apps] ReplicationController should release no longer matching pods [Conformance]","total":311,"completed":183,"skipped":3067,"failed":0}

------------------------------
[k8s.io] Pods 
  should run through the lifecycle of Pods and PodStatus [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 22:47:00.803: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:187
[It] should run through the lifecycle of Pods and PodStatus [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a Pod with a static label
STEP: watching for Pod to be ready
Aug 23 22:47:01.783: INFO: observed Pod pod-test in namespace pods-7989 in phase Pending conditions []
Aug 23 22:47:01.783: INFO: observed Pod pod-test in namespace pods-7989 in phase Pending conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-08-23 22:47:01 +0000 UTC  }]
Aug 23 22:47:01.783: INFO: observed Pod pod-test in namespace pods-7989 in phase Pending conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-08-23 22:47:01 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-08-23 22:47:01 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-08-23 22:47:01 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-08-23 22:47:01 +0000 UTC  }]
Aug 23 22:47:04.924: INFO: observed Pod pod-test in namespace pods-7989 in phase Pending conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-08-23 22:47:01 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-08-23 22:47:01 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-08-23 22:47:01 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-08-23 22:47:01 +0000 UTC  }]
Aug 23 22:47:05.198: INFO: observed Pod pod-test in namespace pods-7989 in phase Pending conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-08-23 22:47:01 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-08-23 22:47:01 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-08-23 22:47:01 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-08-23 22:47:01 +0000 UTC  }]
STEP: patching the Pod with a new Label and updated data
Aug 23 22:47:06.639: INFO: observed event type ADDED
STEP: getting the Pod and ensuring that it's patched
STEP: getting the PodStatus
STEP: replacing the Pod's status Ready condition to False
STEP: check the Pod again to ensure its Ready conditions are False
STEP: deleting the Pod via a Collection with a LabelSelector
STEP: watching for the Pod to be deleted
Aug 23 22:47:07.201: INFO: observed event type ADDED
Aug 23 22:47:07.201: INFO: observed event type MODIFIED
Aug 23 22:47:07.201: INFO: observed event type MODIFIED
Aug 23 22:47:07.201: INFO: observed event type MODIFIED
Aug 23 22:47:07.201: INFO: observed event type MODIFIED
Aug 23 22:47:07.274: INFO: observed event type MODIFIED
Aug 23 22:47:07.275: INFO: observed event type MODIFIED
Aug 23 22:47:07.275: INFO: observed event type MODIFIED
Aug 23 22:47:07.275: INFO: observed event type MODIFIED
[AfterEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 22:47:07.275: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-7989" for this suite.

• [SLOW TEST:6.920 seconds]
[k8s.io] Pods
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should run through the lifecycle of Pods and PodStatus [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Pods should run through the lifecycle of Pods and PodStatus [Conformance]","total":311,"completed":184,"skipped":3067,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for the cluster  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 22:47:07.723: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for the cluster  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-1332.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-1332.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Aug 23 22:47:14.829: INFO: DNS probes using dns-1332/dns-test-dbb2f33f-a7bf-4394-9a26-63cdc06cc0d2 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 22:47:14.889: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-1332" for this suite.

• [SLOW TEST:7.254 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for the cluster  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for the cluster  [Conformance]","total":311,"completed":185,"skipped":3091,"failed":0}
SSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 22:47:14.978: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:53
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating pod busybox-60741e69-918a-49a2-8976-71b063c24785 in namespace container-probe-3113
Aug 23 22:47:25.405: INFO: Started pod busybox-60741e69-918a-49a2-8976-71b063c24785 in namespace container-probe-3113
STEP: checking the pod's current state and verifying that restartCount is present
Aug 23 22:47:25.434: INFO: Initial restart count of pod busybox-60741e69-918a-49a2-8976-71b063c24785 is 0
Aug 23 22:48:10.007: INFO: Restart count of pod container-probe-3113/busybox-60741e69-918a-49a2-8976-71b063c24785 is now 1 (44.572788504s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 22:48:10.153: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-3113" for this suite.

• [SLOW TEST:55.824 seconds]
[k8s.io] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Probing container should be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","total":311,"completed":186,"skipped":3101,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 22:48:10.804: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name configmap-test-volume-map-4e1cf53c-af43-445a-ba14-cdf2c5ee4d38
STEP: Creating a pod to test consume configMaps
Aug 23 22:48:11.754: INFO: Waiting up to 5m0s for pod "pod-configmaps-31ff648b-515f-4b08-a53d-a0be2c2155ec" in namespace "configmap-6862" to be "Succeeded or Failed"
Aug 23 22:48:11.884: INFO: Pod "pod-configmaps-31ff648b-515f-4b08-a53d-a0be2c2155ec": Phase="Pending", Reason="", readiness=false. Elapsed: 130.266521ms
Aug 23 22:48:13.975: INFO: Pod "pod-configmaps-31ff648b-515f-4b08-a53d-a0be2c2155ec": Phase="Pending", Reason="", readiness=false. Elapsed: 2.221125956s
Aug 23 22:48:16.073: INFO: Pod "pod-configmaps-31ff648b-515f-4b08-a53d-a0be2c2155ec": Phase="Pending", Reason="", readiness=false. Elapsed: 4.318741615s
Aug 23 22:48:18.229: INFO: Pod "pod-configmaps-31ff648b-515f-4b08-a53d-a0be2c2155ec": Phase="Pending", Reason="", readiness=false. Elapsed: 6.475198086s
Aug 23 22:48:20.427: INFO: Pod "pod-configmaps-31ff648b-515f-4b08-a53d-a0be2c2155ec": Phase="Pending", Reason="", readiness=false. Elapsed: 8.672745493s
Aug 23 22:48:22.626: INFO: Pod "pod-configmaps-31ff648b-515f-4b08-a53d-a0be2c2155ec": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.871547828s
STEP: Saw pod success
Aug 23 22:48:22.626: INFO: Pod "pod-configmaps-31ff648b-515f-4b08-a53d-a0be2c2155ec" satisfied condition "Succeeded or Failed"
Aug 23 22:48:22.824: INFO: Trying to get logs from node 10.149.248.24 pod pod-configmaps-31ff648b-515f-4b08-a53d-a0be2c2155ec container agnhost-container: <nil>
STEP: delete the pod
Aug 23 22:48:23.261: INFO: Waiting for pod pod-configmaps-31ff648b-515f-4b08-a53d-a0be2c2155ec to disappear
Aug 23 22:48:23.447: INFO: Pod pod-configmaps-31ff648b-515f-4b08-a53d-a0be2c2155ec no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 22:48:23.447: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6862" for this suite.

• [SLOW TEST:13.230 seconds]
[sig-storage] ConfigMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:36
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","total":311,"completed":187,"skipped":3110,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to create a functioning NodePort service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 22:48:24.036: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should be able to create a functioning NodePort service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating service nodeport-test with type=NodePort in namespace services-9349
STEP: creating replication controller nodeport-test in namespace services-9349
I0823 22:48:24.669330      25 runners.go:190] Created replication controller with name: nodeport-test, namespace: services-9349, replica count: 2
I0823 22:48:27.769692      25 runners.go:190] nodeport-test Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Aug 23 22:48:30.769: INFO: Creating new exec pod
I0823 22:48:30.769880      25 runners.go:190] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Aug 23 22:48:40.310: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585719795 --namespace=services-9349 exec execpodc9j4q -- /bin/sh -x -c nc -zv -t -w 2 nodeport-test 80'
Aug 23 22:48:43.280: INFO: stderr: "+ nc -zv -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Aug 23 22:48:43.280: INFO: stdout: ""
Aug 23 22:48:43.280: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585719795 --namespace=services-9349 exec execpodc9j4q -- /bin/sh -x -c nc -zv -t -w 2 172.21.178.95 80'
Aug 23 22:48:44.475: INFO: stderr: "+ nc -zv -t -w 2 172.21.178.95 80\nConnection to 172.21.178.95 80 port [tcp/http] succeeded!\n"
Aug 23 22:48:44.475: INFO: stdout: ""
Aug 23 22:48:44.475: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585719795 --namespace=services-9349 exec execpodc9j4q -- /bin/sh -x -c nc -zv -t -w 2 10.149.248.9 32565'
Aug 23 22:48:45.961: INFO: stderr: "+ nc -zv -t -w 2 10.149.248.9 32565\nConnection to 10.149.248.9 32565 port [tcp/32565] succeeded!\n"
Aug 23 22:48:45.961: INFO: stdout: ""
Aug 23 22:48:45.961: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585719795 --namespace=services-9349 exec execpodc9j4q -- /bin/sh -x -c nc -zv -t -w 2 10.149.248.24 32565'
Aug 23 22:48:48.377: INFO: stderr: "+ nc -zv -t -w 2 10.149.248.24 32565\nConnection to 10.149.248.24 32565 port [tcp/32565] succeeded!\n"
Aug 23 22:48:48.377: INFO: stdout: ""
Aug 23 22:48:48.377: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585719795 --namespace=services-9349 exec execpodc9j4q -- /bin/sh -x -c nc -zv -t -w 2 169.45.209.221 32565'
Aug 23 22:48:49.386: INFO: stderr: "+ nc -zv -t -w 2 169.45.209.221 32565\nConnection to 169.45.209.221 32565 port [tcp/32565] succeeded!\n"
Aug 23 22:48:49.386: INFO: stdout: ""
Aug 23 22:48:49.386: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585719795 --namespace=services-9349 exec execpodc9j4q -- /bin/sh -x -c nc -zv -t -w 2 169.45.209.222 32565'
Aug 23 22:48:51.012: INFO: stderr: "+ nc -zv -t -w 2 169.45.209.222 32565\nConnection to 169.45.209.222 32565 port [tcp/32565] succeeded!\n"
Aug 23 22:48:51.012: INFO: stdout: ""
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 22:48:51.012: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-9349" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749

• [SLOW TEST:27.866 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to create a functioning NodePort service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Services should be able to create a functioning NodePort service [Conformance]","total":311,"completed":188,"skipped":3121,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 22:48:51.902: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a ResourceQuota with best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a best-effort pod
STEP: Ensuring resource quota with best effort scope captures the pod usage
STEP: Ensuring resource quota with not best effort ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a not best-effort pod
STEP: Ensuring resource quota with not best effort scope captures the pod usage
STEP: Ensuring resource quota with best effort scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 22:49:12.621: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-2241" for this suite.

• [SLOW TEST:21.272 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with best effort scope. [Conformance]","total":311,"completed":189,"skipped":3153,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 22:49:13.174: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name projected-configmap-test-volume-map-14793ddb-20e5-44e9-8286-0266fcbe0667
STEP: Creating a pod to test consume configMaps
Aug 23 22:49:14.346: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-e07e290c-ef41-4ed3-9617-a1cfef349e56" in namespace "projected-2134" to be "Succeeded or Failed"
Aug 23 22:49:14.541: INFO: Pod "pod-projected-configmaps-e07e290c-ef41-4ed3-9617-a1cfef349e56": Phase="Pending", Reason="", readiness=false. Elapsed: 194.612543ms
Aug 23 22:49:17.808: INFO: Pod "pod-projected-configmaps-e07e290c-ef41-4ed3-9617-a1cfef349e56": Phase="Pending", Reason="", readiness=false. Elapsed: 3.462071904s
Aug 23 22:49:19.918: INFO: Pod "pod-projected-configmaps-e07e290c-ef41-4ed3-9617-a1cfef349e56": Phase="Pending", Reason="", readiness=false. Elapsed: 5.57218508s
Aug 23 22:49:22.020: INFO: Pod "pod-projected-configmaps-e07e290c-ef41-4ed3-9617-a1cfef349e56": Phase="Succeeded", Reason="", readiness=false. Elapsed: 7.674038938s
STEP: Saw pod success
Aug 23 22:49:22.021: INFO: Pod "pod-projected-configmaps-e07e290c-ef41-4ed3-9617-a1cfef349e56" satisfied condition "Succeeded or Failed"
Aug 23 22:49:22.135: INFO: Trying to get logs from node 10.149.248.24 pod pod-projected-configmaps-e07e290c-ef41-4ed3-9617-a1cfef349e56 container agnhost-container: <nil>
STEP: delete the pod
Aug 23 22:49:22.872: INFO: Waiting for pod pod-projected-configmaps-e07e290c-ef41-4ed3-9617-a1cfef349e56 to disappear
Aug 23 22:49:23.018: INFO: Pod pod-projected-configmaps-e07e290c-ef41-4ed3-9617-a1cfef349e56 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 22:49:23.018: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2134" for this suite.

• [SLOW TEST:10.584 seconds]
[sig-storage] Projected configMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:36
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":190,"skipped":3167,"failed":0}
SSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 22:49:23.758: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating pod pod-subpath-test-projected-2qrm
STEP: Creating a pod to test atomic-volume-subpath
Aug 23 22:49:25.562: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-2qrm" in namespace "subpath-6441" to be "Succeeded or Failed"
Aug 23 22:49:25.724: INFO: Pod "pod-subpath-test-projected-2qrm": Phase="Pending", Reason="", readiness=false. Elapsed: 161.586564ms
Aug 23 22:49:27.869: INFO: Pod "pod-subpath-test-projected-2qrm": Phase="Pending", Reason="", readiness=false. Elapsed: 2.307000757s
Aug 23 22:49:30.001: INFO: Pod "pod-subpath-test-projected-2qrm": Phase="Running", Reason="", readiness=true. Elapsed: 4.43811031s
Aug 23 22:49:32.126: INFO: Pod "pod-subpath-test-projected-2qrm": Phase="Running", Reason="", readiness=true. Elapsed: 6.563661664s
Aug 23 22:49:34.261: INFO: Pod "pod-subpath-test-projected-2qrm": Phase="Running", Reason="", readiness=true. Elapsed: 8.698947193s
Aug 23 22:49:36.364: INFO: Pod "pod-subpath-test-projected-2qrm": Phase="Running", Reason="", readiness=true. Elapsed: 10.801674928s
Aug 23 22:49:38.467: INFO: Pod "pod-subpath-test-projected-2qrm": Phase="Running", Reason="", readiness=true. Elapsed: 12.904439447s
Aug 23 22:49:40.605: INFO: Pod "pod-subpath-test-projected-2qrm": Phase="Running", Reason="", readiness=true. Elapsed: 15.04228616s
Aug 23 22:49:42.710: INFO: Pod "pod-subpath-test-projected-2qrm": Phase="Running", Reason="", readiness=true. Elapsed: 17.147636837s
Aug 23 22:49:44.777: INFO: Pod "pod-subpath-test-projected-2qrm": Phase="Running", Reason="", readiness=true. Elapsed: 19.214781922s
Aug 23 22:49:46.880: INFO: Pod "pod-subpath-test-projected-2qrm": Phase="Running", Reason="", readiness=true. Elapsed: 21.317202768s
Aug 23 22:49:49.107: INFO: Pod "pod-subpath-test-projected-2qrm": Phase="Running", Reason="", readiness=true. Elapsed: 23.544530809s
Aug 23 22:49:51.133: INFO: Pod "pod-subpath-test-projected-2qrm": Phase="Succeeded", Reason="", readiness=false. Elapsed: 25.570841897s
STEP: Saw pod success
Aug 23 22:49:51.133: INFO: Pod "pod-subpath-test-projected-2qrm" satisfied condition "Succeeded or Failed"
Aug 23 22:49:51.164: INFO: Trying to get logs from node 10.149.248.24 pod pod-subpath-test-projected-2qrm container test-container-subpath-projected-2qrm: <nil>
STEP: delete the pod
Aug 23 22:49:51.308: INFO: Waiting for pod pod-subpath-test-projected-2qrm to disappear
Aug 23 22:49:51.339: INFO: Pod pod-subpath-test-projected-2qrm no longer exists
STEP: Deleting pod pod-subpath-test-projected-2qrm
Aug 23 22:49:51.340: INFO: Deleting pod "pod-subpath-test-projected-2qrm" in namespace "subpath-6441"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 22:49:51.378: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-6441" for this suite.

• [SLOW TEST:27.825 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with projected pod [LinuxOnly] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with projected pod [LinuxOnly] [Conformance]","total":311,"completed":191,"skipped":3171,"failed":0}
SSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: http [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 22:49:51.583: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: http [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Performing setup for networking test in namespace pod-network-test-851
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Aug 23 22:49:51.830: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Aug 23 22:49:52.168: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Aug 23 22:49:54.219: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Aug 23 22:49:56.217: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Aug 23 22:49:58.200: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Aug 23 22:50:00.203: INFO: The status of Pod netserver-0 is Running (Ready = false)
Aug 23 22:50:02.240: INFO: The status of Pod netserver-0 is Running (Ready = false)
Aug 23 22:50:04.283: INFO: The status of Pod netserver-0 is Running (Ready = false)
Aug 23 22:50:06.367: INFO: The status of Pod netserver-0 is Running (Ready = false)
Aug 23 22:50:08.359: INFO: The status of Pod netserver-0 is Running (Ready = false)
Aug 23 22:50:10.428: INFO: The status of Pod netserver-0 is Running (Ready = false)
Aug 23 22:50:13.509: INFO: The status of Pod netserver-0 is Running (Ready = false)
Aug 23 22:50:14.372: INFO: The status of Pod netserver-0 is Running (Ready = false)
Aug 23 22:50:16.414: INFO: The status of Pod netserver-0 is Running (Ready = false)
Aug 23 22:50:18.362: INFO: The status of Pod netserver-0 is Running (Ready = true)
Aug 23 22:50:18.738: INFO: The status of Pod netserver-1 is Running (Ready = true)
Aug 23 22:50:19.122: INFO: The status of Pod netserver-2 is Running (Ready = true)
STEP: Creating test pods
Aug 23 22:50:28.296: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
Aug 23 22:50:28.296: INFO: Breadth first check of 172.30.87.65 on host 10.149.248.24...
Aug 23 22:50:28.529: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.87.92:9080/dial?request=hostname&protocol=http&host=172.30.87.65&port=8080&tries=1'] Namespace:pod-network-test-851 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 23 22:50:28.529: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
Aug 23 22:50:29.550: INFO: Waiting for responses: map[]
Aug 23 22:50:29.551: INFO: reached 172.30.87.65 after 0/1 tries
Aug 23 22:50:29.551: INFO: Breadth first check of 172.30.240.115 on host 10.149.248.25...
Aug 23 22:50:29.886: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.87.92:9080/dial?request=hostname&protocol=http&host=172.30.240.115&port=8080&tries=1'] Namespace:pod-network-test-851 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 23 22:50:29.886: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
Aug 23 22:50:30.855: INFO: Waiting for responses: map[]
Aug 23 22:50:30.855: INFO: reached 172.30.240.115 after 0/1 tries
Aug 23 22:50:30.855: INFO: Breadth first check of 172.30.210.172 on host 10.149.248.9...
Aug 23 22:50:31.102: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.87.92:9080/dial?request=hostname&protocol=http&host=172.30.210.172&port=8080&tries=1'] Namespace:pod-network-test-851 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 23 22:50:31.102: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
Aug 23 22:50:31.358: INFO: Waiting for responses: map[]
Aug 23 22:50:31.358: INFO: reached 172.30.210.172 after 0/1 tries
Aug 23 22:50:31.358: INFO: Going to retry 0 out of 3 pods....
[AfterEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 22:50:31.358: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-851" for this suite.

• [SLOW TEST:41.197 seconds]
[sig-network] Networking
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:27
  Granular Checks: Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:30
    should function for intra-pod communication: http [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: http [NodeConformance] [Conformance]","total":311,"completed":192,"skipped":3180,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  creating/deleting custom resource definition objects works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 22:50:32.781: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] creating/deleting custom resource definition objects works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Aug 23 22:50:33.954: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 22:50:34.986: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-7756" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition creating/deleting custom resource definition objects works  [Conformance]","total":311,"completed":193,"skipped":3205,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] 
  custom resource defaulting for requests and from storage works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 22:50:36.225: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] custom resource defaulting for requests and from storage works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Aug 23 22:50:37.602: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 22:50:39.879: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-5393" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] custom resource defaulting for requests and from storage works  [Conformance]","total":311,"completed":194,"skipped":3230,"failed":0}
S
------------------------------
[k8s.io] Docker Containers 
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 22:50:40.719: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[AfterEach] [k8s.io] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 22:50:50.078: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-1244" for this suite.

• [SLOW TEST:9.501 seconds]
[k8s.io] Docker Containers
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Docker Containers should use the image defaults if command and args are blank [NodeConformance] [Conformance]","total":311,"completed":195,"skipped":3231,"failed":0}
SSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 22:50:50.220: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:162
[It] should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating the pod
Aug 23 22:50:50.444: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 22:50:57.107: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-950" for this suite.

• [SLOW TEST:6.960 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should invoke init containers on a RestartAlways pod [Conformance]","total":311,"completed":196,"skipped":3235,"failed":0}
SSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 22:50:57.180: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:41
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating the pod
Aug 23 22:51:04.441: INFO: Successfully updated pod "annotationupdate6b60be84-0af3-471c-9268-acfe7ad7c2c8"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 22:51:06.657: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1665" for this suite.

• [SLOW TEST:10.218 seconds]
[sig-storage] Projected downwardAPI
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:35
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should update annotations on modification [NodeConformance] [Conformance]","total":311,"completed":197,"skipped":3240,"failed":0}
SSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 22:51:07.398: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:53
[It] should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating pod liveness-9c802fe2-f290-4ab1-bdea-782ddeaee32a in namespace container-probe-9699
Aug 23 22:51:14.336: INFO: Started pod liveness-9c802fe2-f290-4ab1-bdea-782ddeaee32a in namespace container-probe-9699
STEP: checking the pod's current state and verifying that restartCount is present
Aug 23 22:51:14.453: INFO: Initial restart count of pod liveness-9c802fe2-f290-4ab1-bdea-782ddeaee32a is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 22:55:15.681: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-9699" for this suite.

• [SLOW TEST:249.775 seconds]
[k8s.io] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Probing container should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]","total":311,"completed":198,"skipped":3248,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should find a service from listing all namespaces [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 22:55:17.173: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should find a service from listing all namespaces [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: fetching services
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 22:55:19.138: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-5571" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
•{"msg":"PASSED [sig-network] Services should find a service from listing all namespaces [Conformance]","total":311,"completed":199,"skipped":3269,"failed":0}
SSSS
------------------------------
[sig-cli] Kubectl client Kubectl api-versions 
  should check if v1 is in available api versions  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 22:55:19.814: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should check if v1 is in available api versions  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: validating api versions
Aug 23 22:55:20.604: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585719795 --namespace=kubectl-5166 api-versions'
Aug 23 22:55:20.940: INFO: stderr: ""
Aug 23 22:55:20.940: INFO: stdout: "admissionregistration.k8s.io/v1\nadmissionregistration.k8s.io/v1beta1\napiextensions.k8s.io/v1\napiextensions.k8s.io/v1beta1\napiregistration.k8s.io/v1\napiregistration.k8s.io/v1beta1\napps.openshift.io/v1\napps/v1\nauthentication.k8s.io/v1\nauthentication.k8s.io/v1beta1\nauthorization.k8s.io/v1\nauthorization.k8s.io/v1beta1\nauthorization.openshift.io/v1\nautoscaling/v1\nautoscaling/v2beta1\nautoscaling/v2beta2\nbatch/v1\nbatch/v1beta1\nbuild.openshift.io/v1\ncertificates.k8s.io/v1\ncertificates.k8s.io/v1beta1\ncloudcredential.openshift.io/v1\nconfig.openshift.io/v1\nconsole.openshift.io/v1\nconsole.openshift.io/v1alpha1\ncontrolplane.operator.openshift.io/v1alpha1\ncoordination.k8s.io/v1\ncoordination.k8s.io/v1beta1\ncrd.projectcalico.org/v1\ndiscovery.k8s.io/v1beta1\nevents.k8s.io/v1\nevents.k8s.io/v1beta1\nextensions/v1beta1\nflowcontrol.apiserver.k8s.io/v1alpha1\nflowcontrol.apiserver.k8s.io/v1beta1\nhelm.openshift.io/v1beta1\nibm.com/v1alpha1\nimage.openshift.io/v1\nimageregistry.operator.openshift.io/v1\ningress.operator.openshift.io/v1\nk8s.cni.cncf.io/v1\nmachineconfiguration.openshift.io/v1\nmetal3.io/v1alpha1\nmetrics.k8s.io/v1beta1\nmigration.k8s.io/v1alpha1\nmonitoring.coreos.com/v1\nmonitoring.coreos.com/v1alpha1\nnetwork.operator.openshift.io/v1\nnetworking.k8s.io/v1\nnetworking.k8s.io/v1beta1\nnode.k8s.io/v1\nnode.k8s.io/v1beta1\noauth.openshift.io/v1\noperator.openshift.io/v1\noperator.openshift.io/v1alpha1\noperator.tigera.io/v1\noperators.coreos.com/v1\noperators.coreos.com/v1alpha1\noperators.coreos.com/v1alpha2\noperators.coreos.com/v2\npackages.operators.coreos.com/v1\npolicy/v1beta1\nproject.openshift.io/v1\nquota.openshift.io/v1\nrbac.authorization.k8s.io/v1\nrbac.authorization.k8s.io/v1beta1\nroute.openshift.io/v1\nsamples.operator.openshift.io/v1\nscheduling.k8s.io/v1\nscheduling.k8s.io/v1beta1\nsecurity.internal.openshift.io/v1\nsecurity.openshift.io/v1\nsnapshot.storage.k8s.io/v1\nsnapshot.storage.k8s.io/v1beta1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\ntemplate.openshift.io/v1\ntuned.openshift.io/v1\nuser.openshift.io/v1\nv1\nwhereabouts.cni.cncf.io/v1alpha1\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 22:55:20.940: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5166" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl api-versions should check if v1 is in available api versions  [Conformance]","total":311,"completed":200,"skipped":3273,"failed":0}
SSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 22:55:21.580: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name configmap-test-volume-map-10767242-9545-4821-aeb3-e3ce7bb18124
STEP: Creating a pod to test consume configMaps
Aug 23 22:55:22.427: INFO: Waiting up to 5m0s for pod "pod-configmaps-ef0556fd-213c-4105-b0c4-f7e4770c3dc9" in namespace "configmap-5379" to be "Succeeded or Failed"
Aug 23 22:55:22.527: INFO: Pod "pod-configmaps-ef0556fd-213c-4105-b0c4-f7e4770c3dc9": Phase="Pending", Reason="", readiness=false. Elapsed: 99.379663ms
Aug 23 22:55:24.652: INFO: Pod "pod-configmaps-ef0556fd-213c-4105-b0c4-f7e4770c3dc9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.224764502s
Aug 23 22:55:26.789: INFO: Pod "pod-configmaps-ef0556fd-213c-4105-b0c4-f7e4770c3dc9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.361671953s
Aug 23 22:55:28.894: INFO: Pod "pod-configmaps-ef0556fd-213c-4105-b0c4-f7e4770c3dc9": Phase="Pending", Reason="", readiness=false. Elapsed: 6.466971166s
Aug 23 22:55:31.029: INFO: Pod "pod-configmaps-ef0556fd-213c-4105-b0c4-f7e4770c3dc9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.601884783s
STEP: Saw pod success
Aug 23 22:55:31.029: INFO: Pod "pod-configmaps-ef0556fd-213c-4105-b0c4-f7e4770c3dc9" satisfied condition "Succeeded or Failed"
Aug 23 22:55:31.164: INFO: Trying to get logs from node 10.149.248.24 pod pod-configmaps-ef0556fd-213c-4105-b0c4-f7e4770c3dc9 container agnhost-container: <nil>
STEP: delete the pod
Aug 23 22:55:31.727: INFO: Waiting for pod pod-configmaps-ef0556fd-213c-4105-b0c4-f7e4770c3dc9 to disappear
Aug 23 22:55:31.799: INFO: Pod pod-configmaps-ef0556fd-213c-4105-b0c4-f7e4770c3dc9 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 22:55:31.799: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5379" for this suite.

• [SLOW TEST:10.690 seconds]
[sig-storage] ConfigMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:36
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":201,"skipped":3279,"failed":0}
SSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 22:55:32.272: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:52
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Aug 23 22:55:48.037: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Aug 23 22:55:48.134: INFO: Pod pod-with-prestop-http-hook still exists
Aug 23 22:55:50.134: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Aug 23 22:55:50.254: INFO: Pod pod-with-prestop-http-hook still exists
Aug 23 22:55:52.134: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Aug 23 22:55:52.228: INFO: Pod pod-with-prestop-http-hook still exists
Aug 23 22:55:54.134: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Aug 23 22:55:54.211: INFO: Pod pod-with-prestop-http-hook still exists
Aug 23 22:55:56.134: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Aug 23 22:55:56.247: INFO: Pod pod-with-prestop-http-hook still exists
Aug 23 22:55:58.134: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Aug 23 22:55:58.272: INFO: Pod pod-with-prestop-http-hook still exists
Aug 23 22:56:00.134: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Aug 23 22:56:00.256: INFO: Pod pod-with-prestop-http-hook still exists
Aug 23 22:56:02.134: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Aug 23 22:56:02.315: INFO: Pod pod-with-prestop-http-hook still exists
Aug 23 22:56:04.134: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Aug 23 22:56:04.311: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 22:56:04.475: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-4484" for this suite.

• [SLOW TEST:32.932 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:43
    should execute prestop http hook properly [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop http hook properly [NodeConformance] [Conformance]","total":311,"completed":202,"skipped":3289,"failed":0}
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 22:56:05.204: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Pod that fits quota
STEP: Ensuring ResourceQuota status captures the pod usage
STEP: Not allowing a pod to be created that exceeds remaining quota
STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources)
STEP: Ensuring a pod cannot update its resource requirements
STEP: Ensuring attempts to update pod resource requirements did not change quota usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 22:56:21.388: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-4663" for this suite.

• [SLOW TEST:17.023 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a pod. [Conformance]","total":311,"completed":203,"skipped":3289,"failed":0}
SSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl label 
  should update the label on a resource  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 22:56:22.227: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[BeforeEach] Kubectl label
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1314
STEP: creating the pod
Aug 23 22:56:23.344: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585719795 --namespace=kubectl-1226 create -f -'
Aug 23 22:56:24.296: INFO: stderr: ""
Aug 23 22:56:24.296: INFO: stdout: "pod/pause created\n"
Aug 23 22:56:24.296: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Aug 23 22:56:24.296: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-1226" to be "running and ready"
Aug 23 22:56:24.460: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 163.424502ms
Aug 23 22:56:26.678: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 2.381811208s
Aug 23 22:56:28.779: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 4.483004409s
Aug 23 22:56:28.779: INFO: Pod "pause" satisfied condition "running and ready"
Aug 23 22:56:28.779: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: adding the label testing-label with value testing-label-value to a pod
Aug 23 22:56:28.779: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585719795 --namespace=kubectl-1226 label pods pause testing-label=testing-label-value'
Aug 23 22:56:29.454: INFO: stderr: ""
Aug 23 22:56:29.454: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value
Aug 23 22:56:29.454: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585719795 --namespace=kubectl-1226 get pod pause -L testing-label'
Aug 23 22:56:29.883: INFO: stderr: ""
Aug 23 22:56:29.883: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          5s    testing-label-value\n"
STEP: removing the label testing-label of a pod
Aug 23 22:56:29.883: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585719795 --namespace=kubectl-1226 label pods pause testing-label-'
Aug 23 22:56:30.062: INFO: stderr: ""
Aug 23 22:56:30.062: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod doesn't have the label testing-label
Aug 23 22:56:30.062: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585719795 --namespace=kubectl-1226 get pod pause -L testing-label'
Aug 23 22:56:30.265: INFO: stderr: ""
Aug 23 22:56:30.265: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          6s    \n"
[AfterEach] Kubectl label
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1320
STEP: using delete to clean up resources
Aug 23 22:56:30.265: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585719795 --namespace=kubectl-1226 delete --grace-period=0 --force -f -'
Aug 23 22:56:30.813: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug 23 22:56:30.813: INFO: stdout: "pod \"pause\" force deleted\n"
Aug 23 22:56:30.813: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585719795 --namespace=kubectl-1226 get rc,svc -l name=pause --no-headers'
Aug 23 22:56:30.950: INFO: stderr: "No resources found in kubectl-1226 namespace.\n"
Aug 23 22:56:30.950: INFO: stdout: ""
Aug 23 22:56:30.950: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585719795 --namespace=kubectl-1226 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Aug 23 22:56:31.624: INFO: stderr: ""
Aug 23 22:56:31.624: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 22:56:31.624: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1226" for this suite.

• [SLOW TEST:9.735 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl label
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1312
    should update the label on a resource  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl label should update the label on a resource  [Conformance]","total":311,"completed":204,"skipped":3296,"failed":0}
SSS
------------------------------
[sig-storage] Projected configMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 22:56:31.963: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Aug 23 22:56:32.651: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
STEP: Creating configMap with name cm-test-opt-del-0f744276-e0e6-4634-8396-92863f055fc8
STEP: Creating configMap with name cm-test-opt-upd-fa3a7bd2-8ec3-4875-83b2-d28223764d6c
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-0f744276-e0e6-4634-8396-92863f055fc8
STEP: Updating configmap cm-test-opt-upd-fa3a7bd2-8ec3-4875-83b2-d28223764d6c
STEP: Creating configMap with name cm-test-opt-create-1eb1aa74-1041-448f-847d-410a786f014c
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 22:57:44.772: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8468" for this suite.

• [SLOW TEST:73.276 seconds]
[sig-storage] Projected configMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:36
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap optional updates should be reflected in volume [NodeConformance] [Conformance]","total":311,"completed":205,"skipped":3299,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 22:57:45.240: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:53
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Aug 23 22:57:45.968: INFO: The status of Pod test-webserver-bc26bf62-9d5b-4cbb-bf5a-cd64f9e59731 is Pending, waiting for it to be Running (with Ready = true)
Aug 23 22:57:48.094: INFO: The status of Pod test-webserver-bc26bf62-9d5b-4cbb-bf5a-cd64f9e59731 is Pending, waiting for it to be Running (with Ready = true)
Aug 23 22:57:50.050: INFO: The status of Pod test-webserver-bc26bf62-9d5b-4cbb-bf5a-cd64f9e59731 is Pending, waiting for it to be Running (with Ready = true)
Aug 23 22:57:52.002: INFO: The status of Pod test-webserver-bc26bf62-9d5b-4cbb-bf5a-cd64f9e59731 is Pending, waiting for it to be Running (with Ready = true)
Aug 23 22:57:54.024: INFO: The status of Pod test-webserver-bc26bf62-9d5b-4cbb-bf5a-cd64f9e59731 is Running (Ready = false)
Aug 23 22:57:56.073: INFO: The status of Pod test-webserver-bc26bf62-9d5b-4cbb-bf5a-cd64f9e59731 is Running (Ready = false)
Aug 23 22:57:58.057: INFO: The status of Pod test-webserver-bc26bf62-9d5b-4cbb-bf5a-cd64f9e59731 is Running (Ready = false)
Aug 23 22:58:00.045: INFO: The status of Pod test-webserver-bc26bf62-9d5b-4cbb-bf5a-cd64f9e59731 is Running (Ready = false)
Aug 23 22:58:02.043: INFO: The status of Pod test-webserver-bc26bf62-9d5b-4cbb-bf5a-cd64f9e59731 is Running (Ready = false)
Aug 23 22:58:04.004: INFO: The status of Pod test-webserver-bc26bf62-9d5b-4cbb-bf5a-cd64f9e59731 is Running (Ready = false)
Aug 23 22:58:06.006: INFO: The status of Pod test-webserver-bc26bf62-9d5b-4cbb-bf5a-cd64f9e59731 is Running (Ready = false)
Aug 23 22:58:07.992: INFO: The status of Pod test-webserver-bc26bf62-9d5b-4cbb-bf5a-cd64f9e59731 is Running (Ready = false)
Aug 23 22:58:10.075: INFO: The status of Pod test-webserver-bc26bf62-9d5b-4cbb-bf5a-cd64f9e59731 is Running (Ready = false)
Aug 23 22:58:12.121: INFO: The status of Pod test-webserver-bc26bf62-9d5b-4cbb-bf5a-cd64f9e59731 is Running (Ready = false)
Aug 23 22:58:14.147: INFO: The status of Pod test-webserver-bc26bf62-9d5b-4cbb-bf5a-cd64f9e59731 is Running (Ready = true)
Aug 23 22:58:14.278: INFO: Container started at 2021-08-23 22:57:52 +0000 UTC, pod became ready at 2021-08-23 22:58:12 +0000 UTC
[AfterEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 22:58:14.278: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-6427" for this suite.

• [SLOW TEST:29.692 seconds]
[k8s.io] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Probing container with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]","total":311,"completed":206,"skipped":3321,"failed":0}
[sig-network] Services 
  should provide secure master service  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 22:58:14.932: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should provide secure master service  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 22:58:16.008: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-9700" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
•{"msg":"PASSED [sig-network] Services should provide secure master service  [Conformance]","total":311,"completed":207,"skipped":3321,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 22:58:16.978: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward api env vars
Aug 23 22:58:18.262: INFO: Waiting up to 5m0s for pod "downward-api-fa25cdf4-ce77-4714-9a71-bd4a49267897" in namespace "downward-api-8804" to be "Succeeded or Failed"
Aug 23 22:58:18.432: INFO: Pod "downward-api-fa25cdf4-ce77-4714-9a71-bd4a49267897": Phase="Pending", Reason="", readiness=false. Elapsed: 169.473331ms
Aug 23 22:58:20.621: INFO: Pod "downward-api-fa25cdf4-ce77-4714-9a71-bd4a49267897": Phase="Pending", Reason="", readiness=false. Elapsed: 2.358652455s
Aug 23 22:58:22.779: INFO: Pod "downward-api-fa25cdf4-ce77-4714-9a71-bd4a49267897": Phase="Pending", Reason="", readiness=false. Elapsed: 4.516812549s
Aug 23 22:58:24.904: INFO: Pod "downward-api-fa25cdf4-ce77-4714-9a71-bd4a49267897": Phase="Pending", Reason="", readiness=false. Elapsed: 6.64214372s
Aug 23 22:58:27.082: INFO: Pod "downward-api-fa25cdf4-ce77-4714-9a71-bd4a49267897": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.819707973s
STEP: Saw pod success
Aug 23 22:58:27.082: INFO: Pod "downward-api-fa25cdf4-ce77-4714-9a71-bd4a49267897" satisfied condition "Succeeded or Failed"
Aug 23 22:58:27.241: INFO: Trying to get logs from node 10.149.248.24 pod downward-api-fa25cdf4-ce77-4714-9a71-bd4a49267897 container dapi-container: <nil>
STEP: delete the pod
Aug 23 22:58:27.563: INFO: Waiting for pod downward-api-fa25cdf4-ce77-4714-9a71-bd4a49267897 to disappear
Aug 23 22:58:27.710: INFO: Pod downward-api-fa25cdf4-ce77-4714-9a71-bd4a49267897 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 22:58:27.710: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8804" for this suite.

• [SLOW TEST:11.535 seconds]
[sig-node] Downward API
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:34
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-node] Downward API should provide pod UID as env vars [NodeConformance] [Conformance]","total":311,"completed":208,"skipped":3346,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should deny crd creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 22:58:28.514: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Aug 23 22:58:31.899: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63765356311, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63765356311, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63765356311, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63765356311, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 23 22:58:34.019: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63765356311, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63765356311, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63765356311, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63765356311, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 23 22:58:36.037: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63765356311, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63765356311, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63765356311, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63765356311, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Aug 23 22:58:39.188: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should deny crd creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Registering the crd webhook via the AdmissionRegistration API
STEP: Creating a custom resource definition that should be denied by the webhook
Aug 23 22:58:39.593: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 22:58:39.866: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4572" for this suite.
STEP: Destroying namespace "webhook-4572-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

• [SLOW TEST:12.897 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should deny crd creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should deny crd creation [Conformance]","total":311,"completed":209,"skipped":3392,"failed":0}
SS
------------------------------
[sig-cli] Kubectl client Kubectl describe 
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 22:58:41.412: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Aug 23 22:58:42.095: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585719795 --namespace=kubectl-4555 create -f -'
Aug 23 22:58:45.921: INFO: stderr: ""
Aug 23 22:58:45.921: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
Aug 23 22:58:45.921: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585719795 --namespace=kubectl-4555 create -f -'
Aug 23 22:58:46.622: INFO: stderr: ""
Aug 23 22:58:46.622: INFO: stdout: "service/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
Aug 23 22:58:47.757: INFO: Selector matched 1 pods for map[app:agnhost]
Aug 23 22:58:47.757: INFO: Found 0 / 1
Aug 23 22:58:48.757: INFO: Selector matched 1 pods for map[app:agnhost]
Aug 23 22:58:48.757: INFO: Found 0 / 1
Aug 23 22:58:49.713: INFO: Selector matched 1 pods for map[app:agnhost]
Aug 23 22:58:49.713: INFO: Found 0 / 1
Aug 23 22:58:50.812: INFO: Selector matched 1 pods for map[app:agnhost]
Aug 23 22:58:50.812: INFO: Found 1 / 1
Aug 23 22:58:50.812: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Aug 23 22:58:51.040: INFO: Selector matched 1 pods for map[app:agnhost]
Aug 23 22:58:51.040: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Aug 23 22:58:51.040: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585719795 --namespace=kubectl-4555 describe pod agnhost-primary-whc7n'
Aug 23 22:58:51.831: INFO: stderr: ""
Aug 23 22:58:51.831: INFO: stdout: "Name:         agnhost-primary-whc7n\nNamespace:    kubectl-4555\nPriority:     0\nNode:         10.149.248.24/10.149.248.24\nStart Time:   Mon, 23 Aug 2021 22:58:45 +0000\nLabels:       app=agnhost\n              role=primary\nAnnotations:  cni.projectcalico.org/podIP: 172.30.87.79/32\n              cni.projectcalico.org/podIPs: 172.30.87.79/32\n              k8s.v1.cni.cncf.io/network-status:\n                [{\n                    \"name\": \"\",\n                    \"ips\": [\n                        \"172.30.87.79\"\n                    ],\n                    \"default\": true,\n                    \"dns\": {}\n                }]\n              k8s.v1.cni.cncf.io/networks-status:\n                [{\n                    \"name\": \"\",\n                    \"ips\": [\n                        \"172.30.87.79\"\n                    ],\n                    \"default\": true,\n                    \"dns\": {}\n                }]\n              openshift.io/scc: anyuid\nStatus:       Running\nIP:           172.30.87.79\nIPs:\n  IP:           172.30.87.79\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   cri-o://40391372d45a2b26a082f5ea210782d973ad27c96e7c65ed0b687ce0ef2d6516\n    Image:          k8s.gcr.io/e2e-test-images/agnhost:2.21\n    Image ID:       k8s.gcr.io/e2e-test-images/agnhost@sha256:ab055cd3d45f50b90732c14593a5bf50f210871bb4f91994c756fc22db6d922a\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Mon, 23 Aug 2021 22:58:49 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from default-token-qsfsd (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  default-token-qsfsd:\n    Type:        Secret (a volume populated by a Secret)\n    SecretName:  default-token-qsfsd\n    Optional:    false\nQoS Class:       BestEffort\nNode-Selectors:  <none>\nTolerations:     node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                 node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason          Age   From               Message\n  ----    ------          ----  ----               -------\n  Normal  Scheduled       6s    default-scheduler  Successfully assigned kubectl-4555/agnhost-primary-whc7n to 10.149.248.24\n  Normal  AddedInterface  2s    multus             Add eth0 [172.30.87.79/32]\n  Normal  Pulled          2s    kubelet            Container image \"k8s.gcr.io/e2e-test-images/agnhost:2.21\" already present on machine\n  Normal  Created         2s    kubelet            Created container agnhost-primary\n  Normal  Started         1s    kubelet            Started container agnhost-primary\n"
Aug 23 22:58:51.831: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585719795 --namespace=kubectl-4555 describe rc agnhost-primary'
Aug 23 22:58:51.996: INFO: stderr: ""
Aug 23 22:58:51.996: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-4555\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        k8s.gcr.io/e2e-test-images/agnhost:2.21\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  6s    replication-controller  Created pod: agnhost-primary-whc7n\n"
Aug 23 22:58:51.996: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585719795 --namespace=kubectl-4555 describe service agnhost-primary'
Aug 23 22:58:53.783: INFO: stderr: ""
Aug 23 22:58:53.783: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-4555\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP Families:       <none>\nIP:                172.21.248.177\nIPs:               172.21.248.177\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         172.30.87.79:6379\nSession Affinity:  None\nEvents:            <none>\n"
Aug 23 22:58:54.502: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585719795 --namespace=kubectl-4555 describe node 10.149.248.24'
Aug 23 22:58:54.839: INFO: stderr: ""
Aug 23 22:58:54.839: INFO: stdout: "Name:               10.149.248.24\nRoles:              master,worker\nLabels:             arch=amd64\n                    beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/instance-type=b3c.4x16.encrypted\n                    beta.kubernetes.io/os=linux\n                    failure-domain.beta.kubernetes.io/region=us-east\n                    failure-domain.beta.kubernetes.io/zone=wdc04\n                    ibm-cloud.kubernetes.io/encrypted-docker-data=true\n                    ibm-cloud.kubernetes.io/external-ip=169.45.209.222\n                    ibm-cloud.kubernetes.io/iaas-provider=softlayer\n                    ibm-cloud.kubernetes.io/internal-ip=10.149.248.24\n                    ibm-cloud.kubernetes.io/machine-type=b3c.4x16.encrypted\n                    ibm-cloud.kubernetes.io/os=REDHAT_7_64\n                    ibm-cloud.kubernetes.io/region=us-east\n                    ibm-cloud.kubernetes.io/sgx-enabled=false\n                    ibm-cloud.kubernetes.io/worker-id=kube-c4hve0kw0a01h852nde0-kubee2epvgc-default-000002d9\n                    ibm-cloud.kubernetes.io/worker-pool-id=c4hve0kw0a01h852nde0-973bf04\n                    ibm-cloud.kubernetes.io/worker-pool-name=default\n                    ibm-cloud.kubernetes.io/worker-version=4.7.23_1528_openshift\n                    ibm-cloud.kubernetes.io/zone=wdc04\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=10.149.248.24\n                    kubernetes.io/os=linux\n                    node-role.kubernetes.io/master=\n                    node-role.kubernetes.io/worker=\n                    node.kubernetes.io/instance-type=b3c.4x16.encrypted\n                    node.openshift.io/os_id=rhel\n                    privateVLAN=2722946\n                    publicVLAN=2722944\n                    topology.kubernetes.io/region=us-east\n                    topology.kubernetes.io/zone=wdc04\nAnnotations:        projectcalico.org/IPv4Address: 10.149.248.24/26\n                    projectcalico.org/IPv4IPIPTunnelAddr: 172.30.87.70\nCreationTimestamp:  Mon, 23 Aug 2021 20:00:36 +0000\nTaints:             <none>\nUnschedulable:      false\nLease:\n  HolderIdentity:  10.149.248.24\n  AcquireTime:     <unset>\n  RenewTime:       Mon, 23 Aug 2021 22:58:47 +0000\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable   False   Mon, 23 Aug 2021 20:02:53 +0000   Mon, 23 Aug 2021 20:02:53 +0000   CalicoIsUp                   Calico is running on this node\n  MemoryPressure       False   Mon, 23 Aug 2021 22:55:19 +0000   Mon, 23 Aug 2021 20:00:36 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Mon, 23 Aug 2021 22:55:19 +0000   Mon, 23 Aug 2021 20:00:36 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Mon, 23 Aug 2021 22:55:19 +0000   Mon, 23 Aug 2021 20:00:36 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Mon, 23 Aug 2021 22:55:19 +0000   Mon, 23 Aug 2021 20:02:57 +0000   KubeletReady                 kubelet is posting ready status\nAddresses:\n  InternalIP:  10.149.248.24\n  ExternalIP:  169.45.209.222\n  Hostname:    10.149.248.24\nCapacity:\n  cpu:                4\n  ephemeral-storage:  103078840Ki\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             16253404Ki\n  pods:               110\nAllocatable:\n  cpu:                3910m\n  ephemeral-storage:  94369515442\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             13477340Ki\n  pods:               110\nSystem Info:\n  Machine ID:                                       226c2003e176445eb34cdef7cb0276da\n  System UUID:                                      DC93CA97-4427-1E45-F9C2-73F74296C6A5\n  Boot ID:                                          a1e05c6c-5357-42b3-b7c4-a8dc781d7da4\n  Kernel Version:                                   3.10.0-1160.36.2.el7.x86_64\n  OS Image:                                         Red Hat\n  Operating System:                                 linux\n  Architecture:                                     amd64\n  Container Runtime Version:                        cri-o://1.20.4-7.rhaos4.7.git6287500.el7\n  Kubelet Version:                                  v1.20.0+558d959\n  Kube-Proxy Version:                               v1.20.0+558d959\nPodCIDR:                                            172.30.2.0/24\nPodCIDRs:                                           172.30.2.0/24\nProviderID:                                         ibm://fee034388aa6435883a1f720010ab3a2///c4hve0kw0a01h852nde0/kube-c4hve0kw0a01h852nde0-kubee2epvgc-default-000002d9\nNon-terminated Pods:                                (33 in total)\n  Namespace                                         Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  AGE\n  ---------                                         ----                                                       ------------  ----------  ---------------  -------------  ---\n  calico-system                                     calico-node-9qqn4                                          250m (6%)     0 (0%)      80Mi (0%)        0 (0%)         176m\n  calico-system                                     calico-typha-84b7fc6fc8-9fjkp                              250m (6%)     0 (0%)      80Mi (0%)        0 (0%)         176m\n  kube-system                                       ibm-keepalived-watcher-vrlwd                               5m (0%)       0 (0%)      10Mi (0%)        0 (0%)         178m\n  kube-system                                       ibm-master-proxy-static-10.149.248.24                      25m (0%)      300m (7%)   32M (0%)         512M (3%)      177m\n  kube-system                                       ibm-storage-watcher-5c4d7b6cd6-fbwxc                       50m (1%)      200m (5%)   100Mi (0%)       200Mi (1%)     45m\n  kube-system                                       ibmcloud-block-storage-driver-jgx4r                        50m (1%)      300m (7%)   100Mi (0%)       300Mi (2%)     178m\n  kubectl-4555                                      agnhost-primary-whc7n                                      0 (0%)        0 (0%)      0 (0%)           0 (0%)         9s\n  openshift-cluster-node-tuning-operator            tuned-7sb7t                                                10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         173m\n  openshift-cluster-storage-operator                csi-snapshot-controller-566544547f-88h45                   10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         45m\n  openshift-cluster-storage-operator                csi-snapshot-webhook-585d4946dc-922hg                      10m (0%)      0 (0%)      20Mi (0%)        0 (0%)         45m\n  openshift-console-operator                        console-operator-c7f9f8687-fsq8q                           10m (0%)      0 (0%)      100Mi (0%)       0 (0%)         45m\n  openshift-console                                 console-dc8c686bb-cjdbt                                    10m (0%)      0 (0%)      100Mi (0%)       0 (0%)         45m\n  openshift-dns                                     dns-default-mgbp7                                          65m (1%)      0 (0%)      131Mi (0%)       0 (0%)         173m\n  openshift-image-registry                          node-ca-dl95t                                              10m (0%)      0 (0%)      10Mi (0%)        0 (0%)         174m\n  openshift-ingress-canary                          ingress-canary-8x7dj                                       10m (0%)      0 (0%)      20Mi (0%)        0 (0%)         45m\n  openshift-ingress-operator                        ingress-operator-5d97777898-lmhdt                          20m (0%)      0 (0%)      96Mi (0%)        0 (0%)         45m\n  openshift-kube-proxy                              openshift-kube-proxy-wn8f2                                 110m (2%)     0 (0%)      220Mi (1%)       0 (0%)         177m\n  openshift-kube-storage-version-migrator-operator  kube-storage-version-migrator-operator-6d55bddccb-7zfw2    10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         45m\n  openshift-kube-storage-version-migrator           migrator-f58676cd4-rpd8r                                   10m (0%)      0 (0%)      200Mi (1%)       0 (0%)         45m\n  openshift-marketplace                             marketplace-operator-7fd8fdcc5b-hbf84                      10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         45m\n  openshift-monitoring                              alertmanager-main-0                                        8m (0%)       0 (0%)      270Mi (2%)       0 (0%)         45m\n  openshift-monitoring                              kube-state-metrics-d956df775-lqfms                         4m (0%)       0 (0%)      120Mi (0%)       0 (0%)         45m\n  openshift-monitoring                              node-exporter-b5rwn                                        9m (0%)       0 (0%)      210Mi (1%)       0 (0%)         175m\n  openshift-monitoring                              openshift-state-metrics-74b58f578c-8b6fd                   3m (0%)       0 (0%)      190Mi (1%)       0 (0%)         45m\n  openshift-monitoring                              prometheus-adapter-6685ccb7f-2mrfl                         1m (0%)       0 (0%)      25Mi (0%)        0 (0%)         45m\n  openshift-monitoring                              telemeter-client-9d6d6f95f-r7zwc                           3m (0%)       0 (0%)      70Mi (0%)        0 (0%)         45m\n  openshift-multus                                  multus-48kkt                                               10m (0%)      0 (0%)      150Mi (1%)       0 (0%)         177m\n  openshift-multus                                  multus-admission-controller-ctsqt                          20m (0%)      0 (0%)      20Mi (0%)        0 (0%)         45m\n  openshift-multus                                  network-metrics-daemon-cqr85                               20m (0%)      0 (0%)      120Mi (0%)       0 (0%)         177m\n  openshift-network-diagnostics                     network-check-target-447wc                                 10m (0%)      0 (0%)      15Mi (0%)        0 (0%)         177m\n  openshift-roks-metrics                            push-gateway-59f6bc56d4-62594                              10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         45m\n  openshift-service-ca-operator                     service-ca-operator-64b7cc7c85-sv5w8                       10m (0%)      0 (0%)      80Mi (0%)        0 (0%)         45m\n  sonobuoy                                          sonobuoy-systemd-logs-daemon-set-9034e69b695e4ee2-s28vf    0 (0%)        0 (0%)      0 (0%)           0 (0%)         81m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests         Limits\n  --------           --------         ------\n  cpu                1033m (26%)      800m (20%)\n  memory             2885138Ki (21%)  1036288k (7%)\n  ephemeral-storage  0 (0%)           0 (0%)\n  hugepages-1Gi      0 (0%)           0 (0%)\n  hugepages-2Mi      0 (0%)           0 (0%)\nEvents:\n  Type    Reason                   Age                  From        Message\n  ----    ------                   ----                 ----        -------\n  Normal  Starting                 178m                 kubelet     Starting kubelet.\n  Normal  NodeAllocatableEnforced  178m                 kubelet     Updated Node Allocatable limit across pods\n  Normal  NodeHasSufficientPID     178m (x7 over 178m)  kubelet     Node 10.149.248.24 status is now: NodeHasSufficientPID\n  Normal  NodeHasSufficientMemory  178m (x8 over 178m)  kubelet     Node 10.149.248.24 status is now: NodeHasSufficientMemory\n  Normal  NodeHasNoDiskPressure    178m (x8 over 178m)  kubelet     Node 10.149.248.24 status is now: NodeHasNoDiskPressure\n  Normal  Starting                 177m                 kube-proxy  Starting kube-proxy.\n"
Aug 23 22:58:54.839: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585719795 --namespace=kubectl-4555 describe namespace kubectl-4555'
Aug 23 22:58:55.607: INFO: stderr: ""
Aug 23 22:58:55.607: INFO: stdout: "Name:         kubectl-4555\nLabels:       e2e-framework=kubectl\n              e2e-run=6c3cf079-64c9-4ed1-a2c3-c86e319b7e92\nAnnotations:  openshift.io/sa.scc.mcs: s0:c56,c15\n              openshift.io/sa.scc.supplemental-groups: 1003110000/10000\n              openshift.io/sa.scc.uid-range: 1003110000/10000\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 22:58:55.607: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4555" for this suite.

• [SLOW TEST:14.629 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl describe
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1090
    should check if kubectl describe prints relevant information for rc and pods  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl describe should check if kubectl describe prints relevant information for rc and pods  [Conformance]","total":311,"completed":210,"skipped":3394,"failed":0}
SSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 22:58:56.041: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete pods created by rc when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the rc
STEP: delete the rc
STEP: wait for all pods to be garbage collected
STEP: Gathering metrics
W0823 22:59:09.296877      25 metrics_grabber.go:98] Can't find kube-scheduler pod. Grabbing metrics from kube-scheduler is disabled.
W0823 22:59:09.297158      25 metrics_grabber.go:102] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
W0823 22:59:09.297284      25 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
Aug 23 22:59:09.297: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 22:59:09.297: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-6381" for this suite.

• [SLOW TEST:14.213 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should delete pods created by rc when not orphaning [Conformance]","total":311,"completed":211,"skipped":3401,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should allow opting out of API token automount  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 22:59:10.256: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow opting out of API token automount  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: getting the auto-created API token
Aug 23 22:59:12.659: INFO: created pod pod-service-account-defaultsa
Aug 23 22:59:12.659: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Aug 23 22:59:12.857: INFO: created pod pod-service-account-mountsa
Aug 23 22:59:12.857: INFO: pod pod-service-account-mountsa service account token volume mount: true
Aug 23 22:59:13.115: INFO: created pod pod-service-account-nomountsa
Aug 23 22:59:13.115: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Aug 23 22:59:13.366: INFO: created pod pod-service-account-defaultsa-mountspec
Aug 23 22:59:13.366: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Aug 23 22:59:13.646: INFO: created pod pod-service-account-mountsa-mountspec
Aug 23 22:59:13.646: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Aug 23 22:59:13.911: INFO: created pod pod-service-account-nomountsa-mountspec
Aug 23 22:59:13.911: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Aug 23 22:59:14.184: INFO: created pod pod-service-account-defaultsa-nomountspec
Aug 23 22:59:14.184: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Aug 23 22:59:14.459: INFO: created pod pod-service-account-mountsa-nomountspec
Aug 23 22:59:14.459: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Aug 23 22:59:14.766: INFO: created pod pod-service-account-nomountsa-nomountspec
Aug 23 22:59:14.766: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 22:59:14.766: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-6631" for this suite.

• [SLOW TEST:5.723 seconds]
[sig-auth] ServiceAccounts
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:23
  should allow opting out of API token automount  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-auth] ServiceAccounts should allow opting out of API token automount  [Conformance]","total":311,"completed":212,"skipped":3442,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 22:59:15.980: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name projected-configmap-test-volume-map-45dafb29-52b4-4cdc-8229-63fb82466738
STEP: Creating a pod to test consume configMaps
Aug 23 22:59:17.724: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-0a9514d5-091e-41e4-909d-e11f00bc5817" in namespace "projected-170" to be "Succeeded or Failed"
Aug 23 22:59:17.884: INFO: Pod "pod-projected-configmaps-0a9514d5-091e-41e4-909d-e11f00bc5817": Phase="Pending", Reason="", readiness=false. Elapsed: 159.45035ms
Aug 23 22:59:20.181: INFO: Pod "pod-projected-configmaps-0a9514d5-091e-41e4-909d-e11f00bc5817": Phase="Pending", Reason="", readiness=false. Elapsed: 2.456310369s
Aug 23 22:59:22.384: INFO: Pod "pod-projected-configmaps-0a9514d5-091e-41e4-909d-e11f00bc5817": Phase="Pending", Reason="", readiness=false. Elapsed: 4.659134746s
Aug 23 22:59:24.675: INFO: Pod "pod-projected-configmaps-0a9514d5-091e-41e4-909d-e11f00bc5817": Phase="Pending", Reason="", readiness=false. Elapsed: 6.950748498s
Aug 23 22:59:26.920: INFO: Pod "pod-projected-configmaps-0a9514d5-091e-41e4-909d-e11f00bc5817": Phase="Pending", Reason="", readiness=false. Elapsed: 9.19535898s
Aug 23 22:59:29.152: INFO: Pod "pod-projected-configmaps-0a9514d5-091e-41e4-909d-e11f00bc5817": Phase="Succeeded", Reason="", readiness=false. Elapsed: 11.427333852s
STEP: Saw pod success
Aug 23 22:59:29.152: INFO: Pod "pod-projected-configmaps-0a9514d5-091e-41e4-909d-e11f00bc5817" satisfied condition "Succeeded or Failed"
Aug 23 22:59:29.370: INFO: Trying to get logs from node 10.149.248.24 pod pod-projected-configmaps-0a9514d5-091e-41e4-909d-e11f00bc5817 container agnhost-container: <nil>
STEP: delete the pod
Aug 23 22:59:29.809: INFO: Waiting for pod pod-projected-configmaps-0a9514d5-091e-41e4-909d-e11f00bc5817 to disappear
Aug 23 22:59:30.029: INFO: Pod pod-projected-configmaps-0a9514d5-091e-41e4-909d-e11f00bc5817 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 22:59:30.029: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-170" for this suite.

• [SLOW TEST:14.837 seconds]
[sig-storage] Projected configMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:36
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":311,"completed":213,"skipped":3458,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should patch a secret [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 22:59:30.818: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should patch a secret [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a secret
STEP: listing secrets in all namespaces to ensure that there are more than zero
STEP: patching the secret
STEP: deleting the secret using a LabelSelector
STEP: listing secrets in all namespaces, searching for label name and value in patch
[AfterEach] [sig-api-machinery] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 22:59:33.197: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8573" for this suite.
•{"msg":"PASSED [sig-api-machinery] Secrets should patch a secret [Conformance]","total":311,"completed":214,"skipped":3490,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] PreStop 
  should call prestop when killing a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 22:59:33.806: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename prestop
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pre_stop.go:157
[It] should call prestop when killing a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating server pod server in namespace prestop-2141
STEP: Waiting for pods to come up.
STEP: Creating tester pod tester in namespace prestop-2141
STEP: Deleting pre-stop pod
Aug 23 22:59:53.157: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod
[AfterEach] [k8s.io] [sig-node] PreStop
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 22:59:53.443: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "prestop-2141" for this suite.

• [SLOW TEST:20.356 seconds]
[k8s.io] [sig-node] PreStop
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should call prestop when killing a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] [sig-node] PreStop should call prestop when killing a pod  [Conformance]","total":311,"completed":215,"skipped":3506,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 22:59:54.162: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating service in namespace services-9796
STEP: creating service affinity-nodeport-transition in namespace services-9796
STEP: creating replication controller affinity-nodeport-transition in namespace services-9796
I0823 22:59:55.501961      25 runners.go:190] Created replication controller with name: affinity-nodeport-transition, namespace: services-9796, replica count: 3
I0823 22:59:58.702320      25 runners.go:190] affinity-nodeport-transition Pods: 3 out of 3 created, 0 running, 3 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0823 23:00:01.702693      25 runners.go:190] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Aug 23 23:00:02.255: INFO: Creating new exec pod
Aug 23 23:00:10.177: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585719795 --namespace=services-9796 exec execpod-affinityppjkk -- /bin/sh -x -c nc -zv -t -w 2 affinity-nodeport-transition 80'
Aug 23 23:00:12.270: INFO: stderr: "+ nc -zv -t -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
Aug 23 23:00:12.270: INFO: stdout: ""
Aug 23 23:00:12.271: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585719795 --namespace=services-9796 exec execpod-affinityppjkk -- /bin/sh -x -c nc -zv -t -w 2 172.21.37.21 80'
Aug 23 23:00:15.167: INFO: stderr: "+ nc -zv -t -w 2 172.21.37.21 80\nConnection to 172.21.37.21 80 port [tcp/http] succeeded!\n"
Aug 23 23:00:15.167: INFO: stdout: ""
Aug 23 23:00:15.167: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585719795 --namespace=services-9796 exec execpod-affinityppjkk -- /bin/sh -x -c nc -zv -t -w 2 10.149.248.9 30984'
Aug 23 23:00:17.067: INFO: stderr: "+ nc -zv -t -w 2 10.149.248.9 30984\nConnection to 10.149.248.9 30984 port [tcp/30984] succeeded!\n"
Aug 23 23:00:17.067: INFO: stdout: ""
Aug 23 23:00:17.067: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585719795 --namespace=services-9796 exec execpod-affinityppjkk -- /bin/sh -x -c nc -zv -t -w 2 10.149.248.25 30984'
Aug 23 23:00:19.181: INFO: stderr: "+ nc -zv -t -w 2 10.149.248.25 30984\nConnection to 10.149.248.25 30984 port [tcp/30984] succeeded!\n"
Aug 23 23:00:19.181: INFO: stdout: ""
Aug 23 23:00:19.181: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585719795 --namespace=services-9796 exec execpod-affinityppjkk -- /bin/sh -x -c nc -zv -t -w 2 169.45.209.221 30984'
Aug 23 23:00:21.012: INFO: stderr: "+ nc -zv -t -w 2 169.45.209.221 30984\nConnection to 169.45.209.221 30984 port [tcp/30984] succeeded!\n"
Aug 23 23:00:21.012: INFO: stdout: ""
Aug 23 23:00:21.012: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585719795 --namespace=services-9796 exec execpod-affinityppjkk -- /bin/sh -x -c nc -zv -t -w 2 169.45.232.204 30984'
Aug 23 23:00:23.704: INFO: stderr: "+ nc -zv -t -w 2 169.45.232.204 30984\nConnection to 169.45.232.204 30984 port [tcp/30984] succeeded!\n"
Aug 23 23:00:23.704: INFO: stdout: ""
Aug 23 23:00:24.151: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585719795 --namespace=services-9796 exec execpod-affinityppjkk -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.149.248.24:30984/ ; done'
Aug 23 23:00:25.622: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.149.248.24:30984/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.149.248.24:30984/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.149.248.24:30984/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.149.248.24:30984/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.149.248.24:30984/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.149.248.24:30984/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.149.248.24:30984/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.149.248.24:30984/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.149.248.24:30984/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.149.248.24:30984/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.149.248.24:30984/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.149.248.24:30984/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.149.248.24:30984/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.149.248.24:30984/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.149.248.24:30984/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.149.248.24:30984/\n"
Aug 23 23:00:25.622: INFO: stdout: "\naffinity-nodeport-transition-fq9vr\naffinity-nodeport-transition-gzqcw\naffinity-nodeport-transition-gzqcw\naffinity-nodeport-transition-gzqcw\naffinity-nodeport-transition-8c9f5\naffinity-nodeport-transition-8c9f5\naffinity-nodeport-transition-gzqcw\naffinity-nodeport-transition-gzqcw\naffinity-nodeport-transition-8c9f5\naffinity-nodeport-transition-gzqcw\naffinity-nodeport-transition-fq9vr\naffinity-nodeport-transition-8c9f5\naffinity-nodeport-transition-gzqcw\naffinity-nodeport-transition-fq9vr\naffinity-nodeport-transition-8c9f5\naffinity-nodeport-transition-gzqcw"
Aug 23 23:00:25.622: INFO: Received response from host: affinity-nodeport-transition-fq9vr
Aug 23 23:00:25.622: INFO: Received response from host: affinity-nodeport-transition-gzqcw
Aug 23 23:00:25.622: INFO: Received response from host: affinity-nodeport-transition-gzqcw
Aug 23 23:00:25.622: INFO: Received response from host: affinity-nodeport-transition-gzqcw
Aug 23 23:00:25.622: INFO: Received response from host: affinity-nodeport-transition-8c9f5
Aug 23 23:00:25.622: INFO: Received response from host: affinity-nodeport-transition-8c9f5
Aug 23 23:00:25.622: INFO: Received response from host: affinity-nodeport-transition-gzqcw
Aug 23 23:00:25.622: INFO: Received response from host: affinity-nodeport-transition-gzqcw
Aug 23 23:00:25.622: INFO: Received response from host: affinity-nodeport-transition-8c9f5
Aug 23 23:00:25.622: INFO: Received response from host: affinity-nodeport-transition-gzqcw
Aug 23 23:00:25.622: INFO: Received response from host: affinity-nodeport-transition-fq9vr
Aug 23 23:00:25.622: INFO: Received response from host: affinity-nodeport-transition-8c9f5
Aug 23 23:00:25.622: INFO: Received response from host: affinity-nodeport-transition-gzqcw
Aug 23 23:00:25.622: INFO: Received response from host: affinity-nodeport-transition-fq9vr
Aug 23 23:00:25.622: INFO: Received response from host: affinity-nodeport-transition-8c9f5
Aug 23 23:00:25.622: INFO: Received response from host: affinity-nodeport-transition-gzqcw
Aug 23 23:00:25.893: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585719795 --namespace=services-9796 exec execpod-affinityppjkk -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.149.248.24:30984/ ; done'
Aug 23 23:00:28.863: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.149.248.24:30984/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.149.248.24:30984/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.149.248.24:30984/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.149.248.24:30984/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.149.248.24:30984/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.149.248.24:30984/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.149.248.24:30984/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.149.248.24:30984/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.149.248.24:30984/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.149.248.24:30984/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.149.248.24:30984/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.149.248.24:30984/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.149.248.24:30984/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.149.248.24:30984/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.149.248.24:30984/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.149.248.24:30984/\n"
Aug 23 23:00:28.863: INFO: stdout: "\naffinity-nodeport-transition-gzqcw\naffinity-nodeport-transition-gzqcw\naffinity-nodeport-transition-gzqcw\naffinity-nodeport-transition-gzqcw\naffinity-nodeport-transition-gzqcw\naffinity-nodeport-transition-gzqcw\naffinity-nodeport-transition-gzqcw\naffinity-nodeport-transition-gzqcw\naffinity-nodeport-transition-gzqcw\naffinity-nodeport-transition-gzqcw\naffinity-nodeport-transition-gzqcw\naffinity-nodeport-transition-gzqcw\naffinity-nodeport-transition-gzqcw\naffinity-nodeport-transition-gzqcw\naffinity-nodeport-transition-gzqcw\naffinity-nodeport-transition-gzqcw"
Aug 23 23:00:28.863: INFO: Received response from host: affinity-nodeport-transition-gzqcw
Aug 23 23:00:28.863: INFO: Received response from host: affinity-nodeport-transition-gzqcw
Aug 23 23:00:28.863: INFO: Received response from host: affinity-nodeport-transition-gzqcw
Aug 23 23:00:28.863: INFO: Received response from host: affinity-nodeport-transition-gzqcw
Aug 23 23:00:28.863: INFO: Received response from host: affinity-nodeport-transition-gzqcw
Aug 23 23:00:28.863: INFO: Received response from host: affinity-nodeport-transition-gzqcw
Aug 23 23:00:28.863: INFO: Received response from host: affinity-nodeport-transition-gzqcw
Aug 23 23:00:28.863: INFO: Received response from host: affinity-nodeport-transition-gzqcw
Aug 23 23:00:28.863: INFO: Received response from host: affinity-nodeport-transition-gzqcw
Aug 23 23:00:28.863: INFO: Received response from host: affinity-nodeport-transition-gzqcw
Aug 23 23:00:28.863: INFO: Received response from host: affinity-nodeport-transition-gzqcw
Aug 23 23:00:28.863: INFO: Received response from host: affinity-nodeport-transition-gzqcw
Aug 23 23:00:28.863: INFO: Received response from host: affinity-nodeport-transition-gzqcw
Aug 23 23:00:28.863: INFO: Received response from host: affinity-nodeport-transition-gzqcw
Aug 23 23:00:28.863: INFO: Received response from host: affinity-nodeport-transition-gzqcw
Aug 23 23:00:28.863: INFO: Received response from host: affinity-nodeport-transition-gzqcw
Aug 23 23:00:28.863: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-9796, will wait for the garbage collector to delete the pods
Aug 23 23:00:29.586: INFO: Deleting ReplicationController affinity-nodeport-transition took: 317.132413ms
Aug 23 23:00:29.786: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 200.246104ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 23:00:46.814: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-9796" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749

• [SLOW TEST:53.668 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Services should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]","total":311,"completed":216,"skipped":3519,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 23:00:47.832: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Aug 23 23:00:49.570: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
STEP: Creating secret with name s-test-opt-del-ef69413e-baa4-44d5-8a04-11cbdbafa9e7
STEP: Creating secret with name s-test-opt-upd-c24614f1-90ef-4065-8edc-84d50635cb1c
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-ef69413e-baa4-44d5-8a04-11cbdbafa9e7
STEP: Updating secret s-test-opt-upd-c24614f1-90ef-4065-8edc-84d50635cb1c
STEP: Creating secret with name s-test-opt-create-325d04e1-7e0b-4a8b-abde-999728678e89
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 23:02:10.509: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2081" for this suite.

• [SLOW TEST:83.538 seconds]
[sig-storage] Projected secret
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Projected secret optional updates should be reflected in volume [NodeConformance] [Conformance]","total":311,"completed":217,"skipped":3531,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] Certificates API [Privileged:ClusterAdmin] 
  should support CSR API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 23:02:11.374: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename certificates
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support CSR API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: getting /apis
STEP: getting /apis/certificates.k8s.io
STEP: getting /apis/certificates.k8s.io/v1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Aug 23 23:02:14.641: INFO: starting watch
STEP: patching
STEP: updating
Aug 23 23:02:15.215: INFO: waiting for watch events with expected annotations
Aug 23 23:02:15.215: INFO: saw patched and updated annotations
STEP: getting /approval
STEP: patching /approval
STEP: updating /approval
STEP: getting /status
STEP: patching /status
STEP: updating /status
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 23:02:17.819: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "certificates-8740" for this suite.

• [SLOW TEST:7.117 seconds]
[sig-auth] Certificates API [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:23
  should support CSR API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-auth] Certificates API [Privileged:ClusterAdmin] should support CSR API operations [Conformance]","total":311,"completed":218,"skipped":3581,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl version 
  should check is all data is printed  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 23:02:18.491: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should check is all data is printed  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Aug 23 23:02:19.128: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585719795 --namespace=kubectl-4532 version'
Aug 23 23:02:19.794: INFO: stderr: ""
Aug 23 23:02:19.794: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"20\", GitVersion:\"v1.20.6\", GitCommit:\"8a62859e515889f07e3e3be6a1080413f17cf2c3\", GitTreeState:\"clean\", BuildDate:\"2021-04-15T03:28:42Z\", GoVersion:\"go1.15.10\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"20\", GitVersion:\"v1.20.0+558d959\", GitCommit:\"558d959332b3f1f7bf786673bf294e6e0932bb18\", GitTreeState:\"clean\", BuildDate:\"2021-07-13T20:45:42Z\", GoVersion:\"go1.15.14\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 23:02:19.794: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4532" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl version should check is all data is printed  [Conformance]","total":311,"completed":219,"skipped":3618,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 23:02:20.373: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Aug 23 23:02:21.441: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
STEP: Creating configMap with name configmap-test-upd-c47538b5-e329-4ced-9572-5e092f3b1a40
STEP: Creating the pod
STEP: Waiting for pod with text data
STEP: Waiting for pod with binary data
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 23:02:28.465: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1016" for this suite.

• [SLOW TEST:8.398 seconds]
[sig-storage] ConfigMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:36
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap binary data should be reflected in volume [NodeConformance] [Conformance]","total":311,"completed":220,"skipped":3635,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 23:02:28.771: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir volume type on node default medium
Aug 23 23:02:29.303: INFO: Waiting up to 5m0s for pod "pod-4c366679-7295-400e-a607-31d2c5ebe73c" in namespace "emptydir-8727" to be "Succeeded or Failed"
Aug 23 23:02:29.365: INFO: Pod "pod-4c366679-7295-400e-a607-31d2c5ebe73c": Phase="Pending", Reason="", readiness=false. Elapsed: 62.112337ms
Aug 23 23:02:31.384: INFO: Pod "pod-4c366679-7295-400e-a607-31d2c5ebe73c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.080808584s
Aug 23 23:02:33.446: INFO: Pod "pod-4c366679-7295-400e-a607-31d2c5ebe73c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.143172706s
STEP: Saw pod success
Aug 23 23:02:33.446: INFO: Pod "pod-4c366679-7295-400e-a607-31d2c5ebe73c" satisfied condition "Succeeded or Failed"
Aug 23 23:02:33.528: INFO: Trying to get logs from node 10.149.248.9 pod pod-4c366679-7295-400e-a607-31d2c5ebe73c container test-container: <nil>
STEP: delete the pod
Aug 23 23:02:34.782: INFO: Waiting for pod pod-4c366679-7295-400e-a607-31d2c5ebe73c to disappear
Aug 23 23:02:34.828: INFO: Pod pod-4c366679-7295-400e-a607-31d2c5ebe73c no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 23:02:34.828: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8727" for this suite.

• [SLOW TEST:6.260 seconds]
[sig-storage] EmptyDir volumes
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:45
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":221,"skipped":3659,"failed":0}
SSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 23:02:35.031: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-6607
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-6607
STEP: creating replication controller externalsvc in namespace services-6607
I0823 23:02:36.013410      25 runners.go:190] Created replication controller with name: externalsvc, namespace: services-6607, replica count: 2
I0823 23:02:39.163730      25 runners.go:190] externalsvc Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0823 23:02:42.163940      25 runners.go:190] externalsvc Pods: 2 out of 2 created, 1 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0823 23:02:45.164149      25 runners.go:190] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the ClusterIP service to type=ExternalName
Aug 23 23:02:45.344: INFO: Creating new exec pod
Aug 23 23:02:51.592: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585719795 --namespace=services-6607 exec execpodscx9z -- /bin/sh -x -c nslookup clusterip-service.services-6607.svc.cluster.local'
Aug 23 23:02:53.772: INFO: stderr: "+ nslookup clusterip-service.services-6607.svc.cluster.local\n"
Aug 23 23:02:53.772: INFO: stdout: "Server:\t\t172.21.0.10\nAddress:\t172.21.0.10#53\n\nclusterip-service.services-6607.svc.cluster.local\tcanonical name = externalsvc.services-6607.svc.cluster.local.\nName:\texternalsvc.services-6607.svc.cluster.local\nAddress: 172.21.165.36\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-6607, will wait for the garbage collector to delete the pods
Aug 23 23:02:54.115: INFO: Deleting ReplicationController externalsvc took: 110.864348ms
Aug 23 23:02:54.316: INFO: Terminating ReplicationController externalsvc pods took: 200.242874ms
Aug 23 23:03:06.586: INFO: Cleaning up the ClusterIP to ExternalName test service
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 23:03:06.797: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6607" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749

• [SLOW TEST:32.581 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ClusterIP to ExternalName [Conformance]","total":311,"completed":222,"skipped":3664,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath 
  runs ReplicaSets to verify preemption running path [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 23:03:07.613: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename sched-preemption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:90
Aug 23 23:03:08.747: INFO: Waiting up to 1m0s for all nodes to be ready
Aug 23 23:04:12.859: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 23:04:13.181: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename sched-preemption-path
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:488
STEP: Finding an available node
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
Aug 23 23:04:23.154: INFO: found a healthy node: 10.149.248.24
[It] runs ReplicaSets to verify preemption running path [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Aug 23 23:04:56.062: INFO: pods created so far: [1 1 1]
Aug 23 23:04:56.062: INFO: length of pods created so far: 3
Aug 23 23:05:06.291: INFO: pods created so far: [2 2 1]
[AfterEach] PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 23:05:13.292: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-path-1431" for this suite.
[AfterEach] PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:462
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 23:05:14.888: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-5625" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:78

• [SLOW TEST:128.882 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:451
    runs ReplicaSets to verify preemption running path [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath runs ReplicaSets to verify preemption running path [Conformance]","total":311,"completed":223,"skipped":3700,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl expose 
  should create services for rc  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 23:05:16.495: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should create services for rc  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating Agnhost RC
Aug 23 23:05:17.067: INFO: namespace kubectl-595
Aug 23 23:05:17.067: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585719795 --namespace=kubectl-595 create -f -'
Aug 23 23:05:19.266: INFO: stderr: ""
Aug 23 23:05:19.266: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
Aug 23 23:05:20.419: INFO: Selector matched 1 pods for map[app:agnhost]
Aug 23 23:05:20.419: INFO: Found 0 / 1
Aug 23 23:05:21.371: INFO: Selector matched 1 pods for map[app:agnhost]
Aug 23 23:05:21.371: INFO: Found 0 / 1
Aug 23 23:05:22.481: INFO: Selector matched 1 pods for map[app:agnhost]
Aug 23 23:05:22.481: INFO: Found 0 / 1
Aug 23 23:05:23.520: INFO: Selector matched 1 pods for map[app:agnhost]
Aug 23 23:05:23.520: INFO: Found 0 / 1
Aug 23 23:05:24.542: INFO: Selector matched 1 pods for map[app:agnhost]
Aug 23 23:05:24.542: INFO: Found 0 / 1
Aug 23 23:05:25.497: INFO: Selector matched 1 pods for map[app:agnhost]
Aug 23 23:05:25.497: INFO: Found 1 / 1
Aug 23 23:05:25.497: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Aug 23 23:05:25.659: INFO: Selector matched 1 pods for map[app:agnhost]
Aug 23 23:05:25.659: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Aug 23 23:05:25.659: INFO: wait on agnhost-primary startup in kubectl-595 
Aug 23 23:05:25.659: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585719795 --namespace=kubectl-595 logs agnhost-primary-bh577 agnhost-primary'
Aug 23 23:05:26.760: INFO: stderr: ""
Aug 23 23:05:26.760: INFO: stdout: "Paused\n"
STEP: exposing RC
Aug 23 23:05:26.760: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585719795 --namespace=kubectl-595 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
Aug 23 23:05:26.953: INFO: stderr: ""
Aug 23 23:05:26.953: INFO: stdout: "service/rm2 exposed\n"
Aug 23 23:05:27.191: INFO: Service rm2 in namespace kubectl-595 found.
STEP: exposing service
Aug 23 23:05:29.514: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585719795 --namespace=kubectl-595 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
Aug 23 23:05:30.416: INFO: stderr: ""
Aug 23 23:05:30.417: INFO: stdout: "service/rm3 exposed\n"
Aug 23 23:05:30.593: INFO: Service rm3 in namespace kubectl-595 found.
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 23:05:32.970: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-595" for this suite.

• [SLOW TEST:17.075 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl expose
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1229
    should create services for rc  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl expose should create services for rc  [Conformance]","total":311,"completed":224,"skipped":3711,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 23:05:33.571: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name configmap-test-volume-cd05adef-b976-4d95-848e-5dcc3ce9c1f4
STEP: Creating a pod to test consume configMaps
Aug 23 23:05:34.994: INFO: Waiting up to 5m0s for pod "pod-configmaps-def82883-91b7-4d23-a599-40afe7fcc0fb" in namespace "configmap-2360" to be "Succeeded or Failed"
Aug 23 23:05:35.196: INFO: Pod "pod-configmaps-def82883-91b7-4d23-a599-40afe7fcc0fb": Phase="Pending", Reason="", readiness=false. Elapsed: 202.291764ms
Aug 23 23:05:37.394: INFO: Pod "pod-configmaps-def82883-91b7-4d23-a599-40afe7fcc0fb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.400031246s
Aug 23 23:05:39.589: INFO: Pod "pod-configmaps-def82883-91b7-4d23-a599-40afe7fcc0fb": Phase="Pending", Reason="", readiness=false. Elapsed: 4.595307314s
Aug 23 23:05:41.700: INFO: Pod "pod-configmaps-def82883-91b7-4d23-a599-40afe7fcc0fb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.706345137s
STEP: Saw pod success
Aug 23 23:05:41.700: INFO: Pod "pod-configmaps-def82883-91b7-4d23-a599-40afe7fcc0fb" satisfied condition "Succeeded or Failed"
Aug 23 23:05:41.800: INFO: Trying to get logs from node 10.149.248.24 pod pod-configmaps-def82883-91b7-4d23-a599-40afe7fcc0fb container agnhost-container: <nil>
STEP: delete the pod
Aug 23 23:05:42.073: INFO: Waiting for pod pod-configmaps-def82883-91b7-4d23-a599-40afe7fcc0fb to disappear
Aug 23 23:05:42.171: INFO: Pod pod-configmaps-def82883-91b7-4d23-a599-40afe7fcc0fb no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 23:05:42.171: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2360" for this suite.

• [SLOW TEST:8.971 seconds]
[sig-storage] ConfigMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:36
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","total":311,"completed":225,"skipped":3748,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  listing custom resource definition objects works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 23:05:42.542: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] listing custom resource definition objects works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Aug 23 23:05:43.140: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 23:05:49.252: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-6736" for this suite.

• [SLOW TEST:7.286 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:48
    listing custom resource definition objects works  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition listing custom resource definition objects works  [Conformance]","total":311,"completed":226,"skipped":3773,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 23:05:49.828: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Aug 23 23:05:57.370: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 23:05:57.726: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-1952" for this suite.

• [SLOW TEST:8.341 seconds]
[k8s.io] Container Runtime
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  blackbox test
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:41
    on terminated container
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:134
      should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":311,"completed":227,"skipped":3795,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 23:05:58.172: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating projection with secret that has name projected-secret-test-f3a9fc28-44f9-427a-91ba-9a67231c025a
STEP: Creating a pod to test consume secrets
Aug 23 23:05:59.815: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-a03c08c5-3d7e-4422-9309-a877806b39d7" in namespace "projected-132" to be "Succeeded or Failed"
Aug 23 23:06:00.035: INFO: Pod "pod-projected-secrets-a03c08c5-3d7e-4422-9309-a877806b39d7": Phase="Pending", Reason="", readiness=false. Elapsed: 220.284439ms
Aug 23 23:06:02.272: INFO: Pod "pod-projected-secrets-a03c08c5-3d7e-4422-9309-a877806b39d7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.457315898s
Aug 23 23:06:04.423: INFO: Pod "pod-projected-secrets-a03c08c5-3d7e-4422-9309-a877806b39d7": Phase="Pending", Reason="", readiness=false. Elapsed: 4.607962486s
Aug 23 23:06:06.518: INFO: Pod "pod-projected-secrets-a03c08c5-3d7e-4422-9309-a877806b39d7": Phase="Pending", Reason="", readiness=false. Elapsed: 6.702642337s
Aug 23 23:06:08.573: INFO: Pod "pod-projected-secrets-a03c08c5-3d7e-4422-9309-a877806b39d7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.75761274s
STEP: Saw pod success
Aug 23 23:06:08.573: INFO: Pod "pod-projected-secrets-a03c08c5-3d7e-4422-9309-a877806b39d7" satisfied condition "Succeeded or Failed"
Aug 23 23:06:08.617: INFO: Trying to get logs from node 10.149.248.24 pod pod-projected-secrets-a03c08c5-3d7e-4422-9309-a877806b39d7 container projected-secret-volume-test: <nil>
STEP: delete the pod
Aug 23 23:06:09.013: INFO: Waiting for pod pod-projected-secrets-a03c08c5-3d7e-4422-9309-a877806b39d7 to disappear
Aug 23 23:06:09.036: INFO: Pod pod-projected-secrets-a03c08c5-3d7e-4422-9309-a877806b39d7 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 23:06:09.036: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-132" for this suite.

• [SLOW TEST:10.949 seconds]
[sig-storage] Projected secret
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume [NodeConformance] [Conformance]","total":311,"completed":228,"skipped":3851,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 23:06:09.123: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating projection with secret that has name projected-secret-test-map-878d5e84-dcd1-4685-95f5-9eb694d532a4
STEP: Creating a pod to test consume secrets
Aug 23 23:06:09.455: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-fe66d740-7443-4fcf-9d9b-acca02bd7cdf" in namespace "projected-4124" to be "Succeeded or Failed"
Aug 23 23:06:09.475: INFO: Pod "pod-projected-secrets-fe66d740-7443-4fcf-9d9b-acca02bd7cdf": Phase="Pending", Reason="", readiness=false. Elapsed: 20.417731ms
Aug 23 23:06:11.587: INFO: Pod "pod-projected-secrets-fe66d740-7443-4fcf-9d9b-acca02bd7cdf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.132414203s
Aug 23 23:06:13.732: INFO: Pod "pod-projected-secrets-fe66d740-7443-4fcf-9d9b-acca02bd7cdf": Phase="Pending", Reason="", readiness=false. Elapsed: 4.276488541s
Aug 23 23:06:15.845: INFO: Pod "pod-projected-secrets-fe66d740-7443-4fcf-9d9b-acca02bd7cdf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.389819873s
STEP: Saw pod success
Aug 23 23:06:15.845: INFO: Pod "pod-projected-secrets-fe66d740-7443-4fcf-9d9b-acca02bd7cdf" satisfied condition "Succeeded or Failed"
Aug 23 23:06:15.943: INFO: Trying to get logs from node 10.149.248.24 pod pod-projected-secrets-fe66d740-7443-4fcf-9d9b-acca02bd7cdf container projected-secret-volume-test: <nil>
STEP: delete the pod
Aug 23 23:06:16.172: INFO: Waiting for pod pod-projected-secrets-fe66d740-7443-4fcf-9d9b-acca02bd7cdf to disappear
Aug 23 23:06:16.285: INFO: Pod pod-projected-secrets-fe66d740-7443-4fcf-9d9b-acca02bd7cdf no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 23:06:16.286: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4124" for this suite.

• [SLOW TEST:7.501 seconds]
[sig-storage] Projected secret
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":311,"completed":229,"skipped":3881,"failed":0}
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 23:06:16.624: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename crd-webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:126
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
Aug 23 23:06:18.143: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63765356777, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63765356777, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63765356778, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63765356777, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-7d6697c5b7\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 23 23:06:20.228: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63765356777, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63765356777, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63765356778, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63765356777, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-7d6697c5b7\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 23 23:06:22.212: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63765356777, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63765356777, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63765356778, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63765356777, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-7d6697c5b7\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Aug 23 23:06:25.472: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Aug 23 23:06:25.605: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Creating a v1 custom resource
STEP: Create a v2 custom resource
STEP: List CRs in v1
STEP: List CRs in v2
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 23:06:27.050: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-3825" for this suite.
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:137

• [SLOW TEST:10.695 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert a non homogeneous list of CRs [Conformance]","total":311,"completed":230,"skipped":3881,"failed":0}
SSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 23:06:27.318: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:41
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Aug 23 23:06:27.588: INFO: Waiting up to 5m0s for pod "downwardapi-volume-831fd393-bff8-4dcd-8b1e-2fa0de882acf" in namespace "downward-api-501" to be "Succeeded or Failed"
Aug 23 23:06:27.615: INFO: Pod "downwardapi-volume-831fd393-bff8-4dcd-8b1e-2fa0de882acf": Phase="Pending", Reason="", readiness=false. Elapsed: 26.99811ms
Aug 23 23:06:29.687: INFO: Pod "downwardapi-volume-831fd393-bff8-4dcd-8b1e-2fa0de882acf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.098407819s
Aug 23 23:06:31.750: INFO: Pod "downwardapi-volume-831fd393-bff8-4dcd-8b1e-2fa0de882acf": Phase="Pending", Reason="", readiness=false. Elapsed: 4.161965306s
Aug 23 23:06:33.872: INFO: Pod "downwardapi-volume-831fd393-bff8-4dcd-8b1e-2fa0de882acf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.284384004s
STEP: Saw pod success
Aug 23 23:06:33.873: INFO: Pod "downwardapi-volume-831fd393-bff8-4dcd-8b1e-2fa0de882acf" satisfied condition "Succeeded or Failed"
Aug 23 23:06:34.002: INFO: Trying to get logs from node 10.149.248.24 pod downwardapi-volume-831fd393-bff8-4dcd-8b1e-2fa0de882acf container client-container: <nil>
STEP: delete the pod
Aug 23 23:06:34.298: INFO: Waiting for pod downwardapi-volume-831fd393-bff8-4dcd-8b1e-2fa0de882acf to disappear
Aug 23 23:06:34.402: INFO: Pod downwardapi-volume-831fd393-bff8-4dcd-8b1e-2fa0de882acf no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 23:06:34.402: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-501" for this suite.

• [SLOW TEST:7.412 seconds]
[sig-storage] Downward API volume
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:36
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory limit [NodeConformance] [Conformance]","total":311,"completed":231,"skipped":3885,"failed":0}
SSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 23:06:34.732: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:52
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Aug 23 23:06:49.046: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Aug 23 23:06:49.439: INFO: Pod pod-with-poststart-exec-hook still exists
Aug 23 23:06:51.439: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Aug 23 23:06:51.908: INFO: Pod pod-with-poststart-exec-hook still exists
Aug 23 23:06:53.439: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Aug 23 23:06:53.851: INFO: Pod pod-with-poststart-exec-hook still exists
Aug 23 23:06:55.439: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Aug 23 23:06:55.710: INFO: Pod pod-with-poststart-exec-hook still exists
Aug 23 23:06:57.439: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Aug 23 23:06:57.564: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 23:06:57.564: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-2180" for this suite.

• [SLOW TEST:23.465 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:43
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart exec hook properly [NodeConformance] [Conformance]","total":311,"completed":232,"skipped":3896,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 23:06:58.197: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a watch on configmaps
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: closing the watch once it receives two notifications
Aug 23 23:06:59.215: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-2284 /api/v1/namespaces/watch-2284/configmaps/e2e-watch-test-watch-closed 2554ee0d-f735-4f63-8faf-97b59424cada 107039 0 2021-08-23 23:06:58 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2021-08-23 23:06:58 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Aug 23 23:06:59.215: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-2284 /api/v1/namespaces/watch-2284/configmaps/e2e-watch-test-watch-closed 2554ee0d-f735-4f63-8faf-97b59424cada 107041 0 2021-08-23 23:06:58 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2021-08-23 23:06:59 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time, while the watch is closed
STEP: creating a new watch on configmaps from the last resource version observed by the first watch
STEP: deleting the configmap
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed
Aug 23 23:06:59.821: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-2284 /api/v1/namespaces/watch-2284/configmaps/e2e-watch-test-watch-closed 2554ee0d-f735-4f63-8faf-97b59424cada 107042 0 2021-08-23 23:06:58 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2021-08-23 23:06:59 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Aug 23 23:06:59.821: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-2284 /api/v1/namespaces/watch-2284/configmaps/e2e-watch-test-watch-closed 2554ee0d-f735-4f63-8faf-97b59424cada 107045 0 2021-08-23 23:06:58 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2021-08-23 23:06:59 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 23:06:59.821: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-2284" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should be able to restart watching from the last resource version observed by the previous watch [Conformance]","total":311,"completed":233,"skipped":3902,"failed":0}

------------------------------
[sig-scheduling] SchedulerPreemption [Serial] 
  validates lower priority pod preemption by critical pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 23:07:00.477: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename sched-preemption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:90
Aug 23 23:07:01.358: INFO: Waiting up to 1m0s for all nodes to be ready
Aug 23 23:08:06.663: INFO: Waiting for terminating namespaces to be deleted...
[It] validates lower priority pod preemption by critical pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Create pods that use 2/3 of node resources.
Aug 23 23:08:08.115: INFO: Created pod: pod0-sched-preemption-low-priority
Aug 23 23:08:08.848: INFO: Created pod: pod1-sched-preemption-medium-priority
Aug 23 23:08:09.508: INFO: Created pod: pod2-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled.
STEP: Run a critical pod that use same resources as that of a lower priority pod
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 23:08:49.728: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-8080" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:78

• [SLOW TEST:111.984 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates lower priority pod preemption by critical pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]","total":311,"completed":234,"skipped":3902,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 23:08:52.461: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:85
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Aug 23 23:08:54.050: INFO: Creating deployment "test-recreate-deployment"
Aug 23 23:08:54.282: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Aug 23 23:08:54.840: INFO: Waiting deployment "test-recreate-deployment" to complete
Aug 23 23:08:55.077: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63765356934, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63765356934, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63765356934, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63765356934, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-786dd7c454\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 23 23:08:57.274: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63765356934, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63765356934, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63765356934, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63765356934, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-786dd7c454\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 23 23:08:59.328: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63765356934, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63765356934, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63765356934, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63765356934, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-786dd7c454\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 23 23:09:01.334: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Aug 23 23:09:01.659: INFO: Updating deployment test-recreate-deployment
Aug 23 23:09:01.659: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:79
Aug 23 23:09:02.074: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:{test-recreate-deployment  deployment-3408 /apis/apps/v1/namespaces/deployment-3408/deployments/test-recreate-deployment cb3cb93c-c51d-425e-8fa0-39b808384e4b 107970 2 2021-08-23 23:08:54 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2021-08-23 23:09:01 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-08-23 23:09:01 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}}}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00443c528 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2021-08-23 23:09:01 +0000 UTC,LastTransitionTime:2021-08-23 23:09:01 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-f79dd4667" is progressing.,LastUpdateTime:2021-08-23 23:09:01 +0000 UTC,LastTransitionTime:2021-08-23 23:08:54 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

Aug 23 23:09:02.217: INFO: New ReplicaSet "test-recreate-deployment-f79dd4667" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:{test-recreate-deployment-f79dd4667  deployment-3408 /apis/apps/v1/namespaces/deployment-3408/replicasets/test-recreate-deployment-f79dd4667 8482a632-2c6c-457b-9ad5-3960f2de4e99 107968 1 2021-08-23 23:09:01 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:f79dd4667] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment cb3cb93c-c51d-425e-8fa0-39b808384e4b 0xc00443c9a0 0xc00443c9a1}] []  [{kube-controller-manager Update apps/v1 2021-08-23 23:09:01 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"cb3cb93c-c51d-425e-8fa0-39b808384e4b\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: f79dd4667,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:f79dd4667] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00443ca18 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Aug 23 23:09:02.217: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Aug 23 23:09:02.217: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-786dd7c454  deployment-3408 /apis/apps/v1/namespaces/deployment-3408/replicasets/test-recreate-deployment-786dd7c454 d16a7f1b-68ea-44e4-ac27-cbeecf11ed64 107957 2 2021-08-23 23:08:54 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:786dd7c454] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment cb3cb93c-c51d-425e-8fa0-39b808384e4b 0xc00443c8a7 0xc00443c8a8}] []  [{kube-controller-manager Update apps/v1 2021-08-23 23:09:01 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"cb3cb93c-c51d-425e-8fa0-39b808384e4b\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 786dd7c454,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:786dd7c454] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.21 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00443c938 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Aug 23 23:09:02.406: INFO: Pod "test-recreate-deployment-f79dd4667-zjqgx" is not available:
&Pod{ObjectMeta:{test-recreate-deployment-f79dd4667-zjqgx test-recreate-deployment-f79dd4667- deployment-3408 /api/v1/namespaces/deployment-3408/pods/test-recreate-deployment-f79dd4667-zjqgx 9eeddf00-6254-4db7-b9f5-d610ad17ffc9 107971 0 2021-08-23 23:09:01 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:f79dd4667] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet test-recreate-deployment-f79dd4667 8482a632-2c6c-457b-9ad5-3960f2de4e99 0xc00443ce57 0xc00443ce58}] []  [{kube-controller-manager Update v1 2021-08-23 23:09:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8482a632-2c6c-457b-9ad5-3960f2de4e99\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{".":{},"f:capabilities":{"f:drop":{}}},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{".":{},"f:seLinuxOptions":{"f:level":{}}},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-08-23 23:09:01 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-wlsl5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-wlsl5,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-wlsl5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.149.248.24,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c58,c32,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-4x2xn,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-08-23 23:09:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-08-23 23:09:01 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-08-23 23:09:01 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-08-23 23:09:01 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.149.248.24,PodIP:,StartTime:2021-08-23 23:09:01 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 23:09:02.406: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-3408" for this suite.

• [SLOW TEST:10.442 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] Deployment RecreateDeployment should delete old pods and create new ones [Conformance]","total":311,"completed":235,"skipped":3923,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 23:09:02.903: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name configmap-test-volume-e17c5365-1717-42f0-a7b8-9eb17de98c23
STEP: Creating a pod to test consume configMaps
Aug 23 23:09:03.978: INFO: Waiting up to 5m0s for pod "pod-configmaps-5e2defde-8d10-4f46-a35a-33b87cebe3b4" in namespace "configmap-3480" to be "Succeeded or Failed"
Aug 23 23:09:04.116: INFO: Pod "pod-configmaps-5e2defde-8d10-4f46-a35a-33b87cebe3b4": Phase="Pending", Reason="", readiness=false. Elapsed: 138.053444ms
Aug 23 23:09:06.268: INFO: Pod "pod-configmaps-5e2defde-8d10-4f46-a35a-33b87cebe3b4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.289524219s
Aug 23 23:09:08.439: INFO: Pod "pod-configmaps-5e2defde-8d10-4f46-a35a-33b87cebe3b4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.460816075s
Aug 23 23:09:10.575: INFO: Pod "pod-configmaps-5e2defde-8d10-4f46-a35a-33b87cebe3b4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.597260268s
STEP: Saw pod success
Aug 23 23:09:10.575: INFO: Pod "pod-configmaps-5e2defde-8d10-4f46-a35a-33b87cebe3b4" satisfied condition "Succeeded or Failed"
Aug 23 23:09:10.724: INFO: Trying to get logs from node 10.149.248.24 pod pod-configmaps-5e2defde-8d10-4f46-a35a-33b87cebe3b4 container agnhost-container: <nil>
STEP: delete the pod
Aug 23 23:09:11.074: INFO: Waiting for pod pod-configmaps-5e2defde-8d10-4f46-a35a-33b87cebe3b4 to disappear
Aug 23 23:09:11.213: INFO: Pod pod-configmaps-5e2defde-8d10-4f46-a35a-33b87cebe3b4 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 23:09:11.213: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3480" for this suite.

• [SLOW TEST:8.674 seconds]
[sig-storage] ConfigMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:36
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume [NodeConformance] [Conformance]","total":311,"completed":236,"skipped":3943,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 23:09:11.577: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the rc1
STEP: create the rc2
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well
STEP: delete the rc simpletest-rc-to-be-deleted
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W0823 23:09:23.933599      25 metrics_grabber.go:98] Can't find kube-scheduler pod. Grabbing metrics from kube-scheduler is disabled.
W0823 23:09:23.933625      25 metrics_grabber.go:102] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
W0823 23:09:23.933631      25 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
Aug 23 23:09:23.933: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

Aug 23 23:09:23.933: INFO: Deleting pod "simpletest-rc-to-be-deleted-9kg4s" in namespace "gc-355"
Aug 23 23:09:24.051: INFO: Deleting pod "simpletest-rc-to-be-deleted-bg872" in namespace "gc-355"
Aug 23 23:09:24.124: INFO: Deleting pod "simpletest-rc-to-be-deleted-jnhzt" in namespace "gc-355"
Aug 23 23:09:24.190: INFO: Deleting pod "simpletest-rc-to-be-deleted-mm8g7" in namespace "gc-355"
Aug 23 23:09:24.263: INFO: Deleting pod "simpletest-rc-to-be-deleted-nk5zv" in namespace "gc-355"
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 23:09:24.338: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-355" for this suite.

• [SLOW TEST:12.943 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]","total":311,"completed":237,"skipped":3964,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 23:09:24.521: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating pod pod-subpath-test-secret-bdgs
STEP: Creating a pod to test atomic-volume-subpath
Aug 23 23:09:24.975: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-bdgs" in namespace "subpath-3641" to be "Succeeded or Failed"
Aug 23 23:09:25.038: INFO: Pod "pod-subpath-test-secret-bdgs": Phase="Pending", Reason="", readiness=false. Elapsed: 63.438883ms
Aug 23 23:09:27.066: INFO: Pod "pod-subpath-test-secret-bdgs": Phase="Pending", Reason="", readiness=false. Elapsed: 2.091722041s
Aug 23 23:09:29.152: INFO: Pod "pod-subpath-test-secret-bdgs": Phase="Pending", Reason="", readiness=false. Elapsed: 4.177188728s
Aug 23 23:09:31.235: INFO: Pod "pod-subpath-test-secret-bdgs": Phase="Running", Reason="", readiness=true. Elapsed: 6.259927667s
Aug 23 23:09:33.299: INFO: Pod "pod-subpath-test-secret-bdgs": Phase="Running", Reason="", readiness=true. Elapsed: 8.324237225s
Aug 23 23:09:35.412: INFO: Pod "pod-subpath-test-secret-bdgs": Phase="Running", Reason="", readiness=true. Elapsed: 10.437010487s
Aug 23 23:09:37.523: INFO: Pod "pod-subpath-test-secret-bdgs": Phase="Running", Reason="", readiness=true. Elapsed: 12.54862926s
Aug 23 23:09:39.561: INFO: Pod "pod-subpath-test-secret-bdgs": Phase="Running", Reason="", readiness=true. Elapsed: 14.586033065s
Aug 23 23:09:41.623: INFO: Pod "pod-subpath-test-secret-bdgs": Phase="Running", Reason="", readiness=true. Elapsed: 16.648004465s
Aug 23 23:09:43.734: INFO: Pod "pod-subpath-test-secret-bdgs": Phase="Running", Reason="", readiness=true. Elapsed: 18.759170262s
Aug 23 23:09:45.857: INFO: Pod "pod-subpath-test-secret-bdgs": Phase="Running", Reason="", readiness=true. Elapsed: 20.882156917s
Aug 23 23:09:48.004: INFO: Pod "pod-subpath-test-secret-bdgs": Phase="Running", Reason="", readiness=true. Elapsed: 23.02956979s
Aug 23 23:09:50.079: INFO: Pod "pod-subpath-test-secret-bdgs": Phase="Running", Reason="", readiness=true. Elapsed: 25.103982669s
Aug 23 23:09:52.265: INFO: Pod "pod-subpath-test-secret-bdgs": Phase="Succeeded", Reason="", readiness=false. Elapsed: 27.290267977s
STEP: Saw pod success
Aug 23 23:09:52.265: INFO: Pod "pod-subpath-test-secret-bdgs" satisfied condition "Succeeded or Failed"
Aug 23 23:09:52.497: INFO: Trying to get logs from node 10.149.248.24 pod pod-subpath-test-secret-bdgs container test-container-subpath-secret-bdgs: <nil>
STEP: delete the pod
Aug 23 23:09:52.923: INFO: Waiting for pod pod-subpath-test-secret-bdgs to disappear
Aug 23 23:09:53.100: INFO: Pod pod-subpath-test-secret-bdgs no longer exists
STEP: Deleting pod pod-subpath-test-secret-bdgs
Aug 23 23:09:53.101: INFO: Deleting pod "pod-subpath-test-secret-bdgs" in namespace "subpath-3641"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 23:09:53.286: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-3641" for this suite.

• [SLOW TEST:29.490 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with secret pod [LinuxOnly] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with secret pod [LinuxOnly] [Conformance]","total":311,"completed":238,"skipped":3999,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 23:09:54.011: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ConfigMap
STEP: Ensuring resource quota status captures configMap creation
STEP: Deleting a ConfigMap
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 23:10:25.404: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-8192" for this suite.

• [SLOW TEST:32.717 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a configMap. [Conformance]","total":311,"completed":239,"skipped":4027,"failed":0}
SSSSSSSSS
------------------------------
[sig-apps] Deployment 
  should run the lifecycle of a Deployment [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 23:10:26.729: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:85
[It] should run the lifecycle of a Deployment [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a Deployment
STEP: waiting for Deployment to be created
STEP: waiting for all Replicas to be Ready
Aug 23 23:10:28.533: INFO: observed Deployment test-deployment in namespace deployment-6739 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Aug 23 23:10:28.533: INFO: observed Deployment test-deployment in namespace deployment-6739 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Aug 23 23:10:28.533: INFO: observed Deployment test-deployment in namespace deployment-6739 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Aug 23 23:10:28.533: INFO: observed Deployment test-deployment in namespace deployment-6739 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Aug 23 23:10:28.533: INFO: observed Deployment test-deployment in namespace deployment-6739 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Aug 23 23:10:28.533: INFO: observed Deployment test-deployment in namespace deployment-6739 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Aug 23 23:10:28.533: INFO: observed Deployment test-deployment in namespace deployment-6739 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Aug 23 23:10:28.533: INFO: observed Deployment test-deployment in namespace deployment-6739 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Aug 23 23:10:34.391: INFO: observed Deployment test-deployment in namespace deployment-6739 with ReadyReplicas 1 and labels map[test-deployment-static:true]
Aug 23 23:10:34.391: INFO: observed Deployment test-deployment in namespace deployment-6739 with ReadyReplicas 1 and labels map[test-deployment-static:true]
Aug 23 23:10:35.108: INFO: observed Deployment test-deployment in namespace deployment-6739 with ReadyReplicas 2 and labels map[test-deployment-static:true]
STEP: patching the Deployment
Aug 23 23:10:35.390: INFO: observed event type ADDED
STEP: waiting for Replicas to scale
Aug 23 23:10:35.552: INFO: observed Deployment test-deployment in namespace deployment-6739 with ReadyReplicas 0
Aug 23 23:10:35.552: INFO: observed Deployment test-deployment in namespace deployment-6739 with ReadyReplicas 0
Aug 23 23:10:35.552: INFO: observed Deployment test-deployment in namespace deployment-6739 with ReadyReplicas 0
Aug 23 23:10:35.552: INFO: observed Deployment test-deployment in namespace deployment-6739 with ReadyReplicas 0
Aug 23 23:10:35.552: INFO: observed Deployment test-deployment in namespace deployment-6739 with ReadyReplicas 0
Aug 23 23:10:35.552: INFO: observed Deployment test-deployment in namespace deployment-6739 with ReadyReplicas 0
Aug 23 23:10:35.552: INFO: observed Deployment test-deployment in namespace deployment-6739 with ReadyReplicas 0
Aug 23 23:10:35.552: INFO: observed Deployment test-deployment in namespace deployment-6739 with ReadyReplicas 0
Aug 23 23:10:35.648: INFO: observed Deployment test-deployment in namespace deployment-6739 with ReadyReplicas 1
Aug 23 23:10:35.648: INFO: observed Deployment test-deployment in namespace deployment-6739 with ReadyReplicas 1
Aug 23 23:10:35.648: INFO: observed Deployment test-deployment in namespace deployment-6739 with ReadyReplicas 2
Aug 23 23:10:35.649: INFO: observed Deployment test-deployment in namespace deployment-6739 with ReadyReplicas 2
Aug 23 23:10:35.649: INFO: observed Deployment test-deployment in namespace deployment-6739 with ReadyReplicas 2
Aug 23 23:10:35.649: INFO: observed Deployment test-deployment in namespace deployment-6739 with ReadyReplicas 2
Aug 23 23:10:35.649: INFO: observed Deployment test-deployment in namespace deployment-6739 with ReadyReplicas 2
Aug 23 23:10:35.649: INFO: observed Deployment test-deployment in namespace deployment-6739 with ReadyReplicas 2
Aug 23 23:10:35.656: INFO: observed Deployment test-deployment in namespace deployment-6739 with ReadyReplicas 2
Aug 23 23:10:35.656: INFO: observed Deployment test-deployment in namespace deployment-6739 with ReadyReplicas 2
Aug 23 23:10:35.656: INFO: observed Deployment test-deployment in namespace deployment-6739 with ReadyReplicas 1
STEP: listing Deployments
Aug 23 23:10:35.870: INFO: Found test-deployment with labels: map[test-deployment:patched test-deployment-static:true]
STEP: updating the Deployment
Aug 23 23:10:36.149: INFO: observed Deployment test-deployment in namespace deployment-6739 with ReadyReplicas 1
STEP: fetching the DeploymentStatus
Aug 23 23:10:36.277: INFO: observed Deployment test-deployment in namespace deployment-6739 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Aug 23 23:10:36.277: INFO: observed Deployment test-deployment in namespace deployment-6739 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Aug 23 23:10:36.277: INFO: observed Deployment test-deployment in namespace deployment-6739 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Aug 23 23:10:36.278: INFO: observed Deployment test-deployment in namespace deployment-6739 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Aug 23 23:10:36.278: INFO: observed Deployment test-deployment in namespace deployment-6739 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Aug 23 23:10:36.278: INFO: observed Deployment test-deployment in namespace deployment-6739 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
STEP: patching the DeploymentStatus
STEP: fetching the DeploymentStatus
Aug 23 23:10:41.256: INFO: observed Deployment test-deployment in namespace deployment-6739 with ReadyReplicas 1
Aug 23 23:10:41.256: INFO: observed Deployment test-deployment in namespace deployment-6739 with ReadyReplicas 1
Aug 23 23:10:41.256: INFO: observed Deployment test-deployment in namespace deployment-6739 with ReadyReplicas 1
Aug 23 23:10:41.257: INFO: observed Deployment test-deployment in namespace deployment-6739 with ReadyReplicas 1
Aug 23 23:10:41.257: INFO: observed Deployment test-deployment in namespace deployment-6739 with ReadyReplicas 1
Aug 23 23:10:41.257: INFO: observed Deployment test-deployment in namespace deployment-6739 with ReadyReplicas 1
STEP: deleting the Deployment
Aug 23 23:10:41.323: INFO: observed event type MODIFIED
Aug 23 23:10:41.324: INFO: observed event type MODIFIED
Aug 23 23:10:41.324: INFO: observed event type MODIFIED
Aug 23 23:10:41.324: INFO: observed event type MODIFIED
Aug 23 23:10:41.324: INFO: observed event type MODIFIED
Aug 23 23:10:41.324: INFO: observed event type MODIFIED
Aug 23 23:10:41.324: INFO: observed event type MODIFIED
Aug 23 23:10:41.324: INFO: observed event type MODIFIED
Aug 23 23:10:41.332: INFO: observed event type MODIFIED
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:79
Aug 23 23:10:41.370: INFO: Log out all the ReplicaSets if there is no deployment created
Aug 23 23:10:41.401: INFO: ReplicaSet "test-deployment-768947d6f5":
&ReplicaSet{ObjectMeta:{test-deployment-768947d6f5  deployment-6739 /apis/apps/v1/namespaces/deployment-6739/replicasets/test-deployment-768947d6f5 b929e1a0-aa5d-4660-ac99-5e4c4155550e 109294 3 2021-08-23 23:10:35 +0000 UTC <nil> <nil> map[pod-template-hash:768947d6f5 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:3] [{apps/v1 Deployment test-deployment a48e8ff3-d58f-4d30-a75b-d5c2c2b52b82 0xc000f3e517 0xc000f3e518}] []  [{kube-controller-manager Update apps/v1 2021-08-23 23:10:41 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a48e8ff3-d58f-4d30-a75b-d5c2c2b52b82\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 768947d6f5,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:768947d6f5 test-deployment-static:true] map[] [] []  []} {[] [] [{test-deployment docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc000f3e580 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:3,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}

Aug 23 23:10:41.426: INFO: pod: "test-deployment-768947d6f5-82ffb":
&Pod{ObjectMeta:{test-deployment-768947d6f5-82ffb test-deployment-768947d6f5- deployment-6739 /api/v1/namespaces/deployment-6739/pods/test-deployment-768947d6f5-82ffb d4f7f5a7-9bd5-4e51-8162-1ccd47e0ef9c 109296 0 2021-08-23 23:10:41 +0000 UTC <nil> <nil> map[pod-template-hash:768947d6f5 test-deployment-static:true] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet test-deployment-768947d6f5 b929e1a0-aa5d-4660-ac99-5e4c4155550e 0xc008759e07 0xc008759e08}] []  [{kube-controller-manager Update v1 2021-08-23 23:10:41 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b929e1a0-aa5d-4660-ac99-5e4c4155550e\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{".":{},"f:seLinuxOptions":{"f:level":{}}},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-08-23 23:10:41 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-7v6bv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-7v6bv,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-7v6bv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.149.248.9,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c58,c57,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-nz8m4,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-08-23 23:10:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-08-23 23:10:41 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [test-deployment],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-08-23 23:10:41 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [test-deployment],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-08-23 23:10:41 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.149.248.9,PodIP:,StartTime:2021-08-23 23:10:41 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}

Aug 23 23:10:41.426: INFO: pod: "test-deployment-768947d6f5-9w2fs":
&Pod{ObjectMeta:{test-deployment-768947d6f5-9w2fs test-deployment-768947d6f5- deployment-6739 /api/v1/namespaces/deployment-6739/pods/test-deployment-768947d6f5-9w2fs 754962ce-54f5-42eb-97bb-c9d7f4ed2b17 109275 0 2021-08-23 23:10:36 +0000 UTC <nil> <nil> map[pod-template-hash:768947d6f5 test-deployment-static:true] map[cni.projectcalico.org/podIP:172.30.87.72/32 cni.projectcalico.org/podIPs:172.30.87.72/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "",
    "ips": [
        "172.30.87.72"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "",
    "ips": [
        "172.30.87.72"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet test-deployment-768947d6f5 b929e1a0-aa5d-4660-ac99-5e4c4155550e 0xc008759fe7 0xc008759fe8}] []  [{kube-controller-manager Update v1 2021-08-23 23:10:36 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b929e1a0-aa5d-4660-ac99-5e4c4155550e\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{".":{},"f:seLinuxOptions":{"f:level":{}}},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-08-23 23:10:39 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {multus Update v1 2021-08-23 23:10:39 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}}} {kubelet Update v1 2021-08-23 23:10:41 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.87.72\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-7v6bv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-7v6bv,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-7v6bv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.149.248.24,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c58,c57,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-nz8m4,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-08-23 23:10:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-08-23 23:10:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-08-23 23:10:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-08-23 23:10:36 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.149.248.24,PodIP:172.30.87.72,StartTime:2021-08-23 23:10:36 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-08-23 23:10:40 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:6feb0ea7b0967367da66e8d58ba813fde32bdb92f63bfc21a9e170d211539db4,ContainerID:cri-o://22194a073661f883c43c4583eab95f86cb7152775abc3f876736fcc3bb61b20a,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.87.72,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Aug 23 23:10:41.427: INFO: ReplicaSet "test-deployment-7c65d4bcf9":
&ReplicaSet{ObjectMeta:{test-deployment-7c65d4bcf9  deployment-6739 /apis/apps/v1/namespaces/deployment-6739/replicasets/test-deployment-7c65d4bcf9 aed0da3c-2980-46e5-820e-c4a1d707a0f4 109287 4 2021-08-23 23:10:35 +0000 UTC <nil> <nil> map[pod-template-hash:7c65d4bcf9 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-deployment a48e8ff3-d58f-4d30-a75b-d5c2c2b52b82 0xc000f3e5e7 0xc000f3e5e8}] []  [{kube-controller-manager Update apps/v1 2021-08-23 23:10:41 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a48e8ff3-d58f-4d30-a75b-d5c2c2b52b82\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:command":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 7c65d4bcf9,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:7c65d4bcf9 test-deployment-static:true] map[] [] []  []} {[] [] [{test-deployment k8s.gcr.io/pause:3.2 [/bin/sleep 100000] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc000f3e668 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:4,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

Aug 23 23:10:41.447: INFO: ReplicaSet "test-deployment-8b6954bfb":
&ReplicaSet{ObjectMeta:{test-deployment-8b6954bfb  deployment-6739 /apis/apps/v1/namespaces/deployment-6739/replicasets/test-deployment-8b6954bfb 275e9024-34d7-4a57-8815-3dda8dd21716 109196 2 2021-08-23 23:10:28 +0000 UTC <nil> <nil> map[pod-template-hash:8b6954bfb test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment a48e8ff3-d58f-4d30-a75b-d5c2c2b52b82 0xc000f3e6d7 0xc000f3e6d8}] []  [{kube-controller-manager Update apps/v1 2021-08-23 23:10:34 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a48e8ff3-d58f-4d30-a75b-d5c2c2b52b82\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 8b6954bfb,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:8b6954bfb test-deployment-static:true] map[] [] []  []} {[] [] [{test-deployment k8s.gcr.io/e2e-test-images/agnhost:2.21 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc000f3e740 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}

[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 23:10:41.476: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-6739" for this suite.

• [SLOW TEST:14.899 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run the lifecycle of a Deployment [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] Deployment should run the lifecycle of a Deployment [Conformance]","total":311,"completed":240,"skipped":4036,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 23:10:41.631: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:41
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Aug 23 23:10:41.927: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e81913e1-ee8a-4ea3-b969-90c94e1a43d6" in namespace "projected-3896" to be "Succeeded or Failed"
Aug 23 23:10:41.973: INFO: Pod "downwardapi-volume-e81913e1-ee8a-4ea3-b969-90c94e1a43d6": Phase="Pending", Reason="", readiness=false. Elapsed: 45.500365ms
Aug 23 23:10:44.056: INFO: Pod "downwardapi-volume-e81913e1-ee8a-4ea3-b969-90c94e1a43d6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.128049129s
Aug 23 23:10:46.100: INFO: Pod "downwardapi-volume-e81913e1-ee8a-4ea3-b969-90c94e1a43d6": Phase="Pending", Reason="", readiness=false. Elapsed: 4.171873162s
Aug 23 23:10:48.120: INFO: Pod "downwardapi-volume-e81913e1-ee8a-4ea3-b969-90c94e1a43d6": Phase="Pending", Reason="", readiness=false. Elapsed: 6.192414145s
Aug 23 23:10:50.210: INFO: Pod "downwardapi-volume-e81913e1-ee8a-4ea3-b969-90c94e1a43d6": Phase="Pending", Reason="", readiness=false. Elapsed: 8.282622728s
Aug 23 23:10:52.300: INFO: Pod "downwardapi-volume-e81913e1-ee8a-4ea3-b969-90c94e1a43d6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.372317808s
STEP: Saw pod success
Aug 23 23:10:52.300: INFO: Pod "downwardapi-volume-e81913e1-ee8a-4ea3-b969-90c94e1a43d6" satisfied condition "Succeeded or Failed"
Aug 23 23:10:52.401: INFO: Trying to get logs from node 10.149.248.24 pod downwardapi-volume-e81913e1-ee8a-4ea3-b969-90c94e1a43d6 container client-container: <nil>
STEP: delete the pod
Aug 23 23:10:52.621: INFO: Waiting for pod downwardapi-volume-e81913e1-ee8a-4ea3-b969-90c94e1a43d6 to disappear
Aug 23 23:10:52.709: INFO: Pod downwardapi-volume-e81913e1-ee8a-4ea3-b969-90c94e1a43d6 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 23:10:52.710: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3896" for this suite.

• [SLOW TEST:11.451 seconds]
[sig-storage] Projected downwardAPI
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:35
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":241,"skipped":4050,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should receive events on concurrent watches in same order [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 23:10:53.082: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should receive events on concurrent watches in same order [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: starting a background goroutine to produce watch events
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 23:11:03.340: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-642" for this suite.

• [SLOW TEST:10.579 seconds]
[sig-api-machinery] Watchers
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should receive events on concurrent watches in same order [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should receive events on concurrent watches in same order [Conformance]","total":311,"completed":242,"skipped":4078,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates 
  should run the lifecycle of PodTemplates [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-node] PodTemplates
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 23:11:03.662: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename podtemplate
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run the lifecycle of PodTemplates [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[AfterEach] [sig-node] PodTemplates
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 23:11:05.620: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-6136" for this suite.
•{"msg":"PASSED [sig-node] PodTemplates should run the lifecycle of PodTemplates [Conformance]","total":311,"completed":243,"skipped":4121,"failed":0}
S
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 23:11:06.826: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test substitution in container's command
Aug 23 23:11:08.193: INFO: Waiting up to 5m0s for pod "var-expansion-29606180-4765-4200-b444-bb3a568cd9d9" in namespace "var-expansion-7988" to be "Succeeded or Failed"
Aug 23 23:11:08.418: INFO: Pod "var-expansion-29606180-4765-4200-b444-bb3a568cd9d9": Phase="Pending", Reason="", readiness=false. Elapsed: 225.691904ms
Aug 23 23:11:10.664: INFO: Pod "var-expansion-29606180-4765-4200-b444-bb3a568cd9d9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.470929274s
Aug 23 23:11:12.865: INFO: Pod "var-expansion-29606180-4765-4200-b444-bb3a568cd9d9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.672004472s
Aug 23 23:11:15.032: INFO: Pod "var-expansion-29606180-4765-4200-b444-bb3a568cd9d9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.839258704s
STEP: Saw pod success
Aug 23 23:11:15.032: INFO: Pod "var-expansion-29606180-4765-4200-b444-bb3a568cd9d9" satisfied condition "Succeeded or Failed"
Aug 23 23:11:15.170: INFO: Trying to get logs from node 10.149.248.24 pod var-expansion-29606180-4765-4200-b444-bb3a568cd9d9 container dapi-container: <nil>
STEP: delete the pod
Aug 23 23:11:15.550: INFO: Waiting for pod var-expansion-29606180-4765-4200-b444-bb3a568cd9d9 to disappear
Aug 23 23:11:15.714: INFO: Pod var-expansion-29606180-4765-4200-b444-bb3a568cd9d9 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 23:11:15.714: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-7988" for this suite.

• [SLOW TEST:9.560 seconds]
[k8s.io] Variable Expansion
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Variable Expansion should allow substituting values in a container's command [NodeConformance] [Conformance]","total":311,"completed":244,"skipped":4122,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 23:11:16.386: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir 0777 on tmpfs
Aug 23 23:11:17.313: INFO: Waiting up to 5m0s for pod "pod-78c5ecf5-1d9a-4c50-a0ea-51aa259147c5" in namespace "emptydir-2782" to be "Succeeded or Failed"
Aug 23 23:11:17.450: INFO: Pod "pod-78c5ecf5-1d9a-4c50-a0ea-51aa259147c5": Phase="Pending", Reason="", readiness=false. Elapsed: 136.905302ms
Aug 23 23:11:19.566: INFO: Pod "pod-78c5ecf5-1d9a-4c50-a0ea-51aa259147c5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.252428501s
Aug 23 23:11:21.688: INFO: Pod "pod-78c5ecf5-1d9a-4c50-a0ea-51aa259147c5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.374417879s
Aug 23 23:11:23.826: INFO: Pod "pod-78c5ecf5-1d9a-4c50-a0ea-51aa259147c5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.512702264s
STEP: Saw pod success
Aug 23 23:11:23.826: INFO: Pod "pod-78c5ecf5-1d9a-4c50-a0ea-51aa259147c5" satisfied condition "Succeeded or Failed"
Aug 23 23:11:23.939: INFO: Trying to get logs from node 10.149.248.24 pod pod-78c5ecf5-1d9a-4c50-a0ea-51aa259147c5 container test-container: <nil>
STEP: delete the pod
Aug 23 23:11:24.264: INFO: Waiting for pod pod-78c5ecf5-1d9a-4c50-a0ea-51aa259147c5 to disappear
Aug 23 23:11:24.404: INFO: Pod pod-78c5ecf5-1d9a-4c50-a0ea-51aa259147c5 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 23:11:24.404: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2782" for this suite.

• [SLOW TEST:8.487 seconds]
[sig-storage] EmptyDir volumes
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:45
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":245,"skipped":4133,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-network] IngressClass API 
   should support creating IngressClass API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] IngressClass API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 23:11:24.874: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename ingressclass
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] IngressClass API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/ingressclass.go:148
[It]  should support creating IngressClass API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: getting /apis
STEP: getting /apis/networking.k8s.io
STEP: getting /apis/networking.k8s.iov1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Aug 23 23:11:26.706: INFO: starting watch
STEP: patching
STEP: updating
Aug 23 23:11:27.186: INFO: waiting for watch events with expected annotations
Aug 23 23:11:27.186: INFO: saw patched and updated annotations
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-network] IngressClass API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 23:11:27.853: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ingressclass-3098" for this suite.
•{"msg":"PASSED [sig-network] IngressClass API  should support creating IngressClass API operations [Conformance]","total":311,"completed":246,"skipped":4147,"failed":0}
SSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a mutating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 23:11:28.625: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Aug 23 23:11:33.054: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63765357092, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63765357092, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63765357092, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63765357092, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 23 23:11:35.247: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63765357092, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63765357092, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63765357092, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63765357092, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 23 23:11:37.245: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63765357092, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63765357092, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63765357092, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63765357092, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Aug 23 23:11:40.326: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a mutating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a mutating webhook configuration
STEP: Updating a mutating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that should not be mutated
STEP: Patching a mutating webhook configuration's rules to include the create operation
STEP: Creating a configMap that should be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 23:11:41.744: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-915" for this suite.
STEP: Destroying namespace "webhook-915-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

• [SLOW TEST:14.643 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  patching/updating a mutating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a mutating webhook should work [Conformance]","total":311,"completed":247,"skipped":4151,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 23:11:43.268: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:41
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Aug 23 23:11:44.434: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a27d8640-0480-4d33-9733-5555f0671efe" in namespace "downward-api-2305" to be "Succeeded or Failed"
Aug 23 23:11:44.660: INFO: Pod "downwardapi-volume-a27d8640-0480-4d33-9733-5555f0671efe": Phase="Pending", Reason="", readiness=false. Elapsed: 225.911862ms
Aug 23 23:11:46.842: INFO: Pod "downwardapi-volume-a27d8640-0480-4d33-9733-5555f0671efe": Phase="Pending", Reason="", readiness=false. Elapsed: 2.407446113s
Aug 23 23:11:48.994: INFO: Pod "downwardapi-volume-a27d8640-0480-4d33-9733-5555f0671efe": Phase="Pending", Reason="", readiness=false. Elapsed: 4.559604569s
Aug 23 23:11:51.070: INFO: Pod "downwardapi-volume-a27d8640-0480-4d33-9733-5555f0671efe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.635662041s
STEP: Saw pod success
Aug 23 23:11:51.070: INFO: Pod "downwardapi-volume-a27d8640-0480-4d33-9733-5555f0671efe" satisfied condition "Succeeded or Failed"
Aug 23 23:11:51.131: INFO: Trying to get logs from node 10.149.248.24 pod downwardapi-volume-a27d8640-0480-4d33-9733-5555f0671efe container client-container: <nil>
STEP: delete the pod
Aug 23 23:11:51.338: INFO: Waiting for pod downwardapi-volume-a27d8640-0480-4d33-9733-5555f0671efe to disappear
Aug 23 23:11:51.421: INFO: Pod downwardapi-volume-a27d8640-0480-4d33-9733-5555f0671efe no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 23:11:51.422: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2305" for this suite.

• [SLOW TEST:8.408 seconds]
[sig-storage] Downward API volume
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:36
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","total":311,"completed":248,"skipped":4194,"failed":0}
SSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 23:11:51.678: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:52
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Aug 23 23:12:00.575: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Aug 23 23:12:00.611: INFO: Pod pod-with-poststart-http-hook still exists
Aug 23 23:12:02.611: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Aug 23 23:12:02.681: INFO: Pod pod-with-poststart-http-hook still exists
Aug 23 23:12:04.611: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Aug 23 23:12:04.632: INFO: Pod pod-with-poststart-http-hook still exists
Aug 23 23:12:06.611: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Aug 23 23:12:06.694: INFO: Pod pod-with-poststart-http-hook still exists
Aug 23 23:12:08.611: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Aug 23 23:12:08.796: INFO: Pod pod-with-poststart-http-hook still exists
Aug 23 23:12:10.611: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Aug 23 23:12:10.764: INFO: Pod pod-with-poststart-http-hook still exists
Aug 23 23:12:12.611: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Aug 23 23:12:12.693: INFO: Pod pod-with-poststart-http-hook still exists
Aug 23 23:12:14.611: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Aug 23 23:12:14.731: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 23:12:14.731: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-4308" for this suite.

• [SLOW TEST:23.765 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:43
    should execute poststart http hook properly [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart http hook properly [NodeConformance] [Conformance]","total":311,"completed":249,"skipped":4207,"failed":0}
SSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 23:12:15.445: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:53
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating pod busybox-22db5a9d-244b-49de-8880-ec31ed22f3fd in namespace container-probe-7289
Aug 23 23:12:20.410: INFO: Started pod busybox-22db5a9d-244b-49de-8880-ec31ed22f3fd in namespace container-probe-7289
STEP: checking the pod's current state and verifying that restartCount is present
Aug 23 23:12:20.521: INFO: Initial restart count of pod busybox-22db5a9d-244b-49de-8880-ec31ed22f3fd is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 23:16:21.095: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-7289" for this suite.

• [SLOW TEST:246.772 seconds]
[k8s.io] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Probing container should *not* be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","total":311,"completed":250,"skipped":4215,"failed":0}
SSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount projected service account token [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 23:16:22.218: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount projected service account token [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test service account token: 
Aug 23 23:16:23.040: INFO: Waiting up to 5m0s for pod "test-pod-226d8c5f-9552-42e2-89f4-f741a0ee9981" in namespace "svcaccounts-339" to be "Succeeded or Failed"
Aug 23 23:16:23.203: INFO: Pod "test-pod-226d8c5f-9552-42e2-89f4-f741a0ee9981": Phase="Pending", Reason="", readiness=false. Elapsed: 162.833848ms
Aug 23 23:16:25.370: INFO: Pod "test-pod-226d8c5f-9552-42e2-89f4-f741a0ee9981": Phase="Pending", Reason="", readiness=false. Elapsed: 2.329448848s
Aug 23 23:16:27.534: INFO: Pod "test-pod-226d8c5f-9552-42e2-89f4-f741a0ee9981": Phase="Pending", Reason="", readiness=false. Elapsed: 4.493962288s
Aug 23 23:16:29.696: INFO: Pod "test-pod-226d8c5f-9552-42e2-89f4-f741a0ee9981": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.655413524s
STEP: Saw pod success
Aug 23 23:16:29.696: INFO: Pod "test-pod-226d8c5f-9552-42e2-89f4-f741a0ee9981" satisfied condition "Succeeded or Failed"
Aug 23 23:16:29.834: INFO: Trying to get logs from node 10.149.248.24 pod test-pod-226d8c5f-9552-42e2-89f4-f741a0ee9981 container agnhost-container: <nil>
STEP: delete the pod
Aug 23 23:16:30.405: INFO: Waiting for pod test-pod-226d8c5f-9552-42e2-89f4-f741a0ee9981 to disappear
Aug 23 23:16:30.533: INFO: Pod test-pod-226d8c5f-9552-42e2-89f4-f741a0ee9981 no longer exists
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 23:16:30.533: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-339" for this suite.

• [SLOW TEST:8.994 seconds]
[sig-auth] ServiceAccounts
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:23
  should mount projected service account token [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-auth] ServiceAccounts should mount projected service account token [Conformance]","total":311,"completed":251,"skipped":4218,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 23:16:31.215: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Aug 23 23:16:31.796: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
STEP: Creating secret with name s-test-opt-del-a8b25fa7-09fe-46de-9060-882edb01c134
STEP: Creating secret with name s-test-opt-upd-28848602-7ed1-4700-9aad-9b93aff54560
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-a8b25fa7-09fe-46de-9060-882edb01c134
STEP: Updating secret s-test-opt-upd-28848602-7ed1-4700-9aad-9b93aff54560
STEP: Creating secret with name s-test-opt-create-d3916bd2-6177-4d85-808b-f2c257e01044
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 23:17:42.608: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5411" for this suite.

• [SLOW TEST:71.839 seconds]
[sig-storage] Secrets
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:36
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Secrets optional updates should be reflected in volume [NodeConformance] [Conformance]","total":311,"completed":252,"skipped":4247,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 23:17:43.054: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete RS created by deployment when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for all rs to be garbage collected
STEP: Gathering metrics
Aug 23 23:17:44.228: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 23:17:44.229: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
W0823 23:17:44.228858      25 metrics_grabber.go:98] Can't find kube-scheduler pod. Grabbing metrics from kube-scheduler is disabled.
W0823 23:17:44.228940      25 metrics_grabber.go:102] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
W0823 23:17:44.228951      25 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
STEP: Destroying namespace "gc-4497" for this suite.
•{"msg":"PASSED [sig-api-machinery] Garbage collector should delete RS created by deployment when not orphaning [Conformance]","total":311,"completed":253,"skipped":4261,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 23:17:44.384: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward api env vars
Aug 23 23:17:44.640: INFO: Waiting up to 5m0s for pod "downward-api-c3764b85-ccea-4b18-977d-7a5b6b514cb3" in namespace "downward-api-7270" to be "Succeeded or Failed"
Aug 23 23:17:44.666: INFO: Pod "downward-api-c3764b85-ccea-4b18-977d-7a5b6b514cb3": Phase="Pending", Reason="", readiness=false. Elapsed: 26.130804ms
Aug 23 23:17:46.684: INFO: Pod "downward-api-c3764b85-ccea-4b18-977d-7a5b6b514cb3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.043705659s
Aug 23 23:17:48.770: INFO: Pod "downward-api-c3764b85-ccea-4b18-977d-7a5b6b514cb3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.129663011s
STEP: Saw pod success
Aug 23 23:17:48.770: INFO: Pod "downward-api-c3764b85-ccea-4b18-977d-7a5b6b514cb3" satisfied condition "Succeeded or Failed"
Aug 23 23:17:48.861: INFO: Trying to get logs from node 10.149.248.24 pod downward-api-c3764b85-ccea-4b18-977d-7a5b6b514cb3 container dapi-container: <nil>
STEP: delete the pod
Aug 23 23:17:49.131: INFO: Waiting for pod downward-api-c3764b85-ccea-4b18-977d-7a5b6b514cb3 to disappear
Aug 23 23:17:49.292: INFO: Pod downward-api-c3764b85-ccea-4b18-977d-7a5b6b514cb3 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 23:17:49.292: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7270" for this suite.

• [SLOW TEST:5.609 seconds]
[sig-node] Downward API
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:34
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-node] Downward API should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]","total":311,"completed":254,"skipped":4298,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 23:17:49.997: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:92
Aug 23 23:17:50.641: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Aug 23 23:17:51.559: INFO: Waiting for terminating namespaces to be deleted...
Aug 23 23:17:52.112: INFO: 
Logging pods the apiserver thinks is on node 10.149.248.24 before test
Aug 23 23:17:52.477: INFO: calico-node-9qqn4 from calico-system started at 2021-08-23 20:02:35 +0000 UTC (1 container statuses recorded)
Aug 23 23:17:52.477: INFO: 	Container calico-node ready: true, restart count 0
Aug 23 23:17:52.477: INFO: calico-typha-84b7fc6fc8-9fjkp from calico-system started at 2021-08-23 20:02:39 +0000 UTC (1 container statuses recorded)
Aug 23 23:17:52.477: INFO: 	Container calico-typha ready: true, restart count 0
Aug 23 23:17:52.477: INFO: ibm-keepalived-watcher-vrlwd from kube-system started at 2021-08-23 20:00:36 +0000 UTC (1 container statuses recorded)
Aug 23 23:17:52.477: INFO: 	Container keepalived-watcher ready: true, restart count 0
Aug 23 23:17:52.477: INFO: ibm-master-proxy-static-10.149.248.24 from kube-system started at 2021-08-23 20:00:33 +0000 UTC (2 container statuses recorded)
Aug 23 23:17:52.477: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Aug 23 23:17:52.477: INFO: 	Container pause ready: true, restart count 0
Aug 23 23:17:52.477: INFO: ibm-storage-watcher-5c4d7b6cd6-fbwxc from kube-system started at 2021-08-23 22:12:56 +0000 UTC (1 container statuses recorded)
Aug 23 23:17:52.477: INFO: 	Container ibm-storage-watcher-container ready: true, restart count 0
Aug 23 23:17:52.477: INFO: ibmcloud-block-storage-driver-jgx4r from kube-system started at 2021-08-23 20:00:42 +0000 UTC (1 container statuses recorded)
Aug 23 23:17:52.477: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Aug 23 23:17:52.477: INFO: tuned-7sb7t from openshift-cluster-node-tuning-operator started at 2021-08-23 20:05:27 +0000 UTC (1 container statuses recorded)
Aug 23 23:17:52.477: INFO: 	Container tuned ready: true, restart count 0
Aug 23 23:17:52.477: INFO: csi-snapshot-controller-566544547f-88h45 from openshift-cluster-storage-operator started at 2021-08-23 22:12:56 +0000 UTC (1 container statuses recorded)
Aug 23 23:17:52.477: INFO: 	Container snapshot-controller ready: true, restart count 0
Aug 23 23:17:52.477: INFO: csi-snapshot-webhook-585d4946dc-922hg from openshift-cluster-storage-operator started at 2021-08-23 22:12:56 +0000 UTC (1 container statuses recorded)
Aug 23 23:17:52.477: INFO: 	Container webhook ready: true, restart count 0
Aug 23 23:17:52.477: INFO: console-operator-c7f9f8687-fsq8q from openshift-console-operator started at 2021-08-23 22:12:56 +0000 UTC (1 container statuses recorded)
Aug 23 23:17:52.477: INFO: 	Container console-operator ready: true, restart count 0
Aug 23 23:17:52.477: INFO: console-dc8c686bb-cjdbt from openshift-console started at 2021-08-23 22:12:56 +0000 UTC (1 container statuses recorded)
Aug 23 23:17:52.477: INFO: 	Container console ready: true, restart count 0
Aug 23 23:17:52.477: INFO: dns-default-mgbp7 from openshift-dns started at 2021-08-23 20:05:27 +0000 UTC (3 container statuses recorded)
Aug 23 23:17:52.477: INFO: 	Container dns ready: true, restart count 0
Aug 23 23:17:52.477: INFO: 	Container dns-node-resolver ready: true, restart count 0
Aug 23 23:17:52.477: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 23 23:17:52.477: INFO: node-ca-dl95t from openshift-image-registry started at 2021-08-23 20:04:29 +0000 UTC (1 container statuses recorded)
Aug 23 23:17:52.477: INFO: 	Container node-ca ready: true, restart count 0
Aug 23 23:17:52.477: INFO: ingress-canary-8x7dj from openshift-ingress-canary started at 2021-08-23 22:13:29 +0000 UTC (1 container statuses recorded)
Aug 23 23:17:52.477: INFO: 	Container hello-openshift-canary ready: true, restart count 0
Aug 23 23:17:52.477: INFO: ingress-operator-5d97777898-lmhdt from openshift-ingress-operator started at 2021-08-23 22:12:56 +0000 UTC (2 container statuses recorded)
Aug 23 23:17:52.477: INFO: 	Container ingress-operator ready: true, restart count 0
Aug 23 23:17:52.477: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 23 23:17:52.477: INFO: openshift-kube-proxy-wn8f2 from openshift-kube-proxy started at 2021-08-23 20:01:25 +0000 UTC (2 container statuses recorded)
Aug 23 23:17:52.477: INFO: 	Container kube-proxy ready: true, restart count 0
Aug 23 23:17:52.477: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 23 23:17:52.477: INFO: kube-storage-version-migrator-operator-6d55bddccb-7zfw2 from openshift-kube-storage-version-migrator-operator started at 2021-08-23 22:12:56 +0000 UTC (1 container statuses recorded)
Aug 23 23:17:52.477: INFO: 	Container kube-storage-version-migrator-operator ready: true, restart count 0
Aug 23 23:17:52.477: INFO: migrator-f58676cd4-rpd8r from openshift-kube-storage-version-migrator started at 2021-08-23 22:12:56 +0000 UTC (1 container statuses recorded)
Aug 23 23:17:52.477: INFO: 	Container migrator ready: true, restart count 0
Aug 23 23:17:52.477: INFO: marketplace-operator-7fd8fdcc5b-hbf84 from openshift-marketplace started at 2021-08-23 22:12:56 +0000 UTC (1 container statuses recorded)
Aug 23 23:17:52.477: INFO: 	Container marketplace-operator ready: true, restart count 0
Aug 23 23:17:52.477: INFO: alertmanager-main-0 from openshift-monitoring started at 2021-08-23 22:13:17 +0000 UTC (5 container statuses recorded)
Aug 23 23:17:52.477: INFO: 	Container alertmanager ready: true, restart count 0
Aug 23 23:17:52.477: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Aug 23 23:17:52.477: INFO: 	Container config-reloader ready: true, restart count 0
Aug 23 23:17:52.477: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 23 23:17:52.477: INFO: 	Container prom-label-proxy ready: true, restart count 0
Aug 23 23:17:52.478: INFO: kube-state-metrics-d956df775-lqfms from openshift-monitoring started at 2021-08-23 22:12:56 +0000 UTC (3 container statuses recorded)
Aug 23 23:17:52.478: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Aug 23 23:17:52.478: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Aug 23 23:17:52.478: INFO: 	Container kube-state-metrics ready: true, restart count 0
Aug 23 23:17:52.478: INFO: node-exporter-b5rwn from openshift-monitoring started at 2021-08-23 20:03:19 +0000 UTC (2 container statuses recorded)
Aug 23 23:17:52.478: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 23 23:17:52.478: INFO: 	Container node-exporter ready: true, restart count 0
Aug 23 23:17:52.478: INFO: openshift-state-metrics-74b58f578c-8b6fd from openshift-monitoring started at 2021-08-23 22:12:56 +0000 UTC (3 container statuses recorded)
Aug 23 23:17:52.478: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Aug 23 23:17:52.478: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Aug 23 23:17:52.478: INFO: 	Container openshift-state-metrics ready: true, restart count 0
Aug 23 23:17:52.478: INFO: prometheus-adapter-6685ccb7f-2mrfl from openshift-monitoring started at 2021-08-23 22:12:56 +0000 UTC (1 container statuses recorded)
Aug 23 23:17:52.478: INFO: 	Container prometheus-adapter ready: true, restart count 0
Aug 23 23:17:52.478: INFO: telemeter-client-9d6d6f95f-r7zwc from openshift-monitoring started at 2021-08-23 22:12:56 +0000 UTC (3 container statuses recorded)
Aug 23 23:17:52.478: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 23 23:17:52.478: INFO: 	Container reload ready: true, restart count 0
Aug 23 23:17:52.478: INFO: 	Container telemeter-client ready: true, restart count 0
Aug 23 23:17:52.478: INFO: multus-48kkt from openshift-multus started at 2021-08-23 20:01:12 +0000 UTC (1 container statuses recorded)
Aug 23 23:17:52.478: INFO: 	Container kube-multus ready: true, restart count 0
Aug 23 23:17:52.478: INFO: multus-admission-controller-ctsqt from openshift-multus started at 2021-08-23 22:13:43 +0000 UTC (2 container statuses recorded)
Aug 23 23:17:52.478: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 23 23:17:52.478: INFO: 	Container multus-admission-controller ready: true, restart count 0
Aug 23 23:17:52.478: INFO: network-metrics-daemon-cqr85 from openshift-multus started at 2021-08-23 20:01:15 +0000 UTC (2 container statuses recorded)
Aug 23 23:17:52.478: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 23 23:17:52.478: INFO: 	Container network-metrics-daemon ready: true, restart count 0
Aug 23 23:17:52.478: INFO: network-check-target-447wc from openshift-network-diagnostics started at 2021-08-23 20:01:33 +0000 UTC (1 container statuses recorded)
Aug 23 23:17:52.478: INFO: 	Container network-check-target-container ready: true, restart count 0
Aug 23 23:17:52.478: INFO: push-gateway-59f6bc56d4-62594 from openshift-roks-metrics started at 2021-08-23 22:12:56 +0000 UTC (1 container statuses recorded)
Aug 23 23:17:52.478: INFO: 	Container push-gateway ready: false, restart count 0
Aug 23 23:17:52.478: INFO: service-ca-operator-64b7cc7c85-sv5w8 from openshift-service-ca-operator started at 2021-08-23 22:12:56 +0000 UTC (1 container statuses recorded)
Aug 23 23:17:52.478: INFO: 	Container service-ca-operator ready: true, restart count 0
Aug 23 23:17:52.478: INFO: pod-secrets-8e175e6d-2305-4a09-8190-def778268e72 from secrets-5411 started at 2021-08-23 23:16:31 +0000 UTC (3 container statuses recorded)
Aug 23 23:17:52.478: INFO: 	Container creates-volume-test ready: false, restart count 0
Aug 23 23:17:52.478: INFO: 	Container dels-volume-test ready: false, restart count 0
Aug 23 23:17:52.478: INFO: 	Container upds-volume-test ready: false, restart count 0
Aug 23 23:17:52.478: INFO: sonobuoy-systemd-logs-daemon-set-9034e69b695e4ee2-s28vf from sonobuoy started at 2021-08-23 21:37:23 +0000 UTC (2 container statuses recorded)
Aug 23 23:17:52.478: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 23 23:17:52.478: INFO: 	Container systemd-logs ready: true, restart count 0
Aug 23 23:17:52.478: INFO: 
Logging pods the apiserver thinks is on node 10.149.248.25 before test
Aug 23 23:17:52.812: INFO: calico-node-4m7h2 from calico-system started at 2021-08-23 20:02:35 +0000 UTC (1 container statuses recorded)
Aug 23 23:17:52.813: INFO: 	Container calico-node ready: true, restart count 0
Aug 23 23:17:52.813: INFO: calico-typha-84b7fc6fc8-tf4pd from calico-system started at 2021-08-23 20:02:33 +0000 UTC (1 container statuses recorded)
Aug 23 23:17:52.813: INFO: 	Container calico-typha ready: true, restart count 0
Aug 23 23:17:52.813: INFO: ibm-cloud-provider-ip-169-55-101-126-6bb454748-g6kdf from ibm-system started at 2021-08-23 20:10:53 +0000 UTC (1 container statuses recorded)
Aug 23 23:17:52.813: INFO: 	Container ibm-cloud-provider-ip-169-55-101-126 ready: true, restart count 0
Aug 23 23:17:52.813: INFO: ibm-file-plugin-68fbcccc88-bgbn9 from kube-system started at 2021-08-23 20:03:01 +0000 UTC (1 container statuses recorded)
Aug 23 23:17:52.813: INFO: 	Container ibm-file-plugin-container ready: true, restart count 0
Aug 23 23:17:52.813: INFO: ibm-keepalived-watcher-c94bg from kube-system started at 2021-08-23 20:00:20 +0000 UTC (1 container statuses recorded)
Aug 23 23:17:52.813: INFO: 	Container keepalived-watcher ready: true, restart count 0
Aug 23 23:17:52.813: INFO: ibm-master-proxy-static-10.149.248.25 from kube-system started at 2021-08-23 20:00:16 +0000 UTC (2 container statuses recorded)
Aug 23 23:17:52.813: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Aug 23 23:17:52.813: INFO: 	Container pause ready: true, restart count 0
Aug 23 23:17:52.813: INFO: ibmcloud-block-storage-driver-zdvf4 from kube-system started at 2021-08-23 20:00:24 +0000 UTC (1 container statuses recorded)
Aug 23 23:17:52.813: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Aug 23 23:17:52.813: INFO: ibmcloud-block-storage-plugin-7d6d9649b-zdvgj from kube-system started at 2021-08-23 20:03:01 +0000 UTC (1 container statuses recorded)
Aug 23 23:17:52.813: INFO: 	Container ibmcloud-block-storage-plugin-container ready: true, restart count 0
Aug 23 23:17:52.813: INFO: vpn-db594b4f9-9mxgx from kube-system started at 2021-08-23 20:08:59 +0000 UTC (1 container statuses recorded)
Aug 23 23:17:52.813: INFO: 	Container vpn ready: true, restart count 0
Aug 23 23:17:52.813: INFO: tuned-bfbjn from openshift-cluster-node-tuning-operator started at 2021-08-23 20:05:28 +0000 UTC (1 container statuses recorded)
Aug 23 23:17:52.813: INFO: 	Container tuned ready: true, restart count 0
Aug 23 23:17:52.813: INFO: cluster-storage-operator-6c5c749558-97wht from openshift-cluster-storage-operator started at 2021-08-23 20:03:01 +0000 UTC (1 container statuses recorded)
Aug 23 23:17:52.813: INFO: 	Container cluster-storage-operator ready: true, restart count 1
Aug 23 23:17:52.813: INFO: csi-snapshot-controller-operator-645f4897d-2vvbp from openshift-cluster-storage-operator started at 2021-08-23 20:03:01 +0000 UTC (1 container statuses recorded)
Aug 23 23:17:52.813: INFO: 	Container csi-snapshot-controller-operator ready: true, restart count 0
Aug 23 23:17:52.813: INFO: console-dc8c686bb-8xpf8 from openshift-console started at 2021-08-23 20:07:03 +0000 UTC (1 container statuses recorded)
Aug 23 23:17:52.813: INFO: 	Container console ready: true, restart count 0
Aug 23 23:17:52.813: INFO: downloads-6c96776f98-5w9bj from openshift-console started at 2021-08-23 20:03:01 +0000 UTC (1 container statuses recorded)
Aug 23 23:17:52.813: INFO: 	Container download-server ready: true, restart count 0
Aug 23 23:17:52.813: INFO: downloads-6c96776f98-nqhnf from openshift-console started at 2021-08-23 22:12:56 +0000 UTC (1 container statuses recorded)
Aug 23 23:17:52.813: INFO: 	Container download-server ready: true, restart count 0
Aug 23 23:17:52.813: INFO: dns-operator-6dbb54f776-8k9pq from openshift-dns-operator started at 2021-08-23 20:03:01 +0000 UTC (2 container statuses recorded)
Aug 23 23:17:52.813: INFO: 	Container dns-operator ready: true, restart count 0
Aug 23 23:17:52.813: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 23 23:17:52.813: INFO: dns-default-mxjt9 from openshift-dns started at 2021-08-23 20:05:27 +0000 UTC (3 container statuses recorded)
Aug 23 23:17:52.813: INFO: 	Container dns ready: true, restart count 0
Aug 23 23:17:52.813: INFO: 	Container dns-node-resolver ready: true, restart count 0
Aug 23 23:17:52.813: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 23 23:17:52.813: INFO: cluster-image-registry-operator-85978c675-rdng6 from openshift-image-registry started at 2021-08-23 20:03:01 +0000 UTC (1 container statuses recorded)
Aug 23 23:17:52.813: INFO: 	Container cluster-image-registry-operator ready: true, restart count 0
Aug 23 23:17:52.813: INFO: node-ca-czldz from openshift-image-registry started at 2021-08-23 20:04:29 +0000 UTC (1 container statuses recorded)
Aug 23 23:17:52.813: INFO: 	Container node-ca ready: true, restart count 0
Aug 23 23:17:52.813: INFO: ingress-canary-9skj4 from openshift-ingress-canary started at 2021-08-23 20:05:35 +0000 UTC (1 container statuses recorded)
Aug 23 23:17:52.813: INFO: 	Container hello-openshift-canary ready: true, restart count 0
Aug 23 23:17:52.813: INFO: router-default-678bfcd875-g2q6m from openshift-ingress started at 2021-08-23 20:05:36 +0000 UTC (1 container statuses recorded)
Aug 23 23:17:52.813: INFO: 	Container router ready: true, restart count 0
Aug 23 23:17:52.813: INFO: openshift-kube-proxy-sbzjn from openshift-kube-proxy started at 2021-08-23 20:01:25 +0000 UTC (2 container statuses recorded)
Aug 23 23:17:52.813: INFO: 	Container kube-proxy ready: true, restart count 0
Aug 23 23:17:52.813: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 23 23:17:52.813: INFO: redhat-marketplace-hvcn5 from openshift-marketplace started at 2021-08-23 20:04:56 +0000 UTC (1 container statuses recorded)
Aug 23 23:17:52.813: INFO: 	Container registry-server ready: true, restart count 0
Aug 23 23:17:52.813: INFO: alertmanager-main-1 from openshift-monitoring started at 2021-08-23 20:06:03 +0000 UTC (5 container statuses recorded)
Aug 23 23:17:52.813: INFO: 	Container alertmanager ready: true, restart count 0
Aug 23 23:17:52.813: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Aug 23 23:17:52.813: INFO: 	Container config-reloader ready: true, restart count 0
Aug 23 23:17:52.813: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 23 23:17:52.813: INFO: 	Container prom-label-proxy ready: true, restart count 0
Aug 23 23:17:52.813: INFO: cluster-monitoring-operator-c69d85486-m52jl from openshift-monitoring started at 2021-08-23 20:03:01 +0000 UTC (2 container statuses recorded)
Aug 23 23:17:52.813: INFO: 	Container cluster-monitoring-operator ready: true, restart count 0
Aug 23 23:17:52.813: INFO: 	Container kube-rbac-proxy ready: true, restart count 4
Aug 23 23:17:52.813: INFO: node-exporter-fsg2g from openshift-monitoring started at 2021-08-23 20:03:19 +0000 UTC (2 container statuses recorded)
Aug 23 23:17:52.813: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 23 23:17:52.813: INFO: 	Container node-exporter ready: true, restart count 0
Aug 23 23:17:52.813: INFO: prometheus-adapter-6685ccb7f-gxd72 from openshift-monitoring started at 2021-08-23 20:06:45 +0000 UTC (1 container statuses recorded)
Aug 23 23:17:52.813: INFO: 	Container prometheus-adapter ready: true, restart count 0
Aug 23 23:17:52.813: INFO: prometheus-k8s-1 from openshift-monitoring started at 2021-08-23 20:06:14 +0000 UTC (7 container statuses recorded)
Aug 23 23:17:52.813: INFO: 	Container config-reloader ready: true, restart count 0
Aug 23 23:17:52.813: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 23 23:17:52.813: INFO: 	Container kube-rbac-proxy-thanos ready: true, restart count 0
Aug 23 23:17:52.813: INFO: 	Container prom-label-proxy ready: true, restart count 0
Aug 23 23:17:52.813: INFO: 	Container prometheus ready: true, restart count 1
Aug 23 23:17:52.813: INFO: 	Container prometheus-proxy ready: true, restart count 0
Aug 23 23:17:52.813: INFO: 	Container thanos-sidecar ready: true, restart count 0
Aug 23 23:17:52.813: INFO: thanos-querier-97cd894c4-z4jqj from openshift-monitoring started at 2021-08-23 20:06:10 +0000 UTC (5 container statuses recorded)
Aug 23 23:17:52.813: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 23 23:17:52.813: INFO: 	Container kube-rbac-proxy-rules ready: true, restart count 0
Aug 23 23:17:52.813: INFO: 	Container oauth-proxy ready: true, restart count 0
Aug 23 23:17:52.813: INFO: 	Container prom-label-proxy ready: true, restart count 0
Aug 23 23:17:52.813: INFO: 	Container thanos-query ready: true, restart count 0
Aug 23 23:17:52.813: INFO: multus-2p958 from openshift-multus started at 2021-08-23 20:01:12 +0000 UTC (1 container statuses recorded)
Aug 23 23:17:52.813: INFO: 	Container kube-multus ready: true, restart count 0
Aug 23 23:17:52.813: INFO: multus-admission-controller-ww7jt from openshift-multus started at 2021-08-23 20:02:56 +0000 UTC (2 container statuses recorded)
Aug 23 23:17:52.813: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 23 23:17:52.813: INFO: 	Container multus-admission-controller ready: true, restart count 0
Aug 23 23:17:52.813: INFO: network-metrics-daemon-6bmnm from openshift-multus started at 2021-08-23 20:01:15 +0000 UTC (2 container statuses recorded)
Aug 23 23:17:52.813: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 23 23:17:52.813: INFO: 	Container network-metrics-daemon ready: true, restart count 0
Aug 23 23:17:52.813: INFO: network-check-target-cjqqr from openshift-network-diagnostics started at 2021-08-23 20:01:33 +0000 UTC (1 container statuses recorded)
Aug 23 23:17:52.813: INFO: 	Container network-check-target-container ready: true, restart count 0
Aug 23 23:17:52.813: INFO: olm-operator-d5dc9548d-mwfkk from openshift-operator-lifecycle-manager started at 2021-08-23 20:03:01 +0000 UTC (1 container statuses recorded)
Aug 23 23:17:52.813: INFO: 	Container olm-operator ready: true, restart count 0
Aug 23 23:17:52.813: INFO: packageserver-6d5f99f754-5t8qd from openshift-operator-lifecycle-manager started at 2021-08-23 22:12:56 +0000 UTC (1 container statuses recorded)
Aug 23 23:17:52.813: INFO: 	Container packageserver ready: true, restart count 0
Aug 23 23:17:52.813: INFO: metrics-8fc5c5f56-lq4k6 from openshift-roks-metrics started at 2021-08-23 20:03:01 +0000 UTC (1 container statuses recorded)
Aug 23 23:17:52.813: INFO: 	Container metrics ready: true, restart count 3
Aug 23 23:17:52.813: INFO: sonobuoy-e2e-job-90ed16fed1444409 from sonobuoy started at 2021-08-23 21:37:22 +0000 UTC (2 container statuses recorded)
Aug 23 23:17:52.813: INFO: 	Container e2e ready: true, restart count 0
Aug 23 23:17:52.813: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 23 23:17:52.813: INFO: sonobuoy-systemd-logs-daemon-set-9034e69b695e4ee2-q8h6l from sonobuoy started at 2021-08-23 21:37:23 +0000 UTC (2 container statuses recorded)
Aug 23 23:17:52.813: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 23 23:17:52.813: INFO: 	Container systemd-logs ready: true, restart count 0
Aug 23 23:17:52.813: INFO: 
Logging pods the apiserver thinks is on node 10.149.248.9 before test
Aug 23 23:17:53.227: INFO: calico-kube-controllers-7dcbcc7c66-x65ng from calico-system started at 2021-08-23 20:02:55 +0000 UTC (1 container statuses recorded)
Aug 23 23:17:53.227: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Aug 23 23:17:53.228: INFO: calico-node-cgrcs from calico-system started at 2021-08-23 20:02:35 +0000 UTC (1 container statuses recorded)
Aug 23 23:17:53.228: INFO: 	Container calico-node ready: true, restart count 0
Aug 23 23:17:53.228: INFO: calico-typha-84b7fc6fc8-h6lnw from calico-system started at 2021-08-23 20:02:39 +0000 UTC (1 container statuses recorded)
Aug 23 23:17:53.228: INFO: 	Container calico-typha ready: true, restart count 0
Aug 23 23:17:53.228: INFO: ibm-cloud-provider-ip-169-55-101-126-6bb454748-t7nvk from ibm-system started at 2021-08-23 20:10:53 +0000 UTC (1 container statuses recorded)
Aug 23 23:17:53.228: INFO: 	Container ibm-cloud-provider-ip-169-55-101-126 ready: true, restart count 0
Aug 23 23:17:53.228: INFO: ibm-keepalived-watcher-rsh2g from kube-system started at 2021-08-23 20:00:08 +0000 UTC (1 container statuses recorded)
Aug 23 23:17:53.228: INFO: 	Container keepalived-watcher ready: true, restart count 0
Aug 23 23:17:53.228: INFO: ibm-master-proxy-static-10.149.248.9 from kube-system started at 2021-08-23 20:00:06 +0000 UTC (2 container statuses recorded)
Aug 23 23:17:53.228: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Aug 23 23:17:53.228: INFO: 	Container pause ready: true, restart count 0
Aug 23 23:17:53.228: INFO: ibmcloud-block-storage-driver-5fllx from kube-system started at 2021-08-23 20:00:13 +0000 UTC (1 container statuses recorded)
Aug 23 23:17:53.228: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Aug 23 23:17:53.228: INFO: cluster-node-tuning-operator-84b8576b47-qlh6k from openshift-cluster-node-tuning-operator started at 2021-08-23 22:12:56 +0000 UTC (1 container statuses recorded)
Aug 23 23:17:53.228: INFO: 	Container cluster-node-tuning-operator ready: true, restart count 0
Aug 23 23:17:53.228: INFO: tuned-m585x from openshift-cluster-node-tuning-operator started at 2021-08-23 20:05:28 +0000 UTC (1 container statuses recorded)
Aug 23 23:17:53.228: INFO: 	Container tuned ready: true, restart count 0
Aug 23 23:17:53.228: INFO: cluster-samples-operator-866dcfc6c4-ntqvw from openshift-cluster-samples-operator started at 2021-08-23 22:12:56 +0000 UTC (2 container statuses recorded)
Aug 23 23:17:53.228: INFO: 	Container cluster-samples-operator ready: true, restart count 0
Aug 23 23:17:53.228: INFO: 	Container cluster-samples-operator-watch ready: true, restart count 0
Aug 23 23:17:53.228: INFO: dns-default-4hmcv from openshift-dns started at 2021-08-23 20:05:27 +0000 UTC (3 container statuses recorded)
Aug 23 23:17:53.228: INFO: 	Container dns ready: true, restart count 0
Aug 23 23:17:53.228: INFO: 	Container dns-node-resolver ready: true, restart count 0
Aug 23 23:17:53.228: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 23 23:17:53.229: INFO: image-registry-555cc7f64c-qtjjg from openshift-image-registry started at 2021-08-23 22:12:56 +0000 UTC (1 container statuses recorded)
Aug 23 23:17:53.229: INFO: 	Container registry ready: true, restart count 0
Aug 23 23:17:53.229: INFO: node-ca-g989x from openshift-image-registry started at 2021-08-23 20:04:29 +0000 UTC (1 container statuses recorded)
Aug 23 23:17:53.229: INFO: 	Container node-ca ready: true, restart count 0
Aug 23 23:17:53.229: INFO: registry-pvc-permissions-dbktz from openshift-image-registry started at 2021-08-23 20:08:40 +0000 UTC (1 container statuses recorded)
Aug 23 23:17:53.229: INFO: 	Container pvc-permissions ready: false, restart count 0
Aug 23 23:17:53.229: INFO: ingress-canary-l5l42 from openshift-ingress-canary started at 2021-08-23 20:05:35 +0000 UTC (1 container statuses recorded)
Aug 23 23:17:53.229: INFO: 	Container hello-openshift-canary ready: true, restart count 0
Aug 23 23:17:53.229: INFO: router-default-678bfcd875-2f5hb from openshift-ingress started at 2021-08-23 20:05:36 +0000 UTC (1 container statuses recorded)
Aug 23 23:17:53.229: INFO: 	Container router ready: true, restart count 0
Aug 23 23:17:53.229: INFO: openshift-kube-proxy-wk9m8 from openshift-kube-proxy started at 2021-08-23 20:01:25 +0000 UTC (2 container statuses recorded)
Aug 23 23:17:53.229: INFO: 	Container kube-proxy ready: true, restart count 0
Aug 23 23:17:53.229: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 23 23:17:53.229: INFO: certified-operators-rlpm2 from openshift-marketplace started at 2021-08-23 20:04:55 +0000 UTC (1 container statuses recorded)
Aug 23 23:17:53.229: INFO: 	Container registry-server ready: true, restart count 0
Aug 23 23:17:53.229: INFO: community-operators-ztwk2 from openshift-marketplace started at 2021-08-23 20:04:56 +0000 UTC (1 container statuses recorded)
Aug 23 23:17:53.229: INFO: 	Container registry-server ready: true, restart count 0
Aug 23 23:17:53.229: INFO: redhat-operators-cpqsn from openshift-marketplace started at 2021-08-23 20:04:56 +0000 UTC (1 container statuses recorded)
Aug 23 23:17:53.229: INFO: 	Container registry-server ready: true, restart count 0
Aug 23 23:17:53.229: INFO: alertmanager-main-2 from openshift-monitoring started at 2021-08-23 20:06:04 +0000 UTC (5 container statuses recorded)
Aug 23 23:17:53.229: INFO: 	Container alertmanager ready: true, restart count 0
Aug 23 23:17:53.229: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Aug 23 23:17:53.229: INFO: 	Container config-reloader ready: true, restart count 0
Aug 23 23:17:53.229: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 23 23:17:53.229: INFO: 	Container prom-label-proxy ready: true, restart count 0
Aug 23 23:17:53.230: INFO: grafana-85d4bdb748-9g6gn from openshift-monitoring started at 2021-08-23 22:12:56 +0000 UTC (2 container statuses recorded)
Aug 23 23:17:53.230: INFO: 	Container grafana ready: true, restart count 0
Aug 23 23:17:53.230: INFO: 	Container grafana-proxy ready: true, restart count 0
Aug 23 23:17:53.230: INFO: node-exporter-kcjzv from openshift-monitoring started at 2021-08-23 20:03:19 +0000 UTC (2 container statuses recorded)
Aug 23 23:17:53.230: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 23 23:17:53.230: INFO: 	Container node-exporter ready: true, restart count 0
Aug 23 23:17:53.230: INFO: prometheus-k8s-0 from openshift-monitoring started at 2021-08-23 20:06:14 +0000 UTC (7 container statuses recorded)
Aug 23 23:17:53.230: INFO: 	Container config-reloader ready: true, restart count 0
Aug 23 23:17:53.230: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 23 23:17:53.230: INFO: 	Container kube-rbac-proxy-thanos ready: true, restart count 0
Aug 23 23:17:53.230: INFO: 	Container prom-label-proxy ready: true, restart count 0
Aug 23 23:17:53.230: INFO: 	Container prometheus ready: true, restart count 1
Aug 23 23:17:53.230: INFO: 	Container prometheus-proxy ready: true, restart count 0
Aug 23 23:17:53.230: INFO: 	Container thanos-sidecar ready: true, restart count 0
Aug 23 23:17:53.230: INFO: prometheus-operator-5699bb49dc-8q47w from openshift-monitoring started at 2021-08-23 20:06:44 +0000 UTC (2 container statuses recorded)
Aug 23 23:17:53.230: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 23 23:17:53.230: INFO: 	Container prometheus-operator ready: true, restart count 0
Aug 23 23:17:53.230: INFO: thanos-querier-97cd894c4-g4rfr from openshift-monitoring started at 2021-08-23 22:12:56 +0000 UTC (5 container statuses recorded)
Aug 23 23:17:53.230: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 23 23:17:53.230: INFO: 	Container kube-rbac-proxy-rules ready: true, restart count 0
Aug 23 23:17:53.230: INFO: 	Container oauth-proxy ready: true, restart count 0
Aug 23 23:17:53.230: INFO: 	Container prom-label-proxy ready: true, restart count 0
Aug 23 23:17:53.230: INFO: 	Container thanos-query ready: true, restart count 0
Aug 23 23:17:53.230: INFO: multus-admission-controller-hcwf4 from openshift-multus started at 2021-08-23 20:02:50 +0000 UTC (2 container statuses recorded)
Aug 23 23:17:53.230: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 23 23:17:53.231: INFO: 	Container multus-admission-controller ready: true, restart count 0
Aug 23 23:17:53.231: INFO: multus-z4wqr from openshift-multus started at 2021-08-23 20:01:12 +0000 UTC (1 container statuses recorded)
Aug 23 23:17:53.231: INFO: 	Container kube-multus ready: true, restart count 0
Aug 23 23:17:53.231: INFO: network-metrics-daemon-mbjhv from openshift-multus started at 2021-08-23 20:01:15 +0000 UTC (2 container statuses recorded)
Aug 23 23:17:53.231: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 23 23:17:53.231: INFO: 	Container network-metrics-daemon ready: true, restart count 0
Aug 23 23:17:53.231: INFO: network-check-source-6cd65cf589-6rbmz from openshift-network-diagnostics started at 2021-08-23 20:03:01 +0000 UTC (1 container statuses recorded)
Aug 23 23:17:53.231: INFO: 	Container check-endpoints ready: true, restart count 0
Aug 23 23:17:53.231: INFO: network-check-target-d6zj2 from openshift-network-diagnostics started at 2021-08-23 20:01:33 +0000 UTC (1 container statuses recorded)
Aug 23 23:17:53.231: INFO: 	Container network-check-target-container ready: true, restart count 0
Aug 23 23:17:53.231: INFO: network-operator-86dcc4df56-crx6q from openshift-network-operator started at 2021-08-23 20:00:18 +0000 UTC (1 container statuses recorded)
Aug 23 23:17:53.231: INFO: 	Container network-operator ready: true, restart count 0
Aug 23 23:17:53.231: INFO: catalog-operator-6f8ff86686-tg2d4 from openshift-operator-lifecycle-manager started at 2021-08-23 22:12:56 +0000 UTC (1 container statuses recorded)
Aug 23 23:17:53.231: INFO: 	Container catalog-operator ready: true, restart count 0
Aug 23 23:17:53.231: INFO: packageserver-6d5f99f754-9mfv7 from openshift-operator-lifecycle-manager started at 2021-08-23 20:04:35 +0000 UTC (1 container statuses recorded)
Aug 23 23:17:53.231: INFO: 	Container packageserver ready: true, restart count 0
Aug 23 23:17:53.231: INFO: service-ca-54b7675c-xmpw4 from openshift-service-ca started at 2021-08-23 22:12:56 +0000 UTC (1 container statuses recorded)
Aug 23 23:17:53.231: INFO: 	Container service-ca-controller ready: false, restart count 0
Aug 23 23:17:53.231: INFO: sonobuoy from sonobuoy started at 2021-08-23 21:37:12 +0000 UTC (1 container statuses recorded)
Aug 23 23:17:53.231: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Aug 23 23:17:53.231: INFO: sonobuoy-systemd-logs-daemon-set-9034e69b695e4ee2-zn2c4 from sonobuoy started at 2021-08-23 21:37:23 +0000 UTC (2 container statuses recorded)
Aug 23 23:17:53.231: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 23 23:17:53.231: INFO: 	Container systemd-logs ready: true, restart count 0
Aug 23 23:17:53.231: INFO: tigera-operator-667cd558f7-d78zp from tigera-operator started at 2021-08-23 20:00:19 +0000 UTC (1 container statuses recorded)
Aug 23 23:17:53.231: INFO: 	Container tigera-operator ready: true, restart count 4
[It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-767f6b5f-7012-4d58-9c2f-7c62fb4a1c11 95
STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled
STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 10.149.248.24 on the node which pod4 resides and expect not scheduled
STEP: removing the label kubernetes.io/e2e-767f6b5f-7012-4d58-9c2f-7c62fb4a1c11 off the node 10.149.248.24
STEP: verifying the node doesn't have the label kubernetes.io/e2e-767f6b5f-7012-4d58-9c2f-7c62fb4a1c11
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 23:23:11.175: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-7950" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:83

• [SLOW TEST:322.249 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]","total":311,"completed":255,"skipped":4353,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 23:23:12.245: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating secret with name secret-test-249d324c-9e31-4f07-bd50-0cf7ec837cc7
STEP: Creating a pod to test consume secrets
Aug 23 23:23:13.843: INFO: Waiting up to 5m0s for pod "pod-secrets-e21e27fb-e559-49d9-83b8-84f10e541cbd" in namespace "secrets-5565" to be "Succeeded or Failed"
Aug 23 23:23:14.911: INFO: Pod "pod-secrets-e21e27fb-e559-49d9-83b8-84f10e541cbd": Phase="Pending", Reason="", readiness=false. Elapsed: 1.067939175s
Aug 23 23:23:17.166: INFO: Pod "pod-secrets-e21e27fb-e559-49d9-83b8-84f10e541cbd": Phase="Pending", Reason="", readiness=false. Elapsed: 3.323377392s
Aug 23 23:23:19.392: INFO: Pod "pod-secrets-e21e27fb-e559-49d9-83b8-84f10e541cbd": Phase="Pending", Reason="", readiness=false. Elapsed: 5.549247813s
Aug 23 23:23:21.531: INFO: Pod "pod-secrets-e21e27fb-e559-49d9-83b8-84f10e541cbd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 7.68776685s
STEP: Saw pod success
Aug 23 23:23:21.531: INFO: Pod "pod-secrets-e21e27fb-e559-49d9-83b8-84f10e541cbd" satisfied condition "Succeeded or Failed"
Aug 23 23:23:21.656: INFO: Trying to get logs from node 10.149.248.24 pod pod-secrets-e21e27fb-e559-49d9-83b8-84f10e541cbd container secret-volume-test: <nil>
STEP: delete the pod
Aug 23 23:23:22.071: INFO: Waiting for pod pod-secrets-e21e27fb-e559-49d9-83b8-84f10e541cbd to disappear
Aug 23 23:23:22.269: INFO: Pod pod-secrets-e21e27fb-e559-49d9-83b8-84f10e541cbd no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 23:23:22.269: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5565" for this suite.

• [SLOW TEST:10.903 seconds]
[sig-storage] Secrets
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:36
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Secrets should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","total":311,"completed":256,"skipped":4382,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run pod 
  should create a pod from an image when restart is Never  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 23:23:23.149: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[BeforeEach] Kubectl run pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1520
[It] should create a pod from an image when restart is Never  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Aug 23 23:23:23.833: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585719795 --namespace=kubectl-9724 run e2e-test-httpd-pod --restart=Never --image=docker.io/library/httpd:2.4.38-alpine'
Aug 23 23:23:25.942: INFO: stderr: ""
Aug 23 23:23:25.942: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod was created
[AfterEach] Kubectl run pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1524
Aug 23 23:23:26.035: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585719795 --namespace=kubectl-9724 delete pods e2e-test-httpd-pod'
Aug 23 23:23:33.444: INFO: stderr: ""
Aug 23 23:23:33.444: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 23:23:33.444: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9724" for this suite.

• [SLOW TEST:10.433 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl run pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1517
    should create a pod from an image when restart is Never  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl run pod should create a pod from an image when restart is Never  [Conformance]","total":311,"completed":257,"skipped":4401,"failed":0}
S
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 23:23:33.582: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a ResourceQuota with terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a long running pod
STEP: Ensuring resource quota with not terminating scope captures the pod usage
STEP: Ensuring resource quota with terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a terminating pod
STEP: Ensuring resource quota with terminating scope captures the pod usage
STEP: Ensuring resource quota with not terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 23:23:51.615: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-3926" for this suite.

• [SLOW TEST:19.052 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with terminating scopes. [Conformance]","total":311,"completed":258,"skipped":4402,"failed":0}
SSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 23:23:52.634: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:92
Aug 23 23:23:53.903: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Aug 23 23:23:55.770: INFO: Waiting for terminating namespaces to be deleted...
Aug 23 23:23:58.446: INFO: 
Logging pods the apiserver thinks is on node 10.149.248.24 before test
Aug 23 23:24:01.082: INFO: calico-node-9qqn4 from calico-system started at 2021-08-23 20:02:35 +0000 UTC (1 container statuses recorded)
Aug 23 23:24:01.082: INFO: 	Container calico-node ready: true, restart count 0
Aug 23 23:24:01.082: INFO: calico-typha-84b7fc6fc8-9fjkp from calico-system started at 2021-08-23 20:02:39 +0000 UTC (1 container statuses recorded)
Aug 23 23:24:01.082: INFO: 	Container calico-typha ready: true, restart count 0
Aug 23 23:24:01.082: INFO: ibm-keepalived-watcher-vrlwd from kube-system started at 2021-08-23 20:00:36 +0000 UTC (1 container statuses recorded)
Aug 23 23:24:01.082: INFO: 	Container keepalived-watcher ready: true, restart count 0
Aug 23 23:24:01.082: INFO: ibm-master-proxy-static-10.149.248.24 from kube-system started at 2021-08-23 20:00:33 +0000 UTC (2 container statuses recorded)
Aug 23 23:24:01.082: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Aug 23 23:24:01.082: INFO: 	Container pause ready: true, restart count 0
Aug 23 23:24:01.082: INFO: ibm-storage-watcher-5c4d7b6cd6-fbwxc from kube-system started at 2021-08-23 22:12:56 +0000 UTC (1 container statuses recorded)
Aug 23 23:24:01.082: INFO: 	Container ibm-storage-watcher-container ready: true, restart count 0
Aug 23 23:24:01.082: INFO: ibmcloud-block-storage-driver-jgx4r from kube-system started at 2021-08-23 20:00:42 +0000 UTC (1 container statuses recorded)
Aug 23 23:24:01.082: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Aug 23 23:24:01.082: INFO: tuned-7sb7t from openshift-cluster-node-tuning-operator started at 2021-08-23 20:05:27 +0000 UTC (1 container statuses recorded)
Aug 23 23:24:01.082: INFO: 	Container tuned ready: true, restart count 0
Aug 23 23:24:01.082: INFO: csi-snapshot-controller-566544547f-88h45 from openshift-cluster-storage-operator started at 2021-08-23 22:12:56 +0000 UTC (1 container statuses recorded)
Aug 23 23:24:01.082: INFO: 	Container snapshot-controller ready: true, restart count 0
Aug 23 23:24:01.082: INFO: csi-snapshot-webhook-585d4946dc-922hg from openshift-cluster-storage-operator started at 2021-08-23 22:12:56 +0000 UTC (1 container statuses recorded)
Aug 23 23:24:01.082: INFO: 	Container webhook ready: true, restart count 0
Aug 23 23:24:01.082: INFO: console-operator-c7f9f8687-fsq8q from openshift-console-operator started at 2021-08-23 22:12:56 +0000 UTC (1 container statuses recorded)
Aug 23 23:24:01.083: INFO: 	Container console-operator ready: true, restart count 0
Aug 23 23:24:01.083: INFO: console-dc8c686bb-cjdbt from openshift-console started at 2021-08-23 22:12:56 +0000 UTC (1 container statuses recorded)
Aug 23 23:24:01.083: INFO: 	Container console ready: true, restart count 0
Aug 23 23:24:01.083: INFO: dns-default-mgbp7 from openshift-dns started at 2021-08-23 20:05:27 +0000 UTC (3 container statuses recorded)
Aug 23 23:24:01.083: INFO: 	Container dns ready: true, restart count 0
Aug 23 23:24:01.083: INFO: 	Container dns-node-resolver ready: true, restart count 0
Aug 23 23:24:01.083: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 23 23:24:01.083: INFO: node-ca-dl95t from openshift-image-registry started at 2021-08-23 20:04:29 +0000 UTC (1 container statuses recorded)
Aug 23 23:24:01.083: INFO: 	Container node-ca ready: true, restart count 0
Aug 23 23:24:01.083: INFO: ingress-canary-8x7dj from openshift-ingress-canary started at 2021-08-23 22:13:29 +0000 UTC (1 container statuses recorded)
Aug 23 23:24:01.083: INFO: 	Container hello-openshift-canary ready: true, restart count 0
Aug 23 23:24:01.083: INFO: ingress-operator-5d97777898-lmhdt from openshift-ingress-operator started at 2021-08-23 22:12:56 +0000 UTC (2 container statuses recorded)
Aug 23 23:24:01.083: INFO: 	Container ingress-operator ready: true, restart count 0
Aug 23 23:24:01.083: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 23 23:24:01.083: INFO: openshift-kube-proxy-wn8f2 from openshift-kube-proxy started at 2021-08-23 20:01:25 +0000 UTC (2 container statuses recorded)
Aug 23 23:24:01.083: INFO: 	Container kube-proxy ready: true, restart count 0
Aug 23 23:24:01.083: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 23 23:24:01.083: INFO: kube-storage-version-migrator-operator-6d55bddccb-7zfw2 from openshift-kube-storage-version-migrator-operator started at 2021-08-23 22:12:56 +0000 UTC (1 container statuses recorded)
Aug 23 23:24:01.083: INFO: 	Container kube-storage-version-migrator-operator ready: true, restart count 0
Aug 23 23:24:01.083: INFO: migrator-f58676cd4-rpd8r from openshift-kube-storage-version-migrator started at 2021-08-23 22:12:56 +0000 UTC (1 container statuses recorded)
Aug 23 23:24:01.083: INFO: 	Container migrator ready: true, restart count 0
Aug 23 23:24:01.083: INFO: marketplace-operator-7fd8fdcc5b-hbf84 from openshift-marketplace started at 2021-08-23 22:12:56 +0000 UTC (1 container statuses recorded)
Aug 23 23:24:01.083: INFO: 	Container marketplace-operator ready: true, restart count 0
Aug 23 23:24:01.083: INFO: alertmanager-main-0 from openshift-monitoring started at 2021-08-23 22:13:17 +0000 UTC (5 container statuses recorded)
Aug 23 23:24:01.083: INFO: 	Container alertmanager ready: true, restart count 0
Aug 23 23:24:01.083: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Aug 23 23:24:01.083: INFO: 	Container config-reloader ready: true, restart count 0
Aug 23 23:24:01.083: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 23 23:24:01.083: INFO: 	Container prom-label-proxy ready: true, restart count 0
Aug 23 23:24:01.083: INFO: kube-state-metrics-d956df775-lqfms from openshift-monitoring started at 2021-08-23 22:12:56 +0000 UTC (3 container statuses recorded)
Aug 23 23:24:01.083: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Aug 23 23:24:01.083: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Aug 23 23:24:01.083: INFO: 	Container kube-state-metrics ready: true, restart count 0
Aug 23 23:24:01.083: INFO: node-exporter-b5rwn from openshift-monitoring started at 2021-08-23 20:03:19 +0000 UTC (2 container statuses recorded)
Aug 23 23:24:01.083: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 23 23:24:01.083: INFO: 	Container node-exporter ready: true, restart count 0
Aug 23 23:24:01.083: INFO: openshift-state-metrics-74b58f578c-8b6fd from openshift-monitoring started at 2021-08-23 22:12:56 +0000 UTC (3 container statuses recorded)
Aug 23 23:24:01.083: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Aug 23 23:24:01.083: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Aug 23 23:24:01.083: INFO: 	Container openshift-state-metrics ready: true, restart count 0
Aug 23 23:24:01.083: INFO: prometheus-adapter-6685ccb7f-2mrfl from openshift-monitoring started at 2021-08-23 22:12:56 +0000 UTC (1 container statuses recorded)
Aug 23 23:24:01.083: INFO: 	Container prometheus-adapter ready: true, restart count 0
Aug 23 23:24:01.083: INFO: telemeter-client-9d6d6f95f-r7zwc from openshift-monitoring started at 2021-08-23 22:12:56 +0000 UTC (3 container statuses recorded)
Aug 23 23:24:01.083: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 23 23:24:01.083: INFO: 	Container reload ready: true, restart count 0
Aug 23 23:24:01.083: INFO: 	Container telemeter-client ready: true, restart count 0
Aug 23 23:24:01.083: INFO: multus-48kkt from openshift-multus started at 2021-08-23 20:01:12 +0000 UTC (1 container statuses recorded)
Aug 23 23:24:01.083: INFO: 	Container kube-multus ready: true, restart count 0
Aug 23 23:24:01.083: INFO: multus-admission-controller-ctsqt from openshift-multus started at 2021-08-23 22:13:43 +0000 UTC (2 container statuses recorded)
Aug 23 23:24:01.083: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 23 23:24:01.083: INFO: 	Container multus-admission-controller ready: true, restart count 0
Aug 23 23:24:01.083: INFO: network-metrics-daemon-cqr85 from openshift-multus started at 2021-08-23 20:01:15 +0000 UTC (2 container statuses recorded)
Aug 23 23:24:01.083: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 23 23:24:01.083: INFO: 	Container network-metrics-daemon ready: true, restart count 0
Aug 23 23:24:01.083: INFO: network-check-target-447wc from openshift-network-diagnostics started at 2021-08-23 20:01:33 +0000 UTC (1 container statuses recorded)
Aug 23 23:24:01.083: INFO: 	Container network-check-target-container ready: true, restart count 0
Aug 23 23:24:01.083: INFO: push-gateway-59f6bc56d4-62594 from openshift-roks-metrics started at 2021-08-23 22:12:56 +0000 UTC (1 container statuses recorded)
Aug 23 23:24:01.083: INFO: 	Container push-gateway ready: false, restart count 0
Aug 23 23:24:01.083: INFO: service-ca-operator-64b7cc7c85-sv5w8 from openshift-service-ca-operator started at 2021-08-23 22:12:56 +0000 UTC (1 container statuses recorded)
Aug 23 23:24:01.083: INFO: 	Container service-ca-operator ready: true, restart count 0
Aug 23 23:24:01.083: INFO: sonobuoy-systemd-logs-daemon-set-9034e69b695e4ee2-s28vf from sonobuoy started at 2021-08-23 21:37:23 +0000 UTC (2 container statuses recorded)
Aug 23 23:24:01.083: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 23 23:24:01.083: INFO: 	Container systemd-logs ready: true, restart count 0
Aug 23 23:24:01.083: INFO: 
Logging pods the apiserver thinks is on node 10.149.248.25 before test
Aug 23 23:24:02.564: INFO: calico-node-4m7h2 from calico-system started at 2021-08-23 20:02:35 +0000 UTC (1 container statuses recorded)
Aug 23 23:24:02.564: INFO: 	Container calico-node ready: true, restart count 0
Aug 23 23:24:02.564: INFO: calico-typha-84b7fc6fc8-tf4pd from calico-system started at 2021-08-23 20:02:33 +0000 UTC (1 container statuses recorded)
Aug 23 23:24:02.564: INFO: 	Container calico-typha ready: true, restart count 0
Aug 23 23:24:02.564: INFO: ibm-cloud-provider-ip-169-55-101-126-6bb454748-g6kdf from ibm-system started at 2021-08-23 20:10:53 +0000 UTC (1 container statuses recorded)
Aug 23 23:24:02.564: INFO: 	Container ibm-cloud-provider-ip-169-55-101-126 ready: true, restart count 0
Aug 23 23:24:02.564: INFO: ibm-file-plugin-68fbcccc88-bgbn9 from kube-system started at 2021-08-23 20:03:01 +0000 UTC (1 container statuses recorded)
Aug 23 23:24:02.564: INFO: 	Container ibm-file-plugin-container ready: true, restart count 0
Aug 23 23:24:02.564: INFO: ibm-keepalived-watcher-c94bg from kube-system started at 2021-08-23 20:00:20 +0000 UTC (1 container statuses recorded)
Aug 23 23:24:02.564: INFO: 	Container keepalived-watcher ready: true, restart count 0
Aug 23 23:24:02.564: INFO: ibm-master-proxy-static-10.149.248.25 from kube-system started at 2021-08-23 20:00:16 +0000 UTC (2 container statuses recorded)
Aug 23 23:24:02.564: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Aug 23 23:24:02.564: INFO: 	Container pause ready: true, restart count 0
Aug 23 23:24:02.564: INFO: ibmcloud-block-storage-driver-zdvf4 from kube-system started at 2021-08-23 20:00:24 +0000 UTC (1 container statuses recorded)
Aug 23 23:24:02.564: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Aug 23 23:24:02.564: INFO: ibmcloud-block-storage-plugin-7d6d9649b-zdvgj from kube-system started at 2021-08-23 20:03:01 +0000 UTC (1 container statuses recorded)
Aug 23 23:24:02.564: INFO: 	Container ibmcloud-block-storage-plugin-container ready: true, restart count 0
Aug 23 23:24:02.564: INFO: vpn-db594b4f9-9mxgx from kube-system started at 2021-08-23 20:08:59 +0000 UTC (1 container statuses recorded)
Aug 23 23:24:02.564: INFO: 	Container vpn ready: true, restart count 0
Aug 23 23:24:02.564: INFO: tuned-bfbjn from openshift-cluster-node-tuning-operator started at 2021-08-23 20:05:28 +0000 UTC (1 container statuses recorded)
Aug 23 23:24:02.564: INFO: 	Container tuned ready: true, restart count 0
Aug 23 23:24:02.564: INFO: cluster-storage-operator-6c5c749558-97wht from openshift-cluster-storage-operator started at 2021-08-23 20:03:01 +0000 UTC (1 container statuses recorded)
Aug 23 23:24:02.564: INFO: 	Container cluster-storage-operator ready: true, restart count 1
Aug 23 23:24:02.564: INFO: csi-snapshot-controller-operator-645f4897d-2vvbp from openshift-cluster-storage-operator started at 2021-08-23 20:03:01 +0000 UTC (1 container statuses recorded)
Aug 23 23:24:02.565: INFO: 	Container csi-snapshot-controller-operator ready: true, restart count 0
Aug 23 23:24:02.565: INFO: console-dc8c686bb-8xpf8 from openshift-console started at 2021-08-23 20:07:03 +0000 UTC (1 container statuses recorded)
Aug 23 23:24:02.565: INFO: 	Container console ready: true, restart count 0
Aug 23 23:24:02.565: INFO: downloads-6c96776f98-5w9bj from openshift-console started at 2021-08-23 20:03:01 +0000 UTC (1 container statuses recorded)
Aug 23 23:24:02.565: INFO: 	Container download-server ready: true, restart count 0
Aug 23 23:24:02.565: INFO: downloads-6c96776f98-nqhnf from openshift-console started at 2021-08-23 22:12:56 +0000 UTC (1 container statuses recorded)
Aug 23 23:24:02.565: INFO: 	Container download-server ready: true, restart count 0
Aug 23 23:24:02.565: INFO: dns-operator-6dbb54f776-8k9pq from openshift-dns-operator started at 2021-08-23 20:03:01 +0000 UTC (2 container statuses recorded)
Aug 23 23:24:02.565: INFO: 	Container dns-operator ready: true, restart count 0
Aug 23 23:24:02.565: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 23 23:24:02.565: INFO: dns-default-mxjt9 from openshift-dns started at 2021-08-23 20:05:27 +0000 UTC (3 container statuses recorded)
Aug 23 23:24:02.565: INFO: 	Container dns ready: true, restart count 0
Aug 23 23:24:02.565: INFO: 	Container dns-node-resolver ready: true, restart count 0
Aug 23 23:24:02.565: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 23 23:24:02.565: INFO: cluster-image-registry-operator-85978c675-rdng6 from openshift-image-registry started at 2021-08-23 20:03:01 +0000 UTC (1 container statuses recorded)
Aug 23 23:24:02.565: INFO: 	Container cluster-image-registry-operator ready: true, restart count 0
Aug 23 23:24:02.565: INFO: node-ca-czldz from openshift-image-registry started at 2021-08-23 20:04:29 +0000 UTC (1 container statuses recorded)
Aug 23 23:24:02.565: INFO: 	Container node-ca ready: true, restart count 0
Aug 23 23:24:02.565: INFO: ingress-canary-9skj4 from openshift-ingress-canary started at 2021-08-23 20:05:35 +0000 UTC (1 container statuses recorded)
Aug 23 23:24:02.565: INFO: 	Container hello-openshift-canary ready: true, restart count 0
Aug 23 23:24:02.565: INFO: router-default-678bfcd875-g2q6m from openshift-ingress started at 2021-08-23 20:05:36 +0000 UTC (1 container statuses recorded)
Aug 23 23:24:02.565: INFO: 	Container router ready: true, restart count 0
Aug 23 23:24:02.565: INFO: openshift-kube-proxy-sbzjn from openshift-kube-proxy started at 2021-08-23 20:01:25 +0000 UTC (2 container statuses recorded)
Aug 23 23:24:02.565: INFO: 	Container kube-proxy ready: true, restart count 0
Aug 23 23:24:02.565: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 23 23:24:02.565: INFO: redhat-marketplace-hvcn5 from openshift-marketplace started at 2021-08-23 20:04:56 +0000 UTC (1 container statuses recorded)
Aug 23 23:24:02.565: INFO: 	Container registry-server ready: true, restart count 0
Aug 23 23:24:02.565: INFO: alertmanager-main-1 from openshift-monitoring started at 2021-08-23 20:06:03 +0000 UTC (5 container statuses recorded)
Aug 23 23:24:02.565: INFO: 	Container alertmanager ready: true, restart count 0
Aug 23 23:24:02.565: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Aug 23 23:24:02.565: INFO: 	Container config-reloader ready: true, restart count 0
Aug 23 23:24:02.565: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 23 23:24:02.565: INFO: 	Container prom-label-proxy ready: true, restart count 0
Aug 23 23:24:02.565: INFO: cluster-monitoring-operator-c69d85486-m52jl from openshift-monitoring started at 2021-08-23 20:03:01 +0000 UTC (2 container statuses recorded)
Aug 23 23:24:02.565: INFO: 	Container cluster-monitoring-operator ready: true, restart count 0
Aug 23 23:24:02.565: INFO: 	Container kube-rbac-proxy ready: true, restart count 4
Aug 23 23:24:02.565: INFO: node-exporter-fsg2g from openshift-monitoring started at 2021-08-23 20:03:19 +0000 UTC (2 container statuses recorded)
Aug 23 23:24:02.565: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 23 23:24:02.565: INFO: 	Container node-exporter ready: true, restart count 0
Aug 23 23:24:02.565: INFO: prometheus-adapter-6685ccb7f-gxd72 from openshift-monitoring started at 2021-08-23 20:06:45 +0000 UTC (1 container statuses recorded)
Aug 23 23:24:02.565: INFO: 	Container prometheus-adapter ready: true, restart count 0
Aug 23 23:24:02.565: INFO: prometheus-k8s-1 from openshift-monitoring started at 2021-08-23 20:06:14 +0000 UTC (7 container statuses recorded)
Aug 23 23:24:02.565: INFO: 	Container config-reloader ready: true, restart count 0
Aug 23 23:24:02.565: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 23 23:24:02.565: INFO: 	Container kube-rbac-proxy-thanos ready: true, restart count 0
Aug 23 23:24:02.565: INFO: 	Container prom-label-proxy ready: true, restart count 0
Aug 23 23:24:02.565: INFO: 	Container prometheus ready: true, restart count 1
Aug 23 23:24:02.565: INFO: 	Container prometheus-proxy ready: true, restart count 0
Aug 23 23:24:02.565: INFO: 	Container thanos-sidecar ready: true, restart count 0
Aug 23 23:24:02.565: INFO: thanos-querier-97cd894c4-z4jqj from openshift-monitoring started at 2021-08-23 20:06:10 +0000 UTC (5 container statuses recorded)
Aug 23 23:24:02.565: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 23 23:24:02.565: INFO: 	Container kube-rbac-proxy-rules ready: true, restart count 0
Aug 23 23:24:02.565: INFO: 	Container oauth-proxy ready: true, restart count 0
Aug 23 23:24:02.565: INFO: 	Container prom-label-proxy ready: true, restart count 0
Aug 23 23:24:02.565: INFO: 	Container thanos-query ready: true, restart count 0
Aug 23 23:24:02.565: INFO: multus-2p958 from openshift-multus started at 2021-08-23 20:01:12 +0000 UTC (1 container statuses recorded)
Aug 23 23:24:02.565: INFO: 	Container kube-multus ready: true, restart count 0
Aug 23 23:24:02.565: INFO: multus-admission-controller-ww7jt from openshift-multus started at 2021-08-23 20:02:56 +0000 UTC (2 container statuses recorded)
Aug 23 23:24:02.565: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 23 23:24:02.565: INFO: 	Container multus-admission-controller ready: true, restart count 0
Aug 23 23:24:02.565: INFO: network-metrics-daemon-6bmnm from openshift-multus started at 2021-08-23 20:01:15 +0000 UTC (2 container statuses recorded)
Aug 23 23:24:02.565: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 23 23:24:02.565: INFO: 	Container network-metrics-daemon ready: true, restart count 0
Aug 23 23:24:02.565: INFO: network-check-target-cjqqr from openshift-network-diagnostics started at 2021-08-23 20:01:33 +0000 UTC (1 container statuses recorded)
Aug 23 23:24:02.565: INFO: 	Container network-check-target-container ready: true, restart count 0
Aug 23 23:24:02.565: INFO: olm-operator-d5dc9548d-mwfkk from openshift-operator-lifecycle-manager started at 2021-08-23 20:03:01 +0000 UTC (1 container statuses recorded)
Aug 23 23:24:02.565: INFO: 	Container olm-operator ready: true, restart count 0
Aug 23 23:24:02.565: INFO: packageserver-6d5f99f754-5t8qd from openshift-operator-lifecycle-manager started at 2021-08-23 22:12:56 +0000 UTC (1 container statuses recorded)
Aug 23 23:24:02.565: INFO: 	Container packageserver ready: true, restart count 0
Aug 23 23:24:02.565: INFO: metrics-8fc5c5f56-lq4k6 from openshift-roks-metrics started at 2021-08-23 20:03:01 +0000 UTC (1 container statuses recorded)
Aug 23 23:24:02.565: INFO: 	Container metrics ready: true, restart count 3
Aug 23 23:24:02.565: INFO: sonobuoy-e2e-job-90ed16fed1444409 from sonobuoy started at 2021-08-23 21:37:22 +0000 UTC (2 container statuses recorded)
Aug 23 23:24:02.565: INFO: 	Container e2e ready: true, restart count 0
Aug 23 23:24:02.565: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 23 23:24:02.565: INFO: sonobuoy-systemd-logs-daemon-set-9034e69b695e4ee2-q8h6l from sonobuoy started at 2021-08-23 21:37:23 +0000 UTC (2 container statuses recorded)
Aug 23 23:24:02.565: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 23 23:24:02.565: INFO: 	Container systemd-logs ready: true, restart count 0
Aug 23 23:24:02.565: INFO: 
Logging pods the apiserver thinks is on node 10.149.248.9 before test
Aug 23 23:24:03.973: INFO: calico-kube-controllers-7dcbcc7c66-x65ng from calico-system started at 2021-08-23 20:02:55 +0000 UTC (1 container statuses recorded)
Aug 23 23:24:03.973: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Aug 23 23:24:03.973: INFO: calico-node-cgrcs from calico-system started at 2021-08-23 20:02:35 +0000 UTC (1 container statuses recorded)
Aug 23 23:24:03.973: INFO: 	Container calico-node ready: true, restart count 0
Aug 23 23:24:03.973: INFO: calico-typha-84b7fc6fc8-h6lnw from calico-system started at 2021-08-23 20:02:39 +0000 UTC (1 container statuses recorded)
Aug 23 23:24:03.973: INFO: 	Container calico-typha ready: true, restart count 0
Aug 23 23:24:03.973: INFO: ibm-cloud-provider-ip-169-55-101-126-6bb454748-t7nvk from ibm-system started at 2021-08-23 20:10:53 +0000 UTC (1 container statuses recorded)
Aug 23 23:24:03.974: INFO: 	Container ibm-cloud-provider-ip-169-55-101-126 ready: true, restart count 0
Aug 23 23:24:03.974: INFO: ibm-keepalived-watcher-rsh2g from kube-system started at 2021-08-23 20:00:08 +0000 UTC (1 container statuses recorded)
Aug 23 23:24:03.974: INFO: 	Container keepalived-watcher ready: true, restart count 0
Aug 23 23:24:03.974: INFO: ibm-master-proxy-static-10.149.248.9 from kube-system started at 2021-08-23 20:00:06 +0000 UTC (2 container statuses recorded)
Aug 23 23:24:03.974: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Aug 23 23:24:03.974: INFO: 	Container pause ready: true, restart count 0
Aug 23 23:24:03.974: INFO: ibmcloud-block-storage-driver-5fllx from kube-system started at 2021-08-23 20:00:13 +0000 UTC (1 container statuses recorded)
Aug 23 23:24:03.974: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Aug 23 23:24:03.974: INFO: cluster-node-tuning-operator-84b8576b47-qlh6k from openshift-cluster-node-tuning-operator started at 2021-08-23 22:12:56 +0000 UTC (1 container statuses recorded)
Aug 23 23:24:03.974: INFO: 	Container cluster-node-tuning-operator ready: true, restart count 0
Aug 23 23:24:03.974: INFO: tuned-m585x from openshift-cluster-node-tuning-operator started at 2021-08-23 20:05:28 +0000 UTC (1 container statuses recorded)
Aug 23 23:24:03.974: INFO: 	Container tuned ready: true, restart count 0
Aug 23 23:24:03.974: INFO: cluster-samples-operator-866dcfc6c4-ntqvw from openshift-cluster-samples-operator started at 2021-08-23 22:12:56 +0000 UTC (2 container statuses recorded)
Aug 23 23:24:03.974: INFO: 	Container cluster-samples-operator ready: true, restart count 0
Aug 23 23:24:03.974: INFO: 	Container cluster-samples-operator-watch ready: true, restart count 0
Aug 23 23:24:03.974: INFO: dns-default-4hmcv from openshift-dns started at 2021-08-23 20:05:27 +0000 UTC (3 container statuses recorded)
Aug 23 23:24:03.975: INFO: 	Container dns ready: true, restart count 0
Aug 23 23:24:03.975: INFO: 	Container dns-node-resolver ready: true, restart count 0
Aug 23 23:24:03.975: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 23 23:24:03.975: INFO: image-registry-555cc7f64c-qtjjg from openshift-image-registry started at 2021-08-23 22:12:56 +0000 UTC (1 container statuses recorded)
Aug 23 23:24:03.975: INFO: 	Container registry ready: true, restart count 0
Aug 23 23:24:03.975: INFO: node-ca-g989x from openshift-image-registry started at 2021-08-23 20:04:29 +0000 UTC (1 container statuses recorded)
Aug 23 23:24:03.975: INFO: 	Container node-ca ready: true, restart count 0
Aug 23 23:24:03.975: INFO: registry-pvc-permissions-dbktz from openshift-image-registry started at 2021-08-23 20:08:40 +0000 UTC (1 container statuses recorded)
Aug 23 23:24:03.975: INFO: 	Container pvc-permissions ready: false, restart count 0
Aug 23 23:24:03.975: INFO: ingress-canary-l5l42 from openshift-ingress-canary started at 2021-08-23 20:05:35 +0000 UTC (1 container statuses recorded)
Aug 23 23:24:03.975: INFO: 	Container hello-openshift-canary ready: true, restart count 0
Aug 23 23:24:03.975: INFO: router-default-678bfcd875-2f5hb from openshift-ingress started at 2021-08-23 20:05:36 +0000 UTC (1 container statuses recorded)
Aug 23 23:24:03.975: INFO: 	Container router ready: true, restart count 0
Aug 23 23:24:03.975: INFO: openshift-kube-proxy-wk9m8 from openshift-kube-proxy started at 2021-08-23 20:01:25 +0000 UTC (2 container statuses recorded)
Aug 23 23:24:03.975: INFO: 	Container kube-proxy ready: true, restart count 0
Aug 23 23:24:03.976: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 23 23:24:03.976: INFO: certified-operators-rlpm2 from openshift-marketplace started at 2021-08-23 20:04:55 +0000 UTC (1 container statuses recorded)
Aug 23 23:24:03.976: INFO: 	Container registry-server ready: true, restart count 0
Aug 23 23:24:03.976: INFO: community-operators-ztwk2 from openshift-marketplace started at 2021-08-23 20:04:56 +0000 UTC (1 container statuses recorded)
Aug 23 23:24:03.976: INFO: 	Container registry-server ready: true, restart count 0
Aug 23 23:24:03.976: INFO: redhat-operators-cpqsn from openshift-marketplace started at 2021-08-23 20:04:56 +0000 UTC (1 container statuses recorded)
Aug 23 23:24:03.976: INFO: 	Container registry-server ready: true, restart count 0
Aug 23 23:24:03.976: INFO: alertmanager-main-2 from openshift-monitoring started at 2021-08-23 20:06:04 +0000 UTC (5 container statuses recorded)
Aug 23 23:24:03.976: INFO: 	Container alertmanager ready: true, restart count 0
Aug 23 23:24:03.976: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Aug 23 23:24:03.976: INFO: 	Container config-reloader ready: true, restart count 0
Aug 23 23:24:03.976: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 23 23:24:03.976: INFO: 	Container prom-label-proxy ready: true, restart count 0
Aug 23 23:24:03.976: INFO: grafana-85d4bdb748-9g6gn from openshift-monitoring started at 2021-08-23 22:12:56 +0000 UTC (2 container statuses recorded)
Aug 23 23:24:03.976: INFO: 	Container grafana ready: true, restart count 0
Aug 23 23:24:03.976: INFO: 	Container grafana-proxy ready: true, restart count 0
Aug 23 23:24:03.977: INFO: node-exporter-kcjzv from openshift-monitoring started at 2021-08-23 20:03:19 +0000 UTC (2 container statuses recorded)
Aug 23 23:24:03.977: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 23 23:24:03.977: INFO: 	Container node-exporter ready: true, restart count 0
Aug 23 23:24:03.977: INFO: prometheus-k8s-0 from openshift-monitoring started at 2021-08-23 20:06:14 +0000 UTC (7 container statuses recorded)
Aug 23 23:24:03.977: INFO: 	Container config-reloader ready: true, restart count 0
Aug 23 23:24:03.977: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 23 23:24:03.977: INFO: 	Container kube-rbac-proxy-thanos ready: true, restart count 0
Aug 23 23:24:03.977: INFO: 	Container prom-label-proxy ready: true, restart count 0
Aug 23 23:24:03.977: INFO: 	Container prometheus ready: true, restart count 1
Aug 23 23:24:03.977: INFO: 	Container prometheus-proxy ready: true, restart count 0
Aug 23 23:24:03.977: INFO: 	Container thanos-sidecar ready: true, restart count 0
Aug 23 23:24:03.977: INFO: prometheus-operator-5699bb49dc-8q47w from openshift-monitoring started at 2021-08-23 20:06:44 +0000 UTC (2 container statuses recorded)
Aug 23 23:24:03.977: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 23 23:24:03.977: INFO: 	Container prometheus-operator ready: true, restart count 0
Aug 23 23:24:03.977: INFO: thanos-querier-97cd894c4-g4rfr from openshift-monitoring started at 2021-08-23 22:12:56 +0000 UTC (5 container statuses recorded)
Aug 23 23:24:03.977: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 23 23:24:03.978: INFO: 	Container kube-rbac-proxy-rules ready: true, restart count 0
Aug 23 23:24:03.978: INFO: 	Container oauth-proxy ready: true, restart count 0
Aug 23 23:24:03.978: INFO: 	Container prom-label-proxy ready: true, restart count 0
Aug 23 23:24:03.978: INFO: 	Container thanos-query ready: true, restart count 0
Aug 23 23:24:03.978: INFO: multus-admission-controller-hcwf4 from openshift-multus started at 2021-08-23 20:02:50 +0000 UTC (2 container statuses recorded)
Aug 23 23:24:03.978: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 23 23:24:03.978: INFO: 	Container multus-admission-controller ready: true, restart count 0
Aug 23 23:24:03.978: INFO: multus-z4wqr from openshift-multus started at 2021-08-23 20:01:12 +0000 UTC (1 container statuses recorded)
Aug 23 23:24:03.978: INFO: 	Container kube-multus ready: true, restart count 0
Aug 23 23:24:03.978: INFO: network-metrics-daemon-mbjhv from openshift-multus started at 2021-08-23 20:01:15 +0000 UTC (2 container statuses recorded)
Aug 23 23:24:03.978: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 23 23:24:03.978: INFO: 	Container network-metrics-daemon ready: true, restart count 0
Aug 23 23:24:03.978: INFO: network-check-source-6cd65cf589-6rbmz from openshift-network-diagnostics started at 2021-08-23 20:03:01 +0000 UTC (1 container statuses recorded)
Aug 23 23:24:03.978: INFO: 	Container check-endpoints ready: true, restart count 0
Aug 23 23:24:03.978: INFO: network-check-target-d6zj2 from openshift-network-diagnostics started at 2021-08-23 20:01:33 +0000 UTC (1 container statuses recorded)
Aug 23 23:24:03.978: INFO: 	Container network-check-target-container ready: true, restart count 0
Aug 23 23:24:03.979: INFO: network-operator-86dcc4df56-crx6q from openshift-network-operator started at 2021-08-23 20:00:18 +0000 UTC (1 container statuses recorded)
Aug 23 23:24:03.979: INFO: 	Container network-operator ready: true, restart count 0
Aug 23 23:24:03.979: INFO: catalog-operator-6f8ff86686-tg2d4 from openshift-operator-lifecycle-manager started at 2021-08-23 22:12:56 +0000 UTC (1 container statuses recorded)
Aug 23 23:24:03.979: INFO: 	Container catalog-operator ready: true, restart count 0
Aug 23 23:24:03.979: INFO: packageserver-6d5f99f754-9mfv7 from openshift-operator-lifecycle-manager started at 2021-08-23 20:04:35 +0000 UTC (1 container statuses recorded)
Aug 23 23:24:03.979: INFO: 	Container packageserver ready: true, restart count 0
Aug 23 23:24:03.979: INFO: service-ca-54b7675c-xmpw4 from openshift-service-ca started at 2021-08-23 22:12:56 +0000 UTC (1 container statuses recorded)
Aug 23 23:24:03.979: INFO: 	Container service-ca-controller ready: false, restart count 0
Aug 23 23:24:03.979: INFO: sonobuoy from sonobuoy started at 2021-08-23 21:37:12 +0000 UTC (1 container statuses recorded)
Aug 23 23:24:03.979: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Aug 23 23:24:03.979: INFO: sonobuoy-systemd-logs-daemon-set-9034e69b695e4ee2-zn2c4 from sonobuoy started at 2021-08-23 21:37:23 +0000 UTC (2 container statuses recorded)
Aug 23 23:24:03.979: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 23 23:24:03.979: INFO: 	Container systemd-logs ready: true, restart count 0
Aug 23 23:24:03.979: INFO: tigera-operator-667cd558f7-d78zp from tigera-operator started at 2021-08-23 20:00:19 +0000 UTC (1 container statuses recorded)
Aug 23 23:24:03.979: INFO: 	Container tigera-operator ready: true, restart count 4
[It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-81794f6d-2525-4253-a0b4-983cc6d27a4e 90
STEP: Trying to create a pod(pod1) with hostport 54321 and hostIP 127.0.0.1 and expect scheduled
STEP: Trying to create another pod(pod2) with hostport 54321 but hostIP 10.149.248.24 on the node which pod1 resides and expect scheduled
STEP: Trying to create a third pod(pod3) with hostport 54321, hostIP 10.149.248.24 but use UDP protocol on the node which pod2 resides
STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54321
Aug 23 23:24:32.336: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 10.149.248.24 http://127.0.0.1:54321/hostname] Namespace:sched-pred-3836 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 23 23:24:32.336: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.149.248.24, port: 54321
Aug 23 23:24:32.600: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://10.149.248.24:54321/hostname] Namespace:sched-pred-3836 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 23 23:24:32.600: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.149.248.24, port: 54321 UDP
Aug 23 23:24:33.437: INFO: ExecWithOptions {Command:[/bin/sh -c nc -vuz -w 5 10.149.248.24 54321] Namespace:sched-pred-3836 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 23 23:24:33.437: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54321
Aug 23 23:24:39.920: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 10.149.248.24 http://127.0.0.1:54321/hostname] Namespace:sched-pred-3836 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 23 23:24:39.920: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.149.248.24, port: 54321
Aug 23 23:24:40.138: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://10.149.248.24:54321/hostname] Namespace:sched-pred-3836 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 23 23:24:40.138: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.149.248.24, port: 54321 UDP
Aug 23 23:24:40.760: INFO: ExecWithOptions {Command:[/bin/sh -c nc -vuz -w 5 10.149.248.24 54321] Namespace:sched-pred-3836 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 23 23:24:40.760: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54321
Aug 23 23:24:48.174: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 10.149.248.24 http://127.0.0.1:54321/hostname] Namespace:sched-pred-3836 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 23 23:24:48.174: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.149.248.24, port: 54321
Aug 23 23:24:49.260: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://10.149.248.24:54321/hostname] Namespace:sched-pred-3836 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 23 23:24:49.260: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.149.248.24, port: 54321 UDP
Aug 23 23:24:49.527: INFO: ExecWithOptions {Command:[/bin/sh -c nc -vuz -w 5 10.149.248.24 54321] Namespace:sched-pred-3836 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 23 23:24:49.527: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54321
Aug 23 23:24:56.035: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 10.149.248.24 http://127.0.0.1:54321/hostname] Namespace:sched-pred-3836 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 23 23:24:56.035: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.149.248.24, port: 54321
Aug 23 23:24:56.824: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://10.149.248.24:54321/hostname] Namespace:sched-pred-3836 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 23 23:24:56.824: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.149.248.24, port: 54321 UDP
Aug 23 23:24:57.687: INFO: ExecWithOptions {Command:[/bin/sh -c nc -vuz -w 5 10.149.248.24 54321] Namespace:sched-pred-3836 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 23 23:24:57.687: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54321
Aug 23 23:25:02.919: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 10.149.248.24 http://127.0.0.1:54321/hostname] Namespace:sched-pred-3836 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 23 23:25:02.919: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.149.248.24, port: 54321
Aug 23 23:25:04.321: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://10.149.248.24:54321/hostname] Namespace:sched-pred-3836 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 23 23:25:04.321: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.149.248.24, port: 54321 UDP
Aug 23 23:25:05.971: INFO: ExecWithOptions {Command:[/bin/sh -c nc -vuz -w 5 10.149.248.24 54321] Namespace:sched-pred-3836 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 23 23:25:05.971: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: removing the label kubernetes.io/e2e-81794f6d-2525-4253-a0b4-983cc6d27a4e off the node 10.149.248.24
STEP: verifying the node doesn't have the label kubernetes.io/e2e-81794f6d-2525-4253-a0b4-983cc6d27a4e
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 23:25:14.428: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-3836" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:83

• [SLOW TEST:82.504 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]","total":311,"completed":259,"skipped":4410,"failed":0}
SSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 23:25:15.138: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:41
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Aug 23 23:25:15.929: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ffb3c6bd-e1e9-45a4-868a-7eeda5bf74c1" in namespace "projected-9815" to be "Succeeded or Failed"
Aug 23 23:25:16.056: INFO: Pod "downwardapi-volume-ffb3c6bd-e1e9-45a4-868a-7eeda5bf74c1": Phase="Pending", Reason="", readiness=false. Elapsed: 126.305385ms
Aug 23 23:25:18.283: INFO: Pod "downwardapi-volume-ffb3c6bd-e1e9-45a4-868a-7eeda5bf74c1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.353796734s
Aug 23 23:25:20.449: INFO: Pod "downwardapi-volume-ffb3c6bd-e1e9-45a4-868a-7eeda5bf74c1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.519375247s
Aug 23 23:25:22.680: INFO: Pod "downwardapi-volume-ffb3c6bd-e1e9-45a4-868a-7eeda5bf74c1": Phase="Pending", Reason="", readiness=false. Elapsed: 6.750350123s
Aug 23 23:25:24.954: INFO: Pod "downwardapi-volume-ffb3c6bd-e1e9-45a4-868a-7eeda5bf74c1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 9.024372527s
STEP: Saw pod success
Aug 23 23:25:24.954: INFO: Pod "downwardapi-volume-ffb3c6bd-e1e9-45a4-868a-7eeda5bf74c1" satisfied condition "Succeeded or Failed"
Aug 23 23:25:25.195: INFO: Trying to get logs from node 10.149.248.25 pod downwardapi-volume-ffb3c6bd-e1e9-45a4-868a-7eeda5bf74c1 container client-container: <nil>
STEP: delete the pod
Aug 23 23:25:26.294: INFO: Waiting for pod downwardapi-volume-ffb3c6bd-e1e9-45a4-868a-7eeda5bf74c1 to disappear
Aug 23 23:25:26.451: INFO: Pod downwardapi-volume-ffb3c6bd-e1e9-45a4-868a-7eeda5bf74c1 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 23:25:26.451: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9815" for this suite.

• [SLOW TEST:11.922 seconds]
[sig-storage] Projected downwardAPI
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:35
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu request [NodeConformance] [Conformance]","total":311,"completed":260,"skipped":4413,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial] 
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 23:25:27.062: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename taint-multiple-pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/taints.go:345
Aug 23 23:25:27.653: INFO: Waiting up to 1m0s for all nodes to be ready
Aug 23 23:26:30.289: INFO: Waiting for terminating namespaces to be deleted...
[It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Aug 23 23:26:30.609: INFO: Starting informer...
STEP: Starting pods...
Aug 23 23:26:31.073: INFO: Pod1 is running on 10.149.248.24. Tainting Node
Aug 23 23:26:37.614: INFO: Pod2 is running on 10.149.248.24. Tainting Node
STEP: Trying to apply a taint on the Node
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting for Pod1 and Pod2 to be deleted
Aug 23 23:27:03.391: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
Aug 23 23:27:23.613: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
[AfterEach] [k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 23:27:24.083: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-multiple-pods-140" for this suite.

• [SLOW TEST:117.434 seconds]
[k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]","total":311,"completed":261,"skipped":4436,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a volume subpath [sig-storage] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 23:27:24.496: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a volume subpath [sig-storage] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test substitution in volume subpath
Aug 23 23:27:25.451: INFO: Waiting up to 5m0s for pod "var-expansion-dcbea315-8d22-4c82-82a9-6b4294d7aa01" in namespace "var-expansion-6193" to be "Succeeded or Failed"
Aug 23 23:27:25.672: INFO: Pod "var-expansion-dcbea315-8d22-4c82-82a9-6b4294d7aa01": Phase="Pending", Reason="", readiness=false. Elapsed: 220.647776ms
Aug 23 23:27:27.797: INFO: Pod "var-expansion-dcbea315-8d22-4c82-82a9-6b4294d7aa01": Phase="Pending", Reason="", readiness=false. Elapsed: 2.34607601s
Aug 23 23:27:29.926: INFO: Pod "var-expansion-dcbea315-8d22-4c82-82a9-6b4294d7aa01": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.475230826s
STEP: Saw pod success
Aug 23 23:27:29.927: INFO: Pod "var-expansion-dcbea315-8d22-4c82-82a9-6b4294d7aa01" satisfied condition "Succeeded or Failed"
Aug 23 23:27:30.070: INFO: Trying to get logs from node 10.149.248.24 pod var-expansion-dcbea315-8d22-4c82-82a9-6b4294d7aa01 container dapi-container: <nil>
STEP: delete the pod
Aug 23 23:27:30.435: INFO: Waiting for pod var-expansion-dcbea315-8d22-4c82-82a9-6b4294d7aa01 to disappear
Aug 23 23:27:30.599: INFO: Pod var-expansion-dcbea315-8d22-4c82-82a9-6b4294d7aa01 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 23:27:30.599: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-6193" for this suite.

• [SLOW TEST:6.479 seconds]
[k8s.io] Variable Expansion
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should allow substituting values in a volume subpath [sig-storage] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Variable Expansion should allow substituting values in a volume subpath [sig-storage] [Conformance]","total":311,"completed":262,"skipped":4482,"failed":0}
SSS
------------------------------
[sig-api-machinery] Garbage collector 
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 23:27:30.975: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W0823 23:27:38.993910      25 metrics_grabber.go:98] Can't find kube-scheduler pod. Grabbing metrics from kube-scheduler is disabled.
W0823 23:27:38.994019      25 metrics_grabber.go:102] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
W0823 23:27:38.994055      25 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
Aug 23 23:27:38.994: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 23:27:38.994: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-236" for this suite.

• [SLOW TEST:8.828 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]","total":311,"completed":263,"skipped":4485,"failed":0}
SSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 23:27:39.803: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs
STEP: Gathering metrics
W0823 23:27:42.761535      25 metrics_grabber.go:98] Can't find kube-scheduler pod. Grabbing metrics from kube-scheduler is disabled.
W0823 23:27:42.761690      25 metrics_grabber.go:102] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
W0823 23:27:42.761767      25 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
Aug 23 23:27:42.761: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 23:27:42.761: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-9621" for this suite.
•{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]","total":311,"completed":264,"skipped":4494,"failed":0}
S
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 23:27:43.293: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation
Aug 23 23:27:44.332: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
Aug 23 23:27:55.883: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 23:28:35.674: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-9248" for this suite.

• [SLOW TEST:53.163 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group and version but different kinds [Conformance]","total":311,"completed":265,"skipped":4495,"failed":0}
SSSSSSSSSS
------------------------------
[sig-node] RuntimeClass 
   should support RuntimeClasses API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-node] RuntimeClass
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 23:28:36.456: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename runtimeclass
STEP: Waiting for a default service account to be provisioned in namespace
[It]  should support RuntimeClasses API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: getting /apis
STEP: getting /apis/node.k8s.io
STEP: getting /apis/node.k8s.io/v1
STEP: creating
STEP: watching
Aug 23 23:28:38.798: INFO: starting watch
STEP: getting
STEP: listing
STEP: patching
STEP: updating
Aug 23 23:28:39.647: INFO: waiting for watch events with expected annotations
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-node] RuntimeClass
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 23:28:40.272: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-6384" for this suite.
•{"msg":"PASSED [sig-node] RuntimeClass  should support RuntimeClasses API operations [Conformance]","total":311,"completed":266,"skipped":4505,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 23:28:40.583: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:41
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Aug 23 23:28:41.526: INFO: Waiting up to 5m0s for pod "downwardapi-volume-af1f5eae-37c9-4b68-aa59-fbeba58d6186" in namespace "downward-api-736" to be "Succeeded or Failed"
Aug 23 23:28:41.643: INFO: Pod "downwardapi-volume-af1f5eae-37c9-4b68-aa59-fbeba58d6186": Phase="Pending", Reason="", readiness=false. Elapsed: 117.368455ms
Aug 23 23:28:43.756: INFO: Pod "downwardapi-volume-af1f5eae-37c9-4b68-aa59-fbeba58d6186": Phase="Pending", Reason="", readiness=false. Elapsed: 2.229945239s
Aug 23 23:28:45.837: INFO: Pod "downwardapi-volume-af1f5eae-37c9-4b68-aa59-fbeba58d6186": Phase="Pending", Reason="", readiness=false. Elapsed: 4.310990374s
Aug 23 23:28:47.998: INFO: Pod "downwardapi-volume-af1f5eae-37c9-4b68-aa59-fbeba58d6186": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.472130194s
STEP: Saw pod success
Aug 23 23:28:47.998: INFO: Pod "downwardapi-volume-af1f5eae-37c9-4b68-aa59-fbeba58d6186" satisfied condition "Succeeded or Failed"
Aug 23 23:28:48.170: INFO: Trying to get logs from node 10.149.248.24 pod downwardapi-volume-af1f5eae-37c9-4b68-aa59-fbeba58d6186 container client-container: <nil>
STEP: delete the pod
Aug 23 23:28:49.004: INFO: Waiting for pod downwardapi-volume-af1f5eae-37c9-4b68-aa59-fbeba58d6186 to disappear
Aug 23 23:28:49.181: INFO: Pod downwardapi-volume-af1f5eae-37c9-4b68-aa59-fbeba58d6186 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 23:28:49.181: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-736" for this suite.

• [SLOW TEST:9.076 seconds]
[sig-storage] Downward API volume
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:36
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":267,"skipped":4633,"failed":0}
SSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 23:28:49.659: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Aug 23 23:28:50.303: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Aug 23 23:29:00.330: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585719795 --namespace=crd-publish-openapi-2936 --namespace=crd-publish-openapi-2936 create -f -'
Aug 23 23:29:01.400: INFO: stderr: ""
Aug 23 23:29:01.400: INFO: stdout: "e2e-test-crd-publish-openapi-5783-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Aug 23 23:29:01.400: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585719795 --namespace=crd-publish-openapi-2936 --namespace=crd-publish-openapi-2936 delete e2e-test-crd-publish-openapi-5783-crds test-cr'
Aug 23 23:29:02.063: INFO: stderr: ""
Aug 23 23:29:02.063: INFO: stdout: "e2e-test-crd-publish-openapi-5783-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
Aug 23 23:29:02.063: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585719795 --namespace=crd-publish-openapi-2936 --namespace=crd-publish-openapi-2936 apply -f -'
Aug 23 23:29:03.323: INFO: stderr: ""
Aug 23 23:29:03.323: INFO: stdout: "e2e-test-crd-publish-openapi-5783-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Aug 23 23:29:03.323: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585719795 --namespace=crd-publish-openapi-2936 --namespace=crd-publish-openapi-2936 delete e2e-test-crd-publish-openapi-5783-crds test-cr'
Aug 23 23:29:03.442: INFO: stderr: ""
Aug 23 23:29:03.442: INFO: stdout: "e2e-test-crd-publish-openapi-5783-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
Aug 23 23:29:03.442: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585719795 --namespace=crd-publish-openapi-2936 explain e2e-test-crd-publish-openapi-5783-crds'
Aug 23 23:29:04.114: INFO: stderr: ""
Aug 23 23:29:04.114: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-5783-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 23:29:12.893: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-2936" for this suite.

• [SLOW TEST:23.311 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields at the schema root [Conformance]","total":311,"completed":268,"skipped":4636,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 23:29:12.971: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating secret with name projected-secret-test-9b65dd35-6ad3-4c01-a108-92ce75908d80
STEP: Creating a pod to test consume secrets
Aug 23 23:29:13.384: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-008a70d5-7086-4a0d-9a2c-e3222860742b" in namespace "projected-3009" to be "Succeeded or Failed"
Aug 23 23:29:13.434: INFO: Pod "pod-projected-secrets-008a70d5-7086-4a0d-9a2c-e3222860742b": Phase="Pending", Reason="", readiness=false. Elapsed: 50.18363ms
Aug 23 23:29:15.518: INFO: Pod "pod-projected-secrets-008a70d5-7086-4a0d-9a2c-e3222860742b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.134337331s
Aug 23 23:29:17.602: INFO: Pod "pod-projected-secrets-008a70d5-7086-4a0d-9a2c-e3222860742b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.217817149s
STEP: Saw pod success
Aug 23 23:29:17.602: INFO: Pod "pod-projected-secrets-008a70d5-7086-4a0d-9a2c-e3222860742b" satisfied condition "Succeeded or Failed"
Aug 23 23:29:17.681: INFO: Trying to get logs from node 10.149.248.24 pod pod-projected-secrets-008a70d5-7086-4a0d-9a2c-e3222860742b container secret-volume-test: <nil>
STEP: delete the pod
Aug 23 23:29:17.834: INFO: Waiting for pod pod-projected-secrets-008a70d5-7086-4a0d-9a2c-e3222860742b to disappear
Aug 23 23:29:17.895: INFO: Pod pod-projected-secrets-008a70d5-7086-4a0d-9a2c-e3222860742b no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 23:29:17.895: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3009" for this suite.

• [SLOW TEST:5.085 seconds]
[sig-storage] Projected secret
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Projected secret should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","total":311,"completed":269,"skipped":4654,"failed":0}
[sig-cli] Kubectl client Proxy server 
  should support proxy with --port 0  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 23:29:18.056: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should support proxy with --port 0  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: starting the proxy server
Aug 23 23:29:18.508: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-585719795 --namespace=kubectl-7459 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 23:29:19.128: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7459" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support proxy with --port 0  [Conformance]","total":311,"completed":270,"skipped":4654,"failed":0}
S
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 23:29:19.514: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicationController
STEP: Ensuring resource quota status captures replication controller creation
STEP: Deleting a ReplicationController
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 23:29:32.815: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-2677" for this suite.

• [SLOW TEST:14.142 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replication controller. [Conformance]","total":311,"completed":271,"skipped":4655,"failed":0}
SSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 23:29:33.657: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-3265
[It] should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating statefulset ss in namespace statefulset-3265
Aug 23 23:29:35.584: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Pending - Ready=false
Aug 23 23:29:45.842: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: getting scale subresource
STEP: updating a scale subresource
STEP: verifying the statefulset Spec.Replicas was modified
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
Aug 23 23:29:46.948: INFO: Deleting all statefulset in ns statefulset-3265
Aug 23 23:29:47.126: INFO: Scaling statefulset ss to 0
Aug 23 23:29:57.653: INFO: Waiting for statefulset status.replicas updated to 0
Aug 23 23:29:57.724: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 23:29:57.957: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-3265" for this suite.

• [SLOW TEST:24.531 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
    should have a working scale subresource [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should have a working scale subresource [Conformance]","total":311,"completed":272,"skipped":4658,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job 
  should delete a job [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 23:29:58.189: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a job [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: delete a job
STEP: deleting Job.batch foo in namespace job-3694, will wait for the garbage collector to delete the pods
Aug 23 23:30:07.244: INFO: Deleting Job.batch foo took: 158.86146ms
Aug 23 23:30:07.344: INFO: Terminating Job.batch foo pods took: 100.237055ms
STEP: Ensuring job was deleted
[AfterEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 23:30:53.981: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-3694" for this suite.

• [SLOW TEST:56.225 seconds]
[sig-apps] Job
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should delete a job [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] Job should delete a job [Conformance]","total":311,"completed":273,"skipped":4730,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 23:30:54.415: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:41
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Aug 23 23:30:55.156: INFO: Waiting up to 5m0s for pod "downwardapi-volume-9b3ecc1c-b59c-4302-864e-fe9a34e93bab" in namespace "downward-api-6297" to be "Succeeded or Failed"
Aug 23 23:30:55.324: INFO: Pod "downwardapi-volume-9b3ecc1c-b59c-4302-864e-fe9a34e93bab": Phase="Pending", Reason="", readiness=false. Elapsed: 167.419274ms
Aug 23 23:30:57.531: INFO: Pod "downwardapi-volume-9b3ecc1c-b59c-4302-864e-fe9a34e93bab": Phase="Pending", Reason="", readiness=false. Elapsed: 2.374515231s
Aug 23 23:30:59.674: INFO: Pod "downwardapi-volume-9b3ecc1c-b59c-4302-864e-fe9a34e93bab": Phase="Pending", Reason="", readiness=false. Elapsed: 4.517942013s
Aug 23 23:31:01.777: INFO: Pod "downwardapi-volume-9b3ecc1c-b59c-4302-864e-fe9a34e93bab": Phase="Running", Reason="", readiness=true. Elapsed: 6.620136129s
Aug 23 23:31:03.943: INFO: Pod "downwardapi-volume-9b3ecc1c-b59c-4302-864e-fe9a34e93bab": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.786803297s
STEP: Saw pod success
Aug 23 23:31:03.943: INFO: Pod "downwardapi-volume-9b3ecc1c-b59c-4302-864e-fe9a34e93bab" satisfied condition "Succeeded or Failed"
Aug 23 23:31:04.144: INFO: Trying to get logs from node 10.149.248.24 pod downwardapi-volume-9b3ecc1c-b59c-4302-864e-fe9a34e93bab container client-container: <nil>
STEP: delete the pod
Aug 23 23:31:04.610: INFO: Waiting for pod downwardapi-volume-9b3ecc1c-b59c-4302-864e-fe9a34e93bab to disappear
Aug 23 23:31:04.773: INFO: Pod downwardapi-volume-9b3ecc1c-b59c-4302-864e-fe9a34e93bab no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 23:31:04.773: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6297" for this suite.

• [SLOW TEST:10.778 seconds]
[sig-storage] Downward API volume
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:36
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","total":311,"completed":274,"skipped":4739,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 23:31:05.194: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir 0777 on node default medium
Aug 23 23:31:06.052: INFO: Waiting up to 5m0s for pod "pod-3aa88edf-a63d-453c-ad00-3539ae47978e" in namespace "emptydir-1997" to be "Succeeded or Failed"
Aug 23 23:31:06.246: INFO: Pod "pod-3aa88edf-a63d-453c-ad00-3539ae47978e": Phase="Pending", Reason="", readiness=false. Elapsed: 193.128018ms
Aug 23 23:31:08.478: INFO: Pod "pod-3aa88edf-a63d-453c-ad00-3539ae47978e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.425760914s
Aug 23 23:31:10.671: INFO: Pod "pod-3aa88edf-a63d-453c-ad00-3539ae47978e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.61851206s
STEP: Saw pod success
Aug 23 23:31:10.671: INFO: Pod "pod-3aa88edf-a63d-453c-ad00-3539ae47978e" satisfied condition "Succeeded or Failed"
Aug 23 23:31:10.858: INFO: Trying to get logs from node 10.149.248.24 pod pod-3aa88edf-a63d-453c-ad00-3539ae47978e container test-container: <nil>
STEP: delete the pod
Aug 23 23:31:11.283: INFO: Waiting for pod pod-3aa88edf-a63d-453c-ad00-3539ae47978e to disappear
Aug 23 23:31:11.496: INFO: Pod pod-3aa88edf-a63d-453c-ad00-3539ae47978e no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 23:31:11.496: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1997" for this suite.

• [SLOW TEST:6.848 seconds]
[sig-storage] EmptyDir volumes
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:45
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":275,"skipped":4747,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 23:31:12.042: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating service in namespace services-8373
STEP: creating service affinity-nodeport in namespace services-8373
STEP: creating replication controller affinity-nodeport in namespace services-8373
I0823 23:31:12.993505      25 runners.go:190] Created replication controller with name: affinity-nodeport, namespace: services-8373, replica count: 3
I0823 23:31:16.143754      25 runners.go:190] affinity-nodeport Pods: 3 out of 3 created, 0 running, 3 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0823 23:31:19.143944      25 runners.go:190] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Aug 23 23:31:19.241: INFO: Creating new exec pod
Aug 23 23:31:30.646: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585719795 --namespace=services-8373 exec execpod-affinitykm4hq -- /bin/sh -x -c nc -zv -t -w 2 affinity-nodeport 80'
Aug 23 23:31:31.437: INFO: stderr: "+ nc -zv -t -w 2 affinity-nodeport 80\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
Aug 23 23:31:31.437: INFO: stdout: ""
Aug 23 23:31:31.437: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585719795 --namespace=services-8373 exec execpod-affinitykm4hq -- /bin/sh -x -c nc -zv -t -w 2 172.21.8.7 80'
Aug 23 23:31:32.977: INFO: stderr: "+ nc -zv -t -w 2 172.21.8.7 80\nConnection to 172.21.8.7 80 port [tcp/http] succeeded!\n"
Aug 23 23:31:32.977: INFO: stdout: ""
Aug 23 23:31:32.977: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585719795 --namespace=services-8373 exec execpod-affinitykm4hq -- /bin/sh -x -c nc -zv -t -w 2 10.149.248.9 31462'
Aug 23 23:31:33.661: INFO: stderr: "+ nc -zv -t -w 2 10.149.248.9 31462\nConnection to 10.149.248.9 31462 port [tcp/31462] succeeded!\n"
Aug 23 23:31:33.661: INFO: stdout: ""
Aug 23 23:31:33.661: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585719795 --namespace=services-8373 exec execpod-affinitykm4hq -- /bin/sh -x -c nc -zv -t -w 2 10.149.248.24 31462'
Aug 23 23:31:34.433: INFO: stderr: "+ nc -zv -t -w 2 10.149.248.24 31462\nConnection to 10.149.248.24 31462 port [tcp/31462] succeeded!\n"
Aug 23 23:31:34.433: INFO: stdout: ""
Aug 23 23:31:34.433: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585719795 --namespace=services-8373 exec execpod-affinitykm4hq -- /bin/sh -x -c nc -zv -t -w 2 169.45.209.221 31462'
Aug 23 23:31:35.367: INFO: stderr: "+ nc -zv -t -w 2 169.45.209.221 31462\nConnection to 169.45.209.221 31462 port [tcp/31462] succeeded!\n"
Aug 23 23:31:35.367: INFO: stdout: ""
Aug 23 23:31:35.367: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585719795 --namespace=services-8373 exec execpod-affinitykm4hq -- /bin/sh -x -c nc -zv -t -w 2 169.45.209.222 31462'
Aug 23 23:31:37.115: INFO: stderr: "+ nc -zv -t -w 2 169.45.209.222 31462\nConnection to 169.45.209.222 31462 port [tcp/31462] succeeded!\n"
Aug 23 23:31:37.115: INFO: stdout: ""
Aug 23 23:31:37.115: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585719795 --namespace=services-8373 exec execpod-affinitykm4hq -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.149.248.24:31462/ ; done'
Aug 23 23:31:38.778: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.149.248.24:31462/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.149.248.24:31462/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.149.248.24:31462/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.149.248.24:31462/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.149.248.24:31462/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.149.248.24:31462/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.149.248.24:31462/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.149.248.24:31462/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.149.248.24:31462/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.149.248.24:31462/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.149.248.24:31462/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.149.248.24:31462/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.149.248.24:31462/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.149.248.24:31462/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.149.248.24:31462/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.149.248.24:31462/\n"
Aug 23 23:31:38.778: INFO: stdout: "\naffinity-nodeport-gq6lt\naffinity-nodeport-gq6lt\naffinity-nodeport-gq6lt\naffinity-nodeport-gq6lt\naffinity-nodeport-gq6lt\naffinity-nodeport-gq6lt\naffinity-nodeport-gq6lt\naffinity-nodeport-gq6lt\naffinity-nodeport-gq6lt\naffinity-nodeport-gq6lt\naffinity-nodeport-gq6lt\naffinity-nodeport-gq6lt\naffinity-nodeport-gq6lt\naffinity-nodeport-gq6lt\naffinity-nodeport-gq6lt\naffinity-nodeport-gq6lt"
Aug 23 23:31:38.778: INFO: Received response from host: affinity-nodeport-gq6lt
Aug 23 23:31:38.778: INFO: Received response from host: affinity-nodeport-gq6lt
Aug 23 23:31:38.778: INFO: Received response from host: affinity-nodeport-gq6lt
Aug 23 23:31:38.778: INFO: Received response from host: affinity-nodeport-gq6lt
Aug 23 23:31:38.778: INFO: Received response from host: affinity-nodeport-gq6lt
Aug 23 23:31:38.778: INFO: Received response from host: affinity-nodeport-gq6lt
Aug 23 23:31:38.778: INFO: Received response from host: affinity-nodeport-gq6lt
Aug 23 23:31:38.778: INFO: Received response from host: affinity-nodeport-gq6lt
Aug 23 23:31:38.778: INFO: Received response from host: affinity-nodeport-gq6lt
Aug 23 23:31:38.778: INFO: Received response from host: affinity-nodeport-gq6lt
Aug 23 23:31:38.778: INFO: Received response from host: affinity-nodeport-gq6lt
Aug 23 23:31:38.778: INFO: Received response from host: affinity-nodeport-gq6lt
Aug 23 23:31:38.778: INFO: Received response from host: affinity-nodeport-gq6lt
Aug 23 23:31:38.778: INFO: Received response from host: affinity-nodeport-gq6lt
Aug 23 23:31:38.778: INFO: Received response from host: affinity-nodeport-gq6lt
Aug 23 23:31:38.778: INFO: Received response from host: affinity-nodeport-gq6lt
Aug 23 23:31:38.778: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport in namespace services-8373, will wait for the garbage collector to delete the pods
Aug 23 23:31:39.204: INFO: Deleting ReplicationController affinity-nodeport took: 111.105499ms
Aug 23 23:31:39.405: INFO: Terminating ReplicationController affinity-nodeport pods took: 200.21869ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 23:31:56.649: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8373" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749

• [SLOW TEST:44.899 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity work for NodePort service [LinuxOnly] [Conformance]","total":311,"completed":276,"skipped":4764,"failed":0}
SSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 23:31:56.941: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Aug 23 23:31:57.573: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 23:31:58.674: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-1258" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition getting/updating/patching custom resource definition status sub-resource works  [Conformance]","total":311,"completed":277,"skipped":4773,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 23:31:59.265: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-1265
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a new StatefulSet
Aug 23 23:32:00.538: INFO: Found 1 stateful pods, waiting for 3
Aug 23 23:32:10.722: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Aug 23 23:32:10.722: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Aug 23 23:32:10.722: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Pending - Ready=false
Aug 23 23:32:20.624: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Aug 23 23:32:20.624: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Aug 23 23:32:20.624: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Aug 23 23:32:20.871: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585719795 --namespace=statefulset-1265 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Aug 23 23:32:22.069: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Aug 23 23:32:22.069: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Aug 23 23:32:22.069: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from docker.io/library/httpd:2.4.38-alpine to docker.io/library/httpd:2.4.39-alpine
Aug 23 23:32:32.236: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Updating Pods in reverse ordinal order
Aug 23 23:32:42.583: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585719795 --namespace=statefulset-1265 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 23 23:32:46.288: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Aug 23 23:32:46.288: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Aug 23 23:32:46.288: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Aug 23 23:32:57.191: INFO: Waiting for StatefulSet statefulset-1265/ss2 to complete update
Aug 23 23:32:57.191: INFO: Waiting for Pod statefulset-1265/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Aug 23 23:32:57.191: INFO: Waiting for Pod statefulset-1265/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Aug 23 23:33:07.460: INFO: Waiting for StatefulSet statefulset-1265/ss2 to complete update
Aug 23 23:33:07.460: INFO: Waiting for Pod statefulset-1265/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Aug 23 23:33:17.659: INFO: Waiting for StatefulSet statefulset-1265/ss2 to complete update
Aug 23 23:33:17.659: INFO: Waiting for Pod statefulset-1265/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Aug 23 23:33:27.408: INFO: Waiting for StatefulSet statefulset-1265/ss2 to complete update
STEP: Rolling back to a previous revision
Aug 23 23:33:37.434: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585719795 --namespace=statefulset-1265 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Aug 23 23:33:38.024: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Aug 23 23:33:38.024: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Aug 23 23:33:38.024: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Aug 23 23:33:48.652: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order
Aug 23 23:33:48.892: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585719795 --namespace=statefulset-1265 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 23 23:33:49.575: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Aug 23 23:33:49.575: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Aug 23 23:33:49.575: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Aug 23 23:33:49.911: INFO: Waiting for StatefulSet statefulset-1265/ss2 to complete update
Aug 23 23:33:49.911: INFO: Waiting for Pod statefulset-1265/ss2-0 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Aug 23 23:33:49.911: INFO: Waiting for Pod statefulset-1265/ss2-1 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Aug 23 23:33:49.911: INFO: Waiting for Pod statefulset-1265/ss2-2 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Aug 23 23:34:01.093: INFO: Waiting for StatefulSet statefulset-1265/ss2 to complete update
Aug 23 23:34:01.093: INFO: Waiting for Pod statefulset-1265/ss2-0 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Aug 23 23:34:01.093: INFO: Waiting for Pod statefulset-1265/ss2-1 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Aug 23 23:34:10.322: INFO: Waiting for StatefulSet statefulset-1265/ss2 to complete update
Aug 23 23:34:10.322: INFO: Waiting for Pod statefulset-1265/ss2-0 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Aug 23 23:34:20.245: INFO: Waiting for StatefulSet statefulset-1265/ss2 to complete update
Aug 23 23:34:20.245: INFO: Waiting for Pod statefulset-1265/ss2-0 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
Aug 23 23:34:30.223: INFO: Deleting all statefulset in ns statefulset-1265
Aug 23 23:34:30.336: INFO: Scaling statefulset ss2 to 0
Aug 23 23:35:11.068: INFO: Waiting for statefulset status.replicas updated to 0
Aug 23 23:35:11.185: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 23:35:11.618: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-1265" for this suite.

• [SLOW TEST:194.548 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
    should perform rolling updates and roll backs of template modifications [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should perform rolling updates and roll backs of template modifications [Conformance]","total":311,"completed":278,"skipped":4822,"failed":0}
SSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 23:35:13.813: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicaSet
STEP: Ensuring resource quota status captures replicaset creation
STEP: Deleting a ReplicaSet
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 23:35:26.581: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-420" for this suite.

• [SLOW TEST:14.740 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replica set. [Conformance]","total":311,"completed":279,"skipped":4826,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 23:35:28.553: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a service in the namespace
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there is no service in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 23:35:40.853: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-9673" for this suite.
STEP: Destroying namespace "nsdeletetest-1816" for this suite.
Aug 23 23:35:43.096: INFO: Namespace nsdeletetest-1816 was already deleted
STEP: Destroying namespace "nsdeletetest-9337" for this suite.

• [SLOW TEST:14.797 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all services are removed when a namespace is deleted [Conformance]","total":311,"completed":280,"skipped":4855,"failed":0}
SSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 23:35:43.351: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Aug 23 23:35:44.624: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Aug 23 23:35:57.317: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585719795 --namespace=crd-publish-openapi-2054 --namespace=crd-publish-openapi-2054 create -f -'
Aug 23 23:35:58.974: INFO: stderr: ""
Aug 23 23:35:58.974: INFO: stdout: "e2e-test-crd-publish-openapi-6434-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Aug 23 23:35:58.974: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585719795 --namespace=crd-publish-openapi-2054 --namespace=crd-publish-openapi-2054 delete e2e-test-crd-publish-openapi-6434-crds test-cr'
Aug 23 23:35:59.698: INFO: stderr: ""
Aug 23 23:35:59.698: INFO: stdout: "e2e-test-crd-publish-openapi-6434-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
Aug 23 23:35:59.698: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585719795 --namespace=crd-publish-openapi-2054 --namespace=crd-publish-openapi-2054 apply -f -'
Aug 23 23:36:00.718: INFO: stderr: ""
Aug 23 23:36:00.718: INFO: stdout: "e2e-test-crd-publish-openapi-6434-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Aug 23 23:36:00.718: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585719795 --namespace=crd-publish-openapi-2054 --namespace=crd-publish-openapi-2054 delete e2e-test-crd-publish-openapi-6434-crds test-cr'
Aug 23 23:36:00.872: INFO: stderr: ""
Aug 23 23:36:00.872: INFO: stdout: "e2e-test-crd-publish-openapi-6434-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
Aug 23 23:36:00.872: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585719795 --namespace=crd-publish-openapi-2054 explain e2e-test-crd-publish-openapi-6434-crds'
Aug 23 23:36:02.302: INFO: stderr: ""
Aug 23 23:36:02.302: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-6434-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 23:36:12.493: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-2054" for this suite.

• [SLOW TEST:29.338 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields in an embedded object [Conformance]","total":311,"completed":281,"skipped":4862,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 23:36:12.689: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:41
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating the pod
Aug 23 23:36:22.516: INFO: Successfully updated pod "annotationupdateea8995a2-d16a-461d-be60-ecbda32873f9"
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 23:36:24.811: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7394" for this suite.

• [SLOW TEST:12.704 seconds]
[sig-storage] Downward API volume
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:36
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should update annotations on modification [NodeConformance] [Conformance]","total":311,"completed":282,"skipped":4906,"failed":0}
SSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 23:36:25.393: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating projection with secret that has name projected-secret-test-febdd5d2-8dde-4aa1-acd4-2dad8622f50a
STEP: Creating a pod to test consume secrets
Aug 23 23:36:27.232: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-808f87fa-dc13-4143-a058-24d40ff757df" in namespace "projected-8030" to be "Succeeded or Failed"
Aug 23 23:36:27.466: INFO: Pod "pod-projected-secrets-808f87fa-dc13-4143-a058-24d40ff757df": Phase="Pending", Reason="", readiness=false. Elapsed: 234.297829ms
Aug 23 23:36:29.615: INFO: Pod "pod-projected-secrets-808f87fa-dc13-4143-a058-24d40ff757df": Phase="Pending", Reason="", readiness=false. Elapsed: 2.383053172s
Aug 23 23:36:31.776: INFO: Pod "pod-projected-secrets-808f87fa-dc13-4143-a058-24d40ff757df": Phase="Pending", Reason="", readiness=false. Elapsed: 4.544169048s
Aug 23 23:36:33.969: INFO: Pod "pod-projected-secrets-808f87fa-dc13-4143-a058-24d40ff757df": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.736695706s
STEP: Saw pod success
Aug 23 23:36:33.969: INFO: Pod "pod-projected-secrets-808f87fa-dc13-4143-a058-24d40ff757df" satisfied condition "Succeeded or Failed"
Aug 23 23:36:34.171: INFO: Trying to get logs from node 10.149.248.24 pod pod-projected-secrets-808f87fa-dc13-4143-a058-24d40ff757df container projected-secret-volume-test: <nil>
STEP: delete the pod
Aug 23 23:36:34.601: INFO: Waiting for pod pod-projected-secrets-808f87fa-dc13-4143-a058-24d40ff757df to disappear
Aug 23 23:36:34.829: INFO: Pod pod-projected-secrets-808f87fa-dc13-4143-a058-24d40ff757df no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 23:36:34.829: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8030" for this suite.

• [SLOW TEST:10.124 seconds]
[sig-storage] Projected secret
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":283,"skipped":4909,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Service endpoints latency 
  should not be very high  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Service endpoints latency
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 23:36:35.519: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename svc-latency
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be very high  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Aug 23 23:36:36.544: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: creating replication controller svc-latency-rc in namespace svc-latency-2997
I0823 23:36:36.822948      25 runners.go:190] Created replication controller with name: svc-latency-rc, namespace: svc-latency-2997, replica count: 1
I0823 23:36:38.073254      25 runners.go:190] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0823 23:36:39.073500      25 runners.go:190] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0823 23:36:40.073662      25 runners.go:190] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0823 23:36:41.073852      25 runners.go:190] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0823 23:36:42.074039      25 runners.go:190] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Aug 23 23:36:42.451: INFO: Created: latency-svc-xjcpb
Aug 23 23:36:42.468: INFO: Got endpoints: latency-svc-xjcpb [194.26292ms]
Aug 23 23:36:42.609: INFO: Created: latency-svc-9pwtt
Aug 23 23:36:42.616: INFO: Got endpoints: latency-svc-9pwtt [147.290251ms]
Aug 23 23:36:42.689: INFO: Created: latency-svc-9qvtg
Aug 23 23:36:42.698: INFO: Created: latency-svc-zvgl5
Aug 23 23:36:42.698: INFO: Got endpoints: latency-svc-zvgl5 [228.854622ms]
Aug 23 23:36:42.698: INFO: Got endpoints: latency-svc-9qvtg [229.141068ms]
Aug 23 23:36:42.706: INFO: Created: latency-svc-c4754
Aug 23 23:36:42.715: INFO: Created: latency-svc-sz52j
Aug 23 23:36:42.716: INFO: Got endpoints: latency-svc-c4754 [244.889513ms]
Aug 23 23:36:42.732: INFO: Got endpoints: latency-svc-sz52j [262.414964ms]
Aug 23 23:36:42.740: INFO: Created: latency-svc-wtlsw
Aug 23 23:36:42.748: INFO: Created: latency-svc-6zdjw
Aug 23 23:36:42.749: INFO: Got endpoints: latency-svc-wtlsw [279.010748ms]
Aug 23 23:36:42.765: INFO: Got endpoints: latency-svc-6zdjw [295.11147ms]
Aug 23 23:36:42.772: INFO: Created: latency-svc-jh56v
Aug 23 23:36:42.782: INFO: Got endpoints: latency-svc-jh56v [311.199587ms]
Aug 23 23:36:42.798: INFO: Created: latency-svc-bxhns
Aug 23 23:36:42.817: INFO: Got endpoints: latency-svc-bxhns [345.355422ms]
Aug 23 23:36:42.817: INFO: Created: latency-svc-95mx4
Aug 23 23:36:42.823: INFO: Got endpoints: latency-svc-95mx4 [353.697031ms]
Aug 23 23:36:42.832: INFO: Created: latency-svc-cqmpp
Aug 23 23:36:42.840: INFO: Created: latency-svc-c795l
Aug 23 23:36:42.840: INFO: Got endpoints: latency-svc-cqmpp [369.986392ms]
Aug 23 23:36:42.850: INFO: Got endpoints: latency-svc-c795l [378.726646ms]
Aug 23 23:36:42.858: INFO: Created: latency-svc-zzb2l
Aug 23 23:36:42.866: INFO: Got endpoints: latency-svc-zzb2l [395.025644ms]
Aug 23 23:36:42.882: INFO: Created: latency-svc-7k5fj
Aug 23 23:36:42.889: INFO: Created: latency-svc-m24td
Aug 23 23:36:42.905: INFO: Got endpoints: latency-svc-m24td [288.717917ms]
Aug 23 23:36:42.905: INFO: Got endpoints: latency-svc-7k5fj [433.402213ms]
Aug 23 23:36:42.912: INFO: Created: latency-svc-b7z2d
Aug 23 23:36:42.919: INFO: Got endpoints: latency-svc-b7z2d [447.275754ms]
Aug 23 23:36:42.927: INFO: Created: latency-svc-vb5dr
Aug 23 23:36:42.943: INFO: Got endpoints: latency-svc-vb5dr [244.968088ms]
Aug 23 23:36:42.951: INFO: Created: latency-svc-ltcdm
Aug 23 23:36:42.959: INFO: Got endpoints: latency-svc-ltcdm [260.76557ms]
Aug 23 23:36:42.966: INFO: Created: latency-svc-r4jgf
Aug 23 23:36:42.966: INFO: Got endpoints: latency-svc-r4jgf [233.757217ms]
Aug 23 23:36:42.973: INFO: Created: latency-svc-j2v47
Aug 23 23:36:42.991: INFO: Got endpoints: latency-svc-j2v47 [275.069625ms]
Aug 23 23:36:42.999: INFO: Created: latency-svc-jf9w5
Aug 23 23:36:43.015: INFO: Got endpoints: latency-svc-jf9w5 [266.157233ms]
Aug 23 23:36:43.015: INFO: Created: latency-svc-9xkgp
Aug 23 23:36:43.023: INFO: Created: latency-svc-pkknd
Aug 23 23:36:43.024: INFO: Got endpoints: latency-svc-9xkgp [258.260425ms]
Aug 23 23:36:43.040: INFO: Got endpoints: latency-svc-pkknd [258.468749ms]
Aug 23 23:36:43.048: INFO: Created: latency-svc-7f95l
Aug 23 23:36:43.056: INFO: Created: latency-svc-g9pgq
Aug 23 23:36:43.071: INFO: Created: latency-svc-f65zp
Aug 23 23:36:43.071: INFO: Got endpoints: latency-svc-g9pgq [247.831646ms]
Aug 23 23:36:43.071: INFO: Got endpoints: latency-svc-7f95l [254.56893ms]
Aug 23 23:36:43.079: INFO: Created: latency-svc-wzh9l
Aug 23 23:36:43.079: INFO: Got endpoints: latency-svc-f65zp [228.947043ms]
Aug 23 23:36:43.088: INFO: Got endpoints: latency-svc-wzh9l [247.21618ms]
Aug 23 23:36:43.096: INFO: Created: latency-svc-khq8n
Aug 23 23:36:43.103: INFO: Got endpoints: latency-svc-khq8n [236.257825ms]
Aug 23 23:36:43.103: INFO: Created: latency-svc-mqmm4
Aug 23 23:36:43.119: INFO: Got endpoints: latency-svc-mqmm4 [214.19023ms]
Aug 23 23:36:43.127: INFO: Created: latency-svc-x7c9w
Aug 23 23:36:43.136: INFO: Got endpoints: latency-svc-x7c9w [217.008703ms]
Aug 23 23:36:43.136: INFO: Created: latency-svc-g6cs5
Aug 23 23:36:43.152: INFO: Got endpoints: latency-svc-g6cs5 [247.169187ms]
Aug 23 23:36:43.161: INFO: Created: latency-svc-d2lg4
Aug 23 23:36:43.168: INFO: Got endpoints: latency-svc-d2lg4 [224.685926ms]
Aug 23 23:36:43.168: INFO: Created: latency-svc-v7t8l
Aug 23 23:36:43.175: INFO: Created: latency-svc-s2tsb
Aug 23 23:36:43.175: INFO: Got endpoints: latency-svc-v7t8l [209.145598ms]
Aug 23 23:36:43.184: INFO: Got endpoints: latency-svc-s2tsb [225.057124ms]
Aug 23 23:36:43.192: INFO: Created: latency-svc-h7h2n
Aug 23 23:36:43.209: INFO: Created: latency-svc-r6pbq
Aug 23 23:36:43.209: INFO: Got endpoints: latency-svc-h7h2n [218.249885ms]
Aug 23 23:36:43.227: INFO: Got endpoints: latency-svc-r6pbq [211.592188ms]
Aug 23 23:36:43.234: INFO: Created: latency-svc-vrrfx
Aug 23 23:36:43.241: INFO: Got endpoints: latency-svc-vrrfx [217.470982ms]
Aug 23 23:36:43.258: INFO: Created: latency-svc-wt96r
Aug 23 23:36:43.265: INFO: Created: latency-svc-jqcrp
Aug 23 23:36:43.274: INFO: Got endpoints: latency-svc-wt96r [233.12647ms]
Aug 23 23:36:43.282: INFO: Created: latency-svc-m94fh
Aug 23 23:36:43.282: INFO: Got endpoints: latency-svc-jqcrp [210.855167ms]
Aug 23 23:36:43.307: INFO: Got endpoints: latency-svc-m94fh [235.530362ms]
Aug 23 23:36:43.314: INFO: Created: latency-svc-z5gc4
Aug 23 23:36:43.322: INFO: Got endpoints: latency-svc-z5gc4 [243.044055ms]
Aug 23 23:36:43.330: INFO: Created: latency-svc-dgwmh
Aug 23 23:36:43.338: INFO: Created: latency-svc-787bk
Aug 23 23:36:43.338: INFO: Got endpoints: latency-svc-dgwmh [250.489787ms]
Aug 23 23:36:43.346: INFO: Created: latency-svc-zjqht
Aug 23 23:36:43.346: INFO: Got endpoints: latency-svc-787bk [243.718721ms]
Aug 23 23:36:43.353: INFO: Got endpoints: latency-svc-zjqht [234.114764ms]
Aug 23 23:36:43.364: INFO: Created: latency-svc-l4kjx
Aug 23 23:36:43.368: INFO: Got endpoints: latency-svc-l4kjx [231.511109ms]
Aug 23 23:36:43.376: INFO: Created: latency-svc-kngvp
Aug 23 23:36:43.383: INFO: Got endpoints: latency-svc-kngvp [230.911187ms]
Aug 23 23:36:43.391: INFO: Created: latency-svc-m8wv7
Aug 23 23:36:43.404: INFO: Created: latency-svc-765qv
Aug 23 23:36:43.411: INFO: Got endpoints: latency-svc-765qv [235.763916ms]
Aug 23 23:36:43.412: INFO: Got endpoints: latency-svc-m8wv7 [243.642485ms]
Aug 23 23:36:43.418: INFO: Created: latency-svc-qqv5s
Aug 23 23:36:43.438: INFO: Created: latency-svc-w78jf
Aug 23 23:36:43.438: INFO: Got endpoints: latency-svc-qqv5s [254.336967ms]
Aug 23 23:36:43.455: INFO: Got endpoints: latency-svc-w78jf [245.332601ms]
Aug 23 23:36:43.462: INFO: Created: latency-svc-sx7zn
Aug 23 23:36:43.483: INFO: Got endpoints: latency-svc-sx7zn [256.153293ms]
Aug 23 23:36:43.492: INFO: Created: latency-svc-qt84q
Aug 23 23:36:43.501: INFO: Got endpoints: latency-svc-qt84q [260.318955ms]
Aug 23 23:36:43.502: INFO: Created: latency-svc-b5rvm
Aug 23 23:36:43.523: INFO: Got endpoints: latency-svc-b5rvm [249.383268ms]
Aug 23 23:36:43.532: INFO: Created: latency-svc-wczvq
Aug 23 23:36:43.542: INFO: Created: latency-svc-jvwgl
Aug 23 23:36:43.542: INFO: Got endpoints: latency-svc-wczvq [259.712739ms]
Aug 23 23:36:43.562: INFO: Got endpoints: latency-svc-jvwgl [254.727776ms]
Aug 23 23:36:43.562: INFO: Created: latency-svc-8b4hp
Aug 23 23:36:43.562: INFO: Created: latency-svc-bssv6
Aug 23 23:36:43.562: INFO: Got endpoints: latency-svc-bssv6 [240.172174ms]
Aug 23 23:36:43.571: INFO: Created: latency-svc-lwlld
Aug 23 23:36:43.571: INFO: Got endpoints: latency-svc-8b4hp [232.649062ms]
Aug 23 23:36:43.587: INFO: Got endpoints: latency-svc-lwlld [240.272897ms]
Aug 23 23:36:43.593: INFO: Created: latency-svc-g7bgx
Aug 23 23:36:43.607: INFO: Created: latency-svc-thpw5
Aug 23 23:36:43.607: INFO: Got endpoints: latency-svc-g7bgx [239.761955ms]
Aug 23 23:36:43.616: INFO: Created: latency-svc-qrsjc
Aug 23 23:36:43.616: INFO: Got endpoints: latency-svc-thpw5 [263.174797ms]
Aug 23 23:36:43.625: INFO: Created: latency-svc-qhqdp
Aug 23 23:36:43.625: INFO: Got endpoints: latency-svc-qrsjc [242.121217ms]
Aug 23 23:36:43.633: INFO: Got endpoints: latency-svc-qhqdp [221.153412ms]
Aug 23 23:36:43.633: INFO: Created: latency-svc-rzfls
Aug 23 23:36:43.647: INFO: Created: latency-svc-669qp
Aug 23 23:36:43.647: INFO: Got endpoints: latency-svc-rzfls [208.315535ms]
Aug 23 23:36:43.653: INFO: Got endpoints: latency-svc-669qp [241.467421ms]
Aug 23 23:36:43.654: INFO: Created: latency-svc-zzc66
Aug 23 23:36:43.679: INFO: Created: latency-svc-w4plv
Aug 23 23:36:43.679: INFO: Got endpoints: latency-svc-zzc66 [224.530149ms]
Aug 23 23:36:43.694: INFO: Created: latency-svc-x66ff
Aug 23 23:36:43.694: INFO: Got endpoints: latency-svc-w4plv [211.093214ms]
Aug 23 23:36:43.710: INFO: Got endpoints: latency-svc-x66ff [208.688609ms]
Aug 23 23:36:43.710: INFO: Created: latency-svc-222k8
Aug 23 23:36:43.724: INFO: Got endpoints: latency-svc-222k8 [201.433888ms]
Aug 23 23:36:43.779: INFO: Created: latency-svc-w7nsm
Aug 23 23:36:43.779: INFO: Got endpoints: latency-svc-w7nsm [217.472268ms]
Aug 23 23:36:43.788: INFO: Created: latency-svc-bsjsm
Aug 23 23:36:43.810: INFO: Got endpoints: latency-svc-bsjsm [267.71604ms]
Aug 23 23:36:43.810: INFO: Created: latency-svc-dgccf
Aug 23 23:36:43.817: INFO: Created: latency-svc-29n5z
Aug 23 23:36:43.817: INFO: Got endpoints: latency-svc-dgccf [254.94376ms]
Aug 23 23:36:43.825: INFO: Got endpoints: latency-svc-29n5z [217.542953ms]
Aug 23 23:36:43.831: INFO: Created: latency-svc-rkpgq
Aug 23 23:36:43.844: INFO: Got endpoints: latency-svc-rkpgq [257.439345ms]
Aug 23 23:36:43.844: INFO: Created: latency-svc-jzzn6
Aug 23 23:36:43.861: INFO: Got endpoints: latency-svc-jzzn6 [289.852383ms]
Aug 23 23:36:43.869: INFO: Created: latency-svc-tpvbx
Aug 23 23:36:43.886: INFO: Got endpoints: latency-svc-tpvbx [269.612871ms]
Aug 23 23:36:43.894: INFO: Created: latency-svc-46nnr
Aug 23 23:36:43.926: INFO: Created: latency-svc-2pqqm
Aug 23 23:36:43.926: INFO: Got endpoints: latency-svc-46nnr [300.826204ms]
Aug 23 23:36:43.942: INFO: Got endpoints: latency-svc-2pqqm [294.603768ms]
Aug 23 23:36:43.949: INFO: Created: latency-svc-gtnqs
Aug 23 23:36:43.957: INFO: Created: latency-svc-ddrtf
Aug 23 23:36:43.958: INFO: Got endpoints: latency-svc-gtnqs [325.450454ms]
Aug 23 23:36:43.965: INFO: Created: latency-svc-rdjg6
Aug 23 23:36:43.972: INFO: Got endpoints: latency-svc-rdjg6 [292.218959ms]
Aug 23 23:36:43.972: INFO: Got endpoints: latency-svc-ddrtf [318.52284ms]
Aug 23 23:36:43.978: INFO: Created: latency-svc-2kg7q
Aug 23 23:36:43.991: INFO: Created: latency-svc-ccsq5
Aug 23 23:36:43.991: INFO: Got endpoints: latency-svc-2kg7q [280.803942ms]
Aug 23 23:36:43.998: INFO: Created: latency-svc-tl5lt
Aug 23 23:36:44.008: INFO: Got endpoints: latency-svc-ccsq5 [283.089755ms]
Aug 23 23:36:44.017: INFO: Got endpoints: latency-svc-tl5lt [322.938754ms]
Aug 23 23:36:44.020: INFO: Created: latency-svc-sh849
Aug 23 23:36:44.026: INFO: Created: latency-svc-4g4rg
Aug 23 23:36:44.026: INFO: Got endpoints: latency-svc-sh849 [247.26007ms]
Aug 23 23:36:44.037: INFO: Got endpoints: latency-svc-4g4rg [226.645526ms]
Aug 23 23:36:44.101: INFO: Created: latency-svc-mssgf
Aug 23 23:36:44.107: INFO: Got endpoints: latency-svc-mssgf [282.430817ms]
Aug 23 23:36:44.114: INFO: Created: latency-svc-vftgs
Aug 23 23:36:44.127: INFO: Got endpoints: latency-svc-vftgs [283.040549ms]
Aug 23 23:36:44.128: INFO: Created: latency-svc-p4p9r
Aug 23 23:36:44.134: INFO: Got endpoints: latency-svc-p4p9r [316.450426ms]
Aug 23 23:36:44.141: INFO: Created: latency-svc-4k7bm
Aug 23 23:36:44.158: INFO: Got endpoints: latency-svc-4k7bm [296.802504ms]
Aug 23 23:36:44.166: INFO: Created: latency-svc-rsmxq
Aug 23 23:36:44.184: INFO: Created: latency-svc-96nc5
Aug 23 23:36:44.184: INFO: Got endpoints: latency-svc-rsmxq [257.439669ms]
Aug 23 23:36:44.201: INFO: Created: latency-svc-f78gk
Aug 23 23:36:44.201: INFO: Got endpoints: latency-svc-96nc5 [315.14234ms]
Aug 23 23:36:44.210: INFO: Created: latency-svc-8mrpp
Aug 23 23:36:44.210: INFO: Got endpoints: latency-svc-f78gk [268.078532ms]
Aug 23 23:36:44.223: INFO: Got endpoints: latency-svc-8mrpp [251.451378ms]
Aug 23 23:36:44.229: INFO: Created: latency-svc-dnqbq
Aug 23 23:36:44.237: INFO: Got endpoints: latency-svc-dnqbq [278.287677ms]
Aug 23 23:36:44.243: INFO: Created: latency-svc-7qzq5
Aug 23 23:36:44.252: INFO: Got endpoints: latency-svc-7qzq5 [279.449844ms]
Aug 23 23:36:44.276: INFO: Created: latency-svc-96x2s
Aug 23 23:36:44.285: INFO: Got endpoints: latency-svc-96x2s [294.295175ms]
Aug 23 23:36:44.286: INFO: Created: latency-svc-dc94b
Aug 23 23:36:44.293: INFO: Got endpoints: latency-svc-dc94b [285.373237ms]
Aug 23 23:36:44.301: INFO: Created: latency-svc-25shk
Aug 23 23:36:44.317: INFO: Got endpoints: latency-svc-25shk [289.982315ms]
Aug 23 23:36:44.325: INFO: Created: latency-svc-p6ckc
Aug 23 23:36:44.333: INFO: Created: latency-svc-2bftc
Aug 23 23:36:44.334: INFO: Got endpoints: latency-svc-p6ckc [316.365807ms]
Aug 23 23:36:44.350: INFO: Created: latency-svc-mxkmn
Aug 23 23:36:44.350: INFO: Got endpoints: latency-svc-2bftc [313.732856ms]
Aug 23 23:36:44.365: INFO: Created: latency-svc-4zxxr
Aug 23 23:36:44.365: INFO: Got endpoints: latency-svc-mxkmn [257.869866ms]
Aug 23 23:36:44.384: INFO: Got endpoints: latency-svc-4zxxr [256.667985ms]
Aug 23 23:36:44.384: INFO: Created: latency-svc-d4hhp
Aug 23 23:36:44.390: INFO: Got endpoints: latency-svc-d4hhp [256.888911ms]
Aug 23 23:36:44.459: INFO: Created: latency-svc-htsmf
Aug 23 23:36:44.470: INFO: Got endpoints: latency-svc-htsmf [247.197415ms]
Aug 23 23:36:44.477: INFO: Created: latency-svc-5vzhn
Aug 23 23:36:44.484: INFO: Created: latency-svc-sl9qb
Aug 23 23:36:44.496: INFO: Got endpoints: latency-svc-sl9qb [294.336897ms]
Aug 23 23:36:44.496: INFO: Got endpoints: latency-svc-5vzhn [312.366897ms]
Aug 23 23:36:44.502: INFO: Created: latency-svc-6m56s
Aug 23 23:36:44.517: INFO: Got endpoints: latency-svc-6m56s [307.257128ms]
Aug 23 23:36:44.525: INFO: Created: latency-svc-v6wnq
Aug 23 23:36:44.542: INFO: Got endpoints: latency-svc-v6wnq [383.74819ms]
Aug 23 23:36:44.558: INFO: Created: latency-svc-s4mqr
Aug 23 23:36:44.565: INFO: Created: latency-svc-wbcn5
Aug 23 23:36:44.566: INFO: Got endpoints: latency-svc-s4mqr [328.952537ms]
Aug 23 23:36:44.572: INFO: Got endpoints: latency-svc-wbcn5 [286.689401ms]
Aug 23 23:36:44.579: INFO: Created: latency-svc-q2n27
Aug 23 23:36:44.588: INFO: Got endpoints: latency-svc-q2n27 [336.634926ms]
Aug 23 23:36:44.595: INFO: Created: latency-svc-trcr6
Aug 23 23:36:44.595: INFO: Got endpoints: latency-svc-trcr6 [301.941313ms]
Aug 23 23:36:44.604: INFO: Created: latency-svc-ncw75
Aug 23 23:36:44.612: INFO: Got endpoints: latency-svc-ncw75 [295.63035ms]
Aug 23 23:36:44.641: INFO: Created: latency-svc-n7stm
Aug 23 23:36:44.658: INFO: Got endpoints: latency-svc-n7stm [292.852478ms]
Aug 23 23:36:44.675: INFO: Created: latency-svc-c22js
Aug 23 23:36:44.683: INFO: Created: latency-svc-vfqxr
Aug 23 23:36:44.683: INFO: Got endpoints: latency-svc-c22js [292.4823ms]
Aug 23 23:36:44.698: INFO: Created: latency-svc-l8rsx
Aug 23 23:36:44.707: INFO: Got endpoints: latency-svc-l8rsx [373.674526ms]
Aug 23 23:36:44.708: INFO: Got endpoints: latency-svc-vfqxr [357.624757ms]
Aug 23 23:36:44.715: INFO: Created: latency-svc-v527h
Aug 23 23:36:44.724: INFO: Got endpoints: latency-svc-v527h [339.868065ms]
Aug 23 23:36:44.732: INFO: Created: latency-svc-d8ps4
Aug 23 23:36:44.739: INFO: Created: latency-svc-cg294
Aug 23 23:36:44.739: INFO: Got endpoints: latency-svc-d8ps4 [268.665154ms]
Aug 23 23:36:44.754: INFO: Created: latency-svc-s6xfl
Aug 23 23:36:44.754: INFO: Got endpoints: latency-svc-cg294 [258.095442ms]
Aug 23 23:36:44.762: INFO: Got endpoints: latency-svc-s6xfl [265.54154ms]
Aug 23 23:36:44.784: INFO: Created: latency-svc-vtw8j
Aug 23 23:36:44.793: INFO: Created: latency-svc-nhjvz
Aug 23 23:36:44.802: INFO: Got endpoints: latency-svc-vtw8j [284.770591ms]
Aug 23 23:36:44.825: INFO: Created: latency-svc-9vllw
Aug 23 23:36:44.826: INFO: Got endpoints: latency-svc-nhjvz [230.116843ms]
Aug 23 23:36:44.834: INFO: Got endpoints: latency-svc-9vllw [245.70084ms]
Aug 23 23:36:44.842: INFO: Created: latency-svc-q8ngc
Aug 23 23:36:44.848: INFO: Got endpoints: latency-svc-q8ngc [282.811418ms]
Aug 23 23:36:44.856: INFO: Created: latency-svc-wq7zg
Aug 23 23:36:44.864: INFO: Got endpoints: latency-svc-wq7zg [292.136226ms]
Aug 23 23:36:44.865: INFO: Created: latency-svc-5s78x
Aug 23 23:36:44.881: INFO: Got endpoints: latency-svc-5s78x [338.924845ms]
Aug 23 23:36:44.881: INFO: Created: latency-svc-f89lx
Aug 23 23:36:44.889: INFO: Created: latency-svc-6cf6z
Aug 23 23:36:44.889: INFO: Got endpoints: latency-svc-f89lx [276.415311ms]
Aug 23 23:36:44.905: INFO: Got endpoints: latency-svc-6cf6z [246.89235ms]
Aug 23 23:36:44.913: INFO: Created: latency-svc-qm7r9
Aug 23 23:36:44.922: INFO: Got endpoints: latency-svc-qm7r9 [238.988348ms]
Aug 23 23:36:44.936: INFO: Created: latency-svc-djkks
Aug 23 23:36:44.950: INFO: Created: latency-svc-fzdzn
Aug 23 23:36:44.950: INFO: Got endpoints: latency-svc-fzdzn [195.86436ms]
Aug 23 23:36:44.951: INFO: Got endpoints: latency-svc-djkks [243.117586ms]
Aug 23 23:36:44.958: INFO: Created: latency-svc-fwwpn
Aug 23 23:36:44.974: INFO: Got endpoints: latency-svc-fwwpn [249.631385ms]
Aug 23 23:36:44.974: INFO: Created: latency-svc-dkqdq
Aug 23 23:36:44.982: INFO: Created: latency-svc-r8782
Aug 23 23:36:44.983: INFO: Got endpoints: latency-svc-dkqdq [243.441105ms]
Aug 23 23:36:44.989: INFO: Got endpoints: latency-svc-r8782 [280.384362ms]
Aug 23 23:36:45.013: INFO: Created: latency-svc-757df
Aug 23 23:36:45.028: INFO: Created: latency-svc-654p2
Aug 23 23:36:45.029: INFO: Got endpoints: latency-svc-757df [267.10479ms]
Aug 23 23:36:45.043: INFO: Got endpoints: latency-svc-654p2 [241.411381ms]
Aug 23 23:36:45.050: INFO: Created: latency-svc-96rrf
Aug 23 23:36:45.050: INFO: Got endpoints: latency-svc-96rrf [224.744867ms]
Aug 23 23:36:45.057: INFO: Created: latency-svc-zwgsg
Aug 23 23:36:45.064: INFO: Got endpoints: latency-svc-zwgsg [229.882597ms]
Aug 23 23:36:45.094: INFO: Created: latency-svc-zh9hj
Aug 23 23:36:45.118: INFO: Created: latency-svc-7nhj2
Aug 23 23:36:45.118: INFO: Got endpoints: latency-svc-zh9hj [269.812275ms]
Aug 23 23:36:45.143: INFO: Created: latency-svc-np7wq
Aug 23 23:36:45.143: INFO: Got endpoints: latency-svc-7nhj2 [262.100104ms]
Aug 23 23:36:45.158: INFO: Created: latency-svc-z54df
Aug 23 23:36:45.158: INFO: Got endpoints: latency-svc-np7wq [269.388349ms]
Aug 23 23:36:45.166: INFO: Got endpoints: latency-svc-z54df [301.776798ms]
Aug 23 23:36:45.200: INFO: Created: latency-svc-m2f9x
Aug 23 23:36:45.223: INFO: Created: latency-svc-shvlb
Aug 23 23:36:45.223: INFO: Got endpoints: latency-svc-m2f9x [300.811124ms]
Aug 23 23:36:45.231: INFO: Got endpoints: latency-svc-shvlb [280.38707ms]
Aug 23 23:36:45.237: INFO: Created: latency-svc-l9ch9
Aug 23 23:36:45.244: INFO: Created: latency-svc-fjkzm
Aug 23 23:36:45.244: INFO: Got endpoints: latency-svc-l9ch9 [338.851059ms]
Aug 23 23:36:45.252: INFO: Got endpoints: latency-svc-fjkzm [300.924436ms]
Aug 23 23:36:45.252: INFO: Created: latency-svc-qjsgk
Aug 23 23:36:45.268: INFO: Created: latency-svc-l7nj7
Aug 23 23:36:45.268: INFO: Got endpoints: latency-svc-qjsgk [279.395777ms]
Aug 23 23:36:45.275: INFO: Got endpoints: latency-svc-l7nj7 [301.254733ms]
Aug 23 23:36:45.275: INFO: Created: latency-svc-5tt67
Aug 23 23:36:45.288: INFO: Created: latency-svc-gll2j
Aug 23 23:36:45.288: INFO: Got endpoints: latency-svc-5tt67 [305.483272ms]
Aug 23 23:36:45.302: INFO: Created: latency-svc-krz8f
Aug 23 23:36:45.302: INFO: Got endpoints: latency-svc-gll2j [258.38627ms]
Aug 23 23:36:45.315: INFO: Created: latency-svc-x5hfh
Aug 23 23:36:45.315: INFO: Got endpoints: latency-svc-krz8f [286.519608ms]
Aug 23 23:36:45.328: INFO: Got endpoints: latency-svc-x5hfh [277.130238ms]
Aug 23 23:36:45.362: INFO: Created: latency-svc-29bkz
Aug 23 23:36:45.370: INFO: Created: latency-svc-xvr84
Aug 23 23:36:45.370: INFO: Got endpoints: latency-svc-29bkz [226.742696ms]
Aug 23 23:36:45.385: INFO: Created: latency-svc-swjcq
Aug 23 23:36:45.386: INFO: Got endpoints: latency-svc-xvr84 [267.406862ms]
Aug 23 23:36:45.395: INFO: Got endpoints: latency-svc-swjcq [330.621266ms]
Aug 23 23:36:45.400: INFO: Created: latency-svc-7ll4q
Aug 23 23:36:45.415: INFO: Created: latency-svc-p2ldr
Aug 23 23:36:45.415: INFO: Got endpoints: latency-svc-7ll4q [248.924835ms]
Aug 23 23:36:45.428: INFO: Got endpoints: latency-svc-p2ldr [269.954798ms]
Aug 23 23:36:45.429: INFO: Created: latency-svc-8wk68
Aug 23 23:36:45.453: INFO: Created: latency-svc-9k7mz
Aug 23 23:36:45.460: INFO: Created: latency-svc-grvpc
Aug 23 23:36:45.474: INFO: Created: latency-svc-gm5v5
Aug 23 23:36:45.533: INFO: Got endpoints: latency-svc-8wk68 [288.909197ms]
Aug 23 23:36:45.541: INFO: Got endpoints: latency-svc-grvpc [309.926474ms]
Aug 23 23:36:45.541: INFO: Got endpoints: latency-svc-9k7mz [317.637303ms]
Aug 23 23:36:45.548: INFO: Created: latency-svc-7lqgx
Aug 23 23:36:45.548: INFO: Got endpoints: latency-svc-gm5v5 [295.992947ms]
Aug 23 23:36:45.555: INFO: Created: latency-svc-mvk65
Aug 23 23:36:45.555: INFO: Got endpoints: latency-svc-7lqgx [286.86248ms]
Aug 23 23:36:45.569: INFO: Got endpoints: latency-svc-mvk65 [294.101949ms]
Aug 23 23:36:45.569: INFO: Created: latency-svc-vqhzv
Aug 23 23:36:45.586: INFO: Created: latency-svc-rnhq6
Aug 23 23:36:45.586: INFO: Got endpoints: latency-svc-vqhzv [298.099931ms]
Aug 23 23:36:45.595: INFO: Created: latency-svc-bnjdd
Aug 23 23:36:45.595: INFO: Got endpoints: latency-svc-rnhq6 [293.351309ms]
Aug 23 23:36:45.603: INFO: Got endpoints: latency-svc-bnjdd [287.210456ms]
Aug 23 23:36:45.610: INFO: Created: latency-svc-qt8bs
Aug 23 23:36:45.639: INFO: Created: latency-svc-2k5nm
Aug 23 23:36:45.647: INFO: Got endpoints: latency-svc-qt8bs [319.292073ms]
Aug 23 23:36:45.650: INFO: Got endpoints: latency-svc-2k5nm [279.826946ms]
Aug 23 23:36:45.650: INFO: Created: latency-svc-94vrd
Aug 23 23:36:45.657: INFO: Got endpoints: latency-svc-94vrd [270.950964ms]
Aug 23 23:36:45.665: INFO: Created: latency-svc-kzvzw
Aug 23 23:36:45.680: INFO: Got endpoints: latency-svc-kzvzw [285.377449ms]
Aug 23 23:36:45.690: INFO: Created: latency-svc-ld84m
Aug 23 23:36:45.709: INFO: Created: latency-svc-p6lzx
Aug 23 23:36:45.709: INFO: Got endpoints: latency-svc-ld84m [293.5266ms]
Aug 23 23:36:45.727: INFO: Got endpoints: latency-svc-p6lzx [298.542948ms]
Aug 23 23:36:45.738: INFO: Created: latency-svc-x77sh
Aug 23 23:36:45.760: INFO: Created: latency-svc-4cmfl
Aug 23 23:36:45.761: INFO: Got endpoints: latency-svc-x77sh [227.490606ms]
Aug 23 23:36:45.771: INFO: Created: latency-svc-r86rs
Aug 23 23:36:45.771: INFO: Got endpoints: latency-svc-4cmfl [229.550583ms]
Aug 23 23:36:45.781: INFO: Got endpoints: latency-svc-r86rs [240.38748ms]
Aug 23 23:36:45.791: INFO: Created: latency-svc-jsmmc
Aug 23 23:36:45.813: INFO: Created: latency-svc-vmxp5
Aug 23 23:36:45.813: INFO: Got endpoints: latency-svc-jsmmc [257.910931ms]
Aug 23 23:36:45.824: INFO: Got endpoints: latency-svc-vmxp5 [275.51275ms]
Aug 23 23:36:45.846: INFO: Created: latency-svc-d9nf5
Aug 23 23:36:45.867: INFO: Got endpoints: latency-svc-d9nf5 [298.07576ms]
Aug 23 23:36:45.867: INFO: Created: latency-svc-xnldx
Aug 23 23:36:45.879: INFO: Got endpoints: latency-svc-xnldx [292.404619ms]
Aug 23 23:36:45.888: INFO: Created: latency-svc-qnkmc
Aug 23 23:36:45.888: INFO: Got endpoints: latency-svc-qnkmc [292.872344ms]
Aug 23 23:36:45.898: INFO: Created: latency-svc-bs9s2
Aug 23 23:36:45.907: INFO: Got endpoints: latency-svc-bs9s2 [304.70025ms]
Aug 23 23:36:45.908: INFO: Created: latency-svc-b52lx
Aug 23 23:36:45.926: INFO: Created: latency-svc-kjw4j
Aug 23 23:36:45.926: INFO: Got endpoints: latency-svc-b52lx [278.987961ms]
Aug 23 23:36:45.936: INFO: Got endpoints: latency-svc-kjw4j [286.489959ms]
Aug 23 23:36:45.946: INFO: Created: latency-svc-9w4s9
Aug 23 23:36:45.946: INFO: Created: latency-svc-42mkw
Aug 23 23:36:45.946: INFO: Got endpoints: latency-svc-42mkw [289.586663ms]
Aug 23 23:36:45.957: INFO: Got endpoints: latency-svc-9w4s9 [277.195296ms]
Aug 23 23:36:46.018: INFO: Created: latency-svc-phhhf
Aug 23 23:36:46.035: INFO: Created: latency-svc-ndmtf
Aug 23 23:36:46.036: INFO: Got endpoints: latency-svc-phhhf [211.964864ms]
Aug 23 23:36:46.044: INFO: Created: latency-svc-scl7q
Aug 23 23:36:46.044: INFO: Got endpoints: latency-svc-ndmtf [273.471622ms]
Aug 23 23:36:46.061: INFO: Created: latency-svc-lzwhr
Aug 23 23:36:46.062: INFO: Got endpoints: latency-svc-scl7q [334.626387ms]
Aug 23 23:36:46.072: INFO: Got endpoints: latency-svc-lzwhr [311.017906ms]
Aug 23 23:36:46.080: INFO: Created: latency-svc-5bcw6
Aug 23 23:36:46.095: INFO: Got endpoints: latency-svc-5bcw6 [313.089628ms]
Aug 23 23:36:46.101: INFO: Created: latency-svc-9q57h
Aug 23 23:36:46.106: INFO: Created: latency-svc-r5vjf
Aug 23 23:36:46.113: INFO: Got endpoints: latency-svc-9q57h [403.951521ms]
Aug 23 23:36:46.113: INFO: Got endpoints: latency-svc-r5vjf [299.813876ms]
Aug 23 23:36:46.120: INFO: Created: latency-svc-mn999
Aug 23 23:36:46.127: INFO: Got endpoints: latency-svc-mn999 [259.629963ms]
Aug 23 23:36:46.133: INFO: Created: latency-svc-b2fmp
Aug 23 23:36:46.150: INFO: Got endpoints: latency-svc-b2fmp [261.278901ms]
Aug 23 23:36:46.150: INFO: Created: latency-svc-pt29g
Aug 23 23:36:46.167: INFO: Created: latency-svc-bl5dx
Aug 23 23:36:46.167: INFO: Got endpoints: latency-svc-pt29g [287.615948ms]
Aug 23 23:36:46.174: INFO: Got endpoints: latency-svc-bl5dx [266.287848ms]
Aug 23 23:36:46.181: INFO: Created: latency-svc-rrcdm
Aug 23 23:36:46.190: INFO: Got endpoints: latency-svc-rrcdm [243.641308ms]
Aug 23 23:36:46.199: INFO: Created: latency-svc-h4d8z
Aug 23 23:36:46.212: INFO: Created: latency-svc-2cg4l
Aug 23 23:36:46.213: INFO: Got endpoints: latency-svc-h4d8z [286.481325ms]
Aug 23 23:36:46.219: INFO: Got endpoints: latency-svc-2cg4l [283.191544ms]
Aug 23 23:36:46.220: INFO: Latencies: [147.290251ms 195.86436ms 201.433888ms 208.315535ms 208.688609ms 209.145598ms 210.855167ms 211.093214ms 211.592188ms 211.964864ms 214.19023ms 217.008703ms 217.470982ms 217.472268ms 217.542953ms 218.249885ms 221.153412ms 224.530149ms 224.685926ms 224.744867ms 225.057124ms 226.645526ms 226.742696ms 227.490606ms 228.854622ms 228.947043ms 229.141068ms 229.550583ms 229.882597ms 230.116843ms 230.911187ms 231.511109ms 232.649062ms 233.12647ms 233.757217ms 234.114764ms 235.530362ms 235.763916ms 236.257825ms 238.988348ms 239.761955ms 240.172174ms 240.272897ms 240.38748ms 241.411381ms 241.467421ms 242.121217ms 243.044055ms 243.117586ms 243.441105ms 243.641308ms 243.642485ms 243.718721ms 244.889513ms 244.968088ms 245.332601ms 245.70084ms 246.89235ms 247.169187ms 247.197415ms 247.21618ms 247.26007ms 247.831646ms 248.924835ms 249.383268ms 249.631385ms 250.489787ms 251.451378ms 254.336967ms 254.56893ms 254.727776ms 254.94376ms 256.153293ms 256.667985ms 256.888911ms 257.439345ms 257.439669ms 257.869866ms 257.910931ms 258.095442ms 258.260425ms 258.38627ms 258.468749ms 259.629963ms 259.712739ms 260.318955ms 260.76557ms 261.278901ms 262.100104ms 262.414964ms 263.174797ms 265.54154ms 266.157233ms 266.287848ms 267.10479ms 267.406862ms 267.71604ms 268.078532ms 268.665154ms 269.388349ms 269.612871ms 269.812275ms 269.954798ms 270.950964ms 273.471622ms 275.069625ms 275.51275ms 276.415311ms 277.130238ms 277.195296ms 278.287677ms 278.987961ms 279.010748ms 279.395777ms 279.449844ms 279.826946ms 280.384362ms 280.38707ms 280.803942ms 282.430817ms 282.811418ms 283.040549ms 283.089755ms 283.191544ms 284.770591ms 285.373237ms 285.377449ms 286.481325ms 286.489959ms 286.519608ms 286.689401ms 286.86248ms 287.210456ms 287.615948ms 288.717917ms 288.909197ms 289.586663ms 289.852383ms 289.982315ms 292.136226ms 292.218959ms 292.404619ms 292.4823ms 292.852478ms 292.872344ms 293.351309ms 293.5266ms 294.101949ms 294.295175ms 294.336897ms 294.603768ms 295.11147ms 295.63035ms 295.992947ms 296.802504ms 298.07576ms 298.099931ms 298.542948ms 299.813876ms 300.811124ms 300.826204ms 300.924436ms 301.254733ms 301.776798ms 301.941313ms 304.70025ms 305.483272ms 307.257128ms 309.926474ms 311.017906ms 311.199587ms 312.366897ms 313.089628ms 313.732856ms 315.14234ms 316.365807ms 316.450426ms 317.637303ms 318.52284ms 319.292073ms 322.938754ms 325.450454ms 328.952537ms 330.621266ms 334.626387ms 336.634926ms 338.851059ms 338.924845ms 339.868065ms 345.355422ms 353.697031ms 357.624757ms 369.986392ms 373.674526ms 378.726646ms 383.74819ms 395.025644ms 403.951521ms 433.402213ms 447.275754ms]
Aug 23 23:36:46.220: INFO: 50 %ile: 269.612871ms
Aug 23 23:36:46.220: INFO: 90 %ile: 322.938754ms
Aug 23 23:36:46.220: INFO: 99 %ile: 433.402213ms
Aug 23 23:36:46.220: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 23:36:46.220: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svc-latency-2997" for this suite.

• [SLOW TEST:11.019 seconds]
[sig-network] Service endpoints latency
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should not be very high  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Service endpoints latency should not be very high  [Conformance]","total":311,"completed":284,"skipped":4953,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 23:36:46.540: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:162
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating the pod
Aug 23 23:36:47.087: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 23:36:57.608: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-9725" for this suite.

• [SLOW TEST:11.813 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]","total":311,"completed":285,"skipped":4971,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 23:36:58.355: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name projected-configmap-test-volume-118dc920-2f20-4d3b-8ba5-6467f6e2a5f9
STEP: Creating a pod to test consume configMaps
Aug 23 23:37:00.075: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-fa7b7c15-38e2-4392-8527-465b2465e257" in namespace "projected-8182" to be "Succeeded or Failed"
Aug 23 23:37:00.246: INFO: Pod "pod-projected-configmaps-fa7b7c15-38e2-4392-8527-465b2465e257": Phase="Pending", Reason="", readiness=false. Elapsed: 171.335791ms
Aug 23 23:37:02.482: INFO: Pod "pod-projected-configmaps-fa7b7c15-38e2-4392-8527-465b2465e257": Phase="Pending", Reason="", readiness=false. Elapsed: 2.406838552s
Aug 23 23:37:04.635: INFO: Pod "pod-projected-configmaps-fa7b7c15-38e2-4392-8527-465b2465e257": Phase="Pending", Reason="", readiness=false. Elapsed: 4.560132183s
Aug 23 23:37:06.771: INFO: Pod "pod-projected-configmaps-fa7b7c15-38e2-4392-8527-465b2465e257": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.695394483s
STEP: Saw pod success
Aug 23 23:37:06.771: INFO: Pod "pod-projected-configmaps-fa7b7c15-38e2-4392-8527-465b2465e257" satisfied condition "Succeeded or Failed"
Aug 23 23:37:06.921: INFO: Trying to get logs from node 10.149.248.24 pod pod-projected-configmaps-fa7b7c15-38e2-4392-8527-465b2465e257 container agnhost-container: <nil>
STEP: delete the pod
Aug 23 23:37:07.222: INFO: Waiting for pod pod-projected-configmaps-fa7b7c15-38e2-4392-8527-465b2465e257 to disappear
Aug 23 23:37:07.360: INFO: Pod pod-projected-configmaps-fa7b7c15-38e2-4392-8527-465b2465e257 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 23:37:07.360: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8182" for this suite.

• [SLOW TEST:9.408 seconds]
[sig-storage] Projected configMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:36
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume [NodeConformance] [Conformance]","total":311,"completed":286,"skipped":4990,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 23:37:07.767: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:187
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Aug 23 23:37:08.357: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 23:37:13.674: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-5253" for this suite.

• [SLOW TEST:6.405 seconds]
[k8s.io] Pods
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Pods should support remote command execution over websockets [NodeConformance] [Conformance]","total":311,"completed":287,"skipped":5024,"failed":0}
[sig-storage] ConfigMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 23:37:14.172: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Aug 23 23:37:14.828: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
STEP: Creating configMap with name cm-test-opt-del-b293df29-d727-41eb-a337-6f61b17fe8d5
STEP: Creating configMap with name cm-test-opt-upd-c7617dfb-40df-4bbc-9098-ed82680510d0
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-b293df29-d727-41eb-a337-6f61b17fe8d5
STEP: Updating configmap cm-test-opt-upd-c7617dfb-40df-4bbc-9098-ed82680510d0
STEP: Creating configMap with name cm-test-opt-create-f0d17a51-81e1-4dc1-b713-c12a8bd9c2f0
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 23:38:51.759: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6513" for this suite.

• [SLOW TEST:98.398 seconds]
[sig-storage] ConfigMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:36
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap optional updates should be reflected in volume [NodeConformance] [Conformance]","total":311,"completed":288,"skipped":5024,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 23:38:52.571: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:41
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Aug 23 23:38:54.303: INFO: Waiting up to 5m0s for pod "downwardapi-volume-af3db1fd-354f-4c07-b351-b13542960f2b" in namespace "downward-api-9280" to be "Succeeded or Failed"
Aug 23 23:38:54.510: INFO: Pod "downwardapi-volume-af3db1fd-354f-4c07-b351-b13542960f2b": Phase="Pending", Reason="", readiness=false. Elapsed: 206.76872ms
Aug 23 23:38:56.675: INFO: Pod "downwardapi-volume-af3db1fd-354f-4c07-b351-b13542960f2b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.371790918s
Aug 23 23:38:58.958: INFO: Pod "downwardapi-volume-af3db1fd-354f-4c07-b351-b13542960f2b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.654564694s
Aug 23 23:39:01.165: INFO: Pod "downwardapi-volume-af3db1fd-354f-4c07-b351-b13542960f2b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.861910224s
STEP: Saw pod success
Aug 23 23:39:01.165: INFO: Pod "downwardapi-volume-af3db1fd-354f-4c07-b351-b13542960f2b" satisfied condition "Succeeded or Failed"
Aug 23 23:39:01.411: INFO: Trying to get logs from node 10.149.248.24 pod downwardapi-volume-af3db1fd-354f-4c07-b351-b13542960f2b container client-container: <nil>
STEP: delete the pod
Aug 23 23:39:01.988: INFO: Waiting for pod downwardapi-volume-af3db1fd-354f-4c07-b351-b13542960f2b to disappear
Aug 23 23:39:02.188: INFO: Pod downwardapi-volume-af3db1fd-354f-4c07-b351-b13542960f2b no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 23:39:02.188: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9280" for this suite.

• [SLOW TEST:10.308 seconds]
[sig-storage] Downward API volume
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:36
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu limit [NodeConformance] [Conformance]","total":311,"completed":289,"skipped":5097,"failed":0}
SSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should create and stop a replication controller  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 23:39:02.880: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[BeforeEach] Update Demo
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:299
[It] should create and stop a replication controller  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a replication controller
Aug 23 23:39:05.096: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585719795 --namespace=kubectl-8849 create -f -'
Aug 23 23:39:05.734: INFO: stderr: ""
Aug 23 23:39:05.734: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Aug 23 23:39:05.734: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585719795 --namespace=kubectl-8849 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Aug 23 23:39:06.738: INFO: stderr: ""
Aug 23 23:39:06.738: INFO: stdout: "update-demo-nautilus-m7vgl update-demo-nautilus-z6gwp "
Aug 23 23:39:06.739: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585719795 --namespace=kubectl-8849 get pods update-demo-nautilus-m7vgl -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Aug 23 23:39:06.873: INFO: stderr: ""
Aug 23 23:39:06.873: INFO: stdout: ""
Aug 23 23:39:06.873: INFO: update-demo-nautilus-m7vgl is created but not running
Aug 23 23:39:11.873: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585719795 --namespace=kubectl-8849 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Aug 23 23:39:12.009: INFO: stderr: ""
Aug 23 23:39:12.009: INFO: stdout: "update-demo-nautilus-m7vgl update-demo-nautilus-z6gwp "
Aug 23 23:39:12.009: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585719795 --namespace=kubectl-8849 get pods update-demo-nautilus-m7vgl -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Aug 23 23:39:12.473: INFO: stderr: ""
Aug 23 23:39:12.473: INFO: stdout: "true"
Aug 23 23:39:12.473: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585719795 --namespace=kubectl-8849 get pods update-demo-nautilus-m7vgl -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Aug 23 23:39:13.077: INFO: stderr: ""
Aug 23 23:39:13.077: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Aug 23 23:39:13.077: INFO: validating pod update-demo-nautilus-m7vgl
Aug 23 23:39:13.217: INFO: got data: {
  "image": "nautilus.jpg"
}

Aug 23 23:39:13.217: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Aug 23 23:39:13.217: INFO: update-demo-nautilus-m7vgl is verified up and running
Aug 23 23:39:13.217: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585719795 --namespace=kubectl-8849 get pods update-demo-nautilus-z6gwp -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Aug 23 23:39:13.340: INFO: stderr: ""
Aug 23 23:39:13.340: INFO: stdout: "true"
Aug 23 23:39:13.340: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585719795 --namespace=kubectl-8849 get pods update-demo-nautilus-z6gwp -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Aug 23 23:39:14.004: INFO: stderr: ""
Aug 23 23:39:14.004: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Aug 23 23:39:14.004: INFO: validating pod update-demo-nautilus-z6gwp
Aug 23 23:39:14.157: INFO: got data: {
  "image": "nautilus.jpg"
}

Aug 23 23:39:14.157: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Aug 23 23:39:14.157: INFO: update-demo-nautilus-z6gwp is verified up and running
STEP: using delete to clean up resources
Aug 23 23:39:14.157: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585719795 --namespace=kubectl-8849 delete --grace-period=0 --force -f -'
Aug 23 23:39:14.678: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug 23 23:39:14.678: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Aug 23 23:39:14.678: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585719795 --namespace=kubectl-8849 get rc,svc -l name=update-demo --no-headers'
Aug 23 23:39:14.811: INFO: stderr: "No resources found in kubectl-8849 namespace.\n"
Aug 23 23:39:14.811: INFO: stdout: ""
Aug 23 23:39:14.811: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585719795 --namespace=kubectl-8849 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Aug 23 23:39:16.420: INFO: stderr: ""
Aug 23 23:39:16.420: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 23:39:16.420: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8849" for this suite.

• [SLOW TEST:13.956 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:297
    should create and stop a replication controller  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should create and stop a replication controller  [Conformance]","total":311,"completed":290,"skipped":5103,"failed":0}
SSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 23:39:16.836: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:52
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Aug 23 23:39:28.627: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Aug 23 23:39:28.730: INFO: Pod pod-with-prestop-exec-hook still exists
Aug 23 23:39:30.730: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Aug 23 23:39:30.896: INFO: Pod pod-with-prestop-exec-hook still exists
Aug 23 23:39:32.730: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Aug 23 23:39:32.810: INFO: Pod pod-with-prestop-exec-hook still exists
Aug 23 23:39:34.730: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Aug 23 23:39:34.801: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 23:39:34.875: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-2459" for this suite.

• [SLOW TEST:18.235 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:43
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop exec hook properly [NodeConformance] [Conformance]","total":311,"completed":291,"skipped":5106,"failed":0}
SSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 23:39:35.072: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:92
Aug 23 23:39:35.370: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Aug 23 23:39:35.529: INFO: Waiting for terminating namespaces to be deleted...
Aug 23 23:39:35.616: INFO: 
Logging pods the apiserver thinks is on node 10.149.248.24 before test
Aug 23 23:39:35.722: INFO: calico-node-9qqn4 from calico-system started at 2021-08-23 20:02:35 +0000 UTC (1 container statuses recorded)
Aug 23 23:39:35.722: INFO: 	Container calico-node ready: true, restart count 0
Aug 23 23:39:35.722: INFO: calico-typha-84b7fc6fc8-9fjkp from calico-system started at 2021-08-23 20:02:39 +0000 UTC (1 container statuses recorded)
Aug 23 23:39:35.722: INFO: 	Container calico-typha ready: true, restart count 0
Aug 23 23:39:35.722: INFO: pod-handle-http-request from container-lifecycle-hook-2459 started at 2021-08-23 23:39:17 +0000 UTC (1 container statuses recorded)
Aug 23 23:39:35.722: INFO: 	Container agnhost-container ready: true, restart count 0
Aug 23 23:39:35.722: INFO: ibm-keepalived-watcher-vrlwd from kube-system started at 2021-08-23 20:00:36 +0000 UTC (1 container statuses recorded)
Aug 23 23:39:35.722: INFO: 	Container keepalived-watcher ready: true, restart count 0
Aug 23 23:39:35.722: INFO: ibm-master-proxy-static-10.149.248.24 from kube-system started at 2021-08-23 20:00:33 +0000 UTC (2 container statuses recorded)
Aug 23 23:39:35.722: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Aug 23 23:39:35.722: INFO: 	Container pause ready: true, restart count 0
Aug 23 23:39:35.722: INFO: ibmcloud-block-storage-driver-jgx4r from kube-system started at 2021-08-23 20:00:42 +0000 UTC (1 container statuses recorded)
Aug 23 23:39:35.722: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Aug 23 23:39:35.722: INFO: tuned-7sb7t from openshift-cluster-node-tuning-operator started at 2021-08-23 20:05:27 +0000 UTC (1 container statuses recorded)
Aug 23 23:39:35.722: INFO: 	Container tuned ready: true, restart count 0
Aug 23 23:39:35.722: INFO: dns-default-mgbp7 from openshift-dns started at 2021-08-23 20:05:27 +0000 UTC (3 container statuses recorded)
Aug 23 23:39:35.722: INFO: 	Container dns ready: true, restart count 0
Aug 23 23:39:35.722: INFO: 	Container dns-node-resolver ready: true, restart count 0
Aug 23 23:39:35.722: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 23 23:39:35.722: INFO: node-ca-dl95t from openshift-image-registry started at 2021-08-23 20:04:29 +0000 UTC (1 container statuses recorded)
Aug 23 23:39:35.722: INFO: 	Container node-ca ready: true, restart count 0
Aug 23 23:39:35.722: INFO: ingress-canary-hx8m6 from openshift-ingress-canary started at 2021-08-23 23:27:23 +0000 UTC (1 container statuses recorded)
Aug 23 23:39:35.722: INFO: 	Container hello-openshift-canary ready: true, restart count 0
Aug 23 23:39:35.722: INFO: openshift-kube-proxy-wn8f2 from openshift-kube-proxy started at 2021-08-23 20:01:25 +0000 UTC (2 container statuses recorded)
Aug 23 23:39:35.722: INFO: 	Container kube-proxy ready: true, restart count 0
Aug 23 23:39:35.722: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 23 23:39:35.722: INFO: node-exporter-b5rwn from openshift-monitoring started at 2021-08-23 20:03:19 +0000 UTC (2 container statuses recorded)
Aug 23 23:39:35.722: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 23 23:39:35.722: INFO: 	Container node-exporter ready: true, restart count 0
Aug 23 23:39:35.722: INFO: multus-48kkt from openshift-multus started at 2021-08-23 20:01:12 +0000 UTC (1 container statuses recorded)
Aug 23 23:39:35.722: INFO: 	Container kube-multus ready: true, restart count 0
Aug 23 23:39:35.722: INFO: multus-admission-controller-tp5tk from openshift-multus started at 2021-08-23 23:27:23 +0000 UTC (2 container statuses recorded)
Aug 23 23:39:35.722: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 23 23:39:35.722: INFO: 	Container multus-admission-controller ready: true, restart count 0
Aug 23 23:39:35.722: INFO: network-metrics-daemon-cqr85 from openshift-multus started at 2021-08-23 20:01:15 +0000 UTC (2 container statuses recorded)
Aug 23 23:39:35.722: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 23 23:39:35.722: INFO: 	Container network-metrics-daemon ready: true, restart count 0
Aug 23 23:39:35.722: INFO: network-check-target-447wc from openshift-network-diagnostics started at 2021-08-23 20:01:33 +0000 UTC (1 container statuses recorded)
Aug 23 23:39:35.722: INFO: 	Container network-check-target-container ready: true, restart count 0
Aug 23 23:39:35.722: INFO: sonobuoy-systemd-logs-daemon-set-9034e69b695e4ee2-s28vf from sonobuoy started at 2021-08-23 21:37:23 +0000 UTC (2 container statuses recorded)
Aug 23 23:39:35.722: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 23 23:39:35.722: INFO: 	Container systemd-logs ready: true, restart count 0
Aug 23 23:39:35.722: INFO: 
Logging pods the apiserver thinks is on node 10.149.248.25 before test
Aug 23 23:39:35.843: INFO: calico-node-4m7h2 from calico-system started at 2021-08-23 20:02:35 +0000 UTC (1 container statuses recorded)
Aug 23 23:39:35.843: INFO: 	Container calico-node ready: true, restart count 0
Aug 23 23:39:35.843: INFO: calico-typha-84b7fc6fc8-tf4pd from calico-system started at 2021-08-23 20:02:33 +0000 UTC (1 container statuses recorded)
Aug 23 23:39:35.843: INFO: 	Container calico-typha ready: true, restart count 0
Aug 23 23:39:35.843: INFO: ibm-cloud-provider-ip-169-55-101-126-6bb454748-g6kdf from ibm-system started at 2021-08-23 20:10:53 +0000 UTC (1 container statuses recorded)
Aug 23 23:39:35.843: INFO: 	Container ibm-cloud-provider-ip-169-55-101-126 ready: true, restart count 0
Aug 23 23:39:35.843: INFO: ibm-file-plugin-68fbcccc88-bgbn9 from kube-system started at 2021-08-23 20:03:01 +0000 UTC (1 container statuses recorded)
Aug 23 23:39:35.843: INFO: 	Container ibm-file-plugin-container ready: true, restart count 0
Aug 23 23:39:35.843: INFO: ibm-keepalived-watcher-c94bg from kube-system started at 2021-08-23 20:00:20 +0000 UTC (1 container statuses recorded)
Aug 23 23:39:35.843: INFO: 	Container keepalived-watcher ready: true, restart count 0
Aug 23 23:39:35.843: INFO: ibm-master-proxy-static-10.149.248.25 from kube-system started at 2021-08-23 20:00:16 +0000 UTC (2 container statuses recorded)
Aug 23 23:39:35.843: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Aug 23 23:39:35.843: INFO: 	Container pause ready: true, restart count 0
Aug 23 23:39:35.843: INFO: ibm-storage-watcher-5c4d7b6cd6-zxwb2 from kube-system started at 2021-08-23 23:26:37 +0000 UTC (1 container statuses recorded)
Aug 23 23:39:35.843: INFO: 	Container ibm-storage-watcher-container ready: true, restart count 0
Aug 23 23:39:35.843: INFO: ibmcloud-block-storage-driver-zdvf4 from kube-system started at 2021-08-23 20:00:24 +0000 UTC (1 container statuses recorded)
Aug 23 23:39:35.843: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Aug 23 23:39:35.843: INFO: ibmcloud-block-storage-plugin-7d6d9649b-zdvgj from kube-system started at 2021-08-23 20:03:01 +0000 UTC (1 container statuses recorded)
Aug 23 23:39:35.843: INFO: 	Container ibmcloud-block-storage-plugin-container ready: true, restart count 0
Aug 23 23:39:35.843: INFO: vpn-db594b4f9-9mxgx from kube-system started at 2021-08-23 20:08:59 +0000 UTC (1 container statuses recorded)
Aug 23 23:39:35.843: INFO: 	Container vpn ready: true, restart count 0
Aug 23 23:39:35.843: INFO: tuned-bfbjn from openshift-cluster-node-tuning-operator started at 2021-08-23 20:05:28 +0000 UTC (1 container statuses recorded)
Aug 23 23:39:35.843: INFO: 	Container tuned ready: true, restart count 0
Aug 23 23:39:35.843: INFO: cluster-storage-operator-6c5c749558-97wht from openshift-cluster-storage-operator started at 2021-08-23 20:03:01 +0000 UTC (1 container statuses recorded)
Aug 23 23:39:35.843: INFO: 	Container cluster-storage-operator ready: true, restart count 1
Aug 23 23:39:35.843: INFO: csi-snapshot-controller-operator-645f4897d-2vvbp from openshift-cluster-storage-operator started at 2021-08-23 20:03:01 +0000 UTC (1 container statuses recorded)
Aug 23 23:39:35.843: INFO: 	Container csi-snapshot-controller-operator ready: true, restart count 0
Aug 23 23:39:35.843: INFO: console-dc8c686bb-8xpf8 from openshift-console started at 2021-08-23 20:07:03 +0000 UTC (1 container statuses recorded)
Aug 23 23:39:35.843: INFO: 	Container console ready: true, restart count 0
Aug 23 23:39:35.843: INFO: downloads-6c96776f98-5w9bj from openshift-console started at 2021-08-23 20:03:01 +0000 UTC (1 container statuses recorded)
Aug 23 23:39:35.843: INFO: 	Container download-server ready: true, restart count 0
Aug 23 23:39:35.843: INFO: downloads-6c96776f98-nqhnf from openshift-console started at 2021-08-23 22:12:56 +0000 UTC (1 container statuses recorded)
Aug 23 23:39:35.843: INFO: 	Container download-server ready: true, restart count 0
Aug 23 23:39:35.843: INFO: dns-operator-6dbb54f776-8k9pq from openshift-dns-operator started at 2021-08-23 20:03:01 +0000 UTC (2 container statuses recorded)
Aug 23 23:39:35.843: INFO: 	Container dns-operator ready: true, restart count 0
Aug 23 23:39:35.843: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 23 23:39:35.843: INFO: dns-default-mxjt9 from openshift-dns started at 2021-08-23 20:05:27 +0000 UTC (3 container statuses recorded)
Aug 23 23:39:35.843: INFO: 	Container dns ready: true, restart count 0
Aug 23 23:39:35.843: INFO: 	Container dns-node-resolver ready: true, restart count 0
Aug 23 23:39:35.843: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 23 23:39:35.843: INFO: cluster-image-registry-operator-85978c675-rdng6 from openshift-image-registry started at 2021-08-23 20:03:01 +0000 UTC (1 container statuses recorded)
Aug 23 23:39:35.843: INFO: 	Container cluster-image-registry-operator ready: true, restart count 0
Aug 23 23:39:35.843: INFO: node-ca-czldz from openshift-image-registry started at 2021-08-23 20:04:29 +0000 UTC (1 container statuses recorded)
Aug 23 23:39:35.843: INFO: 	Container node-ca ready: true, restart count 0
Aug 23 23:39:35.843: INFO: ingress-canary-9skj4 from openshift-ingress-canary started at 2021-08-23 20:05:35 +0000 UTC (1 container statuses recorded)
Aug 23 23:39:35.843: INFO: 	Container hello-openshift-canary ready: true, restart count 0
Aug 23 23:39:35.843: INFO: ingress-operator-5d97777898-hf6cx from openshift-ingress-operator started at 2021-08-23 23:26:38 +0000 UTC (2 container statuses recorded)
Aug 23 23:39:35.843: INFO: 	Container ingress-operator ready: true, restart count 0
Aug 23 23:39:35.843: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 23 23:39:35.843: INFO: router-default-678bfcd875-g2q6m from openshift-ingress started at 2021-08-23 20:05:36 +0000 UTC (1 container statuses recorded)
Aug 23 23:39:35.843: INFO: 	Container router ready: true, restart count 0
Aug 23 23:39:35.843: INFO: openshift-kube-proxy-sbzjn from openshift-kube-proxy started at 2021-08-23 20:01:25 +0000 UTC (2 container statuses recorded)
Aug 23 23:39:35.843: INFO: 	Container kube-proxy ready: true, restart count 0
Aug 23 23:39:35.843: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 23 23:39:35.843: INFO: migrator-f58676cd4-5q7gc from openshift-kube-storage-version-migrator started at 2021-08-23 23:26:37 +0000 UTC (1 container statuses recorded)
Aug 23 23:39:35.843: INFO: 	Container migrator ready: true, restart count 0
Aug 23 23:39:35.843: INFO: redhat-marketplace-hvcn5 from openshift-marketplace started at 2021-08-23 20:04:56 +0000 UTC (1 container statuses recorded)
Aug 23 23:39:35.843: INFO: 	Container registry-server ready: true, restart count 0
Aug 23 23:39:35.843: INFO: alertmanager-main-1 from openshift-monitoring started at 2021-08-23 20:06:03 +0000 UTC (5 container statuses recorded)
Aug 23 23:39:35.843: INFO: 	Container alertmanager ready: true, restart count 0
Aug 23 23:39:35.843: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Aug 23 23:39:35.843: INFO: 	Container config-reloader ready: true, restart count 0
Aug 23 23:39:35.843: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 23 23:39:35.843: INFO: 	Container prom-label-proxy ready: true, restart count 0
Aug 23 23:39:35.843: INFO: cluster-monitoring-operator-c69d85486-m52jl from openshift-monitoring started at 2021-08-23 20:03:01 +0000 UTC (2 container statuses recorded)
Aug 23 23:39:35.843: INFO: 	Container cluster-monitoring-operator ready: true, restart count 0
Aug 23 23:39:35.843: INFO: 	Container kube-rbac-proxy ready: true, restart count 4
Aug 23 23:39:35.843: INFO: node-exporter-fsg2g from openshift-monitoring started at 2021-08-23 20:03:19 +0000 UTC (2 container statuses recorded)
Aug 23 23:39:35.843: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 23 23:39:35.843: INFO: 	Container node-exporter ready: true, restart count 0
Aug 23 23:39:35.843: INFO: prometheus-adapter-6685ccb7f-gxd72 from openshift-monitoring started at 2021-08-23 20:06:45 +0000 UTC (1 container statuses recorded)
Aug 23 23:39:35.843: INFO: 	Container prometheus-adapter ready: true, restart count 0
Aug 23 23:39:35.843: INFO: prometheus-k8s-1 from openshift-monitoring started at 2021-08-23 20:06:14 +0000 UTC (7 container statuses recorded)
Aug 23 23:39:35.843: INFO: 	Container config-reloader ready: true, restart count 0
Aug 23 23:39:35.843: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 23 23:39:35.843: INFO: 	Container kube-rbac-proxy-thanos ready: true, restart count 0
Aug 23 23:39:35.843: INFO: 	Container prom-label-proxy ready: true, restart count 0
Aug 23 23:39:35.843: INFO: 	Container prometheus ready: true, restart count 1
Aug 23 23:39:35.843: INFO: 	Container prometheus-proxy ready: true, restart count 0
Aug 23 23:39:35.843: INFO: 	Container thanos-sidecar ready: true, restart count 0
Aug 23 23:39:35.843: INFO: thanos-querier-97cd894c4-z4jqj from openshift-monitoring started at 2021-08-23 20:06:10 +0000 UTC (5 container statuses recorded)
Aug 23 23:39:35.843: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 23 23:39:35.843: INFO: 	Container kube-rbac-proxy-rules ready: true, restart count 0
Aug 23 23:39:35.843: INFO: 	Container oauth-proxy ready: true, restart count 0
Aug 23 23:39:35.843: INFO: 	Container prom-label-proxy ready: true, restart count 0
Aug 23 23:39:35.843: INFO: 	Container thanos-query ready: true, restart count 0
Aug 23 23:39:35.843: INFO: multus-2p958 from openshift-multus started at 2021-08-23 20:01:12 +0000 UTC (1 container statuses recorded)
Aug 23 23:39:35.843: INFO: 	Container kube-multus ready: true, restart count 0
Aug 23 23:39:35.843: INFO: multus-admission-controller-ww7jt from openshift-multus started at 2021-08-23 20:02:56 +0000 UTC (2 container statuses recorded)
Aug 23 23:39:35.843: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 23 23:39:35.843: INFO: 	Container multus-admission-controller ready: true, restart count 0
Aug 23 23:39:35.843: INFO: network-metrics-daemon-6bmnm from openshift-multus started at 2021-08-23 20:01:15 +0000 UTC (2 container statuses recorded)
Aug 23 23:39:35.843: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 23 23:39:35.843: INFO: 	Container network-metrics-daemon ready: true, restart count 0
Aug 23 23:39:35.843: INFO: network-check-target-cjqqr from openshift-network-diagnostics started at 2021-08-23 20:01:33 +0000 UTC (1 container statuses recorded)
Aug 23 23:39:35.843: INFO: 	Container network-check-target-container ready: true, restart count 0
Aug 23 23:39:35.843: INFO: olm-operator-d5dc9548d-mwfkk from openshift-operator-lifecycle-manager started at 2021-08-23 20:03:01 +0000 UTC (1 container statuses recorded)
Aug 23 23:39:35.843: INFO: 	Container olm-operator ready: true, restart count 0
Aug 23 23:39:35.843: INFO: packageserver-6d5f99f754-5t8qd from openshift-operator-lifecycle-manager started at 2021-08-23 22:12:56 +0000 UTC (1 container statuses recorded)
Aug 23 23:39:35.843: INFO: 	Container packageserver ready: true, restart count 0
Aug 23 23:39:35.843: INFO: metrics-8fc5c5f56-lq4k6 from openshift-roks-metrics started at 2021-08-23 20:03:01 +0000 UTC (1 container statuses recorded)
Aug 23 23:39:35.843: INFO: 	Container metrics ready: true, restart count 3
Aug 23 23:39:35.843: INFO: push-gateway-59f6bc56d4-bx994 from openshift-roks-metrics started at 2021-08-23 23:26:37 +0000 UTC (1 container statuses recorded)
Aug 23 23:39:35.843: INFO: 	Container push-gateway ready: false, restart count 0
Aug 23 23:39:35.843: INFO: sonobuoy-e2e-job-90ed16fed1444409 from sonobuoy started at 2021-08-23 21:37:22 +0000 UTC (2 container statuses recorded)
Aug 23 23:39:35.843: INFO: 	Container e2e ready: true, restart count 0
Aug 23 23:39:35.843: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 23 23:39:35.843: INFO: sonobuoy-systemd-logs-daemon-set-9034e69b695e4ee2-q8h6l from sonobuoy started at 2021-08-23 21:37:23 +0000 UTC (2 container statuses recorded)
Aug 23 23:39:35.843: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 23 23:39:35.843: INFO: 	Container systemd-logs ready: true, restart count 0
Aug 23 23:39:35.843: INFO: 
Logging pods the apiserver thinks is on node 10.149.248.9 before test
Aug 23 23:39:35.954: INFO: calico-kube-controllers-7dcbcc7c66-x65ng from calico-system started at 2021-08-23 20:02:55 +0000 UTC (1 container statuses recorded)
Aug 23 23:39:35.954: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Aug 23 23:39:35.954: INFO: calico-node-cgrcs from calico-system started at 2021-08-23 20:02:35 +0000 UTC (1 container statuses recorded)
Aug 23 23:39:35.954: INFO: 	Container calico-node ready: true, restart count 0
Aug 23 23:39:35.954: INFO: calico-typha-84b7fc6fc8-h6lnw from calico-system started at 2021-08-23 20:02:39 +0000 UTC (1 container statuses recorded)
Aug 23 23:39:35.954: INFO: 	Container calico-typha ready: true, restart count 0
Aug 23 23:39:35.954: INFO: ibm-cloud-provider-ip-169-55-101-126-6bb454748-t7nvk from ibm-system started at 2021-08-23 20:10:53 +0000 UTC (1 container statuses recorded)
Aug 23 23:39:35.954: INFO: 	Container ibm-cloud-provider-ip-169-55-101-126 ready: true, restart count 0
Aug 23 23:39:35.954: INFO: ibm-keepalived-watcher-rsh2g from kube-system started at 2021-08-23 20:00:08 +0000 UTC (1 container statuses recorded)
Aug 23 23:39:35.954: INFO: 	Container keepalived-watcher ready: true, restart count 0
Aug 23 23:39:35.954: INFO: ibm-master-proxy-static-10.149.248.9 from kube-system started at 2021-08-23 20:00:06 +0000 UTC (2 container statuses recorded)
Aug 23 23:39:35.954: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Aug 23 23:39:35.954: INFO: 	Container pause ready: true, restart count 0
Aug 23 23:39:35.954: INFO: ibmcloud-block-storage-driver-5fllx from kube-system started at 2021-08-23 20:00:13 +0000 UTC (1 container statuses recorded)
Aug 23 23:39:35.954: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Aug 23 23:39:35.954: INFO: cluster-node-tuning-operator-84b8576b47-qlh6k from openshift-cluster-node-tuning-operator started at 2021-08-23 22:12:56 +0000 UTC (1 container statuses recorded)
Aug 23 23:39:35.954: INFO: 	Container cluster-node-tuning-operator ready: true, restart count 0
Aug 23 23:39:35.954: INFO: tuned-m585x from openshift-cluster-node-tuning-operator started at 2021-08-23 20:05:28 +0000 UTC (1 container statuses recorded)
Aug 23 23:39:35.954: INFO: 	Container tuned ready: true, restart count 0
Aug 23 23:39:35.954: INFO: cluster-samples-operator-866dcfc6c4-ntqvw from openshift-cluster-samples-operator started at 2021-08-23 22:12:56 +0000 UTC (2 container statuses recorded)
Aug 23 23:39:35.954: INFO: 	Container cluster-samples-operator ready: true, restart count 0
Aug 23 23:39:35.954: INFO: 	Container cluster-samples-operator-watch ready: true, restart count 0
Aug 23 23:39:35.954: INFO: csi-snapshot-controller-566544547f-sfjlv from openshift-cluster-storage-operator started at 2021-08-23 23:26:38 +0000 UTC (1 container statuses recorded)
Aug 23 23:39:35.954: INFO: 	Container snapshot-controller ready: true, restart count 0
Aug 23 23:39:35.954: INFO: csi-snapshot-webhook-585d4946dc-kd8jt from openshift-cluster-storage-operator started at 2021-08-23 23:26:37 +0000 UTC (1 container statuses recorded)
Aug 23 23:39:35.954: INFO: 	Container webhook ready: true, restart count 0
Aug 23 23:39:35.954: INFO: console-operator-c7f9f8687-s8qw7 from openshift-console-operator started at 2021-08-23 23:26:37 +0000 UTC (1 container statuses recorded)
Aug 23 23:39:35.954: INFO: 	Container console-operator ready: true, restart count 0
Aug 23 23:39:35.954: INFO: console-dc8c686bb-z5xcx from openshift-console started at 2021-08-23 23:26:38 +0000 UTC (1 container statuses recorded)
Aug 23 23:39:35.954: INFO: 	Container console ready: true, restart count 0
Aug 23 23:39:35.954: INFO: dns-default-4hmcv from openshift-dns started at 2021-08-23 20:05:27 +0000 UTC (3 container statuses recorded)
Aug 23 23:39:35.954: INFO: 	Container dns ready: true, restart count 0
Aug 23 23:39:35.954: INFO: 	Container dns-node-resolver ready: true, restart count 0
Aug 23 23:39:35.954: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 23 23:39:35.954: INFO: image-registry-555cc7f64c-qtjjg from openshift-image-registry started at 2021-08-23 22:12:56 +0000 UTC (1 container statuses recorded)
Aug 23 23:39:35.954: INFO: 	Container registry ready: true, restart count 0
Aug 23 23:39:35.954: INFO: node-ca-g989x from openshift-image-registry started at 2021-08-23 20:04:29 +0000 UTC (1 container statuses recorded)
Aug 23 23:39:35.954: INFO: 	Container node-ca ready: true, restart count 0
Aug 23 23:39:35.954: INFO: registry-pvc-permissions-dbktz from openshift-image-registry started at 2021-08-23 20:08:40 +0000 UTC (1 container statuses recorded)
Aug 23 23:39:35.955: INFO: 	Container pvc-permissions ready: false, restart count 0
Aug 23 23:39:35.955: INFO: ingress-canary-l5l42 from openshift-ingress-canary started at 2021-08-23 20:05:35 +0000 UTC (1 container statuses recorded)
Aug 23 23:39:35.955: INFO: 	Container hello-openshift-canary ready: true, restart count 0
Aug 23 23:39:35.955: INFO: router-default-678bfcd875-2f5hb from openshift-ingress started at 2021-08-23 20:05:36 +0000 UTC (1 container statuses recorded)
Aug 23 23:39:35.955: INFO: 	Container router ready: true, restart count 0
Aug 23 23:39:35.955: INFO: openshift-kube-proxy-wk9m8 from openshift-kube-proxy started at 2021-08-23 20:01:25 +0000 UTC (2 container statuses recorded)
Aug 23 23:39:35.955: INFO: 	Container kube-proxy ready: true, restart count 0
Aug 23 23:39:35.955: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 23 23:39:35.955: INFO: kube-storage-version-migrator-operator-6d55bddccb-4jgrz from openshift-kube-storage-version-migrator-operator started at 2021-08-23 23:26:38 +0000 UTC (1 container statuses recorded)
Aug 23 23:39:35.955: INFO: 	Container kube-storage-version-migrator-operator ready: true, restart count 0
Aug 23 23:39:35.955: INFO: certified-operators-rlpm2 from openshift-marketplace started at 2021-08-23 20:04:55 +0000 UTC (1 container statuses recorded)
Aug 23 23:39:35.955: INFO: 	Container registry-server ready: true, restart count 0
Aug 23 23:39:35.955: INFO: community-operators-ztwk2 from openshift-marketplace started at 2021-08-23 20:04:56 +0000 UTC (1 container statuses recorded)
Aug 23 23:39:35.955: INFO: 	Container registry-server ready: true, restart count 0
Aug 23 23:39:35.955: INFO: marketplace-operator-7fd8fdcc5b-ncb77 from openshift-marketplace started at 2021-08-23 23:26:38 +0000 UTC (1 container statuses recorded)
Aug 23 23:39:35.955: INFO: 	Container marketplace-operator ready: true, restart count 0
Aug 23 23:39:35.955: INFO: redhat-operators-cpqsn from openshift-marketplace started at 2021-08-23 20:04:56 +0000 UTC (1 container statuses recorded)
Aug 23 23:39:35.955: INFO: 	Container registry-server ready: true, restart count 0
Aug 23 23:39:35.955: INFO: alertmanager-main-0 from openshift-monitoring started at 2021-08-23 23:27:04 +0000 UTC (5 container statuses recorded)
Aug 23 23:39:35.955: INFO: 	Container alertmanager ready: true, restart count 0
Aug 23 23:39:35.955: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Aug 23 23:39:35.955: INFO: 	Container config-reloader ready: true, restart count 0
Aug 23 23:39:35.955: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 23 23:39:35.955: INFO: 	Container prom-label-proxy ready: true, restart count 0
Aug 23 23:39:35.955: INFO: alertmanager-main-2 from openshift-monitoring started at 2021-08-23 20:06:04 +0000 UTC (5 container statuses recorded)
Aug 23 23:39:35.955: INFO: 	Container alertmanager ready: true, restart count 0
Aug 23 23:39:35.955: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Aug 23 23:39:35.955: INFO: 	Container config-reloader ready: true, restart count 0
Aug 23 23:39:35.955: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 23 23:39:35.955: INFO: 	Container prom-label-proxy ready: true, restart count 0
Aug 23 23:39:35.955: INFO: grafana-85d4bdb748-9g6gn from openshift-monitoring started at 2021-08-23 22:12:56 +0000 UTC (2 container statuses recorded)
Aug 23 23:39:35.955: INFO: 	Container grafana ready: true, restart count 0
Aug 23 23:39:35.955: INFO: 	Container grafana-proxy ready: true, restart count 0
Aug 23 23:39:35.955: INFO: kube-state-metrics-d956df775-bgj5t from openshift-monitoring started at 2021-08-23 23:26:38 +0000 UTC (3 container statuses recorded)
Aug 23 23:39:35.955: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Aug 23 23:39:35.955: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Aug 23 23:39:35.955: INFO: 	Container kube-state-metrics ready: true, restart count 0
Aug 23 23:39:35.955: INFO: node-exporter-kcjzv from openshift-monitoring started at 2021-08-23 20:03:19 +0000 UTC (2 container statuses recorded)
Aug 23 23:39:35.955: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 23 23:39:35.955: INFO: 	Container node-exporter ready: true, restart count 0
Aug 23 23:39:35.955: INFO: openshift-state-metrics-74b58f578c-cmhb9 from openshift-monitoring started at 2021-08-23 23:26:38 +0000 UTC (3 container statuses recorded)
Aug 23 23:39:35.955: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Aug 23 23:39:35.955: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Aug 23 23:39:35.955: INFO: 	Container openshift-state-metrics ready: true, restart count 0
Aug 23 23:39:35.955: INFO: prometheus-adapter-6685ccb7f-9tlvb from openshift-monitoring started at 2021-08-23 23:26:38 +0000 UTC (1 container statuses recorded)
Aug 23 23:39:35.955: INFO: 	Container prometheus-adapter ready: true, restart count 0
Aug 23 23:39:35.955: INFO: prometheus-k8s-0 from openshift-monitoring started at 2021-08-23 20:06:14 +0000 UTC (7 container statuses recorded)
Aug 23 23:39:35.955: INFO: 	Container config-reloader ready: true, restart count 0
Aug 23 23:39:35.955: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 23 23:39:35.955: INFO: 	Container kube-rbac-proxy-thanos ready: true, restart count 0
Aug 23 23:39:35.955: INFO: 	Container prom-label-proxy ready: true, restart count 0
Aug 23 23:39:35.955: INFO: 	Container prometheus ready: true, restart count 1
Aug 23 23:39:35.955: INFO: 	Container prometheus-proxy ready: true, restart count 0
Aug 23 23:39:35.955: INFO: 	Container thanos-sidecar ready: true, restart count 0
Aug 23 23:39:35.955: INFO: prometheus-operator-5699bb49dc-8q47w from openshift-monitoring started at 2021-08-23 20:06:44 +0000 UTC (2 container statuses recorded)
Aug 23 23:39:35.955: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 23 23:39:35.955: INFO: 	Container prometheus-operator ready: true, restart count 0
Aug 23 23:39:35.955: INFO: telemeter-client-9d6d6f95f-ztnrk from openshift-monitoring started at 2021-08-23 23:26:38 +0000 UTC (3 container statuses recorded)
Aug 23 23:39:35.955: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 23 23:39:35.955: INFO: 	Container reload ready: true, restart count 0
Aug 23 23:39:35.955: INFO: 	Container telemeter-client ready: true, restart count 0
Aug 23 23:39:35.955: INFO: thanos-querier-97cd894c4-g4rfr from openshift-monitoring started at 2021-08-23 22:12:56 +0000 UTC (5 container statuses recorded)
Aug 23 23:39:35.955: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 23 23:39:35.955: INFO: 	Container kube-rbac-proxy-rules ready: true, restart count 0
Aug 23 23:39:35.955: INFO: 	Container oauth-proxy ready: true, restart count 0
Aug 23 23:39:35.955: INFO: 	Container prom-label-proxy ready: true, restart count 0
Aug 23 23:39:35.955: INFO: 	Container thanos-query ready: true, restart count 0
Aug 23 23:39:35.955: INFO: multus-admission-controller-hcwf4 from openshift-multus started at 2021-08-23 20:02:50 +0000 UTC (2 container statuses recorded)
Aug 23 23:39:35.955: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 23 23:39:35.955: INFO: 	Container multus-admission-controller ready: true, restart count 0
Aug 23 23:39:35.955: INFO: multus-z4wqr from openshift-multus started at 2021-08-23 20:01:12 +0000 UTC (1 container statuses recorded)
Aug 23 23:39:35.955: INFO: 	Container kube-multus ready: true, restart count 0
Aug 23 23:39:35.955: INFO: network-metrics-daemon-mbjhv from openshift-multus started at 2021-08-23 20:01:15 +0000 UTC (2 container statuses recorded)
Aug 23 23:39:35.955: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 23 23:39:35.955: INFO: 	Container network-metrics-daemon ready: true, restart count 0
Aug 23 23:39:35.955: INFO: network-check-source-6cd65cf589-6rbmz from openshift-network-diagnostics started at 2021-08-23 20:03:01 +0000 UTC (1 container statuses recorded)
Aug 23 23:39:35.955: INFO: 	Container check-endpoints ready: true, restart count 0
Aug 23 23:39:35.955: INFO: network-check-target-d6zj2 from openshift-network-diagnostics started at 2021-08-23 20:01:33 +0000 UTC (1 container statuses recorded)
Aug 23 23:39:35.955: INFO: 	Container network-check-target-container ready: true, restart count 0
Aug 23 23:39:35.955: INFO: network-operator-86dcc4df56-crx6q from openshift-network-operator started at 2021-08-23 20:00:18 +0000 UTC (1 container statuses recorded)
Aug 23 23:39:35.955: INFO: 	Container network-operator ready: true, restart count 0
Aug 23 23:39:35.955: INFO: catalog-operator-6f8ff86686-tg2d4 from openshift-operator-lifecycle-manager started at 2021-08-23 22:12:56 +0000 UTC (1 container statuses recorded)
Aug 23 23:39:35.955: INFO: 	Container catalog-operator ready: true, restart count 0
Aug 23 23:39:35.955: INFO: packageserver-6d5f99f754-9mfv7 from openshift-operator-lifecycle-manager started at 2021-08-23 20:04:35 +0000 UTC (1 container statuses recorded)
Aug 23 23:39:35.955: INFO: 	Container packageserver ready: true, restart count 0
Aug 23 23:39:35.955: INFO: service-ca-operator-64b7cc7c85-bxzpq from openshift-service-ca-operator started at 2021-08-23 23:26:38 +0000 UTC (1 container statuses recorded)
Aug 23 23:39:35.955: INFO: 	Container service-ca-operator ready: true, restart count 0
Aug 23 23:39:35.955: INFO: service-ca-54b7675c-xmpw4 from openshift-service-ca started at 2021-08-23 22:12:56 +0000 UTC (1 container statuses recorded)
Aug 23 23:39:35.955: INFO: 	Container service-ca-controller ready: false, restart count 0
Aug 23 23:39:35.955: INFO: sonobuoy from sonobuoy started at 2021-08-23 21:37:12 +0000 UTC (1 container statuses recorded)
Aug 23 23:39:35.955: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Aug 23 23:39:35.955: INFO: sonobuoy-systemd-logs-daemon-set-9034e69b695e4ee2-zn2c4 from sonobuoy started at 2021-08-23 21:37:23 +0000 UTC (2 container statuses recorded)
Aug 23 23:39:35.955: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 23 23:39:35.955: INFO: 	Container systemd-logs ready: true, restart count 0
Aug 23 23:39:35.955: INFO: tigera-operator-667cd558f7-d78zp from tigera-operator started at 2021-08-23 20:00:19 +0000 UTC (1 container statuses recorded)
Aug 23 23:39:35.955: INFO: 	Container tigera-operator ready: true, restart count 4
[It] validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Trying to schedule Pod with nonempty NodeSelector.
STEP: Considering event: 
Type = [Normal], Name = [sched-pred-4227.169e13e1acd2ab0f], Reason = [CreatedSCCRanges], Message = [created SCC ranges]
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.169e13e1f8e499fb], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 node(s) didn't match Pod's node affinity.]
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 23:39:37.626: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-4227" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:83
•{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if not matching  [Conformance]","total":311,"completed":292,"skipped":5114,"failed":0}
SSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 23:39:37.914: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir 0644 on tmpfs
Aug 23 23:39:38.454: INFO: Waiting up to 5m0s for pod "pod-2296a239-62df-4e76-8bda-ba9142f29ff5" in namespace "emptydir-5173" to be "Succeeded or Failed"
Aug 23 23:39:38.526: INFO: Pod "pod-2296a239-62df-4e76-8bda-ba9142f29ff5": Phase="Pending", Reason="", readiness=false. Elapsed: 72.278888ms
Aug 23 23:39:40.671: INFO: Pod "pod-2296a239-62df-4e76-8bda-ba9142f29ff5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.217037953s
Aug 23 23:39:42.818: INFO: Pod "pod-2296a239-62df-4e76-8bda-ba9142f29ff5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.364419536s
STEP: Saw pod success
Aug 23 23:39:42.818: INFO: Pod "pod-2296a239-62df-4e76-8bda-ba9142f29ff5" satisfied condition "Succeeded or Failed"
Aug 23 23:39:42.912: INFO: Trying to get logs from node 10.149.248.24 pod pod-2296a239-62df-4e76-8bda-ba9142f29ff5 container test-container: <nil>
STEP: delete the pod
Aug 23 23:39:43.118: INFO: Waiting for pod pod-2296a239-62df-4e76-8bda-ba9142f29ff5 to disappear
Aug 23 23:39:43.205: INFO: Pod pod-2296a239-62df-4e76-8bda-ba9142f29ff5 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 23:39:43.205: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5173" for this suite.

• [SLOW TEST:5.532 seconds]
[sig-storage] EmptyDir volumes
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:45
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":293,"skipped":5120,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 23:39:43.446: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename crd-webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:126
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
Aug 23 23:39:45.081: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63765358784, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63765358784, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63765358785, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63765358784, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-7d6697c5b7\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 23 23:39:47.130: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63765358784, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63765358784, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63765358785, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63765358784, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-7d6697c5b7\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 23 23:39:49.133: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63765358784, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63765358784, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63765358785, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63765358784, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-7d6697c5b7\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Aug 23 23:39:52.178: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Aug 23 23:39:52.219: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Creating a v1 custom resource
STEP: v2 custom resource should be converted
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 23:39:54.094: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-7649" for this suite.
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:137

• [SLOW TEST:11.088 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert from CR v1 to CR v2 [Conformance]","total":311,"completed":294,"skipped":5161,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Guestbook application 
  should create and stop a working application  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 23:39:54.538: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should create and stop a working application  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating all guestbook components
Aug 23 23:39:54.934: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-replica
  labels:
    app: agnhost
    role: replica
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: agnhost
    role: replica
    tier: backend

Aug 23 23:39:54.934: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585719795 --namespace=kubectl-1606 create -f -'
Aug 23 23:39:55.688: INFO: stderr: ""
Aug 23 23:39:55.688: INFO: stdout: "service/agnhost-replica created\n"
Aug 23 23:39:55.689: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-primary
  labels:
    app: agnhost
    role: primary
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: agnhost
    role: primary
    tier: backend

Aug 23 23:39:55.689: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585719795 --namespace=kubectl-1606 create -f -'
Aug 23 23:39:56.574: INFO: stderr: ""
Aug 23 23:39:56.574: INFO: stdout: "service/agnhost-primary created\n"
Aug 23 23:39:56.574: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Aug 23 23:39:56.574: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585719795 --namespace=kubectl-1606 create -f -'
Aug 23 23:39:57.822: INFO: stderr: ""
Aug 23 23:39:57.822: INFO: stdout: "service/frontend created\n"
Aug 23 23:39:57.822: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: guestbook-frontend
        image: k8s.gcr.io/e2e-test-images/agnhost:2.21
        args: [ "guestbook", "--backend-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 80

Aug 23 23:39:57.822: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585719795 --namespace=kubectl-1606 create -f -'
Aug 23 23:39:58.540: INFO: stderr: ""
Aug 23 23:39:58.541: INFO: stdout: "deployment.apps/frontend created\n"
Aug 23 23:39:58.541: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-primary
spec:
  replicas: 1
  selector:
    matchLabels:
      app: agnhost
      role: primary
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: primary
        tier: backend
    spec:
      containers:
      - name: primary
        image: k8s.gcr.io/e2e-test-images/agnhost:2.21
        args: [ "guestbook", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Aug 23 23:39:58.541: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585719795 --namespace=kubectl-1606 create -f -'
Aug 23 23:40:09.306: INFO: stderr: ""
Aug 23 23:40:09.306: INFO: stdout: "deployment.apps/agnhost-primary created\n"
Aug 23 23:40:09.307: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-replica
spec:
  replicas: 2
  selector:
    matchLabels:
      app: agnhost
      role: replica
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: replica
        tier: backend
    spec:
      containers:
      - name: replica
        image: k8s.gcr.io/e2e-test-images/agnhost:2.21
        args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Aug 23 23:40:09.307: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585719795 --namespace=kubectl-1606 create -f -'
Aug 23 23:40:10.035: INFO: stderr: ""
Aug 23 23:40:10.035: INFO: stdout: "deployment.apps/agnhost-replica created\n"
STEP: validating guestbook app
Aug 23 23:40:10.035: INFO: Waiting for all frontend pods to be Running.
Aug 23 23:40:10.235: INFO: Waiting for frontend to serve content.
Aug 23 23:40:11.418: INFO: Failed to get response from guestbook. err: the server responded with the status code 417 but did not return more information (get services frontend), response: 
Aug 23 23:40:16.717: INFO: Trying to add a new entry to the guestbook.
Aug 23 23:40:17.002: INFO: Failed to get response from guestbook. err: the server responded with the status code 417 but did not return more information (get services frontend), response: 
Aug 23 23:40:22.317: INFO: Verifying that added entry can be retrieved.
STEP: using delete to clean up resources
Aug 23 23:40:22.603: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585719795 --namespace=kubectl-1606 delete --grace-period=0 --force -f -'
Aug 23 23:40:23.689: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug 23 23:40:23.689: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
STEP: using delete to clean up resources
Aug 23 23:40:23.693: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585719795 --namespace=kubectl-1606 delete --grace-period=0 --force -f -'
Aug 23 23:40:25.225: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug 23 23:40:25.225: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources
Aug 23 23:40:25.226: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585719795 --namespace=kubectl-1606 delete --grace-period=0 --force -f -'
Aug 23 23:40:26.161: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug 23 23:40:26.161: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Aug 23 23:40:26.161: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585719795 --namespace=kubectl-1606 delete --grace-period=0 --force -f -'
Aug 23 23:40:28.854: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug 23 23:40:28.854: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Aug 23 23:40:28.854: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585719795 --namespace=kubectl-1606 delete --grace-period=0 --force -f -'
Aug 23 23:40:29.802: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug 23 23:40:29.802: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources
Aug 23 23:40:29.802: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585719795 --namespace=kubectl-1606 delete --grace-period=0 --force -f -'
Aug 23 23:40:30.611: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug 23 23:40:30.611: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 23:40:30.611: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1606" for this suite.

• [SLOW TEST:36.643 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Guestbook application
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:342
    should create and stop a working application  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Guestbook application should create and stop a working application  [Conformance]","total":311,"completed":295,"skipped":5196,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 23:40:31.181: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating 50 configmaps
STEP: Creating RC which spawns configmap-volume pods
Aug 23 23:40:42.647: INFO: Pod name wrapped-volume-race-a978eb91-7cbb-4342-b469-22e9e45e04f6: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-a978eb91-7cbb-4342-b469-22e9e45e04f6 in namespace emptydir-wrapper-9135, will wait for the garbage collector to delete the pods
Aug 23 23:40:54.201: INFO: Deleting ReplicationController wrapped-volume-race-a978eb91-7cbb-4342-b469-22e9e45e04f6 took: 355.242249ms
Aug 23 23:40:54.501: INFO: Terminating ReplicationController wrapped-volume-race-a978eb91-7cbb-4342-b469-22e9e45e04f6 pods took: 300.166102ms
STEP: Creating RC which spawns configmap-volume pods
Aug 23 23:41:15.350: INFO: Pod name wrapped-volume-race-7086bc7e-8798-4786-8259-5f93e6426199: Found 3 pods out of 5
Aug 23 23:41:20.733: INFO: Pod name wrapped-volume-race-7086bc7e-8798-4786-8259-5f93e6426199: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-7086bc7e-8798-4786-8259-5f93e6426199 in namespace emptydir-wrapper-9135, will wait for the garbage collector to delete the pods
Aug 23 23:41:28.371: INFO: Deleting ReplicationController wrapped-volume-race-7086bc7e-8798-4786-8259-5f93e6426199 took: 248.48111ms
Aug 23 23:41:28.771: INFO: Terminating ReplicationController wrapped-volume-race-7086bc7e-8798-4786-8259-5f93e6426199 pods took: 400.189734ms
STEP: Creating RC which spawns configmap-volume pods
Aug 23 23:41:46.027: INFO: Pod name wrapped-volume-race-452d6e00-8f86-4f59-8798-d387bfd44a30: Found 3 pods out of 5
Aug 23 23:41:51.348: INFO: Pod name wrapped-volume-race-452d6e00-8f86-4f59-8798-d387bfd44a30: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-452d6e00-8f86-4f59-8798-d387bfd44a30 in namespace emptydir-wrapper-9135, will wait for the garbage collector to delete the pods
Aug 23 23:41:57.868: INFO: Deleting ReplicationController wrapped-volume-race-452d6e00-8f86-4f59-8798-d387bfd44a30 took: 340.750842ms
Aug 23 23:41:58.268: INFO: Terminating ReplicationController wrapped-volume-race-452d6e00-8f86-4f59-8798-d387bfd44a30 pods took: 400.190869ms
STEP: Cleaning up the configMaps
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 23:42:22.506: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-9135" for this suite.

• [SLOW TEST:111.524 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not cause race condition when used for configmaps [Serial] [Conformance]","total":311,"completed":296,"skipped":5208,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 23:42:22.707: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward api env vars
Aug 23 23:42:23.142: INFO: Waiting up to 5m0s for pod "downward-api-28fe872b-ef50-410b-9a74-a46bedfd4bc4" in namespace "downward-api-1970" to be "Succeeded or Failed"
Aug 23 23:42:23.214: INFO: Pod "downward-api-28fe872b-ef50-410b-9a74-a46bedfd4bc4": Phase="Pending", Reason="", readiness=false. Elapsed: 71.699953ms
Aug 23 23:42:25.354: INFO: Pod "downward-api-28fe872b-ef50-410b-9a74-a46bedfd4bc4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.211163084s
Aug 23 23:42:27.533: INFO: Pod "downward-api-28fe872b-ef50-410b-9a74-a46bedfd4bc4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.390314975s
STEP: Saw pod success
Aug 23 23:42:27.533: INFO: Pod "downward-api-28fe872b-ef50-410b-9a74-a46bedfd4bc4" satisfied condition "Succeeded or Failed"
Aug 23 23:42:27.746: INFO: Trying to get logs from node 10.149.248.24 pod downward-api-28fe872b-ef50-410b-9a74-a46bedfd4bc4 container dapi-container: <nil>
STEP: delete the pod
Aug 23 23:42:28.291: INFO: Waiting for pod downward-api-28fe872b-ef50-410b-9a74-a46bedfd4bc4 to disappear
Aug 23 23:42:28.533: INFO: Pod downward-api-28fe872b-ef50-410b-9a74-a46bedfd4bc4 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 23:42:28.533: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1970" for this suite.

• [SLOW TEST:6.929 seconds]
[sig-node] Downward API
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:34
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-node] Downward API should provide host IP as an env var [NodeConformance] [Conformance]","total":311,"completed":297,"skipped":5223,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny pod and configmap creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 23:42:29.638: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Aug 23 23:42:35.173: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63765358954, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63765358954, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63765358954, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63765358954, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 23 23:42:37.392: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63765358954, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63765358954, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63765358954, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63765358954, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 23 23:42:39.371: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63765358954, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63765358954, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63765358954, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63765358954, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 23 23:42:41.382: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63765358954, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63765358954, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63765358954, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63765358954, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Aug 23 23:42:44.727: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny pod and configmap creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod that should be denied by the webhook
STEP: create a pod that causes the webhook to hang
STEP: create a configmap that should be denied by the webhook
STEP: create a configmap that should be admitted by the webhook
STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: create a namespace that bypass the webhook
STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 23:42:57.965: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-894" for this suite.
STEP: Destroying namespace "webhook-894-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

• [SLOW TEST:29.536 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny pod and configmap creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny pod and configmap creation [Conformance]","total":311,"completed":298,"skipped":5245,"failed":0}
SSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 23:42:59.174: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:129
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Aug 23 23:43:00.596: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster.
Aug 23 23:43:00.894: INFO: Number of nodes with available pods: 0
Aug 23 23:43:00.894: INFO: Node 10.149.248.24 is running more than one daemon pod
Aug 23 23:43:02.384: INFO: Number of nodes with available pods: 0
Aug 23 23:43:02.384: INFO: Node 10.149.248.24 is running more than one daemon pod
Aug 23 23:43:03.153: INFO: Number of nodes with available pods: 0
Aug 23 23:43:03.153: INFO: Node 10.149.248.24 is running more than one daemon pod
Aug 23 23:43:04.301: INFO: Number of nodes with available pods: 0
Aug 23 23:43:04.301: INFO: Node 10.149.248.24 is running more than one daemon pod
Aug 23 23:43:06.358: INFO: Number of nodes with available pods: 1
Aug 23 23:43:06.358: INFO: Node 10.149.248.24 is running more than one daemon pod
Aug 23 23:43:07.503: INFO: Number of nodes with available pods: 1
Aug 23 23:43:07.504: INFO: Node 10.149.248.24 is running more than one daemon pod
Aug 23 23:43:08.217: INFO: Number of nodes with available pods: 2
Aug 23 23:43:08.217: INFO: Node 10.149.248.25 is running more than one daemon pod
Aug 23 23:43:09.353: INFO: Number of nodes with available pods: 2
Aug 23 23:43:09.353: INFO: Node 10.149.248.25 is running more than one daemon pod
Aug 23 23:43:10.376: INFO: Number of nodes with available pods: 2
Aug 23 23:43:10.376: INFO: Node 10.149.248.25 is running more than one daemon pod
Aug 23 23:43:11.302: INFO: Number of nodes with available pods: 2
Aug 23 23:43:11.302: INFO: Node 10.149.248.25 is running more than one daemon pod
Aug 23 23:43:12.225: INFO: Number of nodes with available pods: 2
Aug 23 23:43:12.226: INFO: Node 10.149.248.25 is running more than one daemon pod
Aug 23 23:43:13.134: INFO: Number of nodes with available pods: 3
Aug 23 23:43:13.134: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Update daemon pods image.
STEP: Check that daemon pods images are updated.
Aug 23 23:43:13.789: INFO: Wrong image for pod: daemon-set-dvdtd. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Aug 23 23:43:13.789: INFO: Wrong image for pod: daemon-set-jvknz. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Aug 23 23:43:13.789: INFO: Wrong image for pod: daemon-set-z4k87. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Aug 23 23:43:14.976: INFO: Wrong image for pod: daemon-set-dvdtd. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Aug 23 23:43:14.977: INFO: Wrong image for pod: daemon-set-jvknz. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Aug 23 23:43:14.977: INFO: Wrong image for pod: daemon-set-z4k87. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Aug 23 23:43:15.973: INFO: Wrong image for pod: daemon-set-dvdtd. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Aug 23 23:43:15.973: INFO: Wrong image for pod: daemon-set-jvknz. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Aug 23 23:43:15.973: INFO: Wrong image for pod: daemon-set-z4k87. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Aug 23 23:43:16.961: INFO: Wrong image for pod: daemon-set-dvdtd. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Aug 23 23:43:16.961: INFO: Wrong image for pod: daemon-set-jvknz. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Aug 23 23:43:16.961: INFO: Wrong image for pod: daemon-set-z4k87. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Aug 23 23:43:17.915: INFO: Wrong image for pod: daemon-set-dvdtd. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Aug 23 23:43:17.915: INFO: Wrong image for pod: daemon-set-jvknz. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Aug 23 23:43:17.915: INFO: Pod daemon-set-jvknz is not available
Aug 23 23:43:17.915: INFO: Wrong image for pod: daemon-set-z4k87. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Aug 23 23:43:18.942: INFO: Wrong image for pod: daemon-set-dvdtd. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Aug 23 23:43:18.942: INFO: Wrong image for pod: daemon-set-jvknz. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Aug 23 23:43:18.942: INFO: Pod daemon-set-jvknz is not available
Aug 23 23:43:18.942: INFO: Wrong image for pod: daemon-set-z4k87. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Aug 23 23:43:19.920: INFO: Wrong image for pod: daemon-set-dvdtd. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Aug 23 23:43:19.920: INFO: Wrong image for pod: daemon-set-jvknz. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Aug 23 23:43:19.920: INFO: Pod daemon-set-jvknz is not available
Aug 23 23:43:19.920: INFO: Wrong image for pod: daemon-set-z4k87. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Aug 23 23:43:20.915: INFO: Wrong image for pod: daemon-set-dvdtd. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Aug 23 23:43:20.915: INFO: Wrong image for pod: daemon-set-jvknz. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Aug 23 23:43:20.915: INFO: Pod daemon-set-jvknz is not available
Aug 23 23:43:20.915: INFO: Wrong image for pod: daemon-set-z4k87. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Aug 23 23:43:21.918: INFO: Wrong image for pod: daemon-set-dvdtd. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Aug 23 23:43:21.918: INFO: Wrong image for pod: daemon-set-jvknz. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Aug 23 23:43:21.918: INFO: Pod daemon-set-jvknz is not available
Aug 23 23:43:21.918: INFO: Wrong image for pod: daemon-set-z4k87. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Aug 23 23:43:22.946: INFO: Wrong image for pod: daemon-set-dvdtd. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Aug 23 23:43:22.946: INFO: Wrong image for pod: daemon-set-jvknz. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Aug 23 23:43:22.946: INFO: Pod daemon-set-jvknz is not available
Aug 23 23:43:22.946: INFO: Wrong image for pod: daemon-set-z4k87. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Aug 23 23:43:23.958: INFO: Pod daemon-set-6gjh4 is not available
Aug 23 23:43:23.958: INFO: Wrong image for pod: daemon-set-dvdtd. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Aug 23 23:43:23.958: INFO: Wrong image for pod: daemon-set-z4k87. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Aug 23 23:43:25.001: INFO: Pod daemon-set-6gjh4 is not available
Aug 23 23:43:25.001: INFO: Wrong image for pod: daemon-set-dvdtd. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Aug 23 23:43:25.001: INFO: Wrong image for pod: daemon-set-z4k87. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Aug 23 23:43:26.009: INFO: Pod daemon-set-6gjh4 is not available
Aug 23 23:43:26.009: INFO: Wrong image for pod: daemon-set-dvdtd. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Aug 23 23:43:26.009: INFO: Wrong image for pod: daemon-set-z4k87. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Aug 23 23:43:27.017: INFO: Wrong image for pod: daemon-set-dvdtd. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Aug 23 23:43:27.017: INFO: Wrong image for pod: daemon-set-z4k87. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Aug 23 23:43:28.094: INFO: Wrong image for pod: daemon-set-dvdtd. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Aug 23 23:43:28.094: INFO: Wrong image for pod: daemon-set-z4k87. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Aug 23 23:43:29.040: INFO: Wrong image for pod: daemon-set-dvdtd. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Aug 23 23:43:29.040: INFO: Wrong image for pod: daemon-set-z4k87. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Aug 23 23:43:29.040: INFO: Pod daemon-set-z4k87 is not available
Aug 23 23:43:30.059: INFO: Wrong image for pod: daemon-set-dvdtd. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Aug 23 23:43:30.059: INFO: Wrong image for pod: daemon-set-z4k87. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Aug 23 23:43:30.059: INFO: Pod daemon-set-z4k87 is not available
Aug 23 23:43:31.069: INFO: Wrong image for pod: daemon-set-dvdtd. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Aug 23 23:43:31.069: INFO: Wrong image for pod: daemon-set-z4k87. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Aug 23 23:43:31.069: INFO: Pod daemon-set-z4k87 is not available
Aug 23 23:43:31.990: INFO: Wrong image for pod: daemon-set-dvdtd. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Aug 23 23:43:31.991: INFO: Wrong image for pod: daemon-set-z4k87. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Aug 23 23:43:31.991: INFO: Pod daemon-set-z4k87 is not available
Aug 23 23:43:33.061: INFO: Wrong image for pod: daemon-set-dvdtd. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Aug 23 23:43:33.061: INFO: Wrong image for pod: daemon-set-z4k87. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Aug 23 23:43:33.061: INFO: Pod daemon-set-z4k87 is not available
Aug 23 23:43:34.171: INFO: Wrong image for pod: daemon-set-dvdtd. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Aug 23 23:43:34.171: INFO: Wrong image for pod: daemon-set-z4k87. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Aug 23 23:43:34.171: INFO: Pod daemon-set-z4k87 is not available
Aug 23 23:43:35.104: INFO: Wrong image for pod: daemon-set-dvdtd. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Aug 23 23:43:35.104: INFO: Wrong image for pod: daemon-set-z4k87. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Aug 23 23:43:35.104: INFO: Pod daemon-set-z4k87 is not available
Aug 23 23:43:36.059: INFO: Wrong image for pod: daemon-set-dvdtd. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Aug 23 23:43:36.059: INFO: Wrong image for pod: daemon-set-z4k87. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Aug 23 23:43:36.059: INFO: Pod daemon-set-z4k87 is not available
Aug 23 23:43:37.112: INFO: Wrong image for pod: daemon-set-dvdtd. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Aug 23 23:43:37.113: INFO: Pod daemon-set-zbgzz is not available
Aug 23 23:43:38.066: INFO: Wrong image for pod: daemon-set-dvdtd. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Aug 23 23:43:38.066: INFO: Pod daemon-set-zbgzz is not available
Aug 23 23:43:39.023: INFO: Wrong image for pod: daemon-set-dvdtd. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Aug 23 23:43:39.023: INFO: Pod daemon-set-zbgzz is not available
Aug 23 23:43:40.010: INFO: Wrong image for pod: daemon-set-dvdtd. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Aug 23 23:43:40.010: INFO: Pod daemon-set-zbgzz is not available
Aug 23 23:43:40.966: INFO: Wrong image for pod: daemon-set-dvdtd. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Aug 23 23:43:42.041: INFO: Wrong image for pod: daemon-set-dvdtd. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Aug 23 23:43:43.198: INFO: Wrong image for pod: daemon-set-dvdtd. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Aug 23 23:43:44.036: INFO: Wrong image for pod: daemon-set-dvdtd. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Aug 23 23:43:45.100: INFO: Wrong image for pod: daemon-set-dvdtd. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Aug 23 23:43:45.100: INFO: Pod daemon-set-dvdtd is not available
Aug 23 23:43:46.054: INFO: Wrong image for pod: daemon-set-dvdtd. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Aug 23 23:43:46.054: INFO: Pod daemon-set-dvdtd is not available
Aug 23 23:43:47.026: INFO: Wrong image for pod: daemon-set-dvdtd. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Aug 23 23:43:47.026: INFO: Pod daemon-set-dvdtd is not available
Aug 23 23:43:48.031: INFO: Wrong image for pod: daemon-set-dvdtd. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Aug 23 23:43:48.031: INFO: Pod daemon-set-dvdtd is not available
Aug 23 23:43:50.065: INFO: Wrong image for pod: daemon-set-dvdtd. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Aug 23 23:43:50.065: INFO: Pod daemon-set-dvdtd is not available
Aug 23 23:43:51.087: INFO: Wrong image for pod: daemon-set-dvdtd. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Aug 23 23:43:51.087: INFO: Pod daemon-set-dvdtd is not available
Aug 23 23:43:52.038: INFO: Wrong image for pod: daemon-set-dvdtd. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Aug 23 23:43:52.038: INFO: Pod daemon-set-dvdtd is not available
Aug 23 23:43:53.038: INFO: Wrong image for pod: daemon-set-dvdtd. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Aug 23 23:43:53.038: INFO: Pod daemon-set-dvdtd is not available
Aug 23 23:43:54.011: INFO: Wrong image for pod: daemon-set-dvdtd. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Aug 23 23:43:54.011: INFO: Pod daemon-set-dvdtd is not available
Aug 23 23:43:55.016: INFO: Wrong image for pod: daemon-set-dvdtd. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Aug 23 23:43:55.016: INFO: Pod daemon-set-dvdtd is not available
Aug 23 23:43:56.019: INFO: Wrong image for pod: daemon-set-dvdtd. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Aug 23 23:43:56.019: INFO: Pod daemon-set-dvdtd is not available
Aug 23 23:43:57.061: INFO: Pod daemon-set-wsjlj is not available
STEP: Check that daemon pods are still running on every node of the cluster.
Aug 23 23:43:58.074: INFO: Number of nodes with available pods: 2
Aug 23 23:43:58.074: INFO: Node 10.149.248.9 is running more than one daemon pod
Aug 23 23:43:59.709: INFO: Number of nodes with available pods: 3
Aug 23 23:43:59.709: INFO: Number of running nodes: 3, number of available pods: 3
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:95
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-2890, will wait for the garbage collector to delete the pods
Aug 23 23:44:00.771: INFO: Deleting DaemonSet.extensions daemon-set took: 301.505038ms
Aug 23 23:44:00.971: INFO: Terminating DaemonSet.extensions daemon-set pods took: 200.21619ms
Aug 23 23:44:16.882: INFO: Number of nodes with available pods: 0
Aug 23 23:44:16.882: INFO: Number of running nodes: 0, number of available pods: 0
Aug 23 23:44:17.151: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-2890/daemonsets","resourceVersion":"130180"},"items":null}

Aug 23 23:44:17.452: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-2890/pods","resourceVersion":"130180"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 23:44:18.641: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-2890" for this suite.

• [SLOW TEST:80.316 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]","total":311,"completed":299,"skipped":5249,"failed":0}
SS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 23:44:19.490: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir 0666 on node default medium
Aug 23 23:44:20.608: INFO: Waiting up to 5m0s for pod "pod-68ba404e-4203-40e0-b794-399b9fccfaef" in namespace "emptydir-9750" to be "Succeeded or Failed"
Aug 23 23:44:20.745: INFO: Pod "pod-68ba404e-4203-40e0-b794-399b9fccfaef": Phase="Pending", Reason="", readiness=false. Elapsed: 136.801951ms
Aug 23 23:44:22.876: INFO: Pod "pod-68ba404e-4203-40e0-b794-399b9fccfaef": Phase="Pending", Reason="", readiness=false. Elapsed: 2.267411947s
Aug 23 23:44:24.943: INFO: Pod "pod-68ba404e-4203-40e0-b794-399b9fccfaef": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.334587683s
STEP: Saw pod success
Aug 23 23:44:24.943: INFO: Pod "pod-68ba404e-4203-40e0-b794-399b9fccfaef" satisfied condition "Succeeded or Failed"
Aug 23 23:44:25.023: INFO: Trying to get logs from node 10.149.248.24 pod pod-68ba404e-4203-40e0-b794-399b9fccfaef container test-container: <nil>
STEP: delete the pod
Aug 23 23:44:25.229: INFO: Waiting for pod pod-68ba404e-4203-40e0-b794-399b9fccfaef to disappear
Aug 23 23:44:25.286: INFO: Pod pod-68ba404e-4203-40e0-b794-399b9fccfaef no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 23:44:25.286: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9750" for this suite.

• [SLOW TEST:6.048 seconds]
[sig-storage] EmptyDir volumes
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:45
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":300,"skipped":5251,"failed":0}
SSS
------------------------------
[k8s.io] KubeletManagedEtcHosts 
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 23:44:25.540: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Setting up the test
STEP: Creating hostNetwork=false pod
STEP: Creating hostNetwork=true pod
STEP: Running the test
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false
Aug 23 23:44:34.492: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-3170 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 23 23:44:34.492: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
Aug 23 23:44:36.296: INFO: Exec stderr: ""
Aug 23 23:44:36.296: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-3170 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 23 23:44:36.296: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
Aug 23 23:44:38.800: INFO: Exec stderr: ""
Aug 23 23:44:38.800: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-3170 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 23 23:44:38.800: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
Aug 23 23:44:39.035: INFO: Exec stderr: ""
Aug 23 23:44:39.036: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-3170 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 23 23:44:39.036: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
Aug 23 23:44:40.761: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount
Aug 23 23:44:40.761: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-3170 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 23 23:44:40.761: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
Aug 23 23:44:41.637: INFO: Exec stderr: ""
Aug 23 23:44:41.637: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-3170 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 23 23:44:41.637: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
Aug 23 23:44:41.879: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true
Aug 23 23:44:41.879: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-3170 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 23 23:44:41.879: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
Aug 23 23:44:42.786: INFO: Exec stderr: ""
Aug 23 23:44:42.786: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-3170 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 23 23:44:42.786: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
Aug 23 23:44:43.783: INFO: Exec stderr: ""
Aug 23 23:44:43.783: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-3170 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 23 23:44:43.783: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
Aug 23 23:44:43.979: INFO: Exec stderr: ""
Aug 23 23:44:43.979: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-3170 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 23 23:44:43.979: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
Aug 23 23:44:44.268: INFO: Exec stderr: ""
[AfterEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 23:44:44.268: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-kubelet-etc-hosts-3170" for this suite.

• [SLOW TEST:18.846 seconds]
[k8s.io] KubeletManagedEtcHosts
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] KubeletManagedEtcHosts should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":301,"skipped":5254,"failed":0}
S
------------------------------
[sig-api-machinery] Garbage collector 
  should not be blocked by dependency circle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 23:44:44.386: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be blocked by dependency circle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Aug 23 23:44:44.782: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"63050e20-149d-4826-8cbc-7b1715bfbc4f", Controller:(*bool)(0xc008758526), BlockOwnerDeletion:(*bool)(0xc008758527)}}
Aug 23 23:44:44.822: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"444223ea-b2e7-4cfc-9503-1c7795dda96d", Controller:(*bool)(0xc003cd5872), BlockOwnerDeletion:(*bool)(0xc003cd5873)}}
Aug 23 23:44:44.878: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"3a9871ba-0d95-4a45-b50f-3ecabeb0bf8f", Controller:(*bool)(0xc00089a9e6), BlockOwnerDeletion:(*bool)(0xc00089a9e7)}}
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 23:44:50.157: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-3097" for this suite.

• [SLOW TEST:6.607 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be blocked by dependency circle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should not be blocked by dependency circle [Conformance]","total":311,"completed":302,"skipped":5255,"failed":0}
S
------------------------------
[sig-network] Services 
  should test the lifecycle of an Endpoint [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 23:44:50.994: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should test the lifecycle of an Endpoint [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating an Endpoint
STEP: waiting for available Endpoint
STEP: listing all Endpoints
STEP: updating the Endpoint
STEP: fetching the Endpoint
STEP: patching the Endpoint
STEP: fetching the Endpoint
STEP: deleting the Endpoint by Collection
STEP: waiting for Endpoint deletion
STEP: fetching the Endpoint
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 23:44:56.191: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8834" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749

• [SLOW TEST:6.261 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should test the lifecycle of an Endpoint [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Services should test the lifecycle of an Endpoint [Conformance]","total":311,"completed":303,"skipped":5256,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 23:44:57.255: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a pod in the namespace
STEP: Waiting for the pod to have running status
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there are no pods in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 23:45:36.531: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-4774" for this suite.
STEP: Destroying namespace "nsdeletetest-5529" for this suite.
Aug 23 23:45:38.161: INFO: Namespace nsdeletetest-5529 was already deleted
STEP: Destroying namespace "nsdeletetest-6254" for this suite.

• [SLOW TEST:41.046 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all pods are removed when a namespace is deleted [Conformance]","total":311,"completed":304,"skipped":5264,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 23:45:38.303: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating projection with secret that has name projected-secret-test-map-e08500b3-924a-45b9-bd40-c3e47c63914f
STEP: Creating a pod to test consume secrets
Aug 23 23:45:39.585: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-8a5f0b38-d63f-47bd-b651-e361d7a276d1" in namespace "projected-5635" to be "Succeeded or Failed"
Aug 23 23:45:39.692: INFO: Pod "pod-projected-secrets-8a5f0b38-d63f-47bd-b651-e361d7a276d1": Phase="Pending", Reason="", readiness=false. Elapsed: 107.117486ms
Aug 23 23:45:41.766: INFO: Pod "pod-projected-secrets-8a5f0b38-d63f-47bd-b651-e361d7a276d1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.18135986s
Aug 23 23:45:43.797: INFO: Pod "pod-projected-secrets-8a5f0b38-d63f-47bd-b651-e361d7a276d1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.211661219s
Aug 23 23:45:45.812: INFO: Pod "pod-projected-secrets-8a5f0b38-d63f-47bd-b651-e361d7a276d1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.226807986s
STEP: Saw pod success
Aug 23 23:45:45.812: INFO: Pod "pod-projected-secrets-8a5f0b38-d63f-47bd-b651-e361d7a276d1" satisfied condition "Succeeded or Failed"
Aug 23 23:45:45.826: INFO: Trying to get logs from node 10.149.248.24 pod pod-projected-secrets-8a5f0b38-d63f-47bd-b651-e361d7a276d1 container projected-secret-volume-test: <nil>
STEP: delete the pod
Aug 23 23:45:45.910: INFO: Waiting for pod pod-projected-secrets-8a5f0b38-d63f-47bd-b651-e361d7a276d1 to disappear
Aug 23 23:45:45.937: INFO: Pod pod-projected-secrets-8a5f0b38-d63f-47bd-b651-e361d7a276d1 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 23:45:45.937: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5635" for this suite.

• [SLOW TEST:7.760 seconds]
[sig-storage] Projected secret
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":305,"skipped":5292,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve a basic endpoint from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 23:45:46.064: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should serve a basic endpoint from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating service endpoint-test2 in namespace services-1348
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-1348 to expose endpoints map[]
Aug 23 23:45:46.387: INFO: successfully validated that service endpoint-test2 in namespace services-1348 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-1348
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-1348 to expose endpoints map[pod1:[80]]
Aug 23 23:45:49.789: INFO: successfully validated that service endpoint-test2 in namespace services-1348 exposes endpoints map[pod1:[80]]
STEP: Creating pod pod2 in namespace services-1348
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-1348 to expose endpoints map[pod1:[80] pod2:[80]]
Aug 23 23:45:54.111: INFO: Unexpected endpoints: found map[7f672216-fe22-4f2c-879b-195c12a2c4a4:[80]], expected map[pod1:[80] pod2:[80]], will retry
Aug 23 23:45:57.318: INFO: successfully validated that service endpoint-test2 in namespace services-1348 exposes endpoints map[pod1:[80] pod2:[80]]
STEP: Deleting pod pod1 in namespace services-1348
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-1348 to expose endpoints map[pod2:[80]]
Aug 23 23:45:57.850: INFO: successfully validated that service endpoint-test2 in namespace services-1348 exposes endpoints map[pod2:[80]]
STEP: Deleting pod pod2 in namespace services-1348
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-1348 to expose endpoints map[]
Aug 23 23:45:58.324: INFO: successfully validated that service endpoint-test2 in namespace services-1348 exposes endpoints map[]
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 23:45:58.504: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1348" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749

• [SLOW TEST:13.049 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should serve a basic endpoint from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Services should serve a basic endpoint from pods  [Conformance]","total":311,"completed":306,"skipped":5318,"failed":0}
SSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 23:45:59.113: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:92
Aug 23 23:45:59.732: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Aug 23 23:46:00.091: INFO: Waiting for terminating namespaces to be deleted...
Aug 23 23:46:00.275: INFO: 
Logging pods the apiserver thinks is on node 10.149.248.24 before test
Aug 23 23:46:00.458: INFO: calico-node-9qqn4 from calico-system started at 2021-08-23 20:02:35 +0000 UTC (1 container statuses recorded)
Aug 23 23:46:00.458: INFO: 	Container calico-node ready: true, restart count 0
Aug 23 23:46:00.458: INFO: calico-typha-84b7fc6fc8-9fjkp from calico-system started at 2021-08-23 20:02:39 +0000 UTC (1 container statuses recorded)
Aug 23 23:46:00.458: INFO: 	Container calico-typha ready: true, restart count 0
Aug 23 23:46:00.458: INFO: ibm-keepalived-watcher-vrlwd from kube-system started at 2021-08-23 20:00:36 +0000 UTC (1 container statuses recorded)
Aug 23 23:46:00.458: INFO: 	Container keepalived-watcher ready: true, restart count 0
Aug 23 23:46:00.458: INFO: ibm-master-proxy-static-10.149.248.24 from kube-system started at 2021-08-23 20:00:33 +0000 UTC (2 container statuses recorded)
Aug 23 23:46:00.458: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Aug 23 23:46:00.458: INFO: 	Container pause ready: true, restart count 0
Aug 23 23:46:00.458: INFO: ibmcloud-block-storage-driver-jgx4r from kube-system started at 2021-08-23 20:00:42 +0000 UTC (1 container statuses recorded)
Aug 23 23:46:00.458: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Aug 23 23:46:00.458: INFO: tuned-7sb7t from openshift-cluster-node-tuning-operator started at 2021-08-23 20:05:27 +0000 UTC (1 container statuses recorded)
Aug 23 23:46:00.458: INFO: 	Container tuned ready: true, restart count 0
Aug 23 23:46:00.458: INFO: dns-default-mgbp7 from openshift-dns started at 2021-08-23 20:05:27 +0000 UTC (3 container statuses recorded)
Aug 23 23:46:00.458: INFO: 	Container dns ready: true, restart count 0
Aug 23 23:46:00.458: INFO: 	Container dns-node-resolver ready: true, restart count 0
Aug 23 23:46:00.458: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 23 23:46:00.458: INFO: node-ca-dl95t from openshift-image-registry started at 2021-08-23 20:04:29 +0000 UTC (1 container statuses recorded)
Aug 23 23:46:00.458: INFO: 	Container node-ca ready: true, restart count 0
Aug 23 23:46:00.458: INFO: ingress-canary-hx8m6 from openshift-ingress-canary started at 2021-08-23 23:27:23 +0000 UTC (1 container statuses recorded)
Aug 23 23:46:00.458: INFO: 	Container hello-openshift-canary ready: true, restart count 0
Aug 23 23:46:00.458: INFO: openshift-kube-proxy-wn8f2 from openshift-kube-proxy started at 2021-08-23 20:01:25 +0000 UTC (2 container statuses recorded)
Aug 23 23:46:00.458: INFO: 	Container kube-proxy ready: true, restart count 0
Aug 23 23:46:00.458: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 23 23:46:00.458: INFO: node-exporter-b5rwn from openshift-monitoring started at 2021-08-23 20:03:19 +0000 UTC (2 container statuses recorded)
Aug 23 23:46:00.458: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 23 23:46:00.458: INFO: 	Container node-exporter ready: true, restart count 0
Aug 23 23:46:00.458: INFO: multus-48kkt from openshift-multus started at 2021-08-23 20:01:12 +0000 UTC (1 container statuses recorded)
Aug 23 23:46:00.458: INFO: 	Container kube-multus ready: true, restart count 0
Aug 23 23:46:00.458: INFO: multus-admission-controller-tp5tk from openshift-multus started at 2021-08-23 23:27:23 +0000 UTC (2 container statuses recorded)
Aug 23 23:46:00.458: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 23 23:46:00.458: INFO: 	Container multus-admission-controller ready: true, restart count 0
Aug 23 23:46:00.458: INFO: network-metrics-daemon-cqr85 from openshift-multus started at 2021-08-23 20:01:15 +0000 UTC (2 container statuses recorded)
Aug 23 23:46:00.458: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 23 23:46:00.458: INFO: 	Container network-metrics-daemon ready: true, restart count 0
Aug 23 23:46:00.458: INFO: network-check-target-447wc from openshift-network-diagnostics started at 2021-08-23 20:01:33 +0000 UTC (1 container statuses recorded)
Aug 23 23:46:00.458: INFO: 	Container network-check-target-container ready: true, restart count 0
Aug 23 23:46:00.458: INFO: sonobuoy-systemd-logs-daemon-set-9034e69b695e4ee2-s28vf from sonobuoy started at 2021-08-23 21:37:23 +0000 UTC (2 container statuses recorded)
Aug 23 23:46:00.458: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 23 23:46:00.458: INFO: 	Container systemd-logs ready: true, restart count 0
Aug 23 23:46:00.458: INFO: 
Logging pods the apiserver thinks is on node 10.149.248.25 before test
Aug 23 23:46:00.558: INFO: calico-node-4m7h2 from calico-system started at 2021-08-23 20:02:35 +0000 UTC (1 container statuses recorded)
Aug 23 23:46:00.558: INFO: 	Container calico-node ready: true, restart count 0
Aug 23 23:46:00.558: INFO: calico-typha-84b7fc6fc8-tf4pd from calico-system started at 2021-08-23 20:02:33 +0000 UTC (1 container statuses recorded)
Aug 23 23:46:00.558: INFO: 	Container calico-typha ready: true, restart count 0
Aug 23 23:46:00.558: INFO: ibm-cloud-provider-ip-169-55-101-126-6bb454748-g6kdf from ibm-system started at 2021-08-23 20:10:53 +0000 UTC (1 container statuses recorded)
Aug 23 23:46:00.558: INFO: 	Container ibm-cloud-provider-ip-169-55-101-126 ready: true, restart count 0
Aug 23 23:46:00.558: INFO: ibm-file-plugin-68fbcccc88-bgbn9 from kube-system started at 2021-08-23 20:03:01 +0000 UTC (1 container statuses recorded)
Aug 23 23:46:00.558: INFO: 	Container ibm-file-plugin-container ready: true, restart count 0
Aug 23 23:46:00.558: INFO: ibm-keepalived-watcher-c94bg from kube-system started at 2021-08-23 20:00:20 +0000 UTC (1 container statuses recorded)
Aug 23 23:46:00.558: INFO: 	Container keepalived-watcher ready: true, restart count 0
Aug 23 23:46:00.558: INFO: ibm-master-proxy-static-10.149.248.25 from kube-system started at 2021-08-23 20:00:16 +0000 UTC (2 container statuses recorded)
Aug 23 23:46:00.558: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Aug 23 23:46:00.558: INFO: 	Container pause ready: true, restart count 0
Aug 23 23:46:00.558: INFO: ibm-storage-watcher-5c4d7b6cd6-zxwb2 from kube-system started at 2021-08-23 23:26:37 +0000 UTC (1 container statuses recorded)
Aug 23 23:46:00.558: INFO: 	Container ibm-storage-watcher-container ready: true, restart count 0
Aug 23 23:46:00.558: INFO: ibmcloud-block-storage-driver-zdvf4 from kube-system started at 2021-08-23 20:00:24 +0000 UTC (1 container statuses recorded)
Aug 23 23:46:00.558: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Aug 23 23:46:00.558: INFO: ibmcloud-block-storage-plugin-7d6d9649b-zdvgj from kube-system started at 2021-08-23 20:03:01 +0000 UTC (1 container statuses recorded)
Aug 23 23:46:00.558: INFO: 	Container ibmcloud-block-storage-plugin-container ready: true, restart count 0
Aug 23 23:46:00.558: INFO: vpn-db594b4f9-9mxgx from kube-system started at 2021-08-23 20:08:59 +0000 UTC (1 container statuses recorded)
Aug 23 23:46:00.558: INFO: 	Container vpn ready: true, restart count 0
Aug 23 23:46:00.559: INFO: tuned-bfbjn from openshift-cluster-node-tuning-operator started at 2021-08-23 20:05:28 +0000 UTC (1 container statuses recorded)
Aug 23 23:46:00.559: INFO: 	Container tuned ready: true, restart count 0
Aug 23 23:46:00.559: INFO: cluster-storage-operator-6c5c749558-97wht from openshift-cluster-storage-operator started at 2021-08-23 20:03:01 +0000 UTC (1 container statuses recorded)
Aug 23 23:46:00.559: INFO: 	Container cluster-storage-operator ready: true, restart count 1
Aug 23 23:46:00.559: INFO: csi-snapshot-controller-operator-645f4897d-2vvbp from openshift-cluster-storage-operator started at 2021-08-23 20:03:01 +0000 UTC (1 container statuses recorded)
Aug 23 23:46:00.559: INFO: 	Container csi-snapshot-controller-operator ready: true, restart count 0
Aug 23 23:46:00.559: INFO: console-dc8c686bb-8xpf8 from openshift-console started at 2021-08-23 20:07:03 +0000 UTC (1 container statuses recorded)
Aug 23 23:46:00.559: INFO: 	Container console ready: true, restart count 0
Aug 23 23:46:00.559: INFO: downloads-6c96776f98-5w9bj from openshift-console started at 2021-08-23 20:03:01 +0000 UTC (1 container statuses recorded)
Aug 23 23:46:00.559: INFO: 	Container download-server ready: true, restart count 0
Aug 23 23:46:00.559: INFO: downloads-6c96776f98-nqhnf from openshift-console started at 2021-08-23 22:12:56 +0000 UTC (1 container statuses recorded)
Aug 23 23:46:00.559: INFO: 	Container download-server ready: true, restart count 0
Aug 23 23:46:00.559: INFO: dns-operator-6dbb54f776-8k9pq from openshift-dns-operator started at 2021-08-23 20:03:01 +0000 UTC (2 container statuses recorded)
Aug 23 23:46:00.559: INFO: 	Container dns-operator ready: true, restart count 0
Aug 23 23:46:00.559: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 23 23:46:00.559: INFO: dns-default-mxjt9 from openshift-dns started at 2021-08-23 20:05:27 +0000 UTC (3 container statuses recorded)
Aug 23 23:46:00.559: INFO: 	Container dns ready: true, restart count 0
Aug 23 23:46:00.559: INFO: 	Container dns-node-resolver ready: true, restart count 0
Aug 23 23:46:00.559: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 23 23:46:00.559: INFO: cluster-image-registry-operator-85978c675-rdng6 from openshift-image-registry started at 2021-08-23 20:03:01 +0000 UTC (1 container statuses recorded)
Aug 23 23:46:00.559: INFO: 	Container cluster-image-registry-operator ready: true, restart count 0
Aug 23 23:46:00.559: INFO: node-ca-czldz from openshift-image-registry started at 2021-08-23 20:04:29 +0000 UTC (1 container statuses recorded)
Aug 23 23:46:00.559: INFO: 	Container node-ca ready: true, restart count 0
Aug 23 23:46:00.559: INFO: ingress-canary-9skj4 from openshift-ingress-canary started at 2021-08-23 20:05:35 +0000 UTC (1 container statuses recorded)
Aug 23 23:46:00.559: INFO: 	Container hello-openshift-canary ready: true, restart count 0
Aug 23 23:46:00.559: INFO: ingress-operator-5d97777898-hf6cx from openshift-ingress-operator started at 2021-08-23 23:26:38 +0000 UTC (2 container statuses recorded)
Aug 23 23:46:00.559: INFO: 	Container ingress-operator ready: true, restart count 0
Aug 23 23:46:00.559: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 23 23:46:00.559: INFO: router-default-678bfcd875-g2q6m from openshift-ingress started at 2021-08-23 20:05:36 +0000 UTC (1 container statuses recorded)
Aug 23 23:46:00.559: INFO: 	Container router ready: true, restart count 0
Aug 23 23:46:00.559: INFO: openshift-kube-proxy-sbzjn from openshift-kube-proxy started at 2021-08-23 20:01:25 +0000 UTC (2 container statuses recorded)
Aug 23 23:46:00.559: INFO: 	Container kube-proxy ready: true, restart count 0
Aug 23 23:46:00.559: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 23 23:46:00.559: INFO: migrator-f58676cd4-5q7gc from openshift-kube-storage-version-migrator started at 2021-08-23 23:26:37 +0000 UTC (1 container statuses recorded)
Aug 23 23:46:00.559: INFO: 	Container migrator ready: true, restart count 0
Aug 23 23:46:00.559: INFO: redhat-marketplace-hvcn5 from openshift-marketplace started at 2021-08-23 20:04:56 +0000 UTC (1 container statuses recorded)
Aug 23 23:46:00.559: INFO: 	Container registry-server ready: true, restart count 0
Aug 23 23:46:00.559: INFO: alertmanager-main-1 from openshift-monitoring started at 2021-08-23 20:06:03 +0000 UTC (5 container statuses recorded)
Aug 23 23:46:00.559: INFO: 	Container alertmanager ready: true, restart count 0
Aug 23 23:46:00.559: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Aug 23 23:46:00.559: INFO: 	Container config-reloader ready: true, restart count 0
Aug 23 23:46:00.559: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 23 23:46:00.559: INFO: 	Container prom-label-proxy ready: true, restart count 0
Aug 23 23:46:00.559: INFO: cluster-monitoring-operator-c69d85486-m52jl from openshift-monitoring started at 2021-08-23 20:03:01 +0000 UTC (2 container statuses recorded)
Aug 23 23:46:00.559: INFO: 	Container cluster-monitoring-operator ready: true, restart count 0
Aug 23 23:46:00.559: INFO: 	Container kube-rbac-proxy ready: true, restart count 4
Aug 23 23:46:00.559: INFO: node-exporter-fsg2g from openshift-monitoring started at 2021-08-23 20:03:19 +0000 UTC (2 container statuses recorded)
Aug 23 23:46:00.559: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 23 23:46:00.559: INFO: 	Container node-exporter ready: true, restart count 0
Aug 23 23:46:00.559: INFO: prometheus-adapter-6685ccb7f-gxd72 from openshift-monitoring started at 2021-08-23 20:06:45 +0000 UTC (1 container statuses recorded)
Aug 23 23:46:00.559: INFO: 	Container prometheus-adapter ready: true, restart count 0
Aug 23 23:46:00.559: INFO: prometheus-k8s-1 from openshift-monitoring started at 2021-08-23 20:06:14 +0000 UTC (7 container statuses recorded)
Aug 23 23:46:00.559: INFO: 	Container config-reloader ready: true, restart count 0
Aug 23 23:46:00.559: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 23 23:46:00.559: INFO: 	Container kube-rbac-proxy-thanos ready: true, restart count 0
Aug 23 23:46:00.559: INFO: 	Container prom-label-proxy ready: true, restart count 0
Aug 23 23:46:00.559: INFO: 	Container prometheus ready: true, restart count 1
Aug 23 23:46:00.559: INFO: 	Container prometheus-proxy ready: true, restart count 0
Aug 23 23:46:00.559: INFO: 	Container thanos-sidecar ready: true, restart count 0
Aug 23 23:46:00.559: INFO: thanos-querier-97cd894c4-z4jqj from openshift-monitoring started at 2021-08-23 20:06:10 +0000 UTC (5 container statuses recorded)
Aug 23 23:46:00.559: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 23 23:46:00.559: INFO: 	Container kube-rbac-proxy-rules ready: true, restart count 0
Aug 23 23:46:00.559: INFO: 	Container oauth-proxy ready: true, restart count 0
Aug 23 23:46:00.559: INFO: 	Container prom-label-proxy ready: true, restart count 0
Aug 23 23:46:00.559: INFO: 	Container thanos-query ready: true, restart count 0
Aug 23 23:46:00.559: INFO: multus-2p958 from openshift-multus started at 2021-08-23 20:01:12 +0000 UTC (1 container statuses recorded)
Aug 23 23:46:00.559: INFO: 	Container kube-multus ready: true, restart count 0
Aug 23 23:46:00.559: INFO: multus-admission-controller-ww7jt from openshift-multus started at 2021-08-23 20:02:56 +0000 UTC (2 container statuses recorded)
Aug 23 23:46:00.559: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 23 23:46:00.559: INFO: 	Container multus-admission-controller ready: true, restart count 0
Aug 23 23:46:00.559: INFO: network-metrics-daemon-6bmnm from openshift-multus started at 2021-08-23 20:01:15 +0000 UTC (2 container statuses recorded)
Aug 23 23:46:00.559: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 23 23:46:00.559: INFO: 	Container network-metrics-daemon ready: true, restart count 0
Aug 23 23:46:00.559: INFO: network-check-target-cjqqr from openshift-network-diagnostics started at 2021-08-23 20:01:33 +0000 UTC (1 container statuses recorded)
Aug 23 23:46:00.559: INFO: 	Container network-check-target-container ready: true, restart count 0
Aug 23 23:46:00.559: INFO: olm-operator-d5dc9548d-mwfkk from openshift-operator-lifecycle-manager started at 2021-08-23 20:03:01 +0000 UTC (1 container statuses recorded)
Aug 23 23:46:00.559: INFO: 	Container olm-operator ready: true, restart count 0
Aug 23 23:46:00.559: INFO: packageserver-6d5f99f754-5t8qd from openshift-operator-lifecycle-manager started at 2021-08-23 22:12:56 +0000 UTC (1 container statuses recorded)
Aug 23 23:46:00.559: INFO: 	Container packageserver ready: true, restart count 0
Aug 23 23:46:00.559: INFO: metrics-8fc5c5f56-lq4k6 from openshift-roks-metrics started at 2021-08-23 20:03:01 +0000 UTC (1 container statuses recorded)
Aug 23 23:46:00.559: INFO: 	Container metrics ready: true, restart count 3
Aug 23 23:46:00.559: INFO: push-gateway-59f6bc56d4-bx994 from openshift-roks-metrics started at 2021-08-23 23:26:37 +0000 UTC (1 container statuses recorded)
Aug 23 23:46:00.559: INFO: 	Container push-gateway ready: false, restart count 0
Aug 23 23:46:00.559: INFO: sonobuoy-e2e-job-90ed16fed1444409 from sonobuoy started at 2021-08-23 21:37:22 +0000 UTC (2 container statuses recorded)
Aug 23 23:46:00.559: INFO: 	Container e2e ready: true, restart count 0
Aug 23 23:46:00.559: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 23 23:46:00.559: INFO: sonobuoy-systemd-logs-daemon-set-9034e69b695e4ee2-q8h6l from sonobuoy started at 2021-08-23 21:37:23 +0000 UTC (2 container statuses recorded)
Aug 23 23:46:00.559: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 23 23:46:00.559: INFO: 	Container systemd-logs ready: true, restart count 0
Aug 23 23:46:00.559: INFO: 
Logging pods the apiserver thinks is on node 10.149.248.9 before test
Aug 23 23:46:00.641: INFO: calico-kube-controllers-7dcbcc7c66-x65ng from calico-system started at 2021-08-23 20:02:55 +0000 UTC (1 container statuses recorded)
Aug 23 23:46:00.641: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Aug 23 23:46:00.641: INFO: calico-node-cgrcs from calico-system started at 2021-08-23 20:02:35 +0000 UTC (1 container statuses recorded)
Aug 23 23:46:00.641: INFO: 	Container calico-node ready: true, restart count 0
Aug 23 23:46:00.641: INFO: calico-typha-84b7fc6fc8-h6lnw from calico-system started at 2021-08-23 20:02:39 +0000 UTC (1 container statuses recorded)
Aug 23 23:46:00.641: INFO: 	Container calico-typha ready: true, restart count 0
Aug 23 23:46:00.641: INFO: ibm-cloud-provider-ip-169-55-101-126-6bb454748-t7nvk from ibm-system started at 2021-08-23 20:10:53 +0000 UTC (1 container statuses recorded)
Aug 23 23:46:00.641: INFO: 	Container ibm-cloud-provider-ip-169-55-101-126 ready: true, restart count 0
Aug 23 23:46:00.641: INFO: ibm-keepalived-watcher-rsh2g from kube-system started at 2021-08-23 20:00:08 +0000 UTC (1 container statuses recorded)
Aug 23 23:46:00.641: INFO: 	Container keepalived-watcher ready: true, restart count 0
Aug 23 23:46:00.641: INFO: ibm-master-proxy-static-10.149.248.9 from kube-system started at 2021-08-23 20:00:06 +0000 UTC (2 container statuses recorded)
Aug 23 23:46:00.641: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Aug 23 23:46:00.641: INFO: 	Container pause ready: true, restart count 0
Aug 23 23:46:00.641: INFO: ibmcloud-block-storage-driver-5fllx from kube-system started at 2021-08-23 20:00:13 +0000 UTC (1 container statuses recorded)
Aug 23 23:46:00.641: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Aug 23 23:46:00.641: INFO: cluster-node-tuning-operator-84b8576b47-qlh6k from openshift-cluster-node-tuning-operator started at 2021-08-23 22:12:56 +0000 UTC (1 container statuses recorded)
Aug 23 23:46:00.642: INFO: 	Container cluster-node-tuning-operator ready: true, restart count 0
Aug 23 23:46:00.642: INFO: tuned-m585x from openshift-cluster-node-tuning-operator started at 2021-08-23 20:05:28 +0000 UTC (1 container statuses recorded)
Aug 23 23:46:00.642: INFO: 	Container tuned ready: true, restart count 0
Aug 23 23:46:00.642: INFO: cluster-samples-operator-866dcfc6c4-ntqvw from openshift-cluster-samples-operator started at 2021-08-23 22:12:56 +0000 UTC (2 container statuses recorded)
Aug 23 23:46:00.642: INFO: 	Container cluster-samples-operator ready: true, restart count 0
Aug 23 23:46:00.642: INFO: 	Container cluster-samples-operator-watch ready: true, restart count 0
Aug 23 23:46:00.642: INFO: csi-snapshot-controller-566544547f-sfjlv from openshift-cluster-storage-operator started at 2021-08-23 23:26:38 +0000 UTC (1 container statuses recorded)
Aug 23 23:46:00.642: INFO: 	Container snapshot-controller ready: true, restart count 0
Aug 23 23:46:00.642: INFO: csi-snapshot-webhook-585d4946dc-kd8jt from openshift-cluster-storage-operator started at 2021-08-23 23:26:37 +0000 UTC (1 container statuses recorded)
Aug 23 23:46:00.642: INFO: 	Container webhook ready: true, restart count 0
Aug 23 23:46:00.642: INFO: console-operator-c7f9f8687-s8qw7 from openshift-console-operator started at 2021-08-23 23:26:37 +0000 UTC (1 container statuses recorded)
Aug 23 23:46:00.642: INFO: 	Container console-operator ready: true, restart count 0
Aug 23 23:46:00.642: INFO: console-dc8c686bb-z5xcx from openshift-console started at 2021-08-23 23:26:38 +0000 UTC (1 container statuses recorded)
Aug 23 23:46:00.642: INFO: 	Container console ready: true, restart count 0
Aug 23 23:46:00.642: INFO: dns-default-4hmcv from openshift-dns started at 2021-08-23 20:05:27 +0000 UTC (3 container statuses recorded)
Aug 23 23:46:00.642: INFO: 	Container dns ready: true, restart count 0
Aug 23 23:46:00.642: INFO: 	Container dns-node-resolver ready: true, restart count 0
Aug 23 23:46:00.642: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 23 23:46:00.642: INFO: image-registry-555cc7f64c-qtjjg from openshift-image-registry started at 2021-08-23 22:12:56 +0000 UTC (1 container statuses recorded)
Aug 23 23:46:00.642: INFO: 	Container registry ready: true, restart count 0
Aug 23 23:46:00.642: INFO: node-ca-g989x from openshift-image-registry started at 2021-08-23 20:04:29 +0000 UTC (1 container statuses recorded)
Aug 23 23:46:00.643: INFO: 	Container node-ca ready: true, restart count 0
Aug 23 23:46:00.643: INFO: registry-pvc-permissions-dbktz from openshift-image-registry started at 2021-08-23 20:08:40 +0000 UTC (1 container statuses recorded)
Aug 23 23:46:00.643: INFO: 	Container pvc-permissions ready: false, restart count 0
Aug 23 23:46:00.643: INFO: ingress-canary-l5l42 from openshift-ingress-canary started at 2021-08-23 20:05:35 +0000 UTC (1 container statuses recorded)
Aug 23 23:46:00.643: INFO: 	Container hello-openshift-canary ready: true, restart count 0
Aug 23 23:46:00.643: INFO: router-default-678bfcd875-2f5hb from openshift-ingress started at 2021-08-23 20:05:36 +0000 UTC (1 container statuses recorded)
Aug 23 23:46:00.643: INFO: 	Container router ready: true, restart count 0
Aug 23 23:46:00.643: INFO: openshift-kube-proxy-wk9m8 from openshift-kube-proxy started at 2021-08-23 20:01:25 +0000 UTC (2 container statuses recorded)
Aug 23 23:46:00.643: INFO: 	Container kube-proxy ready: true, restart count 0
Aug 23 23:46:00.643: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 23 23:46:00.643: INFO: kube-storage-version-migrator-operator-6d55bddccb-4jgrz from openshift-kube-storage-version-migrator-operator started at 2021-08-23 23:26:38 +0000 UTC (1 container statuses recorded)
Aug 23 23:46:00.643: INFO: 	Container kube-storage-version-migrator-operator ready: true, restart count 0
Aug 23 23:46:00.643: INFO: certified-operators-rlpm2 from openshift-marketplace started at 2021-08-23 20:04:55 +0000 UTC (1 container statuses recorded)
Aug 23 23:46:00.643: INFO: 	Container registry-server ready: true, restart count 0
Aug 23 23:46:00.643: INFO: community-operators-ztwk2 from openshift-marketplace started at 2021-08-23 20:04:56 +0000 UTC (1 container statuses recorded)
Aug 23 23:46:00.643: INFO: 	Container registry-server ready: true, restart count 0
Aug 23 23:46:00.643: INFO: marketplace-operator-7fd8fdcc5b-ncb77 from openshift-marketplace started at 2021-08-23 23:26:38 +0000 UTC (1 container statuses recorded)
Aug 23 23:46:00.643: INFO: 	Container marketplace-operator ready: true, restart count 0
Aug 23 23:46:00.643: INFO: redhat-operators-cpqsn from openshift-marketplace started at 2021-08-23 20:04:56 +0000 UTC (1 container statuses recorded)
Aug 23 23:46:00.643: INFO: 	Container registry-server ready: true, restart count 0
Aug 23 23:46:00.644: INFO: alertmanager-main-0 from openshift-monitoring started at 2021-08-23 23:27:04 +0000 UTC (5 container statuses recorded)
Aug 23 23:46:00.644: INFO: 	Container alertmanager ready: true, restart count 0
Aug 23 23:46:00.644: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Aug 23 23:46:00.644: INFO: 	Container config-reloader ready: true, restart count 0
Aug 23 23:46:00.644: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 23 23:46:00.644: INFO: 	Container prom-label-proxy ready: true, restart count 0
Aug 23 23:46:00.644: INFO: alertmanager-main-2 from openshift-monitoring started at 2021-08-23 20:06:04 +0000 UTC (5 container statuses recorded)
Aug 23 23:46:00.644: INFO: 	Container alertmanager ready: true, restart count 0
Aug 23 23:46:00.644: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Aug 23 23:46:00.644: INFO: 	Container config-reloader ready: true, restart count 0
Aug 23 23:46:00.644: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 23 23:46:00.644: INFO: 	Container prom-label-proxy ready: true, restart count 0
Aug 23 23:46:00.644: INFO: grafana-85d4bdb748-9g6gn from openshift-monitoring started at 2021-08-23 22:12:56 +0000 UTC (2 container statuses recorded)
Aug 23 23:46:00.644: INFO: 	Container grafana ready: true, restart count 0
Aug 23 23:46:00.644: INFO: 	Container grafana-proxy ready: true, restart count 0
Aug 23 23:46:00.644: INFO: kube-state-metrics-d956df775-bgj5t from openshift-monitoring started at 2021-08-23 23:26:38 +0000 UTC (3 container statuses recorded)
Aug 23 23:46:00.644: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Aug 23 23:46:00.644: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Aug 23 23:46:00.644: INFO: 	Container kube-state-metrics ready: true, restart count 0
Aug 23 23:46:00.644: INFO: node-exporter-kcjzv from openshift-monitoring started at 2021-08-23 20:03:19 +0000 UTC (2 container statuses recorded)
Aug 23 23:46:00.645: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 23 23:46:00.645: INFO: 	Container node-exporter ready: true, restart count 0
Aug 23 23:46:00.645: INFO: openshift-state-metrics-74b58f578c-cmhb9 from openshift-monitoring started at 2021-08-23 23:26:38 +0000 UTC (3 container statuses recorded)
Aug 23 23:46:00.645: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Aug 23 23:46:00.645: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Aug 23 23:46:00.645: INFO: 	Container openshift-state-metrics ready: true, restart count 0
Aug 23 23:46:00.645: INFO: prometheus-adapter-6685ccb7f-9tlvb from openshift-monitoring started at 2021-08-23 23:26:38 +0000 UTC (1 container statuses recorded)
Aug 23 23:46:00.645: INFO: 	Container prometheus-adapter ready: true, restart count 0
Aug 23 23:46:00.645: INFO: prometheus-k8s-0 from openshift-monitoring started at 2021-08-23 20:06:14 +0000 UTC (7 container statuses recorded)
Aug 23 23:46:00.645: INFO: 	Container config-reloader ready: true, restart count 0
Aug 23 23:46:00.645: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 23 23:46:00.645: INFO: 	Container kube-rbac-proxy-thanos ready: true, restart count 0
Aug 23 23:46:00.645: INFO: 	Container prom-label-proxy ready: true, restart count 0
Aug 23 23:46:00.645: INFO: 	Container prometheus ready: true, restart count 1
Aug 23 23:46:00.645: INFO: 	Container prometheus-proxy ready: true, restart count 0
Aug 23 23:46:00.645: INFO: 	Container thanos-sidecar ready: true, restart count 0
Aug 23 23:46:00.645: INFO: prometheus-operator-5699bb49dc-8q47w from openshift-monitoring started at 2021-08-23 20:06:44 +0000 UTC (2 container statuses recorded)
Aug 23 23:46:00.645: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 23 23:46:00.645: INFO: 	Container prometheus-operator ready: true, restart count 0
Aug 23 23:46:00.645: INFO: telemeter-client-9d6d6f95f-ztnrk from openshift-monitoring started at 2021-08-23 23:26:38 +0000 UTC (3 container statuses recorded)
Aug 23 23:46:00.645: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 23 23:46:00.645: INFO: 	Container reload ready: true, restart count 0
Aug 23 23:46:00.646: INFO: 	Container telemeter-client ready: true, restart count 0
Aug 23 23:46:00.646: INFO: thanos-querier-97cd894c4-g4rfr from openshift-monitoring started at 2021-08-23 22:12:56 +0000 UTC (5 container statuses recorded)
Aug 23 23:46:00.646: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 23 23:46:00.646: INFO: 	Container kube-rbac-proxy-rules ready: true, restart count 0
Aug 23 23:46:00.646: INFO: 	Container oauth-proxy ready: true, restart count 0
Aug 23 23:46:00.646: INFO: 	Container prom-label-proxy ready: true, restart count 0
Aug 23 23:46:00.646: INFO: 	Container thanos-query ready: true, restart count 0
Aug 23 23:46:00.646: INFO: multus-admission-controller-hcwf4 from openshift-multus started at 2021-08-23 20:02:50 +0000 UTC (2 container statuses recorded)
Aug 23 23:46:00.646: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 23 23:46:00.646: INFO: 	Container multus-admission-controller ready: true, restart count 0
Aug 23 23:46:00.646: INFO: multus-z4wqr from openshift-multus started at 2021-08-23 20:01:12 +0000 UTC (1 container statuses recorded)
Aug 23 23:46:00.646: INFO: 	Container kube-multus ready: true, restart count 0
Aug 23 23:46:00.646: INFO: network-metrics-daemon-mbjhv from openshift-multus started at 2021-08-23 20:01:15 +0000 UTC (2 container statuses recorded)
Aug 23 23:46:00.646: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 23 23:46:00.646: INFO: 	Container network-metrics-daemon ready: true, restart count 0
Aug 23 23:46:00.646: INFO: network-check-source-6cd65cf589-6rbmz from openshift-network-diagnostics started at 2021-08-23 20:03:01 +0000 UTC (1 container statuses recorded)
Aug 23 23:46:00.646: INFO: 	Container check-endpoints ready: true, restart count 0
Aug 23 23:46:00.646: INFO: network-check-target-d6zj2 from openshift-network-diagnostics started at 2021-08-23 20:01:33 +0000 UTC (1 container statuses recorded)
Aug 23 23:46:00.646: INFO: 	Container network-check-target-container ready: true, restart count 0
Aug 23 23:46:00.646: INFO: network-operator-86dcc4df56-crx6q from openshift-network-operator started at 2021-08-23 20:00:18 +0000 UTC (1 container statuses recorded)
Aug 23 23:46:00.646: INFO: 	Container network-operator ready: true, restart count 0
Aug 23 23:46:00.646: INFO: catalog-operator-6f8ff86686-tg2d4 from openshift-operator-lifecycle-manager started at 2021-08-23 22:12:56 +0000 UTC (1 container statuses recorded)
Aug 23 23:46:00.647: INFO: 	Container catalog-operator ready: true, restart count 0
Aug 23 23:46:00.647: INFO: packageserver-6d5f99f754-9mfv7 from openshift-operator-lifecycle-manager started at 2021-08-23 20:04:35 +0000 UTC (1 container statuses recorded)
Aug 23 23:46:00.647: INFO: 	Container packageserver ready: true, restart count 0
Aug 23 23:46:00.647: INFO: service-ca-operator-64b7cc7c85-bxzpq from openshift-service-ca-operator started at 2021-08-23 23:26:38 +0000 UTC (1 container statuses recorded)
Aug 23 23:46:00.647: INFO: 	Container service-ca-operator ready: true, restart count 0
Aug 23 23:46:00.647: INFO: service-ca-54b7675c-xmpw4 from openshift-service-ca started at 2021-08-23 22:12:56 +0000 UTC (1 container statuses recorded)
Aug 23 23:46:00.647: INFO: 	Container service-ca-controller ready: false, restart count 0
Aug 23 23:46:00.647: INFO: sonobuoy from sonobuoy started at 2021-08-23 21:37:12 +0000 UTC (1 container statuses recorded)
Aug 23 23:46:00.647: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Aug 23 23:46:00.647: INFO: sonobuoy-systemd-logs-daemon-set-9034e69b695e4ee2-zn2c4 from sonobuoy started at 2021-08-23 21:37:23 +0000 UTC (2 container statuses recorded)
Aug 23 23:46:00.647: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 23 23:46:00.647: INFO: 	Container systemd-logs ready: true, restart count 0
Aug 23 23:46:00.647: INFO: tigera-operator-667cd558f7-d78zp from tigera-operator started at 2021-08-23 20:00:19 +0000 UTC (1 container statuses recorded)
Aug 23 23:46:00.647: INFO: 	Container tigera-operator ready: true, restart count 4
[It] validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: verifying the node has the label node 10.149.248.24
STEP: verifying the node has the label node 10.149.248.25
STEP: verifying the node has the label node 10.149.248.9
Aug 23 23:46:01.294: INFO: Pod calico-kube-controllers-7dcbcc7c66-x65ng requesting resource cpu=10m on Node 10.149.248.9
Aug 23 23:46:01.294: INFO: Pod calico-node-4m7h2 requesting resource cpu=250m on Node 10.149.248.25
Aug 23 23:46:01.294: INFO: Pod calico-node-9qqn4 requesting resource cpu=250m on Node 10.149.248.24
Aug 23 23:46:01.294: INFO: Pod calico-node-cgrcs requesting resource cpu=250m on Node 10.149.248.9
Aug 23 23:46:01.295: INFO: Pod calico-typha-84b7fc6fc8-9fjkp requesting resource cpu=250m on Node 10.149.248.24
Aug 23 23:46:01.295: INFO: Pod calico-typha-84b7fc6fc8-h6lnw requesting resource cpu=250m on Node 10.149.248.9
Aug 23 23:46:01.295: INFO: Pod calico-typha-84b7fc6fc8-tf4pd requesting resource cpu=250m on Node 10.149.248.25
Aug 23 23:46:01.295: INFO: Pod ibm-cloud-provider-ip-169-55-101-126-6bb454748-g6kdf requesting resource cpu=5m on Node 10.149.248.25
Aug 23 23:46:01.295: INFO: Pod ibm-cloud-provider-ip-169-55-101-126-6bb454748-t7nvk requesting resource cpu=5m on Node 10.149.248.9
Aug 23 23:46:01.295: INFO: Pod ibm-file-plugin-68fbcccc88-bgbn9 requesting resource cpu=50m on Node 10.149.248.25
Aug 23 23:46:01.295: INFO: Pod ibm-keepalived-watcher-c94bg requesting resource cpu=5m on Node 10.149.248.25
Aug 23 23:46:01.295: INFO: Pod ibm-keepalived-watcher-rsh2g requesting resource cpu=5m on Node 10.149.248.9
Aug 23 23:46:01.295: INFO: Pod ibm-keepalived-watcher-vrlwd requesting resource cpu=5m on Node 10.149.248.24
Aug 23 23:46:01.295: INFO: Pod ibm-master-proxy-static-10.149.248.24 requesting resource cpu=25m on Node 10.149.248.24
Aug 23 23:46:01.295: INFO: Pod ibm-master-proxy-static-10.149.248.25 requesting resource cpu=25m on Node 10.149.248.25
Aug 23 23:46:01.295: INFO: Pod ibm-master-proxy-static-10.149.248.9 requesting resource cpu=25m on Node 10.149.248.9
Aug 23 23:46:01.295: INFO: Pod ibm-storage-watcher-5c4d7b6cd6-zxwb2 requesting resource cpu=50m on Node 10.149.248.25
Aug 23 23:46:01.295: INFO: Pod ibmcloud-block-storage-driver-5fllx requesting resource cpu=50m on Node 10.149.248.9
Aug 23 23:46:01.295: INFO: Pod ibmcloud-block-storage-driver-jgx4r requesting resource cpu=50m on Node 10.149.248.24
Aug 23 23:46:01.295: INFO: Pod ibmcloud-block-storage-driver-zdvf4 requesting resource cpu=50m on Node 10.149.248.25
Aug 23 23:46:01.295: INFO: Pod ibmcloud-block-storage-plugin-7d6d9649b-zdvgj requesting resource cpu=50m on Node 10.149.248.25
Aug 23 23:46:01.295: INFO: Pod vpn-db594b4f9-9mxgx requesting resource cpu=5m on Node 10.149.248.25
Aug 23 23:46:01.295: INFO: Pod cluster-node-tuning-operator-84b8576b47-qlh6k requesting resource cpu=10m on Node 10.149.248.9
Aug 23 23:46:01.295: INFO: Pod tuned-7sb7t requesting resource cpu=10m on Node 10.149.248.24
Aug 23 23:46:01.295: INFO: Pod tuned-bfbjn requesting resource cpu=10m on Node 10.149.248.25
Aug 23 23:46:01.295: INFO: Pod tuned-m585x requesting resource cpu=10m on Node 10.149.248.9
Aug 23 23:46:01.295: INFO: Pod cluster-samples-operator-866dcfc6c4-ntqvw requesting resource cpu=20m on Node 10.149.248.9
Aug 23 23:46:01.295: INFO: Pod cluster-storage-operator-6c5c749558-97wht requesting resource cpu=10m on Node 10.149.248.25
Aug 23 23:46:01.295: INFO: Pod csi-snapshot-controller-566544547f-sfjlv requesting resource cpu=10m on Node 10.149.248.9
Aug 23 23:46:01.295: INFO: Pod csi-snapshot-controller-operator-645f4897d-2vvbp requesting resource cpu=10m on Node 10.149.248.25
Aug 23 23:46:01.296: INFO: Pod csi-snapshot-webhook-585d4946dc-kd8jt requesting resource cpu=10m on Node 10.149.248.9
Aug 23 23:46:01.296: INFO: Pod console-operator-c7f9f8687-s8qw7 requesting resource cpu=10m on Node 10.149.248.9
Aug 23 23:46:01.296: INFO: Pod console-dc8c686bb-8xpf8 requesting resource cpu=10m on Node 10.149.248.25
Aug 23 23:46:01.296: INFO: Pod console-dc8c686bb-z5xcx requesting resource cpu=10m on Node 10.149.248.9
Aug 23 23:46:01.296: INFO: Pod downloads-6c96776f98-5w9bj requesting resource cpu=10m on Node 10.149.248.25
Aug 23 23:46:01.296: INFO: Pod downloads-6c96776f98-nqhnf requesting resource cpu=10m on Node 10.149.248.25
Aug 23 23:46:01.296: INFO: Pod dns-operator-6dbb54f776-8k9pq requesting resource cpu=20m on Node 10.149.248.25
Aug 23 23:46:01.296: INFO: Pod dns-default-4hmcv requesting resource cpu=65m on Node 10.149.248.9
Aug 23 23:46:01.296: INFO: Pod dns-default-mgbp7 requesting resource cpu=65m on Node 10.149.248.24
Aug 23 23:46:01.296: INFO: Pod dns-default-mxjt9 requesting resource cpu=65m on Node 10.149.248.25
Aug 23 23:46:01.296: INFO: Pod cluster-image-registry-operator-85978c675-rdng6 requesting resource cpu=10m on Node 10.149.248.25
Aug 23 23:46:01.296: INFO: Pod image-registry-555cc7f64c-qtjjg requesting resource cpu=100m on Node 10.149.248.9
Aug 23 23:46:01.296: INFO: Pod node-ca-czldz requesting resource cpu=10m on Node 10.149.248.25
Aug 23 23:46:01.296: INFO: Pod node-ca-dl95t requesting resource cpu=10m on Node 10.149.248.24
Aug 23 23:46:01.296: INFO: Pod node-ca-g989x requesting resource cpu=10m on Node 10.149.248.9
Aug 23 23:46:01.296: INFO: Pod ingress-canary-9skj4 requesting resource cpu=10m on Node 10.149.248.25
Aug 23 23:46:01.296: INFO: Pod ingress-canary-hx8m6 requesting resource cpu=10m on Node 10.149.248.24
Aug 23 23:46:01.296: INFO: Pod ingress-canary-l5l42 requesting resource cpu=10m on Node 10.149.248.9
Aug 23 23:46:01.296: INFO: Pod ingress-operator-5d97777898-hf6cx requesting resource cpu=20m on Node 10.149.248.25
Aug 23 23:46:01.296: INFO: Pod router-default-678bfcd875-2f5hb requesting resource cpu=100m on Node 10.149.248.9
Aug 23 23:46:01.296: INFO: Pod router-default-678bfcd875-g2q6m requesting resource cpu=100m on Node 10.149.248.25
Aug 23 23:46:01.296: INFO: Pod openshift-kube-proxy-sbzjn requesting resource cpu=110m on Node 10.149.248.25
Aug 23 23:46:01.296: INFO: Pod openshift-kube-proxy-wk9m8 requesting resource cpu=110m on Node 10.149.248.9
Aug 23 23:46:01.296: INFO: Pod openshift-kube-proxy-wn8f2 requesting resource cpu=110m on Node 10.149.248.24
Aug 23 23:46:01.296: INFO: Pod kube-storage-version-migrator-operator-6d55bddccb-4jgrz requesting resource cpu=10m on Node 10.149.248.9
Aug 23 23:46:01.296: INFO: Pod migrator-f58676cd4-5q7gc requesting resource cpu=10m on Node 10.149.248.25
Aug 23 23:46:01.296: INFO: Pod certified-operators-rlpm2 requesting resource cpu=10m on Node 10.149.248.9
Aug 23 23:46:01.296: INFO: Pod community-operators-ztwk2 requesting resource cpu=10m on Node 10.149.248.9
Aug 23 23:46:01.297: INFO: Pod marketplace-operator-7fd8fdcc5b-ncb77 requesting resource cpu=10m on Node 10.149.248.9
Aug 23 23:46:01.297: INFO: Pod redhat-marketplace-hvcn5 requesting resource cpu=10m on Node 10.149.248.25
Aug 23 23:46:01.297: INFO: Pod redhat-operators-cpqsn requesting resource cpu=10m on Node 10.149.248.9
Aug 23 23:46:01.297: INFO: Pod alertmanager-main-0 requesting resource cpu=8m on Node 10.149.248.9
Aug 23 23:46:01.297: INFO: Pod alertmanager-main-1 requesting resource cpu=8m on Node 10.149.248.25
Aug 23 23:46:01.297: INFO: Pod alertmanager-main-2 requesting resource cpu=8m on Node 10.149.248.9
Aug 23 23:46:01.297: INFO: Pod cluster-monitoring-operator-c69d85486-m52jl requesting resource cpu=11m on Node 10.149.248.25
Aug 23 23:46:01.297: INFO: Pod grafana-85d4bdb748-9g6gn requesting resource cpu=5m on Node 10.149.248.9
Aug 23 23:46:01.297: INFO: Pod kube-state-metrics-d956df775-bgj5t requesting resource cpu=4m on Node 10.149.248.9
Aug 23 23:46:01.297: INFO: Pod node-exporter-b5rwn requesting resource cpu=9m on Node 10.149.248.24
Aug 23 23:46:01.297: INFO: Pod node-exporter-fsg2g requesting resource cpu=9m on Node 10.149.248.25
Aug 23 23:46:01.297: INFO: Pod node-exporter-kcjzv requesting resource cpu=9m on Node 10.149.248.9
Aug 23 23:46:01.297: INFO: Pod openshift-state-metrics-74b58f578c-cmhb9 requesting resource cpu=3m on Node 10.149.248.9
Aug 23 23:46:01.297: INFO: Pod prometheus-adapter-6685ccb7f-9tlvb requesting resource cpu=1m on Node 10.149.248.9
Aug 23 23:46:01.297: INFO: Pod prometheus-adapter-6685ccb7f-gxd72 requesting resource cpu=1m on Node 10.149.248.25
Aug 23 23:46:01.297: INFO: Pod prometheus-k8s-0 requesting resource cpu=76m on Node 10.149.248.9
Aug 23 23:46:01.297: INFO: Pod prometheus-k8s-1 requesting resource cpu=76m on Node 10.149.248.25
Aug 23 23:46:01.297: INFO: Pod prometheus-operator-5699bb49dc-8q47w requesting resource cpu=6m on Node 10.149.248.9
Aug 23 23:46:01.297: INFO: Pod telemeter-client-9d6d6f95f-ztnrk requesting resource cpu=3m on Node 10.149.248.9
Aug 23 23:46:01.297: INFO: Pod thanos-querier-97cd894c4-g4rfr requesting resource cpu=9m on Node 10.149.248.9
Aug 23 23:46:01.297: INFO: Pod thanos-querier-97cd894c4-z4jqj requesting resource cpu=9m on Node 10.149.248.25
Aug 23 23:46:01.297: INFO: Pod multus-2p958 requesting resource cpu=10m on Node 10.149.248.25
Aug 23 23:46:01.297: INFO: Pod multus-48kkt requesting resource cpu=10m on Node 10.149.248.24
Aug 23 23:46:01.297: INFO: Pod multus-admission-controller-hcwf4 requesting resource cpu=20m on Node 10.149.248.9
Aug 23 23:46:01.297: INFO: Pod multus-admission-controller-tp5tk requesting resource cpu=20m on Node 10.149.248.24
Aug 23 23:46:01.297: INFO: Pod multus-admission-controller-ww7jt requesting resource cpu=20m on Node 10.149.248.25
Aug 23 23:46:01.297: INFO: Pod multus-z4wqr requesting resource cpu=10m on Node 10.149.248.9
Aug 23 23:46:01.297: INFO: Pod network-metrics-daemon-6bmnm requesting resource cpu=20m on Node 10.149.248.25
Aug 23 23:46:01.298: INFO: Pod network-metrics-daemon-cqr85 requesting resource cpu=20m on Node 10.149.248.24
Aug 23 23:46:01.298: INFO: Pod network-metrics-daemon-mbjhv requesting resource cpu=20m on Node 10.149.248.9
Aug 23 23:46:01.298: INFO: Pod network-check-source-6cd65cf589-6rbmz requesting resource cpu=10m on Node 10.149.248.9
Aug 23 23:46:01.298: INFO: Pod network-check-target-447wc requesting resource cpu=10m on Node 10.149.248.24
Aug 23 23:46:01.298: INFO: Pod network-check-target-cjqqr requesting resource cpu=10m on Node 10.149.248.25
Aug 23 23:46:01.298: INFO: Pod network-check-target-d6zj2 requesting resource cpu=10m on Node 10.149.248.9
Aug 23 23:46:01.298: INFO: Pod network-operator-86dcc4df56-crx6q requesting resource cpu=10m on Node 10.149.248.9
Aug 23 23:46:01.298: INFO: Pod catalog-operator-6f8ff86686-tg2d4 requesting resource cpu=10m on Node 10.149.248.9
Aug 23 23:46:01.298: INFO: Pod olm-operator-d5dc9548d-mwfkk requesting resource cpu=10m on Node 10.149.248.25
Aug 23 23:46:01.298: INFO: Pod packageserver-6d5f99f754-5t8qd requesting resource cpu=10m on Node 10.149.248.25
Aug 23 23:46:01.298: INFO: Pod packageserver-6d5f99f754-9mfv7 requesting resource cpu=10m on Node 10.149.248.9
Aug 23 23:46:01.298: INFO: Pod metrics-8fc5c5f56-lq4k6 requesting resource cpu=10m on Node 10.149.248.25
Aug 23 23:46:01.298: INFO: Pod push-gateway-59f6bc56d4-bx994 requesting resource cpu=10m on Node 10.149.248.25
Aug 23 23:46:01.298: INFO: Pod service-ca-operator-64b7cc7c85-bxzpq requesting resource cpu=10m on Node 10.149.248.9
Aug 23 23:46:01.298: INFO: Pod service-ca-54b7675c-xmpw4 requesting resource cpu=10m on Node 10.149.248.9
Aug 23 23:46:01.298: INFO: Pod sonobuoy requesting resource cpu=0m on Node 10.149.248.9
Aug 23 23:46:01.298: INFO: Pod sonobuoy-e2e-job-90ed16fed1444409 requesting resource cpu=0m on Node 10.149.248.25
Aug 23 23:46:01.298: INFO: Pod sonobuoy-systemd-logs-daemon-set-9034e69b695e4ee2-q8h6l requesting resource cpu=0m on Node 10.149.248.25
Aug 23 23:46:01.298: INFO: Pod sonobuoy-systemd-logs-daemon-set-9034e69b695e4ee2-s28vf requesting resource cpu=0m on Node 10.149.248.24
Aug 23 23:46:01.298: INFO: Pod sonobuoy-systemd-logs-daemon-set-9034e69b695e4ee2-zn2c4 requesting resource cpu=0m on Node 10.149.248.9
Aug 23 23:46:01.298: INFO: Pod tigera-operator-667cd558f7-d78zp requesting resource cpu=100m on Node 10.149.248.9
STEP: Starting Pods to consume most of the cluster CPU.
Aug 23 23:46:01.298: INFO: Creating a pod which consumes cpu=2139m on Node 10.149.248.24
Aug 23 23:46:01.376: INFO: Creating a pod which consumes cpu=1771m on Node 10.149.248.25
Aug 23 23:46:01.435: INFO: Creating a pod which consumes cpu=1706m on Node 10.149.248.9
STEP: Creating another pod that requires unavailable amount of CPU.
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-1e5c8732-ebb0-4ae2-8cb7-d2bb9b245379.169e143b9a1973e6], Reason = [Scheduled], Message = [Successfully assigned sched-pred-9545/filler-pod-1e5c8732-ebb0-4ae2-8cb7-d2bb9b245379 to 10.149.248.24]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-1e5c8732-ebb0-4ae2-8cb7-d2bb9b245379.169e143ccf7767d3], Reason = [AddedInterface], Message = [Add eth0 [172.30.87.126/32]]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-1e5c8732-ebb0-4ae2-8cb7-d2bb9b245379.169e143cf0c8db9e], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.2" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-1e5c8732-ebb0-4ae2-8cb7-d2bb9b245379.169e143d003bfe6b], Reason = [Created], Message = [Created container filler-pod-1e5c8732-ebb0-4ae2-8cb7-d2bb9b245379]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-1e5c8732-ebb0-4ae2-8cb7-d2bb9b245379.169e143d0289f709], Reason = [Started], Message = [Started container filler-pod-1e5c8732-ebb0-4ae2-8cb7-d2bb9b245379]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-4f5c3c6e-0d8d-45f4-8e11-206c9016f80b.169e143ba0bb45bf], Reason = [Scheduled], Message = [Successfully assigned sched-pred-9545/filler-pod-4f5c3c6e-0d8d-45f4-8e11-206c9016f80b to 10.149.248.9]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-4f5c3c6e-0d8d-45f4-8e11-206c9016f80b.169e143c5e9b6a16], Reason = [AddedInterface], Message = [Add eth0 [172.30.210.156/32]]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-4f5c3c6e-0d8d-45f4-8e11-206c9016f80b.169e143cadaf32fe], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.2" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-4f5c3c6e-0d8d-45f4-8e11-206c9016f80b.169e143cbb84f1a4], Reason = [Created], Message = [Created container filler-pod-4f5c3c6e-0d8d-45f4-8e11-206c9016f80b]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-4f5c3c6e-0d8d-45f4-8e11-206c9016f80b.169e143cbdd856be], Reason = [Started], Message = [Started container filler-pod-4f5c3c6e-0d8d-45f4-8e11-206c9016f80b]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-58216327-7bba-4e21-a03e-7df00ee596e0.169e143b9dbab9af], Reason = [Scheduled], Message = [Successfully assigned sched-pred-9545/filler-pod-58216327-7bba-4e21-a03e-7df00ee596e0 to 10.149.248.25]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-58216327-7bba-4e21-a03e-7df00ee596e0.169e143d20728a1b], Reason = [AddedInterface], Message = [Add eth0 [172.30.240.103/32]]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-58216327-7bba-4e21-a03e-7df00ee596e0.169e143d23807779], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.2" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-58216327-7bba-4e21-a03e-7df00ee596e0.169e143d2e656048], Reason = [Created], Message = [Created container filler-pod-58216327-7bba-4e21-a03e-7df00ee596e0]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-58216327-7bba-4e21-a03e-7df00ee596e0.169e143d308f43ae], Reason = [Started], Message = [Started container filler-pod-58216327-7bba-4e21-a03e-7df00ee596e0]
STEP: Considering event: 
Type = [Normal], Name = [sched-pred-9545.169e143b1d1dce67], Reason = [CreatedSCCRanges], Message = [created SCC ranges]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.169e143dccf708a3], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 Insufficient cpu.]
STEP: removing the label node off the node 10.149.248.24
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node 10.149.248.25
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node 10.149.248.9
STEP: verifying the node doesn't have the label node
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 23:46:18.855: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-9545" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:83

• [SLOW TEST:21.166 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates resource limits of pods that are allowed to run  [Conformance]","total":311,"completed":307,"skipped":5322,"failed":0}
S
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 23:46:20.279: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Aug 23 23:46:26.040: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 23:46:26.501: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-1783" for this suite.

• [SLOW TEST:7.265 seconds]
[k8s.io] Container Runtime
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  blackbox test
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:41
    on terminated container
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:134
      should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":311,"completed":308,"skipped":5323,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 23:46:27.545: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-667
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Initializing watcher for selector baz=blah,foo=bar
STEP: Creating stateful set ss in namespace statefulset-667
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-667
Aug 23 23:46:29.124: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Pending - Ready=false
Aug 23 23:46:39.334: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod
Aug 23 23:46:39.554: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585719795 --namespace=statefulset-667 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Aug 23 23:46:40.542: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Aug 23 23:46:40.542: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Aug 23 23:46:40.542: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Aug 23 23:46:40.713: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Aug 23 23:46:50.974: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Aug 23 23:46:50.975: INFO: Waiting for statefulset status.replicas updated to 0
Aug 23 23:46:51.649: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999998166s
Aug 23 23:46:52.749: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.851671339s
Aug 23 23:46:53.920: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.751075445s
Aug 23 23:46:55.065: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.580301473s
Aug 23 23:46:56.206: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.435637805s
Aug 23 23:46:57.323: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.294398803s
Aug 23 23:46:58.456: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.177103315s
Aug 23 23:46:59.610: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.044590506s
Aug 23 23:47:00.766: INFO: Verifying statefulset ss doesn't scale past 1 for another 890.015207ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-667
Aug 23 23:47:01.887: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585719795 --namespace=statefulset-667 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 23 23:47:02.697: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Aug 23 23:47:02.697: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Aug 23 23:47:02.697: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Aug 23 23:47:02.763: INFO: Found 1 stateful pods, waiting for 3
Aug 23 23:47:12.806: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Aug 23 23:47:12.806: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Aug 23 23:47:12.806: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order
STEP: Scale down will halt with unhealthy stateful pod
Aug 23 23:47:12.879: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585719795 --namespace=statefulset-667 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Aug 23 23:47:13.830: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Aug 23 23:47:13.830: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Aug 23 23:47:13.830: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Aug 23 23:47:13.830: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585719795 --namespace=statefulset-667 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Aug 23 23:47:15.014: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Aug 23 23:47:15.014: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Aug 23 23:47:15.014: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Aug 23 23:47:15.014: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585719795 --namespace=statefulset-667 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Aug 23 23:47:17.319: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Aug 23 23:47:17.319: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Aug 23 23:47:17.319: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Aug 23 23:47:17.319: INFO: Waiting for statefulset status.replicas updated to 0
Aug 23 23:47:17.475: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 1
Aug 23 23:47:28.199: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Aug 23 23:47:28.199: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Aug 23 23:47:28.199: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Aug 23 23:47:29.269: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999997971s
Aug 23 23:47:30.592: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.453905198s
Aug 23 23:47:31.946: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.126673813s
Aug 23 23:47:33.378: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.777184765s
Aug 23 23:47:34.647: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.345030165s
Aug 23 23:47:35.834: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.076164887s
Aug 23 23:47:37.153: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.889759373s
Aug 23 23:47:38.349: INFO: Verifying statefulset ss doesn't scale past 3 for another 570.047405ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-667
Aug 23 23:47:39.641: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585719795 --namespace=statefulset-667 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 23 23:47:41.619: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Aug 23 23:47:41.619: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Aug 23 23:47:41.619: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Aug 23 23:47:41.619: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585719795 --namespace=statefulset-667 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 23 23:47:43.021: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Aug 23 23:47:43.021: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Aug 23 23:47:43.021: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Aug 23 23:47:43.021: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585719795 --namespace=statefulset-667 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 23 23:47:43.435: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Aug 23 23:47:43.435: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Aug 23 23:47:43.435: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Aug 23 23:47:43.435: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
Aug 23 23:48:14.188: INFO: Deleting all statefulset in ns statefulset-667
Aug 23 23:48:14.419: INFO: Scaling statefulset ss to 0
Aug 23 23:48:15.140: INFO: Waiting for statefulset status.replicas updated to 0
Aug 23 23:48:15.440: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 23:48:16.330: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-667" for this suite.

• [SLOW TEST:110.490 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]","total":311,"completed":309,"skipped":5343,"failed":0}
SSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 23:48:18.035: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir 0644 on tmpfs
Aug 23 23:48:19.826: INFO: Waiting up to 5m0s for pod "pod-40f313ba-a13f-4a06-9709-c21e3d04b428" in namespace "emptydir-1142" to be "Succeeded or Failed"
Aug 23 23:48:20.093: INFO: Pod "pod-40f313ba-a13f-4a06-9709-c21e3d04b428": Phase="Pending", Reason="", readiness=false. Elapsed: 267.018495ms
Aug 23 23:48:22.222: INFO: Pod "pod-40f313ba-a13f-4a06-9709-c21e3d04b428": Phase="Pending", Reason="", readiness=false. Elapsed: 2.395983442s
Aug 23 23:48:24.528: INFO: Pod "pod-40f313ba-a13f-4a06-9709-c21e3d04b428": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.702270639s
STEP: Saw pod success
Aug 23 23:48:24.528: INFO: Pod "pod-40f313ba-a13f-4a06-9709-c21e3d04b428" satisfied condition "Succeeded or Failed"
Aug 23 23:48:24.768: INFO: Trying to get logs from node 10.149.248.24 pod pod-40f313ba-a13f-4a06-9709-c21e3d04b428 container test-container: <nil>
STEP: delete the pod
Aug 23 23:48:26.113: INFO: Waiting for pod pod-40f313ba-a13f-4a06-9709-c21e3d04b428 to disappear
Aug 23 23:48:26.399: INFO: Pod pod-40f313ba-a13f-4a06-9709-c21e3d04b428 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 23:48:26.399: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1142" for this suite.

• [SLOW TEST:10.182 seconds]
[sig-storage] EmptyDir volumes
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:45
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":310,"skipped":5349,"failed":0}
SS
------------------------------
[sig-apps] ReplicaSet 
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 23 23:48:28.218: INFO: >>> kubeConfig: /tmp/kubeconfig-585719795
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Given a Pod with a 'name' label pod-adoption-release is created
STEP: When a replicaset with a matching selector is created
STEP: Then the orphan pod is adopted
STEP: When the matched label of one of its pods change
Aug 23 23:48:37.211: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 23 23:48:37.719: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-9695" for this suite.

• [SLOW TEST:10.198 seconds]
[sig-apps] ReplicaSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet should adopt matching pods on creation and release no longer matching pods [Conformance]","total":311,"completed":311,"skipped":5351,"failed":0}
SSSSSAug 23 23:48:38.417: INFO: Running AfterSuite actions on all nodes
Aug 23 23:48:38.417: INFO: Running AfterSuite actions on node 1
Aug 23 23:48:38.417: INFO: Skipping dumping logs from cluster

JUnit report was created: /tmp/results/junit_01.xml
{"msg":"Test Suite completed","total":311,"completed":311,"skipped":5356,"failed":0}

Ran 311 of 5667 Specs in 7857.169 seconds
SUCCESS! -- 311 Passed | 0 Failed | 0 Pending | 5356 Skipped
PASS

Ginkgo ran 1 suite in 2h10m59.076943966s
Test Suite Passed
